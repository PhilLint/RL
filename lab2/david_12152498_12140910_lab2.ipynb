{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or tim|elise|david|qi, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"Philipp Lintl, Bogdan Floris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0fd6bc65a6759a8899e024459ccb28ef",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "039c8296578b2834a9a858a1a19a43bd",
     "grade": false,
     "grade_id": "cell-eecfd6fb626abfae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Temporal Difference (TD) learning (8 points)\n",
    "Mention one advantage and one disadvantage of Monte Carlo methods. Mention an example where you would prefer to use TD learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4b81bcd51404511164971c110ffa838f",
     "grade": true,
     "grade_id": "cell-cac4639044ba9074",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "One advantage of Monte Carlo methods is that it does not require a full knowledge of the environment (MDP), but the disadvatange is that it does not learn until the episode ends (and thus only works for episodic tasks). TD learning is prefered when the task is not episodic (we do not need to store sample trajectories) and the model is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e61bd7837d3b364741b4c3aa43597a10",
     "grade": false,
     "grade_id": "cell-21ca38ffcbe1c3ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the TD algorithms, we will skip the prediction algorithm and go straight for the control setting where we optimize the policy that we are using. In other words: implement SARSA. To keep it dynamic, we will use the windy gridworld environment (Example 6.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "609d0f1e1ef6ad89c8dcd96dd43aa798",
     "grade": false,
     "grade_id": "cell-c046fd0377cee46d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        return int(np.random.rand() * nA) if np.random.rand() < epsilon else np.argmax(Q[observation])\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "42b89f13768d1cd3b41fb52cddef0d97",
     "grade": true,
     "grade_id": "cell-6b662771f3762bb1",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1251.28it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, Q=None):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Probability to sample a random action. Float between 0 and 1.\n",
    "        Q: hot-start the algorithm with a Q value function (optional)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is a list of tuples giving the episode lengths and rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    if Q is None:\n",
    "        Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize state\n",
    "        state = env.reset()\n",
    "        # choose action from state based on the policy\n",
    "        action = policy(state)\n",
    "        # loop for each step in the episode\n",
    "        for step in itertools.count():\n",
    "            # take action and observe the next state, reward, and if we are done\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # choose an action from next_state using the policy\n",
    "            next_action = policy(next_state)\n",
    "            # update the total episode reward\n",
    "            episode_reward += reward\n",
    "            # sarsa Q update\n",
    "            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        stats.append((step, episode_reward))\n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)\n",
    "\n",
    "Q_sarsa, (episode_lengths_sarsa, episode_returns_sarsa) = sarsa(env, 1000)\n",
    "\n",
    "# We will help you with plotting this time\n",
    "plt.plot(episode_lengths_sarsa)\n",
    "plt.title('Episode lengths SARSA')\n",
    "plt.show()\n",
    "plt.plot(episode_returns_sarsa)\n",
    "plt.title('Episode returns SARSA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e8df3908ce548708b64f69e11a34896",
     "grade": false,
     "grade_id": "cell-0eaf4b925ab3ea34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We learn the optimal (non-exploring) policy while using another policy to do exploration, which is where we arrive at _off-policy_ learning. In the simplest variant, we learn our own value by bootstrapping based on the action value corresponding to the best action we could take, while the exploration policy actual follows the $\\epsilon$-greedy strategy. This is known as Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "954556134388a34f8d4b9a07834180c5",
     "grade": true,
     "grade_id": "cell-a87637d2e582fec0",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2577.79it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, Q=None):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Probability to sample a random action. Float between 0 and 1.\n",
    "        Q: hot-start the algorithm with a Q value function (optional)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is a list of tuples giving the episode lengths and rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    if Q is None:\n",
    "        Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize the state\n",
    "        state = env.reset()\n",
    "        # loop for each step in the episode\n",
    "        for t in itertools.count():\n",
    "            # choose action from state based on the policy\n",
    "            action = policy(state)\n",
    "            # take a step in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # update statistics\n",
    "            episode_reward += reward\n",
    "            # update Q\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] += alpha * (reward + discount_factor *\n",
    "                Q[next_state][best_next_action] - Q[state][action])\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state\n",
    "            state = next_state\n",
    "\n",
    "        stats.append((t, episode_reward))\n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)\n",
    "\n",
    "Q_q_learning, (episode_lengths_q_learning, episode_returns_q_learning) = q_learning(env, 1000)\n",
    "\n",
    "# We will help you with plotting this time\n",
    "plt.plot(episode_lengths_q_learning)\n",
    "plt.title('Episode lengths Q-learning')\n",
    "plt.show()\n",
    "plt.plot(episode_returns_q_learning)\n",
    "plt.title('Episode returns Q-learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f695c6e9d66afd4fc7a49b565419ba5d",
     "grade": false,
     "grade_id": "cell-9f1fcee44ba712c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now compare the episode returns while learning for Q-learning and Sarsa (maybe run some more iterations?), by plotting the returns for both algorithms in a single plot, like in the book, Example 6.6. In order to be able to compare them, you may want to zoom in on the y-axis and smooth the returns (e.g. plotting the $n$ episode average instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c1a110fe85c38220afed145a8cf09bc",
     "grade": true,
     "grade_id": "cell-69ed62a52a44dd78",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2217.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2145.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1526.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1630.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1707.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1394.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1824.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2487.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2747.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2045.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2725.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1968.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2278.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2136.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2441.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2274.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2858.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2220.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2070.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2680.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2320.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2881.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3134.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2570.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1924.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2450.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2960.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3154.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2765.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1937.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2543.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3215.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3038.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2735.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3223.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2087.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2300.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2731.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2637.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2896.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1358.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 850.69it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1110.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1949.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1989.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2268.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2025.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2533.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2327.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2687.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2280.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1995.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1620.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1989.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2072.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2349.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2087.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2768.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1872.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2183.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1785.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2676.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2256.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2233.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2695.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2282.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2809.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1700.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1824.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1677.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1995.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2175.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2624.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2207.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2762.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2447.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2389.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2403.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2320.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2343.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2129.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2728.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2558.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2404.10it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2680.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2217.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2645.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2543.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2352.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2156.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2173.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2647.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1533.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2352.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2213.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2792.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2437.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2392.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2298.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2127.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2757.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2471.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2695.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2524.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2363.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2792.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2380.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1362.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1815.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1805.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2107.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1884.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2433.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2105.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2032.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2358.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1835.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2082.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2159.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2306.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1495.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1545.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1855.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1950.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2247.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1862.20it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2068.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1968.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2304.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2355.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2910.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2724.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2480.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2874.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2330.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2775.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2722.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2454.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2904.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2381.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2557.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1944.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2169.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2769.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2432.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2857.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2729.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2444.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2377.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1754.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2127.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1426.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2079.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2105.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2693.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1522.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2624.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2320.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2720.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2366.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2750.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2631.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2415.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2457.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2420.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2809.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2680.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2256.59it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2687.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2409.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2849.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2570.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2426.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2644.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2402.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2681.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2676.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2358.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2644.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2221.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1771.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2397.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2901.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2668.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2341.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2827.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2363.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2864.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2791.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2422.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2936.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2659.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2547.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2716.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2369.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2560.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2641.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2303.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2860.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2449.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_episode_rewards_average(algo, n=100):\n",
    "    episode_rewards = np.zeros(1000)\n",
    "    for i in range(n):\n",
    "        _, (_, episode_returns) = algo(env, 1000)\n",
    "        episode_rewards += episode_returns\n",
    "    return episode_rewards / n\n",
    "\n",
    "# Compute episode rewards for SARSA\n",
    "episode_rewards_sarsa = get_episode_rewards_average(sarsa)\n",
    "# COmpute episode rewards for Q-Learning\n",
    "episode_rewards_q_learning = get_episode_rewards_average(q_learning)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_rewards_sarsa[100:], label=\"SARSA\")\n",
    "plt.plot(episode_rewards_q_learning[100:], label=\"Q-Learning\")\n",
    "plt.title(\"Averaged episode rewards for SARSA and Q-Learning from episode 100 to 1000\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed2526b0c0f17f055f520f67072c59ac",
     "grade": false,
     "grade_id": "cell-7ef9de74c57a4f0c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which algorithm achieves higher return during learning? How does this compare to Example 6.6 from the book? Try to explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3357293c326223f2a02cae0f38ca24a",
     "grade": true,
     "grade_id": "cell-7acf9de8c94a171f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For the Windy Gridworld environment, Q-Learning achieves the higher returns during learning, which is opposed to the Cliff Walking environment. Since there is no risk in running of a cliff in this example, Q-Learning can learn the optiomal policy and progress through it even with an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2f954f745662334010f6fb0fcfd9896",
     "grade": false,
     "grade_id": "cell-316d3cfd35d55387",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After we have learned the policy, we do not care about exploration any more and we may switch to a deterministic (greedy) policy instead. If we evaluate this for both Sarsa and Q-learning (actually, for Q-learning the learned policy is already deterministic), which policy would you expect to perform better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "011f8038ac100bfdc5e40b78c1bdc2f8",
     "grade": true,
     "grade_id": "cell-ea5058e6f352d717",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Since Q-Learning seems to have learned the better policy, it should perform slightly better than SARSA even in a deterministic setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57ab54058d433e24421d1e1224a9bc87",
     "grade": false,
     "grade_id": "cell-8bcc6f5839a36860",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Please run the experiments to test your hypothesis (print or plot your results). How many runs do you need to evaluate the policy? Note: without learning, the order of the episodes is not relevant so a normal `plt.plot` may not be the most appropriate choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the policies are deterministic, we only need one run to determine the rewards for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "149c39efef43f1807d2b06e6bc50bf95",
     "grade": true,
     "grade_id": "cell-55f9d1767bb7c011",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 989.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the deterministic policies\n",
    "policy_sarsa = make_epsilon_greedy_policy(Q_sarsa, 0.0, env.action_space.n)\n",
    "policy_q_learning= make_epsilon_greedy_policy(Q_q_learning, 0.0, env.action_space.n)\n",
    "\n",
    "def get_rewards(env, policy, num_episodes=1):\n",
    "    rewards = []\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize the state\n",
    "        state = env.reset()\n",
    "        # loop for each step in the episode\n",
    "        for t in itertools.count():\n",
    "            # choose action from state based on the policy\n",
    "            action = policy(state)\n",
    "            # take a step in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # update statistics\n",
    "            episode_reward += reward\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state\n",
    "            state = next_state\n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "determ_rewards_sarsa = get_rewards(env, policy_sarsa)\n",
    "determ_rewards_q_learning = get_rewards(env, policy_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic rewards SARSA: -17.0\n",
      "Deterministic rewards Q-Learning: -15.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Deterministic rewards SARSA: {}\".format(np.mean(determ_rewards_sarsa)))\n",
    "print(\"Deterministic rewards Q-Learning: {}\".format(np.mean(determ_rewards_q_learning)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e70351edfa59760104962f08d541557b",
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Deep Q-Network (DQN) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e27fe8f72a248bbcf1f7a21e5550e657",
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lintl\\Anaconda3\\envs\\rl2019\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11a9c014ee5fbe790ce999428cc22658",
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9692b7acb09d018d9f80ce95685b81d5",
     "grade": false,
     "grade_id": "cell-bf2ac21267daffbb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you think of a way in which we can still use a tabular approach? Why would this work and can you think of an example problem where this would not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ffce6fca4071a1b543186db1b74cc98",
     "grade": true,
     "grade_id": "cell-b0fa2cb0c2cd2a63",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "One way in which we can still use a tabular approach is to aggregate the values into bins. All the values that make up a state in CartPole have a minimum and a maximum. This approach fails for problems where we cannot determine a minimum or a maximum value and thus we cannot determine the ammount of bins to use for aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c5bddd080e12cb076c845d093a70ed7",
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84b9c38718c952ef8e62486fc9bf5e4a",
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ef7d14363dc2aa4beb638856c57a58c",
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(torch.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b9a48f9aee9ebc46da01c6f11cd789a",
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7227d52671b410864319222a98e27d1",
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b3265bef151a12fe6969c378af76be2",
     "grade": false,
     "grade_id": "cell-b5b012e42dd2029e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What could be a problem with doing gradient updates on a sequence of state, action pairs $((s_t, a_t), (s_{t+1}, a_{t+1}) ...)$ observed while interacting with the environment? How will using *experience replay* help to overcome this (potential problem)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75e1a8b00b2bfa9b7dd8805b371c6a4e",
     "grade": true,
     "grade_id": "cell-70a2e59541668a25",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Doing gradient updates on a sequence of state, action pairs clearly violates the independence assumption of the data. Since the states come in a sequence, they are very much dependent on each other. Experience replay helps overcome this problem by randomly sampling from a big pool of state, action pairs. The data is still dependent but clearly not as much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b3bbd8aaf3aade515736d0d07917a61",
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the `push` function that adds a transition to the replay buffer, and the sample function that returns a batch of samples. It should keep at most the maximum number of transitions. Also implement the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c64677cbc7efad32a949783b7c9b53b7",
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            raise ValueError('Not enough memory to sample batch.')\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6865749b3a8810bdaaf1604a9cea42e7",
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.00572155,  0.02683287, -0.01174029, -0.0230005 ]), 0, 1.0, array([ 0.00625821, -0.16811876, -0.0122003 ,  0.26595524]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "354743bd76d6ba43d95b5b177443a202",
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61d26d0dec0133f2aa737ed4711d6e08",
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "270ab31d4bb29dc9a05223c16a4967a7",
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(1.0, 0.05, 1000)\n",
    "\n",
    "def get_epsilon(it):\n",
    "    if it >= 1000:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return epsilons[it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1a81dd07e1b7a98d2cd06ebc171ebdd",
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x158d846da58>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84685c23e4eb899d7fed3a87b7f8915e",
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function that takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon (which we will pass later). Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Unlike numpy, PyTorch has no argmax function, but Google is your friend... Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "882f51819100c850120e73340aec387d",
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return int(np.random.rand() * 2)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = model(torch.Tensor(state))\n",
    "            return torch.argmax(output).item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21f939075cb0c8dde152dabf47568a9d",
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = select_action(model, s, 0.05)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e66ac58d65710439ddf7cdf19a50cd8c",
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4839aac72a80552046ebecc40c1615cf",
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c466ee49add35cb1ec6a3e4a85f733c9",
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    q_val = model(state)\n",
    "    return q_val[np.arange(action.size()[0]), action]\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    q_val_next = model(next_state)\n",
    "    targets = reward + discount_factor * (1 - done).float() * torch.max(q_val_next, dim=1)[0]\n",
    "    return targets\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "877c400001292b619e6871c1366524b9",
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5898780822753906\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2841924b22cdf411348a0eb6080502",
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06dd71aae5c3c699f2b707b348a88107",
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3f61b2ca270d84ab9b28d989dd65d4c",
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        # Get the initial state\n",
    "        state = env.reset()\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            # Get the epsilon\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            \n",
    "            # Get an action and take a step\n",
    "            action = select_action(model, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Save to memory\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Train\n",
    "            loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "            # Print the step\n",
    "            print(\"Step {} ({}); Episode {}/{}; Loss: {}\".format(t, global_steps, i + 1, num_episodes, loss))\n",
    "            \n",
    "            # Increase the global steps\n",
    "            global_steps += 1         \n",
    "            # Check for done\n",
    "            if done:\n",
    "                episode_durations.append(t)\n",
    "                break\n",
    "            \n",
    "            # Otherwise continue\n",
    "            state = next_state\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (0); Episode 1/100; Loss: None\n",
      "Step 1 (1); Episode 1/100; Loss: None\n",
      "Step 2 (2); Episode 1/100; Loss: None\n",
      "Step 3 (3); Episode 1/100; Loss: None\n",
      "Step 4 (4); Episode 1/100; Loss: None\n",
      "Step 5 (5); Episode 1/100; Loss: None\n",
      "Step 6 (6); Episode 1/100; Loss: None\n",
      "Step 7 (7); Episode 1/100; Loss: None\n",
      "Step 8 (8); Episode 1/100; Loss: None\n",
      "Step 9 (9); Episode 1/100; Loss: None\n",
      "Step 10 (10); Episode 1/100; Loss: None\n",
      "Step 11 (11); Episode 1/100; Loss: None\n",
      "Step 12 (12); Episode 1/100; Loss: None\n",
      "Step 13 (13); Episode 1/100; Loss: None\n",
      "Step 14 (14); Episode 1/100; Loss: None\n",
      "Step 15 (15); Episode 1/100; Loss: None\n",
      "Step 16 (16); Episode 1/100; Loss: None\n",
      "Step 17 (17); Episode 1/100; Loss: None\n",
      "Step 18 (18); Episode 1/100; Loss: None\n",
      "Step 19 (19); Episode 1/100; Loss: None\n",
      "Step 20 (20); Episode 1/100; Loss: None\n",
      "Step 21 (21); Episode 1/100; Loss: None\n",
      "Step 22 (22); Episode 1/100; Loss: None\n",
      "Step 23 (23); Episode 1/100; Loss: None\n",
      "Step 24 (24); Episode 1/100; Loss: None\n",
      "Step 25 (25); Episode 1/100; Loss: None\n",
      "Step 26 (26); Episode 1/100; Loss: None\n",
      "Step 27 (27); Episode 1/100; Loss: None\n",
      "Step 28 (28); Episode 1/100; Loss: None\n",
      "Step 29 (29); Episode 1/100; Loss: None\n",
      "Step 30 (30); Episode 1/100; Loss: None\n",
      "Step 31 (31); Episode 1/100; Loss: None\n",
      "Step 32 (32); Episode 1/100; Loss: None\n",
      "Step 33 (33); Episode 1/100; Loss: None\n",
      "Step 0 (34); Episode 2/100; Loss: None\n",
      "Step 1 (35); Episode 2/100; Loss: None\n",
      "Step 2 (36); Episode 2/100; Loss: None\n",
      "Step 3 (37); Episode 2/100; Loss: None\n",
      "Step 4 (38); Episode 2/100; Loss: None\n",
      "Step 5 (39); Episode 2/100; Loss: None\n",
      "Step 6 (40); Episode 2/100; Loss: None\n",
      "Step 7 (41); Episode 2/100; Loss: None\n",
      "Step 8 (42); Episode 2/100; Loss: None\n",
      "Step 9 (43); Episode 2/100; Loss: None\n",
      "Step 10 (44); Episode 2/100; Loss: None\n",
      "Step 11 (45); Episode 2/100; Loss: None\n",
      "Step 0 (46); Episode 3/100; Loss: None\n",
      "Step 1 (47); Episode 3/100; Loss: None\n",
      "Step 2 (48); Episode 3/100; Loss: None\n",
      "Step 3 (49); Episode 3/100; Loss: None\n",
      "Step 4 (50); Episode 3/100; Loss: None\n",
      "Step 5 (51); Episode 3/100; Loss: None\n",
      "Step 6 (52); Episode 3/100; Loss: None\n",
      "Step 7 (53); Episode 3/100; Loss: None\n",
      "Step 8 (54); Episode 3/100; Loss: None\n",
      "Step 9 (55); Episode 3/100; Loss: None\n",
      "Step 10 (56); Episode 3/100; Loss: None\n",
      "Step 0 (57); Episode 4/100; Loss: None\n",
      "Step 1 (58); Episode 4/100; Loss: None\n",
      "Step 2 (59); Episode 4/100; Loss: None\n",
      "Step 3 (60); Episode 4/100; Loss: None\n",
      "Step 4 (61); Episode 4/100; Loss: None\n",
      "Step 5 (62); Episode 4/100; Loss: None\n",
      "Step 6 (63); Episode 4/100; Loss: 0.6656799912452698\n",
      "Step 7 (64); Episode 4/100; Loss: 0.6620500683784485\n",
      "Step 8 (65); Episode 4/100; Loss: 0.6579010486602783\n",
      "Step 9 (66); Episode 4/100; Loss: 0.6498590707778931\n",
      "Step 10 (67); Episode 4/100; Loss: 0.6455860733985901\n",
      "Step 11 (68); Episode 4/100; Loss: 0.6417188048362732\n",
      "Step 12 (69); Episode 4/100; Loss: 0.6313762068748474\n",
      "Step 13 (70); Episode 4/100; Loss: 0.6331341862678528\n",
      "Step 14 (71); Episode 4/100; Loss: 0.6252973079681396\n",
      "Step 15 (72); Episode 4/100; Loss: 0.6182132959365845\n",
      "Step 16 (73); Episode 4/100; Loss: 0.6181449890136719\n",
      "Step 17 (74); Episode 4/100; Loss: 0.6087644696235657\n",
      "Step 18 (75); Episode 4/100; Loss: 0.5988286733627319\n",
      "Step 19 (76); Episode 4/100; Loss: 0.5914990305900574\n",
      "Step 20 (77); Episode 4/100; Loss: 0.587934136390686\n",
      "Step 21 (78); Episode 4/100; Loss: 0.5956051349639893\n",
      "Step 22 (79); Episode 4/100; Loss: 0.5769655108451843\n",
      "Step 23 (80); Episode 4/100; Loss: 0.580588698387146\n",
      "Step 0 (81); Episode 5/100; Loss: 0.5728309750556946\n",
      "Step 1 (82); Episode 5/100; Loss: 0.5635030269622803\n",
      "Step 2 (83); Episode 5/100; Loss: 0.5616545081138611\n",
      "Step 3 (84); Episode 5/100; Loss: 0.5531212687492371\n",
      "Step 4 (85); Episode 5/100; Loss: 0.5419610738754272\n",
      "Step 5 (86); Episode 5/100; Loss: 0.552812933921814\n",
      "Step 6 (87); Episode 5/100; Loss: 0.5524763464927673\n",
      "Step 7 (88); Episode 5/100; Loss: 0.5378400683403015\n",
      "Step 8 (89); Episode 5/100; Loss: 0.533470630645752\n",
      "Step 9 (90); Episode 5/100; Loss: 0.528815507888794\n",
      "Step 10 (91); Episode 5/100; Loss: 0.5288099646568298\n",
      "Step 11 (92); Episode 5/100; Loss: 0.5296723246574402\n",
      "Step 12 (93); Episode 5/100; Loss: 0.5225751399993896\n",
      "Step 13 (94); Episode 5/100; Loss: 0.5118783116340637\n",
      "Step 0 (95); Episode 6/100; Loss: 0.5011741518974304\n",
      "Step 1 (96); Episode 6/100; Loss: 0.4937552213668823\n",
      "Step 2 (97); Episode 6/100; Loss: 0.483482301235199\n",
      "Step 3 (98); Episode 6/100; Loss: 0.49018943309783936\n",
      "Step 4 (99); Episode 6/100; Loss: 0.4930189847946167\n",
      "Step 5 (100); Episode 6/100; Loss: 0.4818252921104431\n",
      "Step 6 (101); Episode 6/100; Loss: 0.4844115972518921\n",
      "Step 7 (102); Episode 6/100; Loss: 0.5028082132339478\n",
      "Step 8 (103); Episode 6/100; Loss: 0.4776521325111389\n",
      "Step 9 (104); Episode 6/100; Loss: 0.48595714569091797\n",
      "Step 10 (105); Episode 6/100; Loss: 0.4722488820552826\n",
      "Step 11 (106); Episode 6/100; Loss: 0.4953714907169342\n",
      "Step 12 (107); Episode 6/100; Loss: 0.4671602249145508\n",
      "Step 13 (108); Episode 6/100; Loss: 0.46211910247802734\n",
      "Step 14 (109); Episode 6/100; Loss: 0.47250720858573914\n",
      "Step 15 (110); Episode 6/100; Loss: 0.4600598216056824\n",
      "Step 16 (111); Episode 6/100; Loss: 0.4497678577899933\n",
      "Step 17 (112); Episode 6/100; Loss: 0.4622686505317688\n",
      "Step 18 (113); Episode 6/100; Loss: 0.4614793360233307\n",
      "Step 0 (114); Episode 7/100; Loss: 0.4583069682121277\n",
      "Step 1 (115); Episode 7/100; Loss: 0.43706005811691284\n",
      "Step 2 (116); Episode 7/100; Loss: 0.4554370939731598\n",
      "Step 3 (117); Episode 7/100; Loss: 0.4591163098812103\n",
      "Step 4 (118); Episode 7/100; Loss: 0.452548086643219\n",
      "Step 5 (119); Episode 7/100; Loss: 0.4578363001346588\n",
      "Step 6 (120); Episode 7/100; Loss: 0.45428144931793213\n",
      "Step 7 (121); Episode 7/100; Loss: 0.44878023862838745\n",
      "Step 8 (122); Episode 7/100; Loss: 0.4463171362876892\n",
      "Step 9 (123); Episode 7/100; Loss: 0.446209579706192\n",
      "Step 10 (124); Episode 7/100; Loss: 0.4303571283817291\n",
      "Step 11 (125); Episode 7/100; Loss: 0.4538354277610779\n",
      "Step 12 (126); Episode 7/100; Loss: 0.4303358793258667\n",
      "Step 13 (127); Episode 7/100; Loss: 0.43488407135009766\n",
      "Step 14 (128); Episode 7/100; Loss: 0.4287971258163452\n",
      "Step 15 (129); Episode 7/100; Loss: 0.40424486994743347\n",
      "Step 16 (130); Episode 7/100; Loss: 0.4403335452079773\n",
      "Step 17 (131); Episode 7/100; Loss: 0.42473289370536804\n",
      "Step 18 (132); Episode 7/100; Loss: 0.4084095358848572\n",
      "Step 19 (133); Episode 7/100; Loss: 0.407662034034729\n",
      "Step 20 (134); Episode 7/100; Loss: 0.4270833730697632\n",
      "Step 21 (135); Episode 7/100; Loss: 0.41408059000968933\n",
      "Step 22 (136); Episode 7/100; Loss: 0.4259072244167328\n",
      "Step 23 (137); Episode 7/100; Loss: 0.42255699634552\n",
      "Step 24 (138); Episode 7/100; Loss: 0.41787779331207275\n",
      "Step 25 (139); Episode 7/100; Loss: 0.4147951304912567\n",
      "Step 26 (140); Episode 7/100; Loss: 0.446471244096756\n",
      "Step 27 (141); Episode 7/100; Loss: 0.43784964084625244\n",
      "Step 28 (142); Episode 7/100; Loss: 0.3921155333518982\n",
      "Step 29 (143); Episode 7/100; Loss: 0.43182373046875\n",
      "Step 30 (144); Episode 7/100; Loss: 0.40949511528015137\n",
      "Step 0 (145); Episode 8/100; Loss: 0.40481987595558167\n",
      "Step 1 (146); Episode 8/100; Loss: 0.3752456307411194\n",
      "Step 2 (147); Episode 8/100; Loss: 0.4376790523529053\n",
      "Step 3 (148); Episode 8/100; Loss: 0.39732712507247925\n",
      "Step 4 (149); Episode 8/100; Loss: 0.4127822816371918\n",
      "Step 5 (150); Episode 8/100; Loss: 0.4467579424381256\n",
      "Step 6 (151); Episode 8/100; Loss: 0.41936612129211426\n",
      "Step 7 (152); Episode 8/100; Loss: 0.5051941871643066\n",
      "Step 8 (153); Episode 8/100; Loss: 0.42936447262763977\n",
      "Step 9 (154); Episode 8/100; Loss: 0.39366674423217773\n",
      "Step 10 (155); Episode 8/100; Loss: 0.4747236967086792\n",
      "Step 0 (156); Episode 9/100; Loss: 0.41505199670791626\n",
      "Step 1 (157); Episode 9/100; Loss: 0.47036224603652954\n",
      "Step 2 (158); Episode 9/100; Loss: 0.37686580419540405\n",
      "Step 3 (159); Episode 9/100; Loss: 0.4279867112636566\n",
      "Step 4 (160); Episode 9/100; Loss: 0.44367244839668274\n",
      "Step 5 (161); Episode 9/100; Loss: 0.5123571753501892\n",
      "Step 6 (162); Episode 9/100; Loss: 0.48109111189842224\n",
      "Step 7 (163); Episode 9/100; Loss: 0.4064996540546417\n",
      "Step 8 (164); Episode 9/100; Loss: 0.42100435495376587\n",
      "Step 9 (165); Episode 9/100; Loss: 0.44570574164390564\n",
      "Step 10 (166); Episode 9/100; Loss: 0.49181658029556274\n",
      "Step 11 (167); Episode 9/100; Loss: 0.4733455181121826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12 (168); Episode 9/100; Loss: 0.3973735272884369\n",
      "Step 13 (169); Episode 9/100; Loss: 0.5391399264335632\n",
      "Step 0 (170); Episode 10/100; Loss: 0.4440852701663971\n",
      "Step 1 (171); Episode 10/100; Loss: 0.4617070257663727\n",
      "Step 2 (172); Episode 10/100; Loss: 0.4934101402759552\n",
      "Step 3 (173); Episode 10/100; Loss: 0.5751378536224365\n",
      "Step 4 (174); Episode 10/100; Loss: 0.5206555724143982\n",
      "Step 5 (175); Episode 10/100; Loss: 0.4328920841217041\n",
      "Step 6 (176); Episode 10/100; Loss: 0.39477744698524475\n",
      "Step 7 (177); Episode 10/100; Loss: 0.4475782811641693\n",
      "Step 8 (178); Episode 10/100; Loss: 0.411612868309021\n",
      "Step 9 (179); Episode 10/100; Loss: 0.4885333478450775\n",
      "Step 10 (180); Episode 10/100; Loss: 0.34962326288223267\n",
      "Step 11 (181); Episode 10/100; Loss: 0.4501132667064667\n",
      "Step 12 (182); Episode 10/100; Loss: 0.4262615740299225\n",
      "Step 13 (183); Episode 10/100; Loss: 0.4977755546569824\n",
      "Step 14 (184); Episode 10/100; Loss: 0.36069104075431824\n",
      "Step 15 (185); Episode 10/100; Loss: 0.4136691093444824\n",
      "Step 16 (186); Episode 10/100; Loss: 0.6030383706092834\n",
      "Step 17 (187); Episode 10/100; Loss: 0.46906164288520813\n",
      "Step 18 (188); Episode 10/100; Loss: 0.6516097187995911\n",
      "Step 19 (189); Episode 10/100; Loss: 0.430177241563797\n",
      "Step 20 (190); Episode 10/100; Loss: 0.4957132935523987\n",
      "Step 21 (191); Episode 10/100; Loss: 0.34493640065193176\n",
      "Step 0 (192); Episode 11/100; Loss: 0.6129205822944641\n",
      "Step 1 (193); Episode 11/100; Loss: 0.417991042137146\n",
      "Step 2 (194); Episode 11/100; Loss: 0.4050278067588806\n",
      "Step 3 (195); Episode 11/100; Loss: 0.2750573456287384\n",
      "Step 4 (196); Episode 11/100; Loss: 0.4838779866695404\n",
      "Step 5 (197); Episode 11/100; Loss: 0.5330084562301636\n",
      "Step 6 (198); Episode 11/100; Loss: 0.6687424182891846\n",
      "Step 7 (199); Episode 11/100; Loss: 0.5012626051902771\n",
      "Step 8 (200); Episode 11/100; Loss: 0.5282201766967773\n",
      "Step 9 (201); Episode 11/100; Loss: 0.6270981431007385\n",
      "Step 10 (202); Episode 11/100; Loss: 0.5253276228904724\n",
      "Step 11 (203); Episode 11/100; Loss: 0.614151656627655\n",
      "Step 12 (204); Episode 11/100; Loss: 0.6136789321899414\n",
      "Step 0 (205); Episode 12/100; Loss: 0.4608067274093628\n",
      "Step 1 (206); Episode 12/100; Loss: 0.7121788263320923\n",
      "Step 2 (207); Episode 12/100; Loss: 0.3446139693260193\n",
      "Step 3 (208); Episode 12/100; Loss: 0.5412876605987549\n",
      "Step 4 (209); Episode 12/100; Loss: 0.4681203067302704\n",
      "Step 5 (210); Episode 12/100; Loss: 0.40297403931617737\n",
      "Step 6 (211); Episode 12/100; Loss: 0.6730456948280334\n",
      "Step 7 (212); Episode 12/100; Loss: 0.6324928998947144\n",
      "Step 8 (213); Episode 12/100; Loss: 0.36738505959510803\n",
      "Step 9 (214); Episode 12/100; Loss: 0.8898921608924866\n",
      "Step 0 (215); Episode 13/100; Loss: 1.1245863437652588\n",
      "Step 1 (216); Episode 13/100; Loss: 0.6932126879692078\n",
      "Step 2 (217); Episode 13/100; Loss: 0.8364805579185486\n",
      "Step 3 (218); Episode 13/100; Loss: 0.7164238691329956\n",
      "Step 4 (219); Episode 13/100; Loss: 0.5267941951751709\n",
      "Step 5 (220); Episode 13/100; Loss: 0.7319695949554443\n",
      "Step 6 (221); Episode 13/100; Loss: 0.8364747166633606\n",
      "Step 7 (222); Episode 13/100; Loss: 0.627562940120697\n",
      "Step 8 (223); Episode 13/100; Loss: 0.6542298793792725\n",
      "Step 0 (224); Episode 14/100; Loss: 0.716262936592102\n",
      "Step 1 (225); Episode 14/100; Loss: 0.4734286069869995\n",
      "Step 2 (226); Episode 14/100; Loss: 0.6385658383369446\n",
      "Step 3 (227); Episode 14/100; Loss: 0.9011058807373047\n",
      "Step 4 (228); Episode 14/100; Loss: 1.0540488958358765\n",
      "Step 5 (229); Episode 14/100; Loss: 0.6938192248344421\n",
      "Step 6 (230); Episode 14/100; Loss: 0.6045973896980286\n",
      "Step 7 (231); Episode 14/100; Loss: 0.6830989122390747\n",
      "Step 8 (232); Episode 14/100; Loss: 0.34886807203292847\n",
      "Step 9 (233); Episode 14/100; Loss: 0.4167412519454956\n",
      "Step 10 (234); Episode 14/100; Loss: 0.9593698382377625\n",
      "Step 11 (235); Episode 14/100; Loss: 0.6863861083984375\n",
      "Step 12 (236); Episode 14/100; Loss: 1.077666997909546\n",
      "Step 13 (237); Episode 14/100; Loss: 0.9046010375022888\n",
      "Step 14 (238); Episode 14/100; Loss: 0.8963130116462708\n",
      "Step 0 (239); Episode 15/100; Loss: 0.49930763244628906\n",
      "Step 1 (240); Episode 15/100; Loss: 0.5634673833847046\n",
      "Step 2 (241); Episode 15/100; Loss: 1.1267930269241333\n",
      "Step 3 (242); Episode 15/100; Loss: 0.9438521265983582\n",
      "Step 4 (243); Episode 15/100; Loss: 0.7342668771743774\n",
      "Step 5 (244); Episode 15/100; Loss: 0.8323433995246887\n",
      "Step 6 (245); Episode 15/100; Loss: 0.2081947922706604\n",
      "Step 7 (246); Episode 15/100; Loss: 0.6641939878463745\n",
      "Step 8 (247); Episode 15/100; Loss: 0.5945857167243958\n",
      "Step 9 (248); Episode 15/100; Loss: 0.7405328750610352\n",
      "Step 10 (249); Episode 15/100; Loss: 0.5181992053985596\n",
      "Step 11 (250); Episode 15/100; Loss: 0.823310136795044\n",
      "Step 12 (251); Episode 15/100; Loss: 1.461498498916626\n",
      "Step 13 (252); Episode 15/100; Loss: 1.0165514945983887\n",
      "Step 14 (253); Episode 15/100; Loss: 0.6551244854927063\n",
      "Step 0 (254); Episode 16/100; Loss: 0.838782787322998\n",
      "Step 1 (255); Episode 16/100; Loss: 0.4614671468734741\n",
      "Step 2 (256); Episode 16/100; Loss: 0.9167554974555969\n",
      "Step 3 (257); Episode 16/100; Loss: 0.7167527675628662\n",
      "Step 4 (258); Episode 16/100; Loss: 0.6810492277145386\n",
      "Step 5 (259); Episode 16/100; Loss: 0.9619529247283936\n",
      "Step 6 (260); Episode 16/100; Loss: 0.7024874687194824\n",
      "Step 7 (261); Episode 16/100; Loss: 0.9559241533279419\n",
      "Step 8 (262); Episode 16/100; Loss: 0.3940833508968353\n",
      "Step 9 (263); Episode 16/100; Loss: 0.6350939273834229\n",
      "Step 10 (264); Episode 16/100; Loss: 0.9684664011001587\n",
      "Step 11 (265); Episode 16/100; Loss: 0.6325644850730896\n",
      "Step 12 (266); Episode 16/100; Loss: 0.5987251400947571\n",
      "Step 0 (267); Episode 17/100; Loss: 1.4383323192596436\n",
      "Step 1 (268); Episode 17/100; Loss: 1.4370354413986206\n",
      "Step 2 (269); Episode 17/100; Loss: 0.5856584310531616\n",
      "Step 3 (270); Episode 17/100; Loss: 1.0427650213241577\n",
      "Step 4 (271); Episode 17/100; Loss: 0.7296421527862549\n",
      "Step 5 (272); Episode 17/100; Loss: 0.3087994158267975\n",
      "Step 6 (273); Episode 17/100; Loss: 1.5195273160934448\n",
      "Step 7 (274); Episode 17/100; Loss: 0.6820775270462036\n",
      "Step 8 (275); Episode 17/100; Loss: 0.4185253083705902\n",
      "Step 9 (276); Episode 17/100; Loss: 1.0758191347122192\n",
      "Step 10 (277); Episode 17/100; Loss: 1.5774418115615845\n",
      "Step 11 (278); Episode 17/100; Loss: 0.7832777500152588\n",
      "Step 12 (279); Episode 17/100; Loss: 0.5603352785110474\n",
      "Step 13 (280); Episode 17/100; Loss: 1.0266666412353516\n",
      "Step 14 (281); Episode 17/100; Loss: 0.7394917011260986\n",
      "Step 15 (282); Episode 17/100; Loss: 0.7624014616012573\n",
      "Step 16 (283); Episode 17/100; Loss: 0.6747793555259705\n",
      "Step 17 (284); Episode 17/100; Loss: 0.8811432719230652\n",
      "Step 0 (285); Episode 18/100; Loss: 0.8502777814865112\n",
      "Step 1 (286); Episode 18/100; Loss: 0.3161928057670593\n",
      "Step 2 (287); Episode 18/100; Loss: 1.3464980125427246\n",
      "Step 3 (288); Episode 18/100; Loss: 0.979223370552063\n",
      "Step 4 (289); Episode 18/100; Loss: 0.732214629650116\n",
      "Step 5 (290); Episode 18/100; Loss: 0.638974666595459\n",
      "Step 6 (291); Episode 18/100; Loss: 1.3703073263168335\n",
      "Step 7 (292); Episode 18/100; Loss: 0.46997758746147156\n",
      "Step 8 (293); Episode 18/100; Loss: 0.7869735360145569\n",
      "Step 9 (294); Episode 18/100; Loss: 0.7679395079612732\n",
      "Step 10 (295); Episode 18/100; Loss: 0.49631568789482117\n",
      "Step 11 (296); Episode 18/100; Loss: 1.3802303075790405\n",
      "Step 12 (297); Episode 18/100; Loss: 0.742904782295227\n",
      "Step 13 (298); Episode 18/100; Loss: 0.22657093405723572\n",
      "Step 14 (299); Episode 18/100; Loss: 1.2886754274368286\n",
      "Step 15 (300); Episode 18/100; Loss: 1.7768579721450806\n",
      "Step 16 (301); Episode 18/100; Loss: 0.9817697405815125\n",
      "Step 17 (302); Episode 18/100; Loss: 0.5880704522132874\n",
      "Step 18 (303); Episode 18/100; Loss: 0.8172270655632019\n",
      "Step 19 (304); Episode 18/100; Loss: 0.32553479075431824\n",
      "Step 20 (305); Episode 18/100; Loss: 1.0511765480041504\n",
      "Step 21 (306); Episode 18/100; Loss: 0.564323902130127\n",
      "Step 22 (307); Episode 18/100; Loss: 0.9944705367088318\n",
      "Step 23 (308); Episode 18/100; Loss: 0.6222324967384338\n",
      "Step 24 (309); Episode 18/100; Loss: 1.1653987169265747\n",
      "Step 0 (310); Episode 19/100; Loss: 0.7401947975158691\n",
      "Step 1 (311); Episode 19/100; Loss: 0.8661297559738159\n",
      "Step 2 (312); Episode 19/100; Loss: 0.9771541953086853\n",
      "Step 3 (313); Episode 19/100; Loss: 1.176376461982727\n",
      "Step 4 (314); Episode 19/100; Loss: 0.17431659996509552\n",
      "Step 5 (315); Episode 19/100; Loss: 0.9415532350540161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 (316); Episode 19/100; Loss: 1.3587923049926758\n",
      "Step 7 (317); Episode 19/100; Loss: 0.41135773062705994\n",
      "Step 8 (318); Episode 19/100; Loss: 0.6566306948661804\n",
      "Step 0 (319); Episode 20/100; Loss: 1.0070780515670776\n",
      "Step 1 (320); Episode 20/100; Loss: 0.7227301001548767\n",
      "Step 2 (321); Episode 20/100; Loss: 0.5087317228317261\n",
      "Step 3 (322); Episode 20/100; Loss: 0.6251462697982788\n",
      "Step 4 (323); Episode 20/100; Loss: 0.5361431241035461\n",
      "Step 5 (324); Episode 20/100; Loss: 0.9217059016227722\n",
      "Step 6 (325); Episode 20/100; Loss: 1.0377435684204102\n",
      "Step 7 (326); Episode 20/100; Loss: 0.8129849433898926\n",
      "Step 8 (327); Episode 20/100; Loss: 0.9297593235969543\n",
      "Step 9 (328); Episode 20/100; Loss: 0.7574583292007446\n",
      "Step 10 (329); Episode 20/100; Loss: 0.9520177841186523\n",
      "Step 11 (330); Episode 20/100; Loss: 0.3297266960144043\n",
      "Step 12 (331); Episode 20/100; Loss: 1.1841673851013184\n",
      "Step 13 (332); Episode 20/100; Loss: 0.2500457167625427\n",
      "Step 14 (333); Episode 20/100; Loss: 0.9880006909370422\n",
      "Step 15 (334); Episode 20/100; Loss: 0.9369679689407349\n",
      "Step 16 (335); Episode 20/100; Loss: 0.5469056367874146\n",
      "Step 17 (336); Episode 20/100; Loss: 0.5758706331253052\n",
      "Step 18 (337); Episode 20/100; Loss: 1.1807410717010498\n",
      "Step 19 (338); Episode 20/100; Loss: 0.8359482884407043\n",
      "Step 0 (339); Episode 21/100; Loss: 0.6445804238319397\n",
      "Step 1 (340); Episode 21/100; Loss: 0.7091219425201416\n",
      "Step 2 (341); Episode 21/100; Loss: 0.42079636454582214\n",
      "Step 3 (342); Episode 21/100; Loss: 0.804141640663147\n",
      "Step 4 (343); Episode 21/100; Loss: 0.6051506996154785\n",
      "Step 5 (344); Episode 21/100; Loss: 1.0667393207550049\n",
      "Step 6 (345); Episode 21/100; Loss: 0.9234071373939514\n",
      "Step 7 (346); Episode 21/100; Loss: 0.88800448179245\n",
      "Step 8 (347); Episode 21/100; Loss: 1.104824185371399\n",
      "Step 9 (348); Episode 21/100; Loss: 0.5736714601516724\n",
      "Step 10 (349); Episode 21/100; Loss: 0.9254517555236816\n",
      "Step 11 (350); Episode 21/100; Loss: 0.35856375098228455\n",
      "Step 12 (351); Episode 21/100; Loss: 0.32070019841194153\n",
      "Step 13 (352); Episode 21/100; Loss: 0.38588637113571167\n",
      "Step 14 (353); Episode 21/100; Loss: 0.4515141248703003\n",
      "Step 15 (354); Episode 21/100; Loss: 0.2527754306793213\n",
      "Step 16 (355); Episode 21/100; Loss: 0.5777632594108582\n",
      "Step 17 (356); Episode 21/100; Loss: 0.9555094838142395\n",
      "Step 18 (357); Episode 21/100; Loss: 0.7674002051353455\n",
      "Step 19 (358); Episode 21/100; Loss: 0.21618805825710297\n",
      "Step 20 (359); Episode 21/100; Loss: 0.9195418953895569\n",
      "Step 0 (360); Episode 22/100; Loss: 0.8039978742599487\n",
      "Step 1 (361); Episode 22/100; Loss: 0.2702289819717407\n",
      "Step 2 (362); Episode 22/100; Loss: 0.3688895106315613\n",
      "Step 3 (363); Episode 22/100; Loss: 0.6893760561943054\n",
      "Step 4 (364); Episode 22/100; Loss: 0.8747181296348572\n",
      "Step 5 (365); Episode 22/100; Loss: 0.39865484833717346\n",
      "Step 6 (366); Episode 22/100; Loss: 0.5828092098236084\n",
      "Step 7 (367); Episode 22/100; Loss: 0.8166760206222534\n",
      "Step 8 (368); Episode 22/100; Loss: 0.7641122937202454\n",
      "Step 9 (369); Episode 22/100; Loss: 0.6503735780715942\n",
      "Step 10 (370); Episode 22/100; Loss: 0.30606934428215027\n",
      "Step 11 (371); Episode 22/100; Loss: 1.08168363571167\n",
      "Step 0 (372); Episode 23/100; Loss: 0.7310294508934021\n",
      "Step 1 (373); Episode 23/100; Loss: 0.3820190131664276\n",
      "Step 2 (374); Episode 23/100; Loss: 0.1891811043024063\n",
      "Step 3 (375); Episode 23/100; Loss: 0.574489176273346\n",
      "Step 4 (376); Episode 23/100; Loss: 0.8418281674385071\n",
      "Step 5 (377); Episode 23/100; Loss: 0.24971851706504822\n",
      "Step 6 (378); Episode 23/100; Loss: 0.3827170729637146\n",
      "Step 7 (379); Episode 23/100; Loss: 0.28970369696617126\n",
      "Step 8 (380); Episode 23/100; Loss: 0.6338297128677368\n",
      "Step 9 (381); Episode 23/100; Loss: 0.3828382194042206\n",
      "Step 10 (382); Episode 23/100; Loss: 0.835503101348877\n",
      "Step 11 (383); Episode 23/100; Loss: 0.7915945053100586\n",
      "Step 12 (384); Episode 23/100; Loss: 0.6021967530250549\n",
      "Step 13 (385); Episode 23/100; Loss: 0.6538230180740356\n",
      "Step 14 (386); Episode 23/100; Loss: 0.762381911277771\n",
      "Step 15 (387); Episode 23/100; Loss: 0.8380777835845947\n",
      "Step 0 (388); Episode 24/100; Loss: 0.6668716073036194\n",
      "Step 1 (389); Episode 24/100; Loss: 0.7496220469474792\n",
      "Step 2 (390); Episode 24/100; Loss: 0.7904831171035767\n",
      "Step 3 (391); Episode 24/100; Loss: 0.614222526550293\n",
      "Step 4 (392); Episode 24/100; Loss: 0.7301918864250183\n",
      "Step 5 (393); Episode 24/100; Loss: 1.2974238395690918\n",
      "Step 6 (394); Episode 24/100; Loss: 0.8468828201293945\n",
      "Step 7 (395); Episode 24/100; Loss: 1.2824434041976929\n",
      "Step 8 (396); Episode 24/100; Loss: 0.5478977560997009\n",
      "Step 9 (397); Episode 24/100; Loss: 0.6005793809890747\n",
      "Step 10 (398); Episode 24/100; Loss: 0.675503671169281\n",
      "Step 11 (399); Episode 24/100; Loss: 0.4776636064052582\n",
      "Step 12 (400); Episode 24/100; Loss: 0.14405137300491333\n",
      "Step 0 (401); Episode 25/100; Loss: 0.26221999526023865\n",
      "Step 1 (402); Episode 25/100; Loss: 0.9884300231933594\n",
      "Step 2 (403); Episode 25/100; Loss: 0.7439645528793335\n",
      "Step 3 (404); Episode 25/100; Loss: 0.47855329513549805\n",
      "Step 4 (405); Episode 25/100; Loss: 1.653839349746704\n",
      "Step 5 (406); Episode 25/100; Loss: 0.5037631392478943\n",
      "Step 6 (407); Episode 25/100; Loss: 0.6319970488548279\n",
      "Step 7 (408); Episode 25/100; Loss: 0.9247621893882751\n",
      "Step 8 (409); Episode 25/100; Loss: 0.7005523443222046\n",
      "Step 9 (410); Episode 25/100; Loss: 0.31143292784690857\n",
      "Step 10 (411); Episode 25/100; Loss: 0.16691705584526062\n",
      "Step 11 (412); Episode 25/100; Loss: 1.152495265007019\n",
      "Step 12 (413); Episode 25/100; Loss: 0.49162787199020386\n",
      "Step 0 (414); Episode 26/100; Loss: 0.9141429662704468\n",
      "Step 1 (415); Episode 26/100; Loss: 0.5108555555343628\n",
      "Step 2 (416); Episode 26/100; Loss: 1.2011396884918213\n",
      "Step 3 (417); Episode 26/100; Loss: 0.7402808666229248\n",
      "Step 4 (418); Episode 26/100; Loss: 0.5596277117729187\n",
      "Step 5 (419); Episode 26/100; Loss: 0.21661734580993652\n",
      "Step 6 (420); Episode 26/100; Loss: 0.40355172753334045\n",
      "Step 7 (421); Episode 26/100; Loss: 0.6413229703903198\n",
      "Step 8 (422); Episode 26/100; Loss: 0.9732441902160645\n",
      "Step 9 (423); Episode 26/100; Loss: 0.4086345136165619\n",
      "Step 10 (424); Episode 26/100; Loss: 0.46871888637542725\n",
      "Step 0 (425); Episode 27/100; Loss: 0.5477762818336487\n",
      "Step 1 (426); Episode 27/100; Loss: 1.4287413358688354\n",
      "Step 2 (427); Episode 27/100; Loss: 0.8723673820495605\n",
      "Step 3 (428); Episode 27/100; Loss: 0.1496407389640808\n",
      "Step 4 (429); Episode 27/100; Loss: 0.639327883720398\n",
      "Step 5 (430); Episode 27/100; Loss: 0.3446829617023468\n",
      "Step 6 (431); Episode 27/100; Loss: 0.975024402141571\n",
      "Step 7 (432); Episode 27/100; Loss: 0.6114235520362854\n",
      "Step 8 (433); Episode 27/100; Loss: 0.3862690031528473\n",
      "Step 9 (434); Episode 27/100; Loss: 0.49754798412323\n",
      "Step 10 (435); Episode 27/100; Loss: 0.8789636492729187\n",
      "Step 0 (436); Episode 28/100; Loss: 0.44505631923675537\n",
      "Step 1 (437); Episode 28/100; Loss: 0.3531316816806793\n",
      "Step 2 (438); Episode 28/100; Loss: 0.5313390493392944\n",
      "Step 3 (439); Episode 28/100; Loss: 0.9663305878639221\n",
      "Step 4 (440); Episode 28/100; Loss: 0.5844516754150391\n",
      "Step 5 (441); Episode 28/100; Loss: 0.7747105956077576\n",
      "Step 6 (442); Episode 28/100; Loss: 1.6768040657043457\n",
      "Step 7 (443); Episode 28/100; Loss: 1.4540380239486694\n",
      "Step 8 (444); Episode 28/100; Loss: 0.9352859854698181\n",
      "Step 9 (445); Episode 28/100; Loss: 0.4686899185180664\n",
      "Step 10 (446); Episode 28/100; Loss: 0.6716271042823792\n",
      "Step 11 (447); Episode 28/100; Loss: 0.32468533515930176\n",
      "Step 12 (448); Episode 28/100; Loss: 0.9150718450546265\n",
      "Step 0 (449); Episode 29/100; Loss: 0.6693118810653687\n",
      "Step 1 (450); Episode 29/100; Loss: 1.0463504791259766\n",
      "Step 2 (451); Episode 29/100; Loss: 0.0368221290409565\n",
      "Step 3 (452); Episode 29/100; Loss: 0.5864008665084839\n",
      "Step 4 (453); Episode 29/100; Loss: 0.8479222655296326\n",
      "Step 5 (454); Episode 29/100; Loss: 0.49819040298461914\n",
      "Step 6 (455); Episode 29/100; Loss: 0.3749222159385681\n",
      "Step 7 (456); Episode 29/100; Loss: 0.25727835297584534\n",
      "Step 8 (457); Episode 29/100; Loss: 0.2459920197725296\n",
      "Step 9 (458); Episode 29/100; Loss: 0.7276434898376465\n",
      "Step 10 (459); Episode 29/100; Loss: 0.6161874532699585\n",
      "Step 11 (460); Episode 29/100; Loss: 0.3617314398288727\n",
      "Step 12 (461); Episode 29/100; Loss: 0.24003945291042328\n",
      "Step 13 (462); Episode 29/100; Loss: 0.8796169757843018\n",
      "Step 14 (463); Episode 29/100; Loss: 0.378072589635849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 (464); Episode 29/100; Loss: 0.7073884010314941\n",
      "Step 16 (465); Episode 29/100; Loss: 0.49398112297058105\n",
      "Step 17 (466); Episode 29/100; Loss: 0.803697407245636\n",
      "Step 0 (467); Episode 30/100; Loss: 0.35757359862327576\n",
      "Step 1 (468); Episode 30/100; Loss: 0.8127176761627197\n",
      "Step 2 (469); Episode 30/100; Loss: 0.26552432775497437\n",
      "Step 3 (470); Episode 30/100; Loss: 1.0946003198623657\n",
      "Step 4 (471); Episode 30/100; Loss: 0.5700643062591553\n",
      "Step 5 (472); Episode 30/100; Loss: 0.6901544332504272\n",
      "Step 6 (473); Episode 30/100; Loss: 0.17576752603054047\n",
      "Step 7 (474); Episode 30/100; Loss: 0.3118240237236023\n",
      "Step 8 (475); Episode 30/100; Loss: 0.33805862069129944\n",
      "Step 9 (476); Episode 30/100; Loss: 0.8011383414268494\n",
      "Step 10 (477); Episode 30/100; Loss: 0.987947404384613\n",
      "Step 0 (478); Episode 31/100; Loss: 1.040987253189087\n",
      "Step 1 (479); Episode 31/100; Loss: 0.2905968725681305\n",
      "Step 2 (480); Episode 31/100; Loss: 0.6302490234375\n",
      "Step 3 (481); Episode 31/100; Loss: 0.33244433999061584\n",
      "Step 4 (482); Episode 31/100; Loss: 0.3129993975162506\n",
      "Step 5 (483); Episode 31/100; Loss: 0.7403157353401184\n",
      "Step 6 (484); Episode 31/100; Loss: 0.9049254655838013\n",
      "Step 7 (485); Episode 31/100; Loss: 0.521461009979248\n",
      "Step 8 (486); Episode 31/100; Loss: 0.30414941906929016\n",
      "Step 9 (487); Episode 31/100; Loss: 0.10010507702827454\n",
      "Step 10 (488); Episode 31/100; Loss: 0.09256384521722794\n",
      "Step 11 (489); Episode 31/100; Loss: 0.7182124257087708\n",
      "Step 0 (490); Episode 32/100; Loss: 0.5356451272964478\n",
      "Step 1 (491); Episode 32/100; Loss: 0.7065108418464661\n",
      "Step 2 (492); Episode 32/100; Loss: 0.6072382926940918\n",
      "Step 3 (493); Episode 32/100; Loss: 0.27282512187957764\n",
      "Step 4 (494); Episode 32/100; Loss: 0.7713470458984375\n",
      "Step 5 (495); Episode 32/100; Loss: 0.12976819276809692\n",
      "Step 6 (496); Episode 32/100; Loss: 0.20645540952682495\n",
      "Step 7 (497); Episode 32/100; Loss: 0.7338061332702637\n",
      "Step 8 (498); Episode 32/100; Loss: 0.5363050103187561\n",
      "Step 9 (499); Episode 32/100; Loss: 0.46581095457077026\n",
      "Step 10 (500); Episode 32/100; Loss: 0.24880936741828918\n",
      "Step 11 (501); Episode 32/100; Loss: 0.6279177069664001\n",
      "Step 0 (502); Episode 33/100; Loss: 0.18125584721565247\n",
      "Step 1 (503); Episode 33/100; Loss: 0.1317611187696457\n",
      "Step 2 (504); Episode 33/100; Loss: 0.47355222702026367\n",
      "Step 3 (505); Episode 33/100; Loss: 0.3094150722026825\n",
      "Step 4 (506); Episode 33/100; Loss: 0.2798326909542084\n",
      "Step 5 (507); Episode 33/100; Loss: 0.46369796991348267\n",
      "Step 6 (508); Episode 33/100; Loss: 0.244801864027977\n",
      "Step 7 (509); Episode 33/100; Loss: 0.19224724173545837\n",
      "Step 8 (510); Episode 33/100; Loss: 0.4705412983894348\n",
      "Step 9 (511); Episode 33/100; Loss: 0.35911402106285095\n",
      "Step 10 (512); Episode 33/100; Loss: 0.18883128464221954\n",
      "Step 11 (513); Episode 33/100; Loss: 0.40254727005958557\n",
      "Step 12 (514); Episode 33/100; Loss: 0.2880815267562866\n",
      "Step 13 (515); Episode 33/100; Loss: 0.22250045835971832\n",
      "Step 0 (516); Episode 34/100; Loss: 0.4225393235683441\n",
      "Step 1 (517); Episode 34/100; Loss: 0.5596181750297546\n",
      "Step 2 (518); Episode 34/100; Loss: 0.2500418424606323\n",
      "Step 3 (519); Episode 34/100; Loss: 0.06243240460753441\n",
      "Step 4 (520); Episode 34/100; Loss: 0.2518655061721802\n",
      "Step 5 (521); Episode 34/100; Loss: 0.21738441288471222\n",
      "Step 6 (522); Episode 34/100; Loss: 0.4343584179878235\n",
      "Step 7 (523); Episode 34/100; Loss: 0.1447540670633316\n",
      "Step 8 (524); Episode 34/100; Loss: 0.3058873116970062\n",
      "Step 9 (525); Episode 34/100; Loss: 0.31764331459999084\n",
      "Step 10 (526); Episode 34/100; Loss: 0.33020928502082825\n",
      "Step 0 (527); Episode 35/100; Loss: 0.34725674986839294\n",
      "Step 1 (528); Episode 35/100; Loss: 0.288230836391449\n",
      "Step 2 (529); Episode 35/100; Loss: 0.36359503865242004\n",
      "Step 3 (530); Episode 35/100; Loss: 0.19748322665691376\n",
      "Step 4 (531); Episode 35/100; Loss: 0.2939665913581848\n",
      "Step 5 (532); Episode 35/100; Loss: 0.35249432921409607\n",
      "Step 6 (533); Episode 35/100; Loss: 0.5118994116783142\n",
      "Step 7 (534); Episode 35/100; Loss: 0.1752329021692276\n",
      "Step 8 (535); Episode 35/100; Loss: 0.22455640137195587\n",
      "Step 9 (536); Episode 35/100; Loss: 0.20494294166564941\n",
      "Step 10 (537); Episode 35/100; Loss: 0.019882744178175926\n",
      "Step 11 (538); Episode 35/100; Loss: 0.22413119673728943\n",
      "Step 12 (539); Episode 35/100; Loss: 0.36308154463768005\n",
      "Step 13 (540); Episode 35/100; Loss: 0.2792752981185913\n",
      "Step 14 (541); Episode 35/100; Loss: 0.34511780738830566\n",
      "Step 15 (542); Episode 35/100; Loss: 0.3030703067779541\n",
      "Step 16 (543); Episode 35/100; Loss: 0.10940999537706375\n",
      "Step 17 (544); Episode 35/100; Loss: 0.3183475136756897\n",
      "Step 18 (545); Episode 35/100; Loss: 0.1463238149881363\n",
      "Step 0 (546); Episode 36/100; Loss: 0.14827950298786163\n",
      "Step 1 (547); Episode 36/100; Loss: 0.364124596118927\n",
      "Step 2 (548); Episode 36/100; Loss: 0.09249430894851685\n",
      "Step 3 (549); Episode 36/100; Loss: 0.01807115599513054\n",
      "Step 4 (550); Episode 36/100; Loss: 0.16525687277317047\n",
      "Step 5 (551); Episode 36/100; Loss: 0.06115896627306938\n",
      "Step 6 (552); Episode 36/100; Loss: 0.18940821290016174\n",
      "Step 7 (553); Episode 36/100; Loss: 0.15139912068843842\n",
      "Step 8 (554); Episode 36/100; Loss: 0.2971992492675781\n",
      "Step 9 (555); Episode 36/100; Loss: 0.14086271822452545\n",
      "Step 10 (556); Episode 36/100; Loss: 0.151497021317482\n",
      "Step 11 (557); Episode 36/100; Loss: 0.17858906090259552\n",
      "Step 12 (558); Episode 36/100; Loss: 0.2611788213253021\n",
      "Step 13 (559); Episode 36/100; Loss: 0.21461015939712524\n",
      "Step 14 (560); Episode 36/100; Loss: 0.21875210106372833\n",
      "Step 0 (561); Episode 37/100; Loss: 0.22448034584522247\n",
      "Step 1 (562); Episode 37/100; Loss: 0.39429935812950134\n",
      "Step 2 (563); Episode 37/100; Loss: 0.2649523913860321\n",
      "Step 3 (564); Episode 37/100; Loss: 0.2927258610725403\n",
      "Step 4 (565); Episode 37/100; Loss: 0.14514456689357758\n",
      "Step 5 (566); Episode 37/100; Loss: 0.3852769136428833\n",
      "Step 6 (567); Episode 37/100; Loss: 0.3012084364891052\n",
      "Step 7 (568); Episode 37/100; Loss: 0.0561431460082531\n",
      "Step 8 (569); Episode 37/100; Loss: 0.22959387302398682\n",
      "Step 9 (570); Episode 37/100; Loss: 0.25255322456359863\n",
      "Step 10 (571); Episode 37/100; Loss: 0.11223077774047852\n",
      "Step 11 (572); Episode 37/100; Loss: 0.17059339582920074\n",
      "Step 12 (573); Episode 37/100; Loss: 0.15141616761684418\n",
      "Step 13 (574); Episode 37/100; Loss: 0.13080517947673798\n",
      "Step 14 (575); Episode 37/100; Loss: 0.3737233281135559\n",
      "Step 15 (576); Episode 37/100; Loss: 0.05004015192389488\n",
      "Step 16 (577); Episode 37/100; Loss: 0.3578975200653076\n",
      "Step 17 (578); Episode 37/100; Loss: 0.3043580949306488\n",
      "Step 18 (579); Episode 37/100; Loss: 0.34471985697746277\n",
      "Step 0 (580); Episode 38/100; Loss: 0.348644495010376\n",
      "Step 1 (581); Episode 38/100; Loss: 0.26032888889312744\n",
      "Step 2 (582); Episode 38/100; Loss: 0.11832874268293381\n",
      "Step 3 (583); Episode 38/100; Loss: 0.14879319071769714\n",
      "Step 4 (584); Episode 38/100; Loss: 0.2530175745487213\n",
      "Step 5 (585); Episode 38/100; Loss: 0.2450280338525772\n",
      "Step 6 (586); Episode 38/100; Loss: 0.017483655363321304\n",
      "Step 7 (587); Episode 38/100; Loss: 0.17974479496479034\n",
      "Step 8 (588); Episode 38/100; Loss: 0.13361474871635437\n",
      "Step 9 (589); Episode 38/100; Loss: 0.12880760431289673\n",
      "Step 10 (590); Episode 38/100; Loss: 0.07050231844186783\n",
      "Step 11 (591); Episode 38/100; Loss: 0.06969225406646729\n",
      "Step 12 (592); Episode 38/100; Loss: 0.2142130583524704\n",
      "Step 13 (593); Episode 38/100; Loss: 0.10926100611686707\n",
      "Step 14 (594); Episode 38/100; Loss: 0.31636935472488403\n",
      "Step 15 (595); Episode 38/100; Loss: 0.2125598043203354\n",
      "Step 16 (596); Episode 38/100; Loss: 0.2676562964916229\n",
      "Step 17 (597); Episode 38/100; Loss: 0.30231890082359314\n",
      "Step 18 (598); Episode 38/100; Loss: 0.23205237090587616\n",
      "Step 19 (599); Episode 38/100; Loss: 0.16196797788143158\n",
      "Step 20 (600); Episode 38/100; Loss: 0.1880521923303604\n",
      "Step 21 (601); Episode 38/100; Loss: 0.10950329899787903\n",
      "Step 0 (602); Episode 39/100; Loss: 0.15779653191566467\n",
      "Step 1 (603); Episode 39/100; Loss: 0.21525172889232635\n",
      "Step 2 (604); Episode 39/100; Loss: 0.22815687954425812\n",
      "Step 3 (605); Episode 39/100; Loss: 0.2006102055311203\n",
      "Step 4 (606); Episode 39/100; Loss: 0.17302186787128448\n",
      "Step 5 (607); Episode 39/100; Loss: 0.2132292240858078\n",
      "Step 6 (608); Episode 39/100; Loss: 0.19052737951278687\n",
      "Step 7 (609); Episode 39/100; Loss: 0.32564038038253784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 (610); Episode 39/100; Loss: 0.30402231216430664\n",
      "Step 9 (611); Episode 39/100; Loss: 0.1704620122909546\n",
      "Step 10 (612); Episode 39/100; Loss: 0.32231658697128296\n",
      "Step 11 (613); Episode 39/100; Loss: 0.17997221648693085\n",
      "Step 12 (614); Episode 39/100; Loss: 0.19921806454658508\n",
      "Step 13 (615); Episode 39/100; Loss: 0.3086579144001007\n",
      "Step 14 (616); Episode 39/100; Loss: 0.29730430245399475\n",
      "Step 15 (617); Episode 39/100; Loss: 0.21459127962589264\n",
      "Step 16 (618); Episode 39/100; Loss: 0.24124716222286224\n",
      "Step 17 (619); Episode 39/100; Loss: 0.19676056504249573\n",
      "Step 18 (620); Episode 39/100; Loss: 0.28685110807418823\n",
      "Step 19 (621); Episode 39/100; Loss: 0.25071921944618225\n",
      "Step 20 (622); Episode 39/100; Loss: 0.18197759985923767\n",
      "Step 21 (623); Episode 39/100; Loss: 0.17088213562965393\n",
      "Step 22 (624); Episode 39/100; Loss: 0.07523071765899658\n",
      "Step 0 (625); Episode 40/100; Loss: 0.1255449652671814\n",
      "Step 1 (626); Episode 40/100; Loss: 0.1534089595079422\n",
      "Step 2 (627); Episode 40/100; Loss: 0.3097149729728699\n",
      "Step 3 (628); Episode 40/100; Loss: 0.16136956214904785\n",
      "Step 4 (629); Episode 40/100; Loss: 0.07262929528951645\n",
      "Step 5 (630); Episode 40/100; Loss: 0.24917033314704895\n",
      "Step 6 (631); Episode 40/100; Loss: 0.17054994404315948\n",
      "Step 7 (632); Episode 40/100; Loss: 0.22029954195022583\n",
      "Step 8 (633); Episode 40/100; Loss: 0.14987856149673462\n",
      "Step 9 (634); Episode 40/100; Loss: 0.22804869711399078\n",
      "Step 10 (635); Episode 40/100; Loss: 0.23545220494270325\n",
      "Step 11 (636); Episode 40/100; Loss: 0.3647620677947998\n",
      "Step 12 (637); Episode 40/100; Loss: 0.13008734583854675\n",
      "Step 13 (638); Episode 40/100; Loss: 0.1424199938774109\n",
      "Step 14 (639); Episode 40/100; Loss: 0.19848603010177612\n",
      "Step 15 (640); Episode 40/100; Loss: 0.2111315280199051\n",
      "Step 16 (641); Episode 40/100; Loss: 0.2294742614030838\n",
      "Step 17 (642); Episode 40/100; Loss: 0.24914056062698364\n",
      "Step 18 (643); Episode 40/100; Loss: 0.2790481150150299\n",
      "Step 19 (644); Episode 40/100; Loss: 0.21805565059185028\n",
      "Step 20 (645); Episode 40/100; Loss: 0.12565073370933533\n",
      "Step 21 (646); Episode 40/100; Loss: 0.2863692045211792\n",
      "Step 22 (647); Episode 40/100; Loss: 0.1883143186569214\n",
      "Step 0 (648); Episode 41/100; Loss: 0.2836180627346039\n",
      "Step 1 (649); Episode 41/100; Loss: 0.28287753462791443\n",
      "Step 2 (650); Episode 41/100; Loss: 0.10103628039360046\n",
      "Step 3 (651); Episode 41/100; Loss: 0.21105964481830597\n",
      "Step 4 (652); Episode 41/100; Loss: 0.30452272295951843\n",
      "Step 5 (653); Episode 41/100; Loss: 0.10205148905515671\n",
      "Step 6 (654); Episode 41/100; Loss: 0.017491506412625313\n",
      "Step 7 (655); Episode 41/100; Loss: 0.1902574598789215\n",
      "Step 8 (656); Episode 41/100; Loss: 0.28783944249153137\n",
      "Step 9 (657); Episode 41/100; Loss: 0.07699162513017654\n",
      "Step 10 (658); Episode 41/100; Loss: 0.13272739946842194\n",
      "Step 11 (659); Episode 41/100; Loss: 0.3046134412288666\n",
      "Step 12 (660); Episode 41/100; Loss: 0.06891981512308121\n",
      "Step 13 (661); Episode 41/100; Loss: 0.2700977027416229\n",
      "Step 14 (662); Episode 41/100; Loss: 0.20909154415130615\n",
      "Step 15 (663); Episode 41/100; Loss: 0.3168037533760071\n",
      "Step 16 (664); Episode 41/100; Loss: 0.17076528072357178\n",
      "Step 17 (665); Episode 41/100; Loss: 0.23419052362442017\n",
      "Step 18 (666); Episode 41/100; Loss: 0.16025367379188538\n",
      "Step 19 (667); Episode 41/100; Loss: 0.17896389961242676\n",
      "Step 20 (668); Episode 41/100; Loss: 0.13794322311878204\n",
      "Step 21 (669); Episode 41/100; Loss: 0.40671306848526\n",
      "Step 22 (670); Episode 41/100; Loss: 0.14482203125953674\n",
      "Step 23 (671); Episode 41/100; Loss: 0.20241165161132812\n",
      "Step 24 (672); Episode 41/100; Loss: 0.11075126379728317\n",
      "Step 25 (673); Episode 41/100; Loss: 0.057558316737413406\n",
      "Step 26 (674); Episode 41/100; Loss: 0.31518250703811646\n",
      "Step 27 (675); Episode 41/100; Loss: 0.18545372784137726\n",
      "Step 28 (676); Episode 41/100; Loss: 0.2127339094877243\n",
      "Step 29 (677); Episode 41/100; Loss: 0.22686488926410675\n",
      "Step 30 (678); Episode 41/100; Loss: 0.24283839762210846\n",
      "Step 0 (679); Episode 42/100; Loss: 0.16368986666202545\n",
      "Step 1 (680); Episode 42/100; Loss: 0.19323815405368805\n",
      "Step 2 (681); Episode 42/100; Loss: 0.14879952371120453\n",
      "Step 3 (682); Episode 42/100; Loss: 0.17065885663032532\n",
      "Step 4 (683); Episode 42/100; Loss: 0.0707678347826004\n",
      "Step 5 (684); Episode 42/100; Loss: 0.15407319366931915\n",
      "Step 6 (685); Episode 42/100; Loss: 0.11214936524629593\n",
      "Step 7 (686); Episode 42/100; Loss: 0.16332311928272247\n",
      "Step 8 (687); Episode 42/100; Loss: 0.2788639962673187\n",
      "Step 9 (688); Episode 42/100; Loss: 0.31425437331199646\n",
      "Step 10 (689); Episode 42/100; Loss: 0.11823225766420364\n",
      "Step 11 (690); Episode 42/100; Loss: 0.2582848072052002\n",
      "Step 12 (691); Episode 42/100; Loss: 0.13722261786460876\n",
      "Step 13 (692); Episode 42/100; Loss: 0.2658231556415558\n",
      "Step 14 (693); Episode 42/100; Loss: 0.09447794407606125\n",
      "Step 15 (694); Episode 42/100; Loss: 0.34692198038101196\n",
      "Step 16 (695); Episode 42/100; Loss: 0.25167983770370483\n",
      "Step 17 (696); Episode 42/100; Loss: 0.0940377339720726\n",
      "Step 18 (697); Episode 42/100; Loss: 0.270084947347641\n",
      "Step 19 (698); Episode 42/100; Loss: 0.13399559259414673\n",
      "Step 20 (699); Episode 42/100; Loss: 0.16269731521606445\n",
      "Step 21 (700); Episode 42/100; Loss: 0.31381702423095703\n",
      "Step 22 (701); Episode 42/100; Loss: 0.14133477210998535\n",
      "Step 23 (702); Episode 42/100; Loss: 0.3142228424549103\n",
      "Step 24 (703); Episode 42/100; Loss: 0.11268974840641022\n",
      "Step 25 (704); Episode 42/100; Loss: 0.25138944387435913\n",
      "Step 26 (705); Episode 42/100; Loss: 0.1224495992064476\n",
      "Step 27 (706); Episode 42/100; Loss: 0.12812627851963043\n",
      "Step 28 (707); Episode 42/100; Loss: 0.23471106588840485\n",
      "Step 29 (708); Episode 42/100; Loss: 0.16053804755210876\n",
      "Step 30 (709); Episode 42/100; Loss: 0.25748538970947266\n",
      "Step 31 (710); Episode 42/100; Loss: 0.3469929099082947\n",
      "Step 32 (711); Episode 42/100; Loss: 0.2008286565542221\n",
      "Step 33 (712); Episode 42/100; Loss: 0.18396100401878357\n",
      "Step 34 (713); Episode 42/100; Loss: 0.20842330157756805\n",
      "Step 35 (714); Episode 42/100; Loss: 0.23267623782157898\n",
      "Step 36 (715); Episode 42/100; Loss: 0.07592830061912537\n",
      "Step 37 (716); Episode 42/100; Loss: 0.200456440448761\n",
      "Step 38 (717); Episode 42/100; Loss: 0.12774020433425903\n",
      "Step 39 (718); Episode 42/100; Loss: 0.15880455076694489\n",
      "Step 40 (719); Episode 42/100; Loss: 0.07182331383228302\n",
      "Step 41 (720); Episode 42/100; Loss: 0.10490739345550537\n",
      "Step 42 (721); Episode 42/100; Loss: 0.23732680082321167\n",
      "Step 43 (722); Episode 42/100; Loss: 0.12645480036735535\n",
      "Step 44 (723); Episode 42/100; Loss: 0.16560007631778717\n",
      "Step 45 (724); Episode 42/100; Loss: 0.17474740743637085\n",
      "Step 46 (725); Episode 42/100; Loss: 0.06734349578619003\n",
      "Step 47 (726); Episode 42/100; Loss: 0.16054370999336243\n",
      "Step 48 (727); Episode 42/100; Loss: 0.25102028250694275\n",
      "Step 49 (728); Episode 42/100; Loss: 0.09083317220211029\n",
      "Step 0 (729); Episode 43/100; Loss: 0.09111509472131729\n",
      "Step 1 (730); Episode 43/100; Loss: 0.24198181927204132\n",
      "Step 2 (731); Episode 43/100; Loss: 0.1331995129585266\n",
      "Step 3 (732); Episode 43/100; Loss: 0.17842240631580353\n",
      "Step 4 (733); Episode 43/100; Loss: 0.0986347571015358\n",
      "Step 5 (734); Episode 43/100; Loss: 0.15723487734794617\n",
      "Step 6 (735); Episode 43/100; Loss: 0.17189934849739075\n",
      "Step 7 (736); Episode 43/100; Loss: 0.20484811067581177\n",
      "Step 8 (737); Episode 43/100; Loss: 0.166743665933609\n",
      "Step 9 (738); Episode 43/100; Loss: 0.1958949863910675\n",
      "Step 10 (739); Episode 43/100; Loss: 0.19099240005016327\n",
      "Step 11 (740); Episode 43/100; Loss: 0.31263765692710876\n",
      "Step 12 (741); Episode 43/100; Loss: 0.12939892709255219\n",
      "Step 13 (742); Episode 43/100; Loss: 0.11112389713525772\n",
      "Step 14 (743); Episode 43/100; Loss: 0.26584410667419434\n",
      "Step 15 (744); Episode 43/100; Loss: 0.09990165382623672\n",
      "Step 16 (745); Episode 43/100; Loss: 0.2022923082113266\n",
      "Step 17 (746); Episode 43/100; Loss: 0.10989214479923248\n",
      "Step 18 (747); Episode 43/100; Loss: 0.06878223270177841\n",
      "Step 19 (748); Episode 43/100; Loss: 0.05819421634078026\n",
      "Step 20 (749); Episode 43/100; Loss: 0.08892423659563065\n",
      "Step 21 (750); Episode 43/100; Loss: 0.21768727898597717\n",
      "Step 22 (751); Episode 43/100; Loss: 0.24945847690105438\n",
      "Step 23 (752); Episode 43/100; Loss: 0.30627304315567017\n",
      "Step 24 (753); Episode 43/100; Loss: 0.24830743670463562\n",
      "Step 25 (754); Episode 43/100; Loss: 0.2220311015844345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26 (755); Episode 43/100; Loss: 0.21856118738651276\n",
      "Step 27 (756); Episode 43/100; Loss: 0.17664262652397156\n",
      "Step 28 (757); Episode 43/100; Loss: 0.16281193494796753\n",
      "Step 29 (758); Episode 43/100; Loss: 0.12779200077056885\n",
      "Step 30 (759); Episode 43/100; Loss: 0.09217176586389542\n",
      "Step 31 (760); Episode 43/100; Loss: 0.06469357013702393\n",
      "Step 32 (761); Episode 43/100; Loss: 0.35368722677230835\n",
      "Step 33 (762); Episode 43/100; Loss: 0.266706645488739\n",
      "Step 34 (763); Episode 43/100; Loss: 0.27225178480148315\n",
      "Step 35 (764); Episode 43/100; Loss: 0.3314623534679413\n",
      "Step 36 (765); Episode 43/100; Loss: 0.20980651676654816\n",
      "Step 37 (766); Episode 43/100; Loss: 0.21686896681785583\n",
      "Step 38 (767); Episode 43/100; Loss: 0.1032319888472557\n",
      "Step 39 (768); Episode 43/100; Loss: 0.010164420120418072\n",
      "Step 40 (769); Episode 43/100; Loss: 0.1427936851978302\n",
      "Step 41 (770); Episode 43/100; Loss: 0.3498094081878662\n",
      "Step 42 (771); Episode 43/100; Loss: 0.1861080378293991\n",
      "Step 43 (772); Episode 43/100; Loss: 0.159097358584404\n",
      "Step 44 (773); Episode 43/100; Loss: 0.10577873140573502\n",
      "Step 45 (774); Episode 43/100; Loss: 0.32899805903434753\n",
      "Step 46 (775); Episode 43/100; Loss: 0.04873541370034218\n",
      "Step 47 (776); Episode 43/100; Loss: 0.09955160319805145\n",
      "Step 48 (777); Episode 43/100; Loss: 0.04425952583551407\n",
      "Step 49 (778); Episode 43/100; Loss: 0.18402199447155\n",
      "Step 50 (779); Episode 43/100; Loss: 0.14910414814949036\n",
      "Step 51 (780); Episode 43/100; Loss: 0.20197440683841705\n",
      "Step 52 (781); Episode 43/100; Loss: 0.15805014967918396\n",
      "Step 53 (782); Episode 43/100; Loss: 0.1405293196439743\n",
      "Step 54 (783); Episode 43/100; Loss: 0.06152874976396561\n",
      "Step 55 (784); Episode 43/100; Loss: 0.08981698751449585\n",
      "Step 0 (785); Episode 44/100; Loss: 0.10262339562177658\n",
      "Step 1 (786); Episode 44/100; Loss: 0.20057901740074158\n",
      "Step 2 (787); Episode 44/100; Loss: 0.07273414731025696\n",
      "Step 3 (788); Episode 44/100; Loss: 0.23277562856674194\n",
      "Step 4 (789); Episode 44/100; Loss: 0.20895974338054657\n",
      "Step 5 (790); Episode 44/100; Loss: 0.2061559557914734\n",
      "Step 6 (791); Episode 44/100; Loss: 0.1179729700088501\n",
      "Step 7 (792); Episode 44/100; Loss: 0.2542559504508972\n",
      "Step 8 (793); Episode 44/100; Loss: 0.20554494857788086\n",
      "Step 9 (794); Episode 44/100; Loss: 0.2372787594795227\n",
      "Step 10 (795); Episode 44/100; Loss: 0.09897790849208832\n",
      "Step 11 (796); Episode 44/100; Loss: 0.060434237122535706\n",
      "Step 12 (797); Episode 44/100; Loss: 0.17203718423843384\n",
      "Step 13 (798); Episode 44/100; Loss: 0.2124968022108078\n",
      "Step 14 (799); Episode 44/100; Loss: 0.2805711328983307\n",
      "Step 15 (800); Episode 44/100; Loss: 0.09807756543159485\n",
      "Step 16 (801); Episode 44/100; Loss: 0.27781474590301514\n",
      "Step 17 (802); Episode 44/100; Loss: 0.15333257615566254\n",
      "Step 18 (803); Episode 44/100; Loss: 0.32129862904548645\n",
      "Step 19 (804); Episode 44/100; Loss: 0.06060256436467171\n",
      "Step 20 (805); Episode 44/100; Loss: 0.15838269889354706\n",
      "Step 21 (806); Episode 44/100; Loss: 0.20550121366977692\n",
      "Step 22 (807); Episode 44/100; Loss: 0.21143025159835815\n",
      "Step 23 (808); Episode 44/100; Loss: 0.11672473698854446\n",
      "Step 24 (809); Episode 44/100; Loss: 0.1955537647008896\n",
      "Step 25 (810); Episode 44/100; Loss: 0.10164809972047806\n",
      "Step 26 (811); Episode 44/100; Loss: 0.15239165723323822\n",
      "Step 27 (812); Episode 44/100; Loss: 0.1510723978281021\n",
      "Step 28 (813); Episode 44/100; Loss: 0.1168227270245552\n",
      "Step 29 (814); Episode 44/100; Loss: 0.2736755907535553\n",
      "Step 30 (815); Episode 44/100; Loss: 0.2613540589809418\n",
      "Step 31 (816); Episode 44/100; Loss: 0.3323329985141754\n",
      "Step 32 (817); Episode 44/100; Loss: 0.23850469291210175\n",
      "Step 33 (818); Episode 44/100; Loss: 0.1131099984049797\n",
      "Step 34 (819); Episode 44/100; Loss: 0.11391079425811768\n",
      "Step 35 (820); Episode 44/100; Loss: 0.11302214115858078\n",
      "Step 36 (821); Episode 44/100; Loss: 0.21415592730045319\n",
      "Step 37 (822); Episode 44/100; Loss: 0.24051980674266815\n",
      "Step 38 (823); Episode 44/100; Loss: 0.010053915902972221\n",
      "Step 39 (824); Episode 44/100; Loss: 0.23976734280586243\n",
      "Step 40 (825); Episode 44/100; Loss: 0.09559851139783859\n",
      "Step 41 (826); Episode 44/100; Loss: 0.27177420258522034\n",
      "Step 42 (827); Episode 44/100; Loss: 0.23544037342071533\n",
      "Step 43 (828); Episode 44/100; Loss: 0.14203207194805145\n",
      "Step 44 (829); Episode 44/100; Loss: 0.13718187808990479\n",
      "Step 45 (830); Episode 44/100; Loss: 0.15178580582141876\n",
      "Step 46 (831); Episode 44/100; Loss: 0.17981204390525818\n",
      "Step 47 (832); Episode 44/100; Loss: 0.25185856223106384\n",
      "Step 48 (833); Episode 44/100; Loss: 0.05630415305495262\n",
      "Step 0 (834); Episode 45/100; Loss: 0.14115120470523834\n",
      "Step 1 (835); Episode 45/100; Loss: 0.007038446143269539\n",
      "Step 2 (836); Episode 45/100; Loss: 0.216597318649292\n",
      "Step 3 (837); Episode 45/100; Loss: 0.15003080666065216\n",
      "Step 4 (838); Episode 45/100; Loss: 0.15627188980579376\n",
      "Step 5 (839); Episode 45/100; Loss: 0.19080229103565216\n",
      "Step 6 (840); Episode 45/100; Loss: 0.057558611035346985\n",
      "Step 7 (841); Episode 45/100; Loss: 0.1366349756717682\n",
      "Step 8 (842); Episode 45/100; Loss: 0.29191455245018005\n",
      "Step 9 (843); Episode 45/100; Loss: 0.15425372123718262\n",
      "Step 10 (844); Episode 45/100; Loss: 0.042066071182489395\n",
      "Step 11 (845); Episode 45/100; Loss: 0.1158638596534729\n",
      "Step 12 (846); Episode 45/100; Loss: 0.18542379140853882\n",
      "Step 13 (847); Episode 45/100; Loss: 0.24978125095367432\n",
      "Step 14 (848); Episode 45/100; Loss: 0.1772952377796173\n",
      "Step 15 (849); Episode 45/100; Loss: 0.09311646223068237\n",
      "Step 16 (850); Episode 45/100; Loss: 0.26783764362335205\n",
      "Step 17 (851); Episode 45/100; Loss: 0.11976355314254761\n",
      "Step 18 (852); Episode 45/100; Loss: 0.14455172419548035\n",
      "Step 19 (853); Episode 45/100; Loss: 0.09134142845869064\n",
      "Step 20 (854); Episode 45/100; Loss: 0.11899037659168243\n",
      "Step 21 (855); Episode 45/100; Loss: 0.1061432734131813\n",
      "Step 22 (856); Episode 45/100; Loss: 0.07770692557096481\n",
      "Step 23 (857); Episode 45/100; Loss: 0.13747991621494293\n",
      "Step 24 (858); Episode 45/100; Loss: 0.06097938120365143\n",
      "Step 25 (859); Episode 45/100; Loss: 0.17834658920764923\n",
      "Step 26 (860); Episode 45/100; Loss: 0.1989285945892334\n",
      "Step 27 (861); Episode 45/100; Loss: 0.28303736448287964\n",
      "Step 28 (862); Episode 45/100; Loss: 0.16897529363632202\n",
      "Step 29 (863); Episode 45/100; Loss: 0.11631312966346741\n",
      "Step 30 (864); Episode 45/100; Loss: 0.2888098657131195\n",
      "Step 31 (865); Episode 45/100; Loss: 0.14982859790325165\n",
      "Step 32 (866); Episode 45/100; Loss: 0.17141880095005035\n",
      "Step 33 (867); Episode 45/100; Loss: 0.19185477495193481\n",
      "Step 34 (868); Episode 45/100; Loss: 0.18155142664909363\n",
      "Step 35 (869); Episode 45/100; Loss: 0.14743177592754364\n",
      "Step 36 (870); Episode 45/100; Loss: 0.13259382545948029\n",
      "Step 37 (871); Episode 45/100; Loss: 0.15344294905662537\n",
      "Step 38 (872); Episode 45/100; Loss: 0.11200214922428131\n",
      "Step 39 (873); Episode 45/100; Loss: 0.03444969281554222\n",
      "Step 40 (874); Episode 45/100; Loss: 0.05814899876713753\n",
      "Step 41 (875); Episode 45/100; Loss: 0.1504666805267334\n",
      "Step 42 (876); Episode 45/100; Loss: 0.19604548811912537\n",
      "Step 43 (877); Episode 45/100; Loss: 0.05278882011771202\n",
      "Step 44 (878); Episode 45/100; Loss: 0.10072949528694153\n",
      "Step 45 (879); Episode 45/100; Loss: 0.3338228166103363\n",
      "Step 46 (880); Episode 45/100; Loss: 0.05724378302693367\n",
      "Step 47 (881); Episode 45/100; Loss: 0.034634433686733246\n",
      "Step 48 (882); Episode 45/100; Loss: 0.10723167657852173\n",
      "Step 49 (883); Episode 45/100; Loss: 0.10798229277133942\n",
      "Step 50 (884); Episode 45/100; Loss: 0.00948063563555479\n",
      "Step 51 (885); Episode 45/100; Loss: 0.2246839851140976\n",
      "Step 52 (886); Episode 45/100; Loss: 0.32212430238723755\n",
      "Step 53 (887); Episode 45/100; Loss: 0.007603906560689211\n",
      "Step 54 (888); Episode 45/100; Loss: 0.1593814492225647\n",
      "Step 55 (889); Episode 45/100; Loss: 0.13707047700881958\n",
      "Step 56 (890); Episode 45/100; Loss: 0.10984625667333603\n",
      "Step 57 (891); Episode 45/100; Loss: 0.08705010265111923\n",
      "Step 58 (892); Episode 45/100; Loss: 0.1990669220685959\n",
      "Step 59 (893); Episode 45/100; Loss: 0.0693904235959053\n",
      "Step 60 (894); Episode 45/100; Loss: 0.10477910190820694\n",
      "Step 61 (895); Episode 45/100; Loss: 0.21826519072055817\n",
      "Step 62 (896); Episode 45/100; Loss: 0.006270379293709993\n",
      "Step 63 (897); Episode 45/100; Loss: 0.0058750007301568985\n",
      "Step 64 (898); Episode 45/100; Loss: 0.21650594472885132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 65 (899); Episode 45/100; Loss: 0.315833181142807\n",
      "Step 66 (900); Episode 45/100; Loss: 0.11532393842935562\n",
      "Step 67 (901); Episode 45/100; Loss: 0.19809286296367645\n",
      "Step 68 (902); Episode 45/100; Loss: 0.06045602634549141\n",
      "Step 69 (903); Episode 45/100; Loss: 0.04406271129846573\n",
      "Step 70 (904); Episode 45/100; Loss: 0.25211766362190247\n",
      "Step 71 (905); Episode 45/100; Loss: 0.11451482772827148\n",
      "Step 72 (906); Episode 45/100; Loss: 0.21945466101169586\n",
      "Step 0 (907); Episode 46/100; Loss: 0.29902470111846924\n",
      "Step 1 (908); Episode 46/100; Loss: 0.11493662744760513\n",
      "Step 2 (909); Episode 46/100; Loss: 0.24232947826385498\n",
      "Step 3 (910); Episode 46/100; Loss: 0.22202597558498383\n",
      "Step 4 (911); Episode 46/100; Loss: 0.2625243067741394\n",
      "Step 5 (912); Episode 46/100; Loss: 0.15263430774211884\n",
      "Step 6 (913); Episode 46/100; Loss: 0.2049097716808319\n",
      "Step 7 (914); Episode 46/100; Loss: 0.16425198316574097\n",
      "Step 8 (915); Episode 46/100; Loss: 0.14500227570533752\n",
      "Step 9 (916); Episode 46/100; Loss: 0.10908699780702591\n",
      "Step 10 (917); Episode 46/100; Loss: 0.10638877749443054\n",
      "Step 11 (918); Episode 46/100; Loss: 0.11277392506599426\n",
      "Step 12 (919); Episode 46/100; Loss: 0.1064210757613182\n",
      "Step 13 (920); Episode 46/100; Loss: 0.01182499434798956\n",
      "Step 14 (921); Episode 46/100; Loss: 0.10824622958898544\n",
      "Step 15 (922); Episode 46/100; Loss: 0.08732127398252487\n",
      "Step 16 (923); Episode 46/100; Loss: 0.21670223772525787\n",
      "Step 17 (924); Episode 46/100; Loss: 0.25097987055778503\n",
      "Step 18 (925); Episode 46/100; Loss: 0.1533229947090149\n",
      "Step 19 (926); Episode 46/100; Loss: 0.0681178942322731\n",
      "Step 20 (927); Episode 46/100; Loss: 0.19287626445293427\n",
      "Step 21 (928); Episode 46/100; Loss: 0.05571960285305977\n",
      "Step 22 (929); Episode 46/100; Loss: 0.09335774928331375\n",
      "Step 23 (930); Episode 46/100; Loss: 0.014184778556227684\n",
      "Step 24 (931); Episode 46/100; Loss: 0.18969281017780304\n",
      "Step 25 (932); Episode 46/100; Loss: 0.08242161571979523\n",
      "Step 26 (933); Episode 46/100; Loss: 0.21859338879585266\n",
      "Step 27 (934); Episode 46/100; Loss: 0.2531217038631439\n",
      "Step 28 (935); Episode 46/100; Loss: 0.09341611713171005\n",
      "Step 29 (936); Episode 46/100; Loss: 0.13709862530231476\n",
      "Step 30 (937); Episode 46/100; Loss: 0.10035537928342819\n",
      "Step 31 (938); Episode 46/100; Loss: 0.2226017415523529\n",
      "Step 32 (939); Episode 46/100; Loss: 0.14176559448242188\n",
      "Step 33 (940); Episode 46/100; Loss: 0.149922713637352\n",
      "Step 34 (941); Episode 46/100; Loss: 0.23003508150577545\n",
      "Step 35 (942); Episode 46/100; Loss: 0.13406826555728912\n",
      "Step 36 (943); Episode 46/100; Loss: 0.24385201930999756\n",
      "Step 37 (944); Episode 46/100; Loss: 0.13517016172409058\n",
      "Step 38 (945); Episode 46/100; Loss: 0.14826981723308563\n",
      "Step 39 (946); Episode 46/100; Loss: 0.1876627802848816\n",
      "Step 40 (947); Episode 46/100; Loss: 0.10355830937623978\n",
      "Step 41 (948); Episode 46/100; Loss: 0.13830305635929108\n",
      "Step 42 (949); Episode 46/100; Loss: 0.15405617654323578\n",
      "Step 43 (950); Episode 46/100; Loss: 0.188132643699646\n",
      "Step 44 (951); Episode 46/100; Loss: 0.14922919869422913\n",
      "Step 45 (952); Episode 46/100; Loss: 0.09205317497253418\n",
      "Step 46 (953); Episode 46/100; Loss: 0.10646384209394455\n",
      "Step 47 (954); Episode 46/100; Loss: 0.12686510384082794\n",
      "Step 48 (955); Episode 46/100; Loss: 0.1778704971075058\n",
      "Step 49 (956); Episode 46/100; Loss: 0.17985175549983978\n",
      "Step 50 (957); Episode 46/100; Loss: 0.18263326585292816\n",
      "Step 51 (958); Episode 46/100; Loss: 0.04672897979617119\n",
      "Step 52 (959); Episode 46/100; Loss: 0.1487259566783905\n",
      "Step 53 (960); Episode 46/100; Loss: 0.12492828071117401\n",
      "Step 54 (961); Episode 46/100; Loss: 0.18949592113494873\n",
      "Step 55 (962); Episode 46/100; Loss: 0.308631032705307\n",
      "Step 56 (963); Episode 46/100; Loss: 0.1442319005727768\n",
      "Step 57 (964); Episode 46/100; Loss: 0.1768862009048462\n",
      "Step 58 (965); Episode 46/100; Loss: 0.0988209918141365\n",
      "Step 59 (966); Episode 46/100; Loss: 0.09373284876346588\n",
      "Step 60 (967); Episode 46/100; Loss: 0.07724886387586594\n",
      "Step 61 (968); Episode 46/100; Loss: 0.03835997357964516\n",
      "Step 62 (969); Episode 46/100; Loss: 0.143289715051651\n",
      "Step 63 (970); Episode 46/100; Loss: 0.10850279033184052\n",
      "Step 64 (971); Episode 46/100; Loss: 0.15483041107654572\n",
      "Step 0 (972); Episode 47/100; Loss: 0.23617318272590637\n",
      "Step 1 (973); Episode 47/100; Loss: 0.007645796984434128\n",
      "Step 2 (974); Episode 47/100; Loss: 0.17887623608112335\n",
      "Step 3 (975); Episode 47/100; Loss: 0.0966716930270195\n",
      "Step 4 (976); Episode 47/100; Loss: 0.11013465374708176\n",
      "Step 5 (977); Episode 47/100; Loss: 0.09434175491333008\n",
      "Step 6 (978); Episode 47/100; Loss: 0.29308784008026123\n",
      "Step 7 (979); Episode 47/100; Loss: 0.053904201835393906\n",
      "Step 8 (980); Episode 47/100; Loss: 0.14320071041584015\n",
      "Step 9 (981); Episode 47/100; Loss: 0.1921088695526123\n",
      "Step 10 (982); Episode 47/100; Loss: 0.14054405689239502\n",
      "Step 11 (983); Episode 47/100; Loss: 0.13607093691825867\n",
      "Step 12 (984); Episode 47/100; Loss: 0.20464032888412476\n",
      "Step 13 (985); Episode 47/100; Loss: 0.04687076434493065\n",
      "Step 14 (986); Episode 47/100; Loss: 0.08880622684955597\n",
      "Step 15 (987); Episode 47/100; Loss: 0.13627618551254272\n",
      "Step 16 (988); Episode 47/100; Loss: 0.2057962268590927\n",
      "Step 17 (989); Episode 47/100; Loss: 0.2881208658218384\n",
      "Step 18 (990); Episode 47/100; Loss: 0.1834060549736023\n",
      "Step 19 (991); Episode 47/100; Loss: 0.09364832192659378\n",
      "Step 20 (992); Episode 47/100; Loss: 0.21248558163642883\n",
      "Step 21 (993); Episode 47/100; Loss: 0.14729097485542297\n",
      "Step 22 (994); Episode 47/100; Loss: 0.042304929345846176\n",
      "Step 23 (995); Episode 47/100; Loss: 0.08223490417003632\n",
      "Step 24 (996); Episode 47/100; Loss: 0.17689082026481628\n",
      "Step 25 (997); Episode 47/100; Loss: 0.14217227697372437\n",
      "Step 26 (998); Episode 47/100; Loss: 0.27746933698654175\n",
      "Step 27 (999); Episode 47/100; Loss: 0.22884203493595123\n",
      "Step 28 (1000); Episode 47/100; Loss: 0.2780894637107849\n",
      "Step 29 (1001); Episode 47/100; Loss: 0.1401256024837494\n",
      "Step 30 (1002); Episode 47/100; Loss: 0.07238594442605972\n",
      "Step 31 (1003); Episode 47/100; Loss: 0.1901853233575821\n",
      "Step 32 (1004); Episode 47/100; Loss: 0.017609553411602974\n",
      "Step 33 (1005); Episode 47/100; Loss: 0.19624146819114685\n",
      "Step 34 (1006); Episode 47/100; Loss: 0.24360424280166626\n",
      "Step 35 (1007); Episode 47/100; Loss: 0.21276520192623138\n",
      "Step 36 (1008); Episode 47/100; Loss: 0.22153930366039276\n",
      "Step 37 (1009); Episode 47/100; Loss: 0.23591794073581696\n",
      "Step 38 (1010); Episode 47/100; Loss: 0.17167705297470093\n",
      "Step 39 (1011); Episode 47/100; Loss: 0.18380247056484222\n",
      "Step 40 (1012); Episode 47/100; Loss: 0.10086337476968765\n",
      "Step 41 (1013); Episode 47/100; Loss: 0.1510346382856369\n",
      "Step 42 (1014); Episode 47/100; Loss: 0.053510650992393494\n",
      "Step 43 (1015); Episode 47/100; Loss: 0.07798394560813904\n",
      "Step 44 (1016); Episode 47/100; Loss: 0.0461752712726593\n",
      "Step 45 (1017); Episode 47/100; Loss: 0.10765406489372253\n",
      "Step 46 (1018); Episode 47/100; Loss: 0.32484373450279236\n",
      "Step 47 (1019); Episode 47/100; Loss: 0.008778565563261509\n",
      "Step 48 (1020); Episode 47/100; Loss: 0.18665096163749695\n",
      "Step 49 (1021); Episode 47/100; Loss: 0.05430212244391441\n",
      "Step 50 (1022); Episode 47/100; Loss: 0.09291961044073105\n",
      "Step 51 (1023); Episode 47/100; Loss: 0.04825873300433159\n",
      "Step 52 (1024); Episode 47/100; Loss: 0.10449819266796112\n",
      "Step 0 (1025); Episode 48/100; Loss: 0.057734403759241104\n",
      "Step 1 (1026); Episode 48/100; Loss: 0.1573466956615448\n",
      "Step 2 (1027); Episode 48/100; Loss: 0.13690556585788727\n",
      "Step 3 (1028); Episode 48/100; Loss: 0.0905030369758606\n",
      "Step 4 (1029); Episode 48/100; Loss: 0.007860871031880379\n",
      "Step 5 (1030); Episode 48/100; Loss: 0.13389469683170319\n",
      "Step 6 (1031); Episode 48/100; Loss: 0.1489430069923401\n",
      "Step 7 (1032); Episode 48/100; Loss: 0.17955657839775085\n",
      "Step 8 (1033); Episode 48/100; Loss: 0.1519560068845749\n",
      "Step 9 (1034); Episode 48/100; Loss: 0.13440509140491486\n",
      "Step 10 (1035); Episode 48/100; Loss: 0.136554554104805\n",
      "Step 11 (1036); Episode 48/100; Loss: 0.08597814291715622\n",
      "Step 12 (1037); Episode 48/100; Loss: 0.05387889966368675\n",
      "Step 13 (1038); Episode 48/100; Loss: 0.056200604885816574\n",
      "Step 14 (1039); Episode 48/100; Loss: 0.2228647619485855\n",
      "Step 15 (1040); Episode 48/100; Loss: 0.2360159158706665\n",
      "Step 16 (1041); Episode 48/100; Loss: 0.20465466380119324\n",
      "Step 17 (1042); Episode 48/100; Loss: 0.08629158139228821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18 (1043); Episode 48/100; Loss: 0.06885804980993271\n",
      "Step 19 (1044); Episode 48/100; Loss: 0.19752776622772217\n",
      "Step 20 (1045); Episode 48/100; Loss: 0.2149139791727066\n",
      "Step 21 (1046); Episode 48/100; Loss: 0.1497107446193695\n",
      "Step 22 (1047); Episode 48/100; Loss: 0.09612424671649933\n",
      "Step 23 (1048); Episode 48/100; Loss: 0.10278961807489395\n",
      "Step 24 (1049); Episode 48/100; Loss: 0.13926105201244354\n",
      "Step 25 (1050); Episode 48/100; Loss: 0.21217316389083862\n",
      "Step 26 (1051); Episode 48/100; Loss: 0.13618303835391998\n",
      "Step 27 (1052); Episode 48/100; Loss: 0.13905629515647888\n",
      "Step 28 (1053); Episode 48/100; Loss: 0.09669405221939087\n",
      "Step 29 (1054); Episode 48/100; Loss: 0.1546197235584259\n",
      "Step 30 (1055); Episode 48/100; Loss: 0.14177827537059784\n",
      "Step 31 (1056); Episode 48/100; Loss: 0.05214592069387436\n",
      "Step 32 (1057); Episode 48/100; Loss: 0.15150144696235657\n",
      "Step 33 (1058); Episode 48/100; Loss: 0.10099756717681885\n",
      "Step 34 (1059); Episode 48/100; Loss: 0.3430832624435425\n",
      "Step 35 (1060); Episode 48/100; Loss: 0.15683193504810333\n",
      "Step 36 (1061); Episode 48/100; Loss: 0.31688597798347473\n",
      "Step 37 (1062); Episode 48/100; Loss: 0.08317127078771591\n",
      "Step 38 (1063); Episode 48/100; Loss: 0.24193152785301208\n",
      "Step 39 (1064); Episode 48/100; Loss: 0.08437714725732803\n",
      "Step 40 (1065); Episode 48/100; Loss: 0.04746844246983528\n",
      "Step 41 (1066); Episode 48/100; Loss: 0.10339009761810303\n",
      "Step 42 (1067); Episode 48/100; Loss: 0.05184491351246834\n",
      "Step 43 (1068); Episode 48/100; Loss: 0.03537767753005028\n",
      "Step 44 (1069); Episode 48/100; Loss: 0.08865112066268921\n",
      "Step 45 (1070); Episode 48/100; Loss: 0.21429835259914398\n",
      "Step 46 (1071); Episode 48/100; Loss: 0.10620773583650589\n",
      "Step 47 (1072); Episode 48/100; Loss: 0.14652162790298462\n",
      "Step 48 (1073); Episode 48/100; Loss: 0.15337535738945007\n",
      "Step 49 (1074); Episode 48/100; Loss: 0.10173463076353073\n",
      "Step 50 (1075); Episode 48/100; Loss: 0.042799461632966995\n",
      "Step 51 (1076); Episode 48/100; Loss: 0.10423488169908524\n",
      "Step 52 (1077); Episode 48/100; Loss: 0.13774599134922028\n",
      "Step 53 (1078); Episode 48/100; Loss: 0.10824638605117798\n",
      "Step 54 (1079); Episode 48/100; Loss: 0.18968136608600616\n",
      "Step 55 (1080); Episode 48/100; Loss: 0.004145283717662096\n",
      "Step 56 (1081); Episode 48/100; Loss: 0.13106603920459747\n",
      "Step 57 (1082); Episode 48/100; Loss: 0.25963035225868225\n",
      "Step 58 (1083); Episode 48/100; Loss: 0.05526197701692581\n",
      "Step 59 (1084); Episode 48/100; Loss: 0.1589619666337967\n",
      "Step 60 (1085); Episode 48/100; Loss: 0.23546190559864044\n",
      "Step 61 (1086); Episode 48/100; Loss: 0.20789580047130585\n",
      "Step 62 (1087); Episode 48/100; Loss: 0.1162288635969162\n",
      "Step 0 (1088); Episode 49/100; Loss: 0.0763586163520813\n",
      "Step 1 (1089); Episode 49/100; Loss: 0.04340919107198715\n",
      "Step 2 (1090); Episode 49/100; Loss: 0.048578232526779175\n",
      "Step 3 (1091); Episode 49/100; Loss: 0.15214398503303528\n",
      "Step 4 (1092); Episode 49/100; Loss: 0.09589298814535141\n",
      "Step 5 (1093); Episode 49/100; Loss: 0.250550776720047\n",
      "Step 6 (1094); Episode 49/100; Loss: 0.22203469276428223\n",
      "Step 7 (1095); Episode 49/100; Loss: 0.10503257066011429\n",
      "Step 8 (1096); Episode 49/100; Loss: 0.15110579133033752\n",
      "Step 9 (1097); Episode 49/100; Loss: 0.15481075644493103\n",
      "Step 10 (1098); Episode 49/100; Loss: 0.1726115345954895\n",
      "Step 11 (1099); Episode 49/100; Loss: 0.10541848838329315\n",
      "Step 12 (1100); Episode 49/100; Loss: 0.09867994487285614\n",
      "Step 13 (1101); Episode 49/100; Loss: 0.11405527591705322\n",
      "Step 14 (1102); Episode 49/100; Loss: 0.0937126949429512\n",
      "Step 15 (1103); Episode 49/100; Loss: 0.14645957946777344\n",
      "Step 16 (1104); Episode 49/100; Loss: 0.1734621226787567\n",
      "Step 17 (1105); Episode 49/100; Loss: 0.07965677976608276\n",
      "Step 18 (1106); Episode 49/100; Loss: 0.009101062081754208\n",
      "Step 19 (1107); Episode 49/100; Loss: 0.06906267255544662\n",
      "Step 20 (1108); Episode 49/100; Loss: 0.04245941340923309\n",
      "Step 21 (1109); Episode 49/100; Loss: 0.11372867226600647\n",
      "Step 22 (1110); Episode 49/100; Loss: 0.14241886138916016\n",
      "Step 23 (1111); Episode 49/100; Loss: 0.09858494997024536\n",
      "Step 24 (1112); Episode 49/100; Loss: 0.20629392564296722\n",
      "Step 25 (1113); Episode 49/100; Loss: 0.09238855540752411\n",
      "Step 26 (1114); Episode 49/100; Loss: 0.18689081072807312\n",
      "Step 27 (1115); Episode 49/100; Loss: 0.004545786418020725\n",
      "Step 28 (1116); Episode 49/100; Loss: 0.05279690399765968\n",
      "Step 29 (1117); Episode 49/100; Loss: 0.21544185280799866\n",
      "Step 30 (1118); Episode 49/100; Loss: 0.05450130254030228\n",
      "Step 31 (1119); Episode 49/100; Loss: 0.13898272812366486\n",
      "Step 32 (1120); Episode 49/100; Loss: 0.1323310285806656\n",
      "Step 33 (1121); Episode 49/100; Loss: 0.09729968011379242\n",
      "Step 34 (1122); Episode 49/100; Loss: 0.26750677824020386\n",
      "Step 35 (1123); Episode 49/100; Loss: 0.09716545045375824\n",
      "Step 36 (1124); Episode 49/100; Loss: 0.2649959921836853\n",
      "Step 37 (1125); Episode 49/100; Loss: 0.1384112536907196\n",
      "Step 0 (1126); Episode 50/100; Loss: 0.07869599014520645\n",
      "Step 1 (1127); Episode 50/100; Loss: 0.21710044145584106\n",
      "Step 2 (1128); Episode 50/100; Loss: 0.08795388042926788\n",
      "Step 3 (1129); Episode 50/100; Loss: 0.22841037809848785\n",
      "Step 4 (1130); Episode 50/100; Loss: 0.09343565255403519\n",
      "Step 5 (1131); Episode 50/100; Loss: 0.18145938217639923\n",
      "Step 6 (1132); Episode 50/100; Loss: 0.19360607862472534\n",
      "Step 7 (1133); Episode 50/100; Loss: 0.1434747874736786\n",
      "Step 8 (1134); Episode 50/100; Loss: 0.09439202398061752\n",
      "Step 9 (1135); Episode 50/100; Loss: 0.12007496505975723\n",
      "Step 10 (1136); Episode 50/100; Loss: 0.1478496491909027\n",
      "Step 11 (1137); Episode 50/100; Loss: 0.004163680598139763\n",
      "Step 12 (1138); Episode 50/100; Loss: 0.033972300589084625\n",
      "Step 13 (1139); Episode 50/100; Loss: 0.0850122720003128\n",
      "Step 14 (1140); Episode 50/100; Loss: 0.22564290463924408\n",
      "Step 15 (1141); Episode 50/100; Loss: 0.19233664870262146\n",
      "Step 16 (1142); Episode 50/100; Loss: 0.09260934591293335\n",
      "Step 17 (1143); Episode 50/100; Loss: 0.15658380091190338\n",
      "Step 18 (1144); Episode 50/100; Loss: 0.1269511878490448\n",
      "Step 19 (1145); Episode 50/100; Loss: 0.09348437190055847\n",
      "Step 20 (1146); Episode 50/100; Loss: 0.0511876717209816\n",
      "Step 21 (1147); Episode 50/100; Loss: 0.21931491792201996\n",
      "Step 22 (1148); Episode 50/100; Loss: 0.0596003495156765\n",
      "Step 23 (1149); Episode 50/100; Loss: 0.014743365347385406\n",
      "Step 24 (1150); Episode 50/100; Loss: 0.14129404723644257\n",
      "Step 25 (1151); Episode 50/100; Loss: 0.12344291061162949\n",
      "Step 26 (1152); Episode 50/100; Loss: 0.13857994973659515\n",
      "Step 27 (1153); Episode 50/100; Loss: 0.07463294267654419\n",
      "Step 28 (1154); Episode 50/100; Loss: 0.12077274918556213\n",
      "Step 29 (1155); Episode 50/100; Loss: 0.10144287347793579\n",
      "Step 30 (1156); Episode 50/100; Loss: 0.2254982888698578\n",
      "Step 31 (1157); Episode 50/100; Loss: 0.13786937296390533\n",
      "Step 32 (1158); Episode 50/100; Loss: 0.1508161872625351\n",
      "Step 33 (1159); Episode 50/100; Loss: 0.1925998330116272\n",
      "Step 34 (1160); Episode 50/100; Loss: 0.05947130545973778\n",
      "Step 35 (1161); Episode 50/100; Loss: 0.1871175467967987\n",
      "Step 36 (1162); Episode 50/100; Loss: 0.03657050058245659\n",
      "Step 37 (1163); Episode 50/100; Loss: 0.10112014412879944\n",
      "Step 38 (1164); Episode 50/100; Loss: 0.05477868393063545\n",
      "Step 39 (1165); Episode 50/100; Loss: 0.1803353726863861\n",
      "Step 40 (1166); Episode 50/100; Loss: 0.1296834647655487\n",
      "Step 41 (1167); Episode 50/100; Loss: 0.2141285091638565\n",
      "Step 42 (1168); Episode 50/100; Loss: 0.15385061502456665\n",
      "Step 43 (1169); Episode 50/100; Loss: 0.06345595419406891\n",
      "Step 44 (1170); Episode 50/100; Loss: 0.1255960762500763\n",
      "Step 45 (1171); Episode 50/100; Loss: 0.14111420512199402\n",
      "Step 46 (1172); Episode 50/100; Loss: 0.10018990933895111\n",
      "Step 47 (1173); Episode 50/100; Loss: 0.09542884677648544\n",
      "Step 48 (1174); Episode 50/100; Loss: 0.00586963864043355\n",
      "Step 49 (1175); Episode 50/100; Loss: 0.005562002770602703\n",
      "Step 50 (1176); Episode 50/100; Loss: 0.15612520277500153\n",
      "Step 51 (1177); Episode 50/100; Loss: 0.10357651114463806\n",
      "Step 52 (1178); Episode 50/100; Loss: 0.006977604702115059\n",
      "Step 53 (1179); Episode 50/100; Loss: 0.07617083936929703\n",
      "Step 54 (1180); Episode 50/100; Loss: 0.15870186686515808\n",
      "Step 55 (1181); Episode 50/100; Loss: 0.11809675395488739\n",
      "Step 56 (1182); Episode 50/100; Loss: 0.12080951780080795\n",
      "Step 57 (1183); Episode 50/100; Loss: 0.08093573153018951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58 (1184); Episode 50/100; Loss: 0.15147490799427032\n",
      "Step 0 (1185); Episode 51/100; Loss: 0.1350039541721344\n",
      "Step 1 (1186); Episode 51/100; Loss: 0.18596406280994415\n",
      "Step 2 (1187); Episode 51/100; Loss: 0.2074277698993683\n",
      "Step 3 (1188); Episode 51/100; Loss: 0.0800599530339241\n",
      "Step 4 (1189); Episode 51/100; Loss: 0.11976978182792664\n",
      "Step 5 (1190); Episode 51/100; Loss: 0.13101507723331451\n",
      "Step 6 (1191); Episode 51/100; Loss: 0.04516502842307091\n",
      "Step 7 (1192); Episode 51/100; Loss: 0.17865659296512604\n",
      "Step 8 (1193); Episode 51/100; Loss: 0.27618205547332764\n",
      "Step 9 (1194); Episode 51/100; Loss: 0.0965956598520279\n",
      "Step 10 (1195); Episode 51/100; Loss: 0.18116766214370728\n",
      "Step 11 (1196); Episode 51/100; Loss: 0.15726105868816376\n",
      "Step 12 (1197); Episode 51/100; Loss: 0.12232990562915802\n",
      "Step 13 (1198); Episode 51/100; Loss: 0.07806631177663803\n",
      "Step 14 (1199); Episode 51/100; Loss: 0.1375478357076645\n",
      "Step 15 (1200); Episode 51/100; Loss: 0.14516347646713257\n",
      "Step 16 (1201); Episode 51/100; Loss: 0.1998673379421234\n",
      "Step 17 (1202); Episode 51/100; Loss: 0.13914279639720917\n",
      "Step 18 (1203); Episode 51/100; Loss: 0.08022714406251907\n",
      "Step 19 (1204); Episode 51/100; Loss: 0.1626867800951004\n",
      "Step 20 (1205); Episode 51/100; Loss: 0.05784526839852333\n",
      "Step 21 (1206); Episode 51/100; Loss: 0.14710558950901031\n",
      "Step 22 (1207); Episode 51/100; Loss: 0.19176986813545227\n",
      "Step 23 (1208); Episode 51/100; Loss: 0.13664843142032623\n",
      "Step 24 (1209); Episode 51/100; Loss: 0.14465370774269104\n",
      "Step 25 (1210); Episode 51/100; Loss: 0.1930009424686432\n",
      "Step 26 (1211); Episode 51/100; Loss: 0.13047398626804352\n",
      "Step 27 (1212); Episode 51/100; Loss: 0.23031604290008545\n",
      "Step 28 (1213); Episode 51/100; Loss: 0.25998541712760925\n",
      "Step 29 (1214); Episode 51/100; Loss: 0.18571837246418\n",
      "Step 30 (1215); Episode 51/100; Loss: 0.26404979825019836\n",
      "Step 31 (1216); Episode 51/100; Loss: 0.10487803816795349\n",
      "Step 32 (1217); Episode 51/100; Loss: 0.13879966735839844\n",
      "Step 33 (1218); Episode 51/100; Loss: 0.04593651369214058\n",
      "Step 34 (1219); Episode 51/100; Loss: 0.03974771872162819\n",
      "Step 35 (1220); Episode 51/100; Loss: 0.11939547955989838\n",
      "Step 36 (1221); Episode 51/100; Loss: 0.21550941467285156\n",
      "Step 37 (1222); Episode 51/100; Loss: 0.05516069382429123\n",
      "Step 38 (1223); Episode 51/100; Loss: 0.10263115167617798\n",
      "Step 39 (1224); Episode 51/100; Loss: 0.04554670304059982\n",
      "Step 40 (1225); Episode 51/100; Loss: 0.03535907343029976\n",
      "Step 0 (1226); Episode 52/100; Loss: 0.12930706143379211\n",
      "Step 1 (1227); Episode 52/100; Loss: 0.006353265605866909\n",
      "Step 2 (1228); Episode 52/100; Loss: 0.1242762878537178\n",
      "Step 3 (1229); Episode 52/100; Loss: 0.04622497782111168\n",
      "Step 4 (1230); Episode 52/100; Loss: 0.09106995165348053\n",
      "Step 5 (1231); Episode 52/100; Loss: 0.04722610488533974\n",
      "Step 6 (1232); Episode 52/100; Loss: 0.16255219280719757\n",
      "Step 7 (1233); Episode 52/100; Loss: 0.037516552954912186\n",
      "Step 8 (1234); Episode 52/100; Loss: 0.08789444714784622\n",
      "Step 9 (1235); Episode 52/100; Loss: 0.008059325627982616\n",
      "Step 10 (1236); Episode 52/100; Loss: 0.09806693345308304\n",
      "Step 11 (1237); Episode 52/100; Loss: 0.1296941041946411\n",
      "Step 12 (1238); Episode 52/100; Loss: 0.34295183420181274\n",
      "Step 13 (1239); Episode 52/100; Loss: 0.09743166714906693\n",
      "Step 14 (1240); Episode 52/100; Loss: 0.006090990733355284\n",
      "Step 15 (1241); Episode 52/100; Loss: 0.049182042479515076\n",
      "Step 16 (1242); Episode 52/100; Loss: 0.08576028048992157\n",
      "Step 17 (1243); Episode 52/100; Loss: 0.16509945690631866\n",
      "Step 18 (1244); Episode 52/100; Loss: 0.155453160405159\n",
      "Step 19 (1245); Episode 52/100; Loss: 0.18972596526145935\n",
      "Step 20 (1246); Episode 52/100; Loss: 0.10610231757164001\n",
      "Step 21 (1247); Episode 52/100; Loss: 0.005872747860848904\n",
      "Step 22 (1248); Episode 52/100; Loss: 0.09644684940576553\n",
      "Step 23 (1249); Episode 52/100; Loss: 0.08864971995353699\n",
      "Step 24 (1250); Episode 52/100; Loss: 0.15766258537769318\n",
      "Step 25 (1251); Episode 52/100; Loss: 0.05464557185769081\n",
      "Step 26 (1252); Episode 52/100; Loss: 0.08521915972232819\n",
      "Step 27 (1253); Episode 52/100; Loss: 0.04476645588874817\n",
      "Step 28 (1254); Episode 52/100; Loss: 0.14423301815986633\n",
      "Step 29 (1255); Episode 52/100; Loss: 0.1321883350610733\n",
      "Step 30 (1256); Episode 52/100; Loss: 0.16650959849357605\n",
      "Step 31 (1257); Episode 52/100; Loss: 0.04309387505054474\n",
      "Step 32 (1258); Episode 52/100; Loss: 0.1684032529592514\n",
      "Step 33 (1259); Episode 52/100; Loss: 0.0083690844476223\n",
      "Step 34 (1260); Episode 52/100; Loss: 0.23267163336277008\n",
      "Step 35 (1261); Episode 52/100; Loss: 0.15970982611179352\n",
      "Step 36 (1262); Episode 52/100; Loss: 0.17676042020320892\n",
      "Step 37 (1263); Episode 52/100; Loss: 0.0037433612160384655\n",
      "Step 38 (1264); Episode 52/100; Loss: 0.09279356896877289\n",
      "Step 39 (1265); Episode 52/100; Loss: 0.165751650929451\n",
      "Step 40 (1266); Episode 52/100; Loss: 0.1890394538640976\n",
      "Step 41 (1267); Episode 52/100; Loss: 0.09577476978302002\n",
      "Step 42 (1268); Episode 52/100; Loss: 0.08318106830120087\n",
      "Step 43 (1269); Episode 52/100; Loss: 0.04265143349766731\n",
      "Step 44 (1270); Episode 52/100; Loss: 0.04348289221525192\n",
      "Step 45 (1271); Episode 52/100; Loss: 0.1535119265317917\n",
      "Step 46 (1272); Episode 52/100; Loss: 0.040440455079078674\n",
      "Step 47 (1273); Episode 52/100; Loss: 0.11936705559492111\n",
      "Step 48 (1274); Episode 52/100; Loss: 0.15025800466537476\n",
      "Step 49 (1275); Episode 52/100; Loss: 0.20155270397663116\n",
      "Step 50 (1276); Episode 52/100; Loss: 0.15267467498779297\n",
      "Step 51 (1277); Episode 52/100; Loss: 0.05766753852367401\n",
      "Step 52 (1278); Episode 52/100; Loss: 0.2827521860599518\n",
      "Step 53 (1279); Episode 52/100; Loss: 0.11251033842563629\n",
      "Step 54 (1280); Episode 52/100; Loss: 0.1907983273267746\n",
      "Step 55 (1281); Episode 52/100; Loss: 0.09460703283548355\n",
      "Step 56 (1282); Episode 52/100; Loss: 0.11781585216522217\n",
      "Step 57 (1283); Episode 52/100; Loss: 0.17579245567321777\n",
      "Step 58 (1284); Episode 52/100; Loss: 0.09130068868398666\n",
      "Step 59 (1285); Episode 52/100; Loss: 0.11568315327167511\n",
      "Step 60 (1286); Episode 52/100; Loss: 0.16432781517505646\n",
      "Step 61 (1287); Episode 52/100; Loss: 0.19079215824604034\n",
      "Step 62 (1288); Episode 52/100; Loss: 0.1739700734615326\n",
      "Step 63 (1289); Episode 52/100; Loss: 0.12853515148162842\n",
      "Step 64 (1290); Episode 52/100; Loss: 0.006088550202548504\n",
      "Step 65 (1291); Episode 52/100; Loss: 0.09594085067510605\n",
      "Step 0 (1292); Episode 53/100; Loss: 0.15170782804489136\n",
      "Step 1 (1293); Episode 53/100; Loss: 0.19942998886108398\n",
      "Step 2 (1294); Episode 53/100; Loss: 0.08880767971277237\n",
      "Step 3 (1295); Episode 53/100; Loss: 0.07805135101079941\n",
      "Step 4 (1296); Episode 53/100; Loss: 0.22840042412281036\n",
      "Step 5 (1297); Episode 53/100; Loss: 0.0475701168179512\n",
      "Step 6 (1298); Episode 53/100; Loss: 0.15095575153827667\n",
      "Step 7 (1299); Episode 53/100; Loss: 0.0788312703371048\n",
      "Step 8 (1300); Episode 53/100; Loss: 0.16484040021896362\n",
      "Step 9 (1301); Episode 53/100; Loss: 0.11912685632705688\n",
      "Step 10 (1302); Episode 53/100; Loss: 0.011000027880072594\n",
      "Step 11 (1303); Episode 53/100; Loss: 0.16805961728096008\n",
      "Step 12 (1304); Episode 53/100; Loss: 0.0434245802462101\n",
      "Step 13 (1305); Episode 53/100; Loss: 0.05077493563294411\n",
      "Step 14 (1306); Episode 53/100; Loss: 0.07597997784614563\n",
      "Step 15 (1307); Episode 53/100; Loss: 0.08694545179605484\n",
      "Step 16 (1308); Episode 53/100; Loss: 0.035577863454818726\n",
      "Step 17 (1309); Episode 53/100; Loss: 0.1281503438949585\n",
      "Step 18 (1310); Episode 53/100; Loss: 0.09168680012226105\n",
      "Step 19 (1311); Episode 53/100; Loss: 0.12696051597595215\n",
      "Step 20 (1312); Episode 53/100; Loss: 0.06880216300487518\n",
      "Step 21 (1313); Episode 53/100; Loss: 0.08893002569675446\n",
      "Step 22 (1314); Episode 53/100; Loss: 0.010968129150569439\n",
      "Step 23 (1315); Episode 53/100; Loss: 0.12818337976932526\n",
      "Step 24 (1316); Episode 53/100; Loss: 0.14796170592308044\n",
      "Step 25 (1317); Episode 53/100; Loss: 0.14985448122024536\n",
      "Step 26 (1318); Episode 53/100; Loss: 0.09801547229290009\n",
      "Step 27 (1319); Episode 53/100; Loss: 0.18624022603034973\n",
      "Step 28 (1320); Episode 53/100; Loss: 0.12215089797973633\n",
      "Step 29 (1321); Episode 53/100; Loss: 0.054449573159217834\n",
      "Step 30 (1322); Episode 53/100; Loss: 0.196772500872612\n",
      "Step 31 (1323); Episode 53/100; Loss: 0.18596874177455902\n",
      "Step 32 (1324); Episode 53/100; Loss: 0.16259540617465973\n",
      "Step 33 (1325); Episode 53/100; Loss: 0.08748602122068405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34 (1326); Episode 53/100; Loss: 0.24712882936000824\n",
      "Step 35 (1327); Episode 53/100; Loss: 0.16146384179592133\n",
      "Step 36 (1328); Episode 53/100; Loss: 0.08193562179803848\n",
      "Step 37 (1329); Episode 53/100; Loss: 0.04885433241724968\n",
      "Step 38 (1330); Episode 53/100; Loss: 0.21465693414211273\n",
      "Step 39 (1331); Episode 53/100; Loss: 0.04244472458958626\n",
      "Step 40 (1332); Episode 53/100; Loss: 0.08440877497196198\n",
      "Step 41 (1333); Episode 53/100; Loss: 0.0527677945792675\n",
      "Step 42 (1334); Episode 53/100; Loss: 0.059338927268981934\n",
      "Step 43 (1335); Episode 53/100; Loss: 0.10280399769544601\n",
      "Step 44 (1336); Episode 53/100; Loss: 0.07756806910037994\n",
      "Step 45 (1337); Episode 53/100; Loss: 0.12465140968561172\n",
      "Step 46 (1338); Episode 53/100; Loss: 0.1282939463853836\n",
      "Step 47 (1339); Episode 53/100; Loss: 0.07653505355119705\n",
      "Step 48 (1340); Episode 53/100; Loss: 0.1319434940814972\n",
      "Step 49 (1341); Episode 53/100; Loss: 0.20098023116588593\n",
      "Step 50 (1342); Episode 53/100; Loss: 0.20664435625076294\n",
      "Step 51 (1343); Episode 53/100; Loss: 0.09254305809736252\n",
      "Step 52 (1344); Episode 53/100; Loss: 0.13214857876300812\n",
      "Step 53 (1345); Episode 53/100; Loss: 0.0821576863527298\n",
      "Step 54 (1346); Episode 53/100; Loss: 0.07836053520441055\n",
      "Step 55 (1347); Episode 53/100; Loss: 0.06390982121229172\n",
      "Step 56 (1348); Episode 53/100; Loss: 0.1786746382713318\n",
      "Step 57 (1349); Episode 53/100; Loss: 0.18459366261959076\n",
      "Step 58 (1350); Episode 53/100; Loss: 0.17993633449077606\n",
      "Step 59 (1351); Episode 53/100; Loss: 0.147779181599617\n",
      "Step 60 (1352); Episode 53/100; Loss: 0.13120250403881073\n",
      "Step 61 (1353); Episode 53/100; Loss: 0.13485465943813324\n",
      "Step 62 (1354); Episode 53/100; Loss: 0.0814138874411583\n",
      "Step 63 (1355); Episode 53/100; Loss: 0.056333646178245544\n",
      "Step 64 (1356); Episode 53/100; Loss: 0.141151562333107\n",
      "Step 65 (1357); Episode 53/100; Loss: 0.006750200409442186\n",
      "Step 66 (1358); Episode 53/100; Loss: 0.04255437105894089\n",
      "Step 67 (1359); Episode 53/100; Loss: 0.13463729619979858\n",
      "Step 68 (1360); Episode 53/100; Loss: 0.08572471141815186\n",
      "Step 69 (1361); Episode 53/100; Loss: 0.18161962926387787\n",
      "Step 70 (1362); Episode 53/100; Loss: 0.1652923971414566\n",
      "Step 71 (1363); Episode 53/100; Loss: 0.22288845479488373\n",
      "Step 72 (1364); Episode 53/100; Loss: 0.09987479448318481\n",
      "Step 73 (1365); Episode 53/100; Loss: 0.19983385503292084\n",
      "Step 74 (1366); Episode 53/100; Loss: 0.036715276539325714\n",
      "Step 75 (1367); Episode 53/100; Loss: 0.05146356672048569\n",
      "Step 76 (1368); Episode 53/100; Loss: 0.04850427806377411\n",
      "Step 77 (1369); Episode 53/100; Loss: 0.0940997302532196\n",
      "Step 78 (1370); Episode 53/100; Loss: 0.10685818642377853\n",
      "Step 79 (1371); Episode 53/100; Loss: 0.14381085336208344\n",
      "Step 80 (1372); Episode 53/100; Loss: 0.058683332055807114\n",
      "Step 81 (1373); Episode 53/100; Loss: 0.159894198179245\n",
      "Step 82 (1374); Episode 53/100; Loss: 0.05795792490243912\n",
      "Step 83 (1375); Episode 53/100; Loss: 0.07119636237621307\n",
      "Step 84 (1376); Episode 53/100; Loss: 0.17119935154914856\n",
      "Step 85 (1377); Episode 53/100; Loss: 0.08859037607908249\n",
      "Step 86 (1378); Episode 53/100; Loss: 0.006319746375083923\n",
      "Step 87 (1379); Episode 53/100; Loss: 0.0914393961429596\n",
      "Step 88 (1380); Episode 53/100; Loss: 0.04980143532156944\n",
      "Step 89 (1381); Episode 53/100; Loss: 0.3590089678764343\n",
      "Step 90 (1382); Episode 53/100; Loss: 0.09970035403966904\n",
      "Step 91 (1383); Episode 53/100; Loss: 0.03842774033546448\n",
      "Step 92 (1384); Episode 53/100; Loss: 0.06050310656428337\n",
      "Step 93 (1385); Episode 53/100; Loss: 0.054671913385391235\n",
      "Step 94 (1386); Episode 53/100; Loss: 0.19507943093776703\n",
      "Step 95 (1387); Episode 53/100; Loss: 0.14263780415058136\n",
      "Step 96 (1388); Episode 53/100; Loss: 0.06487179547548294\n",
      "Step 97 (1389); Episode 53/100; Loss: 0.1113063246011734\n",
      "Step 98 (1390); Episode 53/100; Loss: 0.12449607998132706\n",
      "Step 99 (1391); Episode 53/100; Loss: 0.16741150617599487\n",
      "Step 100 (1392); Episode 53/100; Loss: 0.09628172963857651\n",
      "Step 101 (1393); Episode 53/100; Loss: 0.03507373109459877\n",
      "Step 102 (1394); Episode 53/100; Loss: 0.15386775135993958\n",
      "Step 103 (1395); Episode 53/100; Loss: 0.10112068057060242\n",
      "Step 104 (1396); Episode 53/100; Loss: 0.17167524993419647\n",
      "Step 105 (1397); Episode 53/100; Loss: 0.07078142464160919\n",
      "Step 106 (1398); Episode 53/100; Loss: 0.12018036842346191\n",
      "Step 107 (1399); Episode 53/100; Loss: 0.010588821955025196\n",
      "Step 108 (1400); Episode 53/100; Loss: 0.18394915759563446\n",
      "Step 109 (1401); Episode 53/100; Loss: 0.17978107929229736\n",
      "Step 110 (1402); Episode 53/100; Loss: 0.01958426460623741\n",
      "Step 111 (1403); Episode 53/100; Loss: 0.23535093665122986\n",
      "Step 0 (1404); Episode 54/100; Loss: 0.2432430237531662\n",
      "Step 1 (1405); Episode 54/100; Loss: 0.24787838757038116\n",
      "Step 2 (1406); Episode 54/100; Loss: 0.14442549645900726\n",
      "Step 3 (1407); Episode 54/100; Loss: 0.1405358612537384\n",
      "Step 4 (1408); Episode 54/100; Loss: 0.23828443884849548\n",
      "Step 5 (1409); Episode 54/100; Loss: 0.04484934359788895\n",
      "Step 6 (1410); Episode 54/100; Loss: 0.0975719690322876\n",
      "Step 7 (1411); Episode 54/100; Loss: 0.2285115271806717\n",
      "Step 8 (1412); Episode 54/100; Loss: 0.15743154287338257\n",
      "Step 9 (1413); Episode 54/100; Loss: 0.012112101539969444\n",
      "Step 10 (1414); Episode 54/100; Loss: 0.1659039705991745\n",
      "Step 11 (1415); Episode 54/100; Loss: 0.10957637429237366\n",
      "Step 12 (1416); Episode 54/100; Loss: 0.15308955311775208\n",
      "Step 13 (1417); Episode 54/100; Loss: 0.06367716938257217\n",
      "Step 14 (1418); Episode 54/100; Loss: 0.19706010818481445\n",
      "Step 15 (1419); Episode 54/100; Loss: 0.20684948563575745\n",
      "Step 16 (1420); Episode 54/100; Loss: 0.2143210470676422\n",
      "Step 17 (1421); Episode 54/100; Loss: 0.10713909566402435\n",
      "Step 18 (1422); Episode 54/100; Loss: 0.11443330347537994\n",
      "Step 19 (1423); Episode 54/100; Loss: 0.11165062338113785\n",
      "Step 20 (1424); Episode 54/100; Loss: 0.12388210743665695\n",
      "Step 21 (1425); Episode 54/100; Loss: 0.14770907163619995\n",
      "Step 22 (1426); Episode 54/100; Loss: 0.061141237616539\n",
      "Step 23 (1427); Episode 54/100; Loss: 0.08581124991178513\n",
      "Step 24 (1428); Episode 54/100; Loss: 0.03768780827522278\n",
      "Step 25 (1429); Episode 54/100; Loss: 0.20798227190971375\n",
      "Step 26 (1430); Episode 54/100; Loss: 0.14574825763702393\n",
      "Step 27 (1431); Episode 54/100; Loss: 0.09393085539340973\n",
      "Step 28 (1432); Episode 54/100; Loss: 0.08478788286447525\n",
      "Step 29 (1433); Episode 54/100; Loss: 0.13670632243156433\n",
      "Step 30 (1434); Episode 54/100; Loss: 0.037983834743499756\n",
      "Step 31 (1435); Episode 54/100; Loss: 0.10909871757030487\n",
      "Step 32 (1436); Episode 54/100; Loss: 0.08365467190742493\n",
      "Step 33 (1437); Episode 54/100; Loss: 0.048021215945482254\n",
      "Step 34 (1438); Episode 54/100; Loss: 0.17512959241867065\n",
      "Step 35 (1439); Episode 54/100; Loss: 0.055151183158159256\n",
      "Step 36 (1440); Episode 54/100; Loss: 0.04678570479154587\n",
      "Step 37 (1441); Episode 54/100; Loss: 0.09519647061824799\n",
      "Step 38 (1442); Episode 54/100; Loss: 0.15586349368095398\n",
      "Step 39 (1443); Episode 54/100; Loss: 0.08425702899694443\n",
      "Step 40 (1444); Episode 54/100; Loss: 0.09861057996749878\n",
      "Step 41 (1445); Episode 54/100; Loss: 0.10866167396306992\n",
      "Step 42 (1446); Episode 54/100; Loss: 0.05359888821840286\n",
      "Step 43 (1447); Episode 54/100; Loss: 0.005915231071412563\n",
      "Step 44 (1448); Episode 54/100; Loss: 0.005274997558444738\n",
      "Step 45 (1449); Episode 54/100; Loss: 0.14834129810333252\n",
      "Step 46 (1450); Episode 54/100; Loss: 0.051818858832120895\n",
      "Step 47 (1451); Episode 54/100; Loss: 0.0818781927227974\n",
      "Step 48 (1452); Episode 54/100; Loss: 0.07722204178571701\n",
      "Step 49 (1453); Episode 54/100; Loss: 0.14091525971889496\n",
      "Step 50 (1454); Episode 54/100; Loss: 0.04739798977971077\n",
      "Step 51 (1455); Episode 54/100; Loss: 0.08108072727918625\n",
      "Step 52 (1456); Episode 54/100; Loss: 0.14396630227565765\n",
      "Step 53 (1457); Episode 54/100; Loss: 0.05411926284432411\n",
      "Step 54 (1458); Episode 54/100; Loss: 0.10830039530992508\n",
      "Step 55 (1459); Episode 54/100; Loss: 0.16386833786964417\n",
      "Step 56 (1460); Episode 54/100; Loss: 0.1558765321969986\n",
      "Step 57 (1461); Episode 54/100; Loss: 0.004452706314623356\n",
      "Step 58 (1462); Episode 54/100; Loss: 0.09405921399593353\n",
      "Step 59 (1463); Episode 54/100; Loss: 0.15660102665424347\n",
      "Step 60 (1464); Episode 54/100; Loss: 0.14320224523544312\n",
      "Step 61 (1465); Episode 54/100; Loss: 0.14904867112636566\n",
      "Step 62 (1466); Episode 54/100; Loss: 0.10355164110660553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 63 (1467); Episode 54/100; Loss: 0.13707676529884338\n",
      "Step 64 (1468); Episode 54/100; Loss: 0.1497276872396469\n",
      "Step 65 (1469); Episode 54/100; Loss: 0.004336529411375523\n",
      "Step 66 (1470); Episode 54/100; Loss: 0.007947773672640324\n",
      "Step 67 (1471); Episode 54/100; Loss: 0.05499504879117012\n",
      "Step 68 (1472); Episode 54/100; Loss: 0.05033993348479271\n",
      "Step 69 (1473); Episode 54/100; Loss: 0.1831403523683548\n",
      "Step 70 (1474); Episode 54/100; Loss: 0.046991124749183655\n",
      "Step 71 (1475); Episode 54/100; Loss: 0.14180982112884521\n",
      "Step 72 (1476); Episode 54/100; Loss: 0.006346509326249361\n",
      "Step 73 (1477); Episode 54/100; Loss: 0.04082205146551132\n",
      "Step 74 (1478); Episode 54/100; Loss: 0.10311700403690338\n",
      "Step 75 (1479); Episode 54/100; Loss: 0.05469902604818344\n",
      "Step 76 (1480); Episode 54/100; Loss: 0.05951111763715744\n",
      "Step 77 (1481); Episode 54/100; Loss: 0.09966960549354553\n",
      "Step 78 (1482); Episode 54/100; Loss: 0.049248240888118744\n",
      "Step 79 (1483); Episode 54/100; Loss: 0.07527764141559601\n",
      "Step 80 (1484); Episode 54/100; Loss: 0.21923042833805084\n",
      "Step 81 (1485); Episode 54/100; Loss: 0.054965514689683914\n",
      "Step 82 (1486); Episode 54/100; Loss: 0.05636751279234886\n",
      "Step 83 (1487); Episode 54/100; Loss: 0.13097338378429413\n",
      "Step 84 (1488); Episode 54/100; Loss: 0.12358137965202332\n",
      "Step 85 (1489); Episode 54/100; Loss: 0.2171596735715866\n",
      "Step 0 (1490); Episode 55/100; Loss: 0.14991796016693115\n",
      "Step 1 (1491); Episode 55/100; Loss: 0.11238884180784225\n",
      "Step 2 (1492); Episode 55/100; Loss: 0.19284196197986603\n",
      "Step 3 (1493); Episode 55/100; Loss: 0.1413477510213852\n",
      "Step 4 (1494); Episode 55/100; Loss: 0.13004794716835022\n",
      "Step 5 (1495); Episode 55/100; Loss: 0.1666986644268036\n",
      "Step 6 (1496); Episode 55/100; Loss: 0.05886043980717659\n",
      "Step 7 (1497); Episode 55/100; Loss: 0.05482042580842972\n",
      "Step 8 (1498); Episode 55/100; Loss: 0.14894290268421173\n",
      "Step 9 (1499); Episode 55/100; Loss: 0.0996902585029602\n",
      "Step 10 (1500); Episode 55/100; Loss: 0.2499927282333374\n",
      "Step 11 (1501); Episode 55/100; Loss: 0.04881809279322624\n",
      "Step 12 (1502); Episode 55/100; Loss: 0.15620769560337067\n",
      "Step 13 (1503); Episode 55/100; Loss: 0.08040887117385864\n",
      "Step 14 (1504); Episode 55/100; Loss: 0.059199362993240356\n",
      "Step 15 (1505); Episode 55/100; Loss: 0.052796319127082825\n",
      "Step 16 (1506); Episode 55/100; Loss: 0.12634150683879852\n",
      "Step 17 (1507); Episode 55/100; Loss: 0.08156929910182953\n",
      "Step 18 (1508); Episode 55/100; Loss: 0.08549969643354416\n",
      "Step 19 (1509); Episode 55/100; Loss: 0.20617982745170593\n",
      "Step 20 (1510); Episode 55/100; Loss: 0.24483998119831085\n",
      "Step 21 (1511); Episode 55/100; Loss: 0.14864671230316162\n",
      "Step 22 (1512); Episode 55/100; Loss: 0.11586913466453552\n",
      "Step 23 (1513); Episode 55/100; Loss: 0.2476859837770462\n",
      "Step 24 (1514); Episode 55/100; Loss: 0.2474074512720108\n",
      "Step 25 (1515); Episode 55/100; Loss: 0.06226859986782074\n",
      "Step 26 (1516); Episode 55/100; Loss: 0.03908885270357132\n",
      "Step 27 (1517); Episode 55/100; Loss: 0.14358918368816376\n",
      "Step 28 (1518); Episode 55/100; Loss: 0.1338801234960556\n",
      "Step 29 (1519); Episode 55/100; Loss: 0.0813383162021637\n",
      "Step 30 (1520); Episode 55/100; Loss: 0.04411647468805313\n",
      "Step 31 (1521); Episode 55/100; Loss: 0.04789353162050247\n",
      "Step 32 (1522); Episode 55/100; Loss: 0.06336962431669235\n",
      "Step 33 (1523); Episode 55/100; Loss: 0.06747007369995117\n",
      "Step 34 (1524); Episode 55/100; Loss: 0.12146252393722534\n",
      "Step 35 (1525); Episode 55/100; Loss: 0.052968937903642654\n",
      "Step 36 (1526); Episode 55/100; Loss: 0.12854240834712982\n",
      "Step 37 (1527); Episode 55/100; Loss: 0.1180940717458725\n",
      "Step 38 (1528); Episode 55/100; Loss: 0.11731917411088943\n",
      "Step 39 (1529); Episode 55/100; Loss: 0.05481890216469765\n",
      "Step 40 (1530); Episode 55/100; Loss: 0.06970706582069397\n",
      "Step 41 (1531); Episode 55/100; Loss: 0.07750068604946136\n",
      "Step 42 (1532); Episode 55/100; Loss: 0.056671224534511566\n",
      "Step 43 (1533); Episode 55/100; Loss: 0.039719440042972565\n",
      "Step 44 (1534); Episode 55/100; Loss: 0.15609446167945862\n",
      "Step 45 (1535); Episode 55/100; Loss: 0.012553178705275059\n",
      "Step 46 (1536); Episode 55/100; Loss: 0.046889133751392365\n",
      "Step 47 (1537); Episode 55/100; Loss: 0.07205534726381302\n",
      "Step 48 (1538); Episode 55/100; Loss: 0.1053866371512413\n",
      "Step 49 (1539); Episode 55/100; Loss: 0.054656460881233215\n",
      "Step 50 (1540); Episode 55/100; Loss: 0.1598290205001831\n",
      "Step 51 (1541); Episode 55/100; Loss: 0.010713118128478527\n",
      "Step 52 (1542); Episode 55/100; Loss: 0.19754339754581451\n",
      "Step 53 (1543); Episode 55/100; Loss: 0.12165700644254684\n",
      "Step 54 (1544); Episode 55/100; Loss: 0.030758008360862732\n",
      "Step 55 (1545); Episode 55/100; Loss: 0.1057572290301323\n",
      "Step 56 (1546); Episode 55/100; Loss: 0.2608723044395447\n",
      "Step 57 (1547); Episode 55/100; Loss: 0.24033419787883759\n",
      "Step 58 (1548); Episode 55/100; Loss: 0.03955741226673126\n",
      "Step 59 (1549); Episode 55/100; Loss: 0.05106128007173538\n",
      "Step 60 (1550); Episode 55/100; Loss: 0.11968114972114563\n",
      "Step 61 (1551); Episode 55/100; Loss: 0.0626002848148346\n",
      "Step 62 (1552); Episode 55/100; Loss: 0.0745418593287468\n",
      "Step 63 (1553); Episode 55/100; Loss: 0.07760030031204224\n",
      "Step 64 (1554); Episode 55/100; Loss: 0.04182538762688637\n",
      "Step 65 (1555); Episode 55/100; Loss: 0.07598279416561127\n",
      "Step 66 (1556); Episode 55/100; Loss: 0.05281157046556473\n",
      "Step 67 (1557); Episode 55/100; Loss: 0.04365378990769386\n",
      "Step 68 (1558); Episode 55/100; Loss: 0.13458071649074554\n",
      "Step 69 (1559); Episode 55/100; Loss: 0.10896238684654236\n",
      "Step 70 (1560); Episode 55/100; Loss: 0.0538538433611393\n",
      "Step 71 (1561); Episode 55/100; Loss: 0.11395370960235596\n",
      "Step 72 (1562); Episode 55/100; Loss: 0.0635640025138855\n",
      "Step 73 (1563); Episode 55/100; Loss: 0.08697441965341568\n",
      "Step 74 (1564); Episode 55/100; Loss: 0.15099157392978668\n",
      "Step 75 (1565); Episode 55/100; Loss: 0.10054051876068115\n",
      "Step 76 (1566); Episode 55/100; Loss: 0.17217513918876648\n",
      "Step 77 (1567); Episode 55/100; Loss: 0.16873285174369812\n",
      "Step 78 (1568); Episode 55/100; Loss: 0.09189672768115997\n",
      "Step 79 (1569); Episode 55/100; Loss: 0.14235824346542358\n",
      "Step 80 (1570); Episode 55/100; Loss: 0.06100843846797943\n",
      "Step 81 (1571); Episode 55/100; Loss: 0.19565275311470032\n",
      "Step 82 (1572); Episode 55/100; Loss: 0.08290259540081024\n",
      "Step 83 (1573); Episode 55/100; Loss: 0.08872900158166885\n",
      "Step 84 (1574); Episode 55/100; Loss: 0.10286395251750946\n",
      "Step 85 (1575); Episode 55/100; Loss: 0.09655645489692688\n",
      "Step 86 (1576); Episode 55/100; Loss: 0.11987662315368652\n",
      "Step 87 (1577); Episode 55/100; Loss: 0.10962424427270889\n",
      "Step 88 (1578); Episode 55/100; Loss: 0.08346445113420486\n",
      "Step 89 (1579); Episode 55/100; Loss: 0.15829327702522278\n",
      "Step 90 (1580); Episode 55/100; Loss: 0.08156037330627441\n",
      "Step 91 (1581); Episode 55/100; Loss: 0.05942057445645332\n",
      "Step 92 (1582); Episode 55/100; Loss: 0.07005570083856583\n",
      "Step 93 (1583); Episode 55/100; Loss: 0.18058764934539795\n",
      "Step 94 (1584); Episode 55/100; Loss: 0.07309230417013168\n",
      "Step 95 (1585); Episode 55/100; Loss: 0.1100526973605156\n",
      "Step 96 (1586); Episode 55/100; Loss: 0.09921400994062424\n",
      "Step 97 (1587); Episode 55/100; Loss: 0.15237738192081451\n",
      "Step 98 (1588); Episode 55/100; Loss: 0.053982529789209366\n",
      "Step 99 (1589); Episode 55/100; Loss: 0.03926169499754906\n",
      "Step 100 (1590); Episode 55/100; Loss: 0.13081586360931396\n",
      "Step 101 (1591); Episode 55/100; Loss: 0.1162262111902237\n",
      "Step 102 (1592); Episode 55/100; Loss: 0.1830635517835617\n",
      "Step 103 (1593); Episode 55/100; Loss: 0.005430547520518303\n",
      "Step 104 (1594); Episode 55/100; Loss: 0.053999219089746475\n",
      "Step 105 (1595); Episode 55/100; Loss: 0.05923597887158394\n",
      "Step 106 (1596); Episode 55/100; Loss: 0.04509858414530754\n",
      "Step 107 (1597); Episode 55/100; Loss: 0.004167856182903051\n",
      "Step 108 (1598); Episode 55/100; Loss: 0.005931128282099962\n",
      "Step 109 (1599); Episode 55/100; Loss: 0.05667292699217796\n",
      "Step 110 (1600); Episode 55/100; Loss: 0.04894699901342392\n",
      "Step 111 (1601); Episode 55/100; Loss: 0.08986125886440277\n",
      "Step 0 (1602); Episode 56/100; Loss: 0.05712424963712692\n",
      "Step 1 (1603); Episode 56/100; Loss: 0.039212826639413834\n",
      "Step 2 (1604); Episode 56/100; Loss: 0.14233972132205963\n",
      "Step 3 (1605); Episode 56/100; Loss: 0.044411011040210724\n",
      "Step 4 (1606); Episode 56/100; Loss: 0.03580155968666077\n",
      "Step 5 (1607); Episode 56/100; Loss: 0.052362363785505295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 (1608); Episode 56/100; Loss: 0.07903477549552917\n",
      "Step 7 (1609); Episode 56/100; Loss: 0.09666623920202255\n",
      "Step 8 (1610); Episode 56/100; Loss: 0.09237079322338104\n",
      "Step 9 (1611); Episode 56/100; Loss: 0.004028932191431522\n",
      "Step 10 (1612); Episode 56/100; Loss: 0.08662452548742294\n",
      "Step 11 (1613); Episode 56/100; Loss: 0.18834221363067627\n",
      "Step 12 (1614); Episode 56/100; Loss: 0.12412357330322266\n",
      "Step 13 (1615); Episode 56/100; Loss: 0.13305586576461792\n",
      "Step 14 (1616); Episode 56/100; Loss: 0.06734883040189743\n",
      "Step 15 (1617); Episode 56/100; Loss: 0.03219406679272652\n",
      "Step 16 (1618); Episode 56/100; Loss: 0.03661108762025833\n",
      "Step 17 (1619); Episode 56/100; Loss: 0.12378637492656708\n",
      "Step 18 (1620); Episode 56/100; Loss: 0.09589022397994995\n",
      "Step 19 (1621); Episode 56/100; Loss: 0.0054604909382760525\n",
      "Step 20 (1622); Episode 56/100; Loss: 0.0569346584379673\n",
      "Step 21 (1623); Episode 56/100; Loss: 0.00382771878503263\n",
      "Step 22 (1624); Episode 56/100; Loss: 0.06619265675544739\n",
      "Step 23 (1625); Episode 56/100; Loss: 0.10276802629232407\n",
      "Step 24 (1626); Episode 56/100; Loss: 0.17080116271972656\n",
      "Step 25 (1627); Episode 56/100; Loss: 0.12383747845888138\n",
      "Step 26 (1628); Episode 56/100; Loss: 0.03445485234260559\n",
      "Step 27 (1629); Episode 56/100; Loss: 0.14076760411262512\n",
      "Step 28 (1630); Episode 56/100; Loss: 0.12793023884296417\n",
      "Step 29 (1631); Episode 56/100; Loss: 0.144792377948761\n",
      "Step 30 (1632); Episode 56/100; Loss: 0.09459219872951508\n",
      "Step 31 (1633); Episode 56/100; Loss: 0.09041233360767365\n",
      "Step 32 (1634); Episode 56/100; Loss: 0.04716083034873009\n",
      "Step 33 (1635); Episode 56/100; Loss: 0.09329264611005783\n",
      "Step 34 (1636); Episode 56/100; Loss: 0.00822942703962326\n",
      "Step 35 (1637); Episode 56/100; Loss: 0.08823273330926895\n",
      "Step 36 (1638); Episode 56/100; Loss: 0.048422347754240036\n",
      "Step 37 (1639); Episode 56/100; Loss: 0.0937536358833313\n",
      "Step 38 (1640); Episode 56/100; Loss: 0.08715112507343292\n",
      "Step 39 (1641); Episode 56/100; Loss: 0.006773368921130896\n",
      "Step 40 (1642); Episode 56/100; Loss: 0.0885419100522995\n",
      "Step 41 (1643); Episode 56/100; Loss: 0.10280486941337585\n",
      "Step 42 (1644); Episode 56/100; Loss: 0.08207251131534576\n",
      "Step 43 (1645); Episode 56/100; Loss: 0.10311979055404663\n",
      "Step 44 (1646); Episode 56/100; Loss: 0.26168373227119446\n",
      "Step 45 (1647); Episode 56/100; Loss: 0.09985385835170746\n",
      "Step 46 (1648); Episode 56/100; Loss: 0.047177933156490326\n",
      "Step 47 (1649); Episode 56/100; Loss: 0.08840067684650421\n",
      "Step 48 (1650); Episode 56/100; Loss: 0.08614413440227509\n",
      "Step 49 (1651); Episode 56/100; Loss: 0.00589181249961257\n",
      "Step 50 (1652); Episode 56/100; Loss: 0.12519998848438263\n",
      "Step 51 (1653); Episode 56/100; Loss: 0.08698368817567825\n",
      "Step 52 (1654); Episode 56/100; Loss: 0.18567460775375366\n",
      "Step 53 (1655); Episode 56/100; Loss: 0.05243076756596565\n",
      "Step 54 (1656); Episode 56/100; Loss: 0.04023642838001251\n",
      "Step 55 (1657); Episode 56/100; Loss: 0.04688109830021858\n",
      "Step 56 (1658); Episode 56/100; Loss: 0.07929108291864395\n",
      "Step 57 (1659); Episode 56/100; Loss: 0.05306818708777428\n",
      "Step 58 (1660); Episode 56/100; Loss: 0.055888768285512924\n",
      "Step 59 (1661); Episode 56/100; Loss: 0.10598748177289963\n",
      "Step 60 (1662); Episode 56/100; Loss: 0.08583091199398041\n",
      "Step 61 (1663); Episode 56/100; Loss: 0.04581834003329277\n",
      "Step 62 (1664); Episode 56/100; Loss: 0.1667110174894333\n",
      "Step 63 (1665); Episode 56/100; Loss: 0.05054888129234314\n",
      "Step 64 (1666); Episode 56/100; Loss: 0.0726504921913147\n",
      "Step 65 (1667); Episode 56/100; Loss: 0.08668912202119827\n",
      "Step 66 (1668); Episode 56/100; Loss: 0.052871301770210266\n",
      "Step 67 (1669); Episode 56/100; Loss: 0.03653308376669884\n",
      "Step 68 (1670); Episode 56/100; Loss: 0.0037090766709297895\n",
      "Step 69 (1671); Episode 56/100; Loss: 0.09615001827478409\n",
      "Step 70 (1672); Episode 56/100; Loss: 0.03805616497993469\n",
      "Step 71 (1673); Episode 56/100; Loss: 0.21471509337425232\n",
      "Step 72 (1674); Episode 56/100; Loss: 0.10029079020023346\n",
      "Step 73 (1675); Episode 56/100; Loss: 0.15392020344734192\n",
      "Step 0 (1676); Episode 57/100; Loss: 0.14194919168949127\n",
      "Step 1 (1677); Episode 57/100; Loss: 0.1451258659362793\n",
      "Step 2 (1678); Episode 57/100; Loss: 0.034276220947504044\n",
      "Step 3 (1679); Episode 57/100; Loss: 0.0898834764957428\n",
      "Step 4 (1680); Episode 57/100; Loss: 0.056698303669691086\n",
      "Step 5 (1681); Episode 57/100; Loss: 0.05388292670249939\n",
      "Step 6 (1682); Episode 57/100; Loss: 0.12237628549337387\n",
      "Step 7 (1683); Episode 57/100; Loss: 0.1982629895210266\n",
      "Step 8 (1684); Episode 57/100; Loss: 0.12276231497526169\n",
      "Step 9 (1685); Episode 57/100; Loss: 0.03913566842675209\n",
      "Step 10 (1686); Episode 57/100; Loss: 0.04237847030162811\n",
      "Step 11 (1687); Episode 57/100; Loss: 0.003037800546735525\n",
      "Step 12 (1688); Episode 57/100; Loss: 0.0952628031373024\n",
      "Step 13 (1689); Episode 57/100; Loss: 0.04208703711628914\n",
      "Step 14 (1690); Episode 57/100; Loss: 0.16284379363059998\n",
      "Step 15 (1691); Episode 57/100; Loss: 0.13758379220962524\n",
      "Step 16 (1692); Episode 57/100; Loss: 0.11522948741912842\n",
      "Step 17 (1693); Episode 57/100; Loss: 0.1461750566959381\n",
      "Step 18 (1694); Episode 57/100; Loss: 0.15026071667671204\n",
      "Step 19 (1695); Episode 57/100; Loss: 0.14763179421424866\n",
      "Step 20 (1696); Episode 57/100; Loss: 0.11755146086215973\n",
      "Step 21 (1697); Episode 57/100; Loss: 0.03836134076118469\n",
      "Step 22 (1698); Episode 57/100; Loss: 0.04342850670218468\n",
      "Step 23 (1699); Episode 57/100; Loss: 0.16436414420604706\n",
      "Step 24 (1700); Episode 57/100; Loss: 0.005288366694003344\n",
      "Step 25 (1701); Episode 57/100; Loss: 0.13071464002132416\n",
      "Step 26 (1702); Episode 57/100; Loss: 0.003583719953894615\n",
      "Step 27 (1703); Episode 57/100; Loss: 0.10744671523571014\n",
      "Step 28 (1704); Episode 57/100; Loss: 0.1618979275226593\n",
      "Step 29 (1705); Episode 57/100; Loss: 0.15635432302951813\n",
      "Step 30 (1706); Episode 57/100; Loss: 0.08359488844871521\n",
      "Step 31 (1707); Episode 57/100; Loss: 0.2218342423439026\n",
      "Step 32 (1708); Episode 57/100; Loss: 0.0486798956990242\n",
      "Step 33 (1709); Episode 57/100; Loss: 0.03955411538481712\n",
      "Step 34 (1710); Episode 57/100; Loss: 0.004682601895183325\n",
      "Step 35 (1711); Episode 57/100; Loss: 0.0590534545481205\n",
      "Step 36 (1712); Episode 57/100; Loss: 0.040012944489717484\n",
      "Step 37 (1713); Episode 57/100; Loss: 0.04139311611652374\n",
      "Step 38 (1714); Episode 57/100; Loss: 0.009750149212777615\n",
      "Step 39 (1715); Episode 57/100; Loss: 0.12823979556560516\n",
      "Step 40 (1716); Episode 57/100; Loss: 0.05344618856906891\n",
      "Step 41 (1717); Episode 57/100; Loss: 0.04371451586484909\n",
      "Step 42 (1718); Episode 57/100; Loss: 0.05751655250787735\n",
      "Step 43 (1719); Episode 57/100; Loss: 0.09246496111154556\n",
      "Step 44 (1720); Episode 57/100; Loss: 0.18887567520141602\n",
      "Step 45 (1721); Episode 57/100; Loss: 0.12312327325344086\n",
      "Step 46 (1722); Episode 57/100; Loss: 0.005036500282585621\n",
      "Step 47 (1723); Episode 57/100; Loss: 0.20840632915496826\n",
      "Step 48 (1724); Episode 57/100; Loss: 0.05242680385708809\n",
      "Step 49 (1725); Episode 57/100; Loss: 0.0586124062538147\n",
      "Step 50 (1726); Episode 57/100; Loss: 0.09824880957603455\n",
      "Step 51 (1727); Episode 57/100; Loss: 0.0459715910255909\n",
      "Step 52 (1728); Episode 57/100; Loss: 0.058836888521909714\n",
      "Step 53 (1729); Episode 57/100; Loss: 0.02967623807489872\n",
      "Step 54 (1730); Episode 57/100; Loss: 0.18328458070755005\n",
      "Step 55 (1731); Episode 57/100; Loss: 0.05604329705238342\n",
      "Step 56 (1732); Episode 57/100; Loss: 0.03675239905714989\n",
      "Step 57 (1733); Episode 57/100; Loss: 0.12853826582431793\n",
      "Step 58 (1734); Episode 57/100; Loss: 0.08420852571725845\n",
      "Step 59 (1735); Episode 57/100; Loss: 0.1407511830329895\n",
      "Step 60 (1736); Episode 57/100; Loss: 0.18101641535758972\n",
      "Step 61 (1737); Episode 57/100; Loss: 0.07725589722394943\n",
      "Step 62 (1738); Episode 57/100; Loss: 0.10353226959705353\n",
      "Step 63 (1739); Episode 57/100; Loss: 0.09277348965406418\n",
      "Step 64 (1740); Episode 57/100; Loss: 0.03521837666630745\n",
      "Step 65 (1741); Episode 57/100; Loss: 0.0469115674495697\n",
      "Step 66 (1742); Episode 57/100; Loss: 0.07625415176153183\n",
      "Step 67 (1743); Episode 57/100; Loss: 0.08007410913705826\n",
      "Step 68 (1744); Episode 57/100; Loss: 0.1315588653087616\n",
      "Step 69 (1745); Episode 57/100; Loss: 0.16209660470485687\n",
      "Step 70 (1746); Episode 57/100; Loss: 0.00814865343272686\n",
      "Step 71 (1747); Episode 57/100; Loss: 0.06897053122520447\n",
      "Step 72 (1748); Episode 57/100; Loss: 0.03646188974380493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 73 (1749); Episode 57/100; Loss: 0.0036777753848582506\n",
      "Step 74 (1750); Episode 57/100; Loss: 0.18734218180179596\n",
      "Step 75 (1751); Episode 57/100; Loss: 0.05327647924423218\n",
      "Step 76 (1752); Episode 57/100; Loss: 0.04133385419845581\n",
      "Step 77 (1753); Episode 57/100; Loss: 0.13177980482578278\n",
      "Step 78 (1754); Episode 57/100; Loss: 0.07495178282260895\n",
      "Step 79 (1755); Episode 57/100; Loss: 0.11394419521093369\n",
      "Step 80 (1756); Episode 57/100; Loss: 0.07926815748214722\n",
      "Step 81 (1757); Episode 57/100; Loss: 0.15260423719882965\n",
      "Step 82 (1758); Episode 57/100; Loss: 0.058371782302856445\n",
      "Step 83 (1759); Episode 57/100; Loss: 0.08644743263721466\n",
      "Step 84 (1760); Episode 57/100; Loss: 0.00661866832524538\n",
      "Step 85 (1761); Episode 57/100; Loss: 0.03657406568527222\n",
      "Step 86 (1762); Episode 57/100; Loss: 0.11216290295124054\n",
      "Step 87 (1763); Episode 57/100; Loss: 0.0029665555339306593\n",
      "Step 88 (1764); Episode 57/100; Loss: 0.14533621072769165\n",
      "Step 89 (1765); Episode 57/100; Loss: 0.1048574447631836\n",
      "Step 90 (1766); Episode 57/100; Loss: 0.10606247186660767\n",
      "Step 91 (1767); Episode 57/100; Loss: 0.08577948808670044\n",
      "Step 92 (1768); Episode 57/100; Loss: 0.1944700926542282\n",
      "Step 93 (1769); Episode 57/100; Loss: 0.006603885907679796\n",
      "Step 94 (1770); Episode 57/100; Loss: 0.05511076748371124\n",
      "Step 95 (1771); Episode 57/100; Loss: 0.09569073468446732\n",
      "Step 96 (1772); Episode 57/100; Loss: 0.0510554313659668\n",
      "Step 97 (1773); Episode 57/100; Loss: 0.006815908011049032\n",
      "Step 98 (1774); Episode 57/100; Loss: 0.054663434624671936\n",
      "Step 99 (1775); Episode 57/100; Loss: 0.002867225557565689\n",
      "Step 100 (1776); Episode 57/100; Loss: 0.031682755798101425\n",
      "Step 101 (1777); Episode 57/100; Loss: 0.002491152146831155\n",
      "Step 102 (1778); Episode 57/100; Loss: 0.1307922899723053\n",
      "Step 103 (1779); Episode 57/100; Loss: 0.11531287431716919\n",
      "Step 104 (1780); Episode 57/100; Loss: 0.04355143755674362\n",
      "Step 105 (1781); Episode 57/100; Loss: 0.21431352198123932\n",
      "Step 106 (1782); Episode 57/100; Loss: 0.13974347710609436\n",
      "Step 107 (1783); Episode 57/100; Loss: 0.15746431052684784\n",
      "Step 108 (1784); Episode 57/100; Loss: 0.11411694437265396\n",
      "Step 109 (1785); Episode 57/100; Loss: 0.12543217837810516\n",
      "Step 110 (1786); Episode 57/100; Loss: 0.0021262695081532\n",
      "Step 111 (1787); Episode 57/100; Loss: 0.005683072376996279\n",
      "Step 112 (1788); Episode 57/100; Loss: 0.0692857876420021\n",
      "Step 113 (1789); Episode 57/100; Loss: 0.11836480349302292\n",
      "Step 0 (1790); Episode 58/100; Loss: 0.16499055922031403\n",
      "Step 1 (1791); Episode 58/100; Loss: 0.006441791541874409\n",
      "Step 2 (1792); Episode 58/100; Loss: 0.11871092766523361\n",
      "Step 3 (1793); Episode 58/100; Loss: 0.029314272105693817\n",
      "Step 4 (1794); Episode 58/100; Loss: 0.05403677746653557\n",
      "Step 5 (1795); Episode 58/100; Loss: 0.18820695579051971\n",
      "Step 6 (1796); Episode 58/100; Loss: 0.1276911497116089\n",
      "Step 7 (1797); Episode 58/100; Loss: 0.08350338786840439\n",
      "Step 8 (1798); Episode 58/100; Loss: 0.05829883739352226\n",
      "Step 9 (1799); Episode 58/100; Loss: 0.06011359766125679\n",
      "Step 10 (1800); Episode 58/100; Loss: 0.0944032147526741\n",
      "Step 11 (1801); Episode 58/100; Loss: 0.10368333011865616\n",
      "Step 12 (1802); Episode 58/100; Loss: 0.041185155510902405\n",
      "Step 13 (1803); Episode 58/100; Loss: 0.04431205242872238\n",
      "Step 14 (1804); Episode 58/100; Loss: 0.12330115586519241\n",
      "Step 15 (1805); Episode 58/100; Loss: 0.13496524095535278\n",
      "Step 16 (1806); Episode 58/100; Loss: 0.21881069242954254\n",
      "Step 17 (1807); Episode 58/100; Loss: 0.05836150795221329\n",
      "Step 18 (1808); Episode 58/100; Loss: 0.06142166629433632\n",
      "Step 19 (1809); Episode 58/100; Loss: 0.19790184497833252\n",
      "Step 20 (1810); Episode 58/100; Loss: 0.08385703712701797\n",
      "Step 21 (1811); Episode 58/100; Loss: 0.005253973416984081\n",
      "Step 22 (1812); Episode 58/100; Loss: 0.1350242793560028\n",
      "Step 23 (1813); Episode 58/100; Loss: 0.0578317753970623\n",
      "Step 24 (1814); Episode 58/100; Loss: 0.1366189569234848\n",
      "Step 25 (1815); Episode 58/100; Loss: 0.04617403820157051\n",
      "Step 26 (1816); Episode 58/100; Loss: 0.007170849479734898\n",
      "Step 27 (1817); Episode 58/100; Loss: 0.036241743713617325\n",
      "Step 28 (1818); Episode 58/100; Loss: 0.05827338621020317\n",
      "Step 29 (1819); Episode 58/100; Loss: 0.12301158159971237\n",
      "Step 30 (1820); Episode 58/100; Loss: 0.06239604577422142\n",
      "Step 31 (1821); Episode 58/100; Loss: 0.044533148407936096\n",
      "Step 32 (1822); Episode 58/100; Loss: 0.13307234644889832\n",
      "Step 33 (1823); Episode 58/100; Loss: 0.08162679523229599\n",
      "Step 34 (1824); Episode 58/100; Loss: 0.1605503410100937\n",
      "Step 35 (1825); Episode 58/100; Loss: 0.03905953839421272\n",
      "Step 36 (1826); Episode 58/100; Loss: 0.05679882690310478\n",
      "Step 37 (1827); Episode 58/100; Loss: 0.02420288883149624\n",
      "Step 38 (1828); Episode 58/100; Loss: 0.17563098669052124\n",
      "Step 39 (1829); Episode 58/100; Loss: 0.06248943880200386\n",
      "Step 40 (1830); Episode 58/100; Loss: 0.1194191500544548\n",
      "Step 41 (1831); Episode 58/100; Loss: 0.1376531422138214\n",
      "Step 42 (1832); Episode 58/100; Loss: 0.09505432844161987\n",
      "Step 43 (1833); Episode 58/100; Loss: 0.036667030304670334\n",
      "Step 44 (1834); Episode 58/100; Loss: 0.12566640973091125\n",
      "Step 45 (1835); Episode 58/100; Loss: 0.1873912811279297\n",
      "Step 46 (1836); Episode 58/100; Loss: 0.004903251305222511\n",
      "Step 47 (1837); Episode 58/100; Loss: 0.09691344946622849\n",
      "Step 48 (1838); Episode 58/100; Loss: 0.09015324711799622\n",
      "Step 49 (1839); Episode 58/100; Loss: 0.03822719678282738\n",
      "Step 50 (1840); Episode 58/100; Loss: 0.10245577991008759\n",
      "Step 51 (1841); Episode 58/100; Loss: 0.05191387981176376\n",
      "Step 52 (1842); Episode 58/100; Loss: 0.030018923804163933\n",
      "Step 53 (1843); Episode 58/100; Loss: 0.20310717821121216\n",
      "Step 54 (1844); Episode 58/100; Loss: 0.14763225615024567\n",
      "Step 55 (1845); Episode 58/100; Loss: 0.16126352548599243\n",
      "Step 56 (1846); Episode 58/100; Loss: 0.0569901280105114\n",
      "Step 57 (1847); Episode 58/100; Loss: 0.006364097818732262\n",
      "Step 58 (1848); Episode 58/100; Loss: 0.17278149724006653\n",
      "Step 59 (1849); Episode 58/100; Loss: 0.04627002030611038\n",
      "Step 60 (1850); Episode 58/100; Loss: 0.00453824270516634\n",
      "Step 61 (1851); Episode 58/100; Loss: 0.06745747476816177\n",
      "Step 62 (1852); Episode 58/100; Loss: 0.015626942738890648\n",
      "Step 63 (1853); Episode 58/100; Loss: 0.2103041112422943\n",
      "Step 64 (1854); Episode 58/100; Loss: 0.056232649832963943\n",
      "Step 65 (1855); Episode 58/100; Loss: 0.08447986841201782\n",
      "Step 66 (1856); Episode 58/100; Loss: 0.05587320029735565\n",
      "Step 67 (1857); Episode 58/100; Loss: 0.11797280609607697\n",
      "Step 68 (1858); Episode 58/100; Loss: 0.006972649600356817\n",
      "Step 69 (1859); Episode 58/100; Loss: 0.06507300585508347\n",
      "Step 70 (1860); Episode 58/100; Loss: 0.12119457870721817\n",
      "Step 71 (1861); Episode 58/100; Loss: 0.2017042636871338\n",
      "Step 72 (1862); Episode 58/100; Loss: 0.1714898943901062\n",
      "Step 73 (1863); Episode 58/100; Loss: 0.10768729448318481\n",
      "Step 74 (1864); Episode 58/100; Loss: 0.058498769998550415\n",
      "Step 75 (1865); Episode 58/100; Loss: 0.1745537370443344\n",
      "Step 76 (1866); Episode 58/100; Loss: 0.08973772078752518\n",
      "Step 77 (1867); Episode 58/100; Loss: 0.00567172234877944\n",
      "Step 78 (1868); Episode 58/100; Loss: 0.10639700293540955\n",
      "Step 79 (1869); Episode 58/100; Loss: 0.08188363909721375\n",
      "Step 80 (1870); Episode 58/100; Loss: 0.02577868103981018\n",
      "Step 81 (1871); Episode 58/100; Loss: 0.04418424889445305\n",
      "Step 82 (1872); Episode 58/100; Loss: 0.05556813254952431\n",
      "Step 83 (1873); Episode 58/100; Loss: 0.05604606121778488\n",
      "Step 84 (1874); Episode 58/100; Loss: 0.09660786390304565\n",
      "Step 85 (1875); Episode 58/100; Loss: 0.13328471779823303\n",
      "Step 86 (1876); Episode 58/100; Loss: 0.07395947724580765\n",
      "Step 87 (1877); Episode 58/100; Loss: 0.006537470500916243\n",
      "Step 88 (1878); Episode 58/100; Loss: 0.03181949257850647\n",
      "Step 89 (1879); Episode 58/100; Loss: 0.10446643084287643\n",
      "Step 90 (1880); Episode 58/100; Loss: 0.09746638685464859\n",
      "Step 91 (1881); Episode 58/100; Loss: 0.07128436863422394\n",
      "Step 92 (1882); Episode 58/100; Loss: 0.03300962597131729\n",
      "Step 93 (1883); Episode 58/100; Loss: 0.04196980595588684\n",
      "Step 94 (1884); Episode 58/100; Loss: 0.055841680616140366\n",
      "Step 95 (1885); Episode 58/100; Loss: 0.005084490869194269\n",
      "Step 96 (1886); Episode 58/100; Loss: 0.005697461310774088\n",
      "Step 97 (1887); Episode 58/100; Loss: 0.12910330295562744\n",
      "Step 98 (1888); Episode 58/100; Loss: 0.036270324140787125\n",
      "Step 99 (1889); Episode 58/100; Loss: 0.1408490687608719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 (1890); Episode 58/100; Loss: 0.03362134471535683\n",
      "Step 101 (1891); Episode 58/100; Loss: 0.039594877511262894\n",
      "Step 102 (1892); Episode 58/100; Loss: 0.1826118379831314\n",
      "Step 103 (1893); Episode 58/100; Loss: 0.16215457022190094\n",
      "Step 104 (1894); Episode 58/100; Loss: 0.05164647474884987\n",
      "Step 105 (1895); Episode 58/100; Loss: 0.007341749034821987\n",
      "Step 106 (1896); Episode 58/100; Loss: 0.0884547159075737\n",
      "Step 107 (1897); Episode 58/100; Loss: 0.04247187823057175\n",
      "Step 108 (1898); Episode 58/100; Loss: 0.028269153088331223\n",
      "Step 109 (1899); Episode 58/100; Loss: 0.051083240658044815\n",
      "Step 110 (1900); Episode 58/100; Loss: 0.10703721642494202\n",
      "Step 111 (1901); Episode 58/100; Loss: 0.0636461153626442\n",
      "Step 112 (1902); Episode 58/100; Loss: 0.004962881561368704\n",
      "Step 113 (1903); Episode 58/100; Loss: 0.06584139913320541\n",
      "Step 114 (1904); Episode 58/100; Loss: 0.14555460214614868\n",
      "Step 115 (1905); Episode 58/100; Loss: 0.13111519813537598\n",
      "Step 116 (1906); Episode 58/100; Loss: 0.11028152704238892\n",
      "Step 117 (1907); Episode 58/100; Loss: 0.19903121888637543\n",
      "Step 118 (1908); Episode 58/100; Loss: 0.050846029072999954\n",
      "Step 119 (1909); Episode 58/100; Loss: 0.006883813533931971\n",
      "Step 120 (1910); Episode 58/100; Loss: 0.0059769838117063046\n",
      "Step 121 (1911); Episode 58/100; Loss: 0.0860905721783638\n",
      "Step 122 (1912); Episode 58/100; Loss: 0.005519701633602381\n",
      "Step 123 (1913); Episode 58/100; Loss: 0.004586419556289911\n",
      "Step 124 (1914); Episode 58/100; Loss: 0.004521429538726807\n",
      "Step 125 (1915); Episode 58/100; Loss: 0.0052483584731817245\n",
      "Step 126 (1916); Episode 58/100; Loss: 0.09425881505012512\n",
      "Step 127 (1917); Episode 58/100; Loss: 0.00398620730265975\n",
      "Step 128 (1918); Episode 58/100; Loss: 0.06121792271733284\n",
      "Step 129 (1919); Episode 58/100; Loss: 0.04218801110982895\n",
      "Step 130 (1920); Episode 58/100; Loss: 0.04571858420968056\n",
      "Step 131 (1921); Episode 58/100; Loss: 0.0024368504527956247\n",
      "Step 132 (1922); Episode 58/100; Loss: 0.16203907132148743\n",
      "Step 133 (1923); Episode 58/100; Loss: 0.11203175783157349\n",
      "Step 134 (1924); Episode 58/100; Loss: 0.0025357219856232405\n",
      "Step 135 (1925); Episode 58/100; Loss: 0.13014143705368042\n",
      "Step 136 (1926); Episode 58/100; Loss: 0.05335617810487747\n",
      "Step 137 (1927); Episode 58/100; Loss: 0.02459288202226162\n",
      "Step 138 (1928); Episode 58/100; Loss: 0.10116370022296906\n",
      "Step 139 (1929); Episode 58/100; Loss: 0.09384489059448242\n",
      "Step 140 (1930); Episode 58/100; Loss: 0.09318628907203674\n",
      "Step 141 (1931); Episode 58/100; Loss: 0.049912888556718826\n",
      "Step 142 (1932); Episode 58/100; Loss: 0.12959806621074677\n",
      "Step 143 (1933); Episode 58/100; Loss: 0.04120789095759392\n",
      "Step 144 (1934); Episode 58/100; Loss: 0.052952054888010025\n",
      "Step 145 (1935); Episode 58/100; Loss: 0.027867857366800308\n",
      "Step 146 (1936); Episode 58/100; Loss: 0.09991481900215149\n",
      "Step 147 (1937); Episode 58/100; Loss: 0.0344543494284153\n",
      "Step 148 (1938); Episode 58/100; Loss: 0.09412860125303268\n",
      "Step 149 (1939); Episode 58/100; Loss: 0.13696111738681793\n",
      "Step 150 (1940); Episode 58/100; Loss: 0.04422394558787346\n",
      "Step 151 (1941); Episode 58/100; Loss: 0.15964777767658234\n",
      "Step 152 (1942); Episode 58/100; Loss: 0.0025213398039340973\n",
      "Step 153 (1943); Episode 58/100; Loss: 0.05662376806139946\n",
      "Step 154 (1944); Episode 58/100; Loss: 0.07573314011096954\n",
      "Step 155 (1945); Episode 58/100; Loss: 0.08101499825716019\n",
      "Step 156 (1946); Episode 58/100; Loss: 0.07157543301582336\n",
      "Step 157 (1947); Episode 58/100; Loss: 0.07253017276525497\n",
      "Step 158 (1948); Episode 58/100; Loss: 0.040078453719615936\n",
      "Step 159 (1949); Episode 58/100; Loss: 0.16199623048305511\n",
      "Step 160 (1950); Episode 58/100; Loss: 0.10430626571178436\n",
      "Step 161 (1951); Episode 58/100; Loss: 0.19347858428955078\n",
      "Step 162 (1952); Episode 58/100; Loss: 0.048718251287937164\n",
      "Step 163 (1953); Episode 58/100; Loss: 0.2191568911075592\n",
      "Step 164 (1954); Episode 58/100; Loss: 0.08049679547548294\n",
      "Step 165 (1955); Episode 58/100; Loss: 0.13775376975536346\n",
      "Step 166 (1956); Episode 58/100; Loss: 0.17779576778411865\n",
      "Step 167 (1957); Episode 58/100; Loss: 0.06918065994977951\n",
      "Step 168 (1958); Episode 58/100; Loss: 0.0343974307179451\n",
      "Step 169 (1959); Episode 58/100; Loss: 0.052874330431222916\n",
      "Step 170 (1960); Episode 58/100; Loss: 0.11362262815237045\n",
      "Step 171 (1961); Episode 58/100; Loss: 0.04884370043873787\n",
      "Step 172 (1962); Episode 58/100; Loss: 0.10631103813648224\n",
      "Step 173 (1963); Episode 58/100; Loss: 0.10104450583457947\n",
      "Step 174 (1964); Episode 58/100; Loss: 0.07026400417089462\n",
      "Step 175 (1965); Episode 58/100; Loss: 0.0367787703871727\n",
      "Step 176 (1966); Episode 58/100; Loss: 0.10844520479440689\n",
      "Step 177 (1967); Episode 58/100; Loss: 0.05780620872974396\n",
      "Step 0 (1968); Episode 59/100; Loss: 0.008360909298062325\n",
      "Step 1 (1969); Episode 59/100; Loss: 0.059671737253665924\n",
      "Step 2 (1970); Episode 59/100; Loss: 0.05399179458618164\n",
      "Step 3 (1971); Episode 59/100; Loss: 0.04515036568045616\n",
      "Step 4 (1972); Episode 59/100; Loss: 0.08587349951267242\n",
      "Step 5 (1973); Episode 59/100; Loss: 0.09537096321582794\n",
      "Step 6 (1974); Episode 59/100; Loss: 0.04189865663647652\n",
      "Step 7 (1975); Episode 59/100; Loss: 0.008270369842648506\n",
      "Step 8 (1976); Episode 59/100; Loss: 0.12787185609340668\n",
      "Step 9 (1977); Episode 59/100; Loss: 0.13238701224327087\n",
      "Step 10 (1978); Episode 59/100; Loss: 0.07745803892612457\n",
      "Step 11 (1979); Episode 59/100; Loss: 0.1694231629371643\n",
      "Step 12 (1980); Episode 59/100; Loss: 0.06609510630369186\n",
      "Step 13 (1981); Episode 59/100; Loss: 0.12126445770263672\n",
      "Step 14 (1982); Episode 59/100; Loss: 0.12475596368312836\n",
      "Step 15 (1983); Episode 59/100; Loss: 0.061420924961566925\n",
      "Step 16 (1984); Episode 59/100; Loss: 0.02243463508784771\n",
      "Step 17 (1985); Episode 59/100; Loss: 0.05757026746869087\n",
      "Step 18 (1986); Episode 59/100; Loss: 0.2298441082239151\n",
      "Step 19 (1987); Episode 59/100; Loss: 0.0585467666387558\n",
      "Step 20 (1988); Episode 59/100; Loss: 0.14667312800884247\n",
      "Step 21 (1989); Episode 59/100; Loss: 0.05220235884189606\n",
      "Step 22 (1990); Episode 59/100; Loss: 0.03395960107445717\n",
      "Step 23 (1991); Episode 59/100; Loss: 0.09042508900165558\n",
      "Step 24 (1992); Episode 59/100; Loss: 0.00980820320546627\n",
      "Step 25 (1993); Episode 59/100; Loss: 0.03643333166837692\n",
      "Step 26 (1994); Episode 59/100; Loss: 0.07819309085607529\n",
      "Step 27 (1995); Episode 59/100; Loss: 0.05393366515636444\n",
      "Step 28 (1996); Episode 59/100; Loss: 0.004521619062870741\n",
      "Step 29 (1997); Episode 59/100; Loss: 0.09420260041952133\n",
      "Step 30 (1998); Episode 59/100; Loss: 0.12016848474740982\n",
      "Step 31 (1999); Episode 59/100; Loss: 0.05738021060824394\n",
      "Step 32 (2000); Episode 59/100; Loss: 0.05781993269920349\n",
      "Step 33 (2001); Episode 59/100; Loss: 0.003992816898971796\n",
      "Step 34 (2002); Episode 59/100; Loss: 0.04127112030982971\n",
      "Step 35 (2003); Episode 59/100; Loss: 0.03155449777841568\n",
      "Step 36 (2004); Episode 59/100; Loss: 0.1588752418756485\n",
      "Step 37 (2005); Episode 59/100; Loss: 0.14708833396434784\n",
      "Step 38 (2006); Episode 59/100; Loss: 0.043748438358306885\n",
      "Step 39 (2007); Episode 59/100; Loss: 0.14842097461223602\n",
      "Step 40 (2008); Episode 59/100; Loss: 0.06766485422849655\n",
      "Step 41 (2009); Episode 59/100; Loss: 0.0985526591539383\n",
      "Step 42 (2010); Episode 59/100; Loss: 0.0754847452044487\n",
      "Step 43 (2011); Episode 59/100; Loss: 0.056413646787405014\n",
      "Step 44 (2012); Episode 59/100; Loss: 0.10215681791305542\n",
      "Step 45 (2013); Episode 59/100; Loss: 0.0568125955760479\n",
      "Step 46 (2014); Episode 59/100; Loss: 0.04792873188853264\n",
      "Step 47 (2015); Episode 59/100; Loss: 0.03801124542951584\n",
      "Step 48 (2016); Episode 59/100; Loss: 0.04249979555606842\n",
      "Step 49 (2017); Episode 59/100; Loss: 0.04185599088668823\n",
      "Step 50 (2018); Episode 59/100; Loss: 0.11956188827753067\n",
      "Step 51 (2019); Episode 59/100; Loss: 0.11024970561265945\n",
      "Step 52 (2020); Episode 59/100; Loss: 0.16409698128700256\n",
      "Step 53 (2021); Episode 59/100; Loss: 0.029044242575764656\n",
      "Step 54 (2022); Episode 59/100; Loss: 0.07431770861148834\n",
      "Step 55 (2023); Episode 59/100; Loss: 0.10714619606733322\n",
      "Step 56 (2024); Episode 59/100; Loss: 0.011184630915522575\n",
      "Step 57 (2025); Episode 59/100; Loss: 0.07088126242160797\n",
      "Step 58 (2026); Episode 59/100; Loss: 0.03744237497448921\n",
      "Step 59 (2027); Episode 59/100; Loss: 0.025565223768353462\n",
      "Step 60 (2028); Episode 59/100; Loss: 0.10807392001152039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61 (2029); Episode 59/100; Loss: 0.054709337651729584\n",
      "Step 62 (2030); Episode 59/100; Loss: 0.0036233190912753344\n",
      "Step 63 (2031); Episode 59/100; Loss: 0.10490680485963821\n",
      "Step 64 (2032); Episode 59/100; Loss: 0.028435789048671722\n",
      "Step 65 (2033); Episode 59/100; Loss: 0.031048288568854332\n",
      "Step 66 (2034); Episode 59/100; Loss: 0.029395151883363724\n",
      "Step 67 (2035); Episode 59/100; Loss: 0.05163035914301872\n",
      "Step 68 (2036); Episode 59/100; Loss: 0.18138279020786285\n",
      "Step 69 (2037); Episode 59/100; Loss: 0.10471341758966446\n",
      "Step 70 (2038); Episode 59/100; Loss: 0.12992537021636963\n",
      "Step 71 (2039); Episode 59/100; Loss: 0.023467862978577614\n",
      "Step 72 (2040); Episode 59/100; Loss: 0.10512686520814896\n",
      "Step 73 (2041); Episode 59/100; Loss: 0.07440699636936188\n",
      "Step 0 (2042); Episode 60/100; Loss: 0.10526488721370697\n",
      "Step 1 (2043); Episode 60/100; Loss: 0.07987482845783234\n",
      "Step 2 (2044); Episode 60/100; Loss: 0.011580292135477066\n",
      "Step 3 (2045); Episode 60/100; Loss: 0.1315172016620636\n",
      "Step 4 (2046); Episode 60/100; Loss: 0.03189845383167267\n",
      "Step 5 (2047); Episode 60/100; Loss: 0.14550119638442993\n",
      "Step 6 (2048); Episode 60/100; Loss: 0.22765251994132996\n",
      "Step 7 (2049); Episode 60/100; Loss: 0.1287873387336731\n",
      "Step 8 (2050); Episode 60/100; Loss: 0.056869037449359894\n",
      "Step 9 (2051); Episode 60/100; Loss: 0.01007444504648447\n",
      "Step 10 (2052); Episode 60/100; Loss: 0.06929244846105576\n",
      "Step 11 (2053); Episode 60/100; Loss: 0.03756430745124817\n",
      "Step 12 (2054); Episode 60/100; Loss: 0.052388161420822144\n",
      "Step 13 (2055); Episode 60/100; Loss: 0.02188216522336006\n",
      "Step 14 (2056); Episode 60/100; Loss: 0.09779883176088333\n",
      "Step 15 (2057); Episode 60/100; Loss: 0.10886753350496292\n",
      "Step 16 (2058); Episode 60/100; Loss: 0.08279948681592941\n",
      "Step 17 (2059); Episode 60/100; Loss: 0.05511806160211563\n",
      "Step 18 (2060); Episode 60/100; Loss: 0.07207276672124863\n",
      "Step 19 (2061); Episode 60/100; Loss: 0.14655181765556335\n",
      "Step 20 (2062); Episode 60/100; Loss: 0.11074700206518173\n",
      "Step 21 (2063); Episode 60/100; Loss: 0.0037282153498381376\n",
      "Step 22 (2064); Episode 60/100; Loss: 0.051320113241672516\n",
      "Step 23 (2065); Episode 60/100; Loss: 0.05959424749016762\n",
      "Step 24 (2066); Episode 60/100; Loss: 0.008703034371137619\n",
      "Step 25 (2067); Episode 60/100; Loss: 0.10396835207939148\n",
      "Step 26 (2068); Episode 60/100; Loss: 0.05013217031955719\n",
      "Step 27 (2069); Episode 60/100; Loss: 0.17887257039546967\n",
      "Step 28 (2070); Episode 60/100; Loss: 0.003532517934218049\n",
      "Step 29 (2071); Episode 60/100; Loss: 0.13256442546844482\n",
      "Step 30 (2072); Episode 60/100; Loss: 0.11072441935539246\n",
      "Step 31 (2073); Episode 60/100; Loss: 0.06973129510879517\n",
      "Step 32 (2074); Episode 60/100; Loss: 0.07722100615501404\n",
      "Step 33 (2075); Episode 60/100; Loss: 0.18633168935775757\n",
      "Step 34 (2076); Episode 60/100; Loss: 0.11340516805648804\n",
      "Step 35 (2077); Episode 60/100; Loss: 0.004638643469661474\n",
      "Step 36 (2078); Episode 60/100; Loss: 0.08141927421092987\n",
      "Step 37 (2079); Episode 60/100; Loss: 0.007604123558849096\n",
      "Step 38 (2080); Episode 60/100; Loss: 0.1650407910346985\n",
      "Step 39 (2081); Episode 60/100; Loss: 0.01022072322666645\n",
      "Step 40 (2082); Episode 60/100; Loss: 0.10619214177131653\n",
      "Step 41 (2083); Episode 60/100; Loss: 0.11760614067316055\n",
      "Step 42 (2084); Episode 60/100; Loss: 0.1811908781528473\n",
      "Step 43 (2085); Episode 60/100; Loss: 0.0414983406662941\n",
      "Step 44 (2086); Episode 60/100; Loss: 0.0068001397885382175\n",
      "Step 45 (2087); Episode 60/100; Loss: 0.011531159281730652\n",
      "Step 46 (2088); Episode 60/100; Loss: 0.08768678456544876\n",
      "Step 47 (2089); Episode 60/100; Loss: 0.1457592248916626\n",
      "Step 48 (2090); Episode 60/100; Loss: 0.09682118892669678\n",
      "Step 49 (2091); Episode 60/100; Loss: 0.1319967806339264\n",
      "Step 50 (2092); Episode 60/100; Loss: 0.1853538304567337\n",
      "Step 51 (2093); Episode 60/100; Loss: 0.03698178380727768\n",
      "Step 52 (2094); Episode 60/100; Loss: 0.0036780531518161297\n",
      "Step 53 (2095); Episode 60/100; Loss: 0.12679538130760193\n",
      "Step 54 (2096); Episode 60/100; Loss: 0.011115483939647675\n",
      "Step 55 (2097); Episode 60/100; Loss: 0.03168374300003052\n",
      "Step 56 (2098); Episode 60/100; Loss: 0.0033669571857899427\n",
      "Step 57 (2099); Episode 60/100; Loss: 0.008722088299691677\n",
      "Step 58 (2100); Episode 60/100; Loss: 0.13340993225574493\n",
      "Step 59 (2101); Episode 60/100; Loss: 0.06843419373035431\n",
      "Step 60 (2102); Episode 60/100; Loss: 0.18381085991859436\n",
      "Step 61 (2103); Episode 60/100; Loss: 0.04315827041864395\n",
      "Step 62 (2104); Episode 60/100; Loss: 0.15290337800979614\n",
      "Step 63 (2105); Episode 60/100; Loss: 0.06894337385892868\n",
      "Step 64 (2106); Episode 60/100; Loss: 0.05589766800403595\n",
      "Step 65 (2107); Episode 60/100; Loss: 0.15398751199245453\n",
      "Step 66 (2108); Episode 60/100; Loss: 0.15361915528774261\n",
      "Step 67 (2109); Episode 60/100; Loss: 0.17731980979442596\n",
      "Step 68 (2110); Episode 60/100; Loss: 0.033475734293460846\n",
      "Step 69 (2111); Episode 60/100; Loss: 0.12162435054779053\n",
      "Step 70 (2112); Episode 60/100; Loss: 0.10659992694854736\n",
      "Step 71 (2113); Episode 60/100; Loss: 0.10709785670042038\n",
      "Step 72 (2114); Episode 60/100; Loss: 0.05227360129356384\n",
      "Step 73 (2115); Episode 60/100; Loss: 0.09480621665716171\n",
      "Step 74 (2116); Episode 60/100; Loss: 0.15986201167106628\n",
      "Step 75 (2117); Episode 60/100; Loss: 0.09108994901180267\n",
      "Step 76 (2118); Episode 60/100; Loss: 0.04637441039085388\n",
      "Step 77 (2119); Episode 60/100; Loss: 0.05198788642883301\n",
      "Step 78 (2120); Episode 60/100; Loss: 0.05661291256546974\n",
      "Step 79 (2121); Episode 60/100; Loss: 0.08278115093708038\n",
      "Step 80 (2122); Episode 60/100; Loss: 0.04893742874264717\n",
      "Step 81 (2123); Episode 60/100; Loss: 0.08069010078907013\n",
      "Step 82 (2124); Episode 60/100; Loss: 0.09878088533878326\n",
      "Step 83 (2125); Episode 60/100; Loss: 0.04286042973399162\n",
      "Step 84 (2126); Episode 60/100; Loss: 0.058288536965847015\n",
      "Step 85 (2127); Episode 60/100; Loss: 0.15601509809494019\n",
      "Step 86 (2128); Episode 60/100; Loss: 0.027397841215133667\n",
      "Step 87 (2129); Episode 60/100; Loss: 0.04436438903212547\n",
      "Step 88 (2130); Episode 60/100; Loss: 0.04462570324540138\n",
      "Step 89 (2131); Episode 60/100; Loss: 0.13694345951080322\n",
      "Step 90 (2132); Episode 60/100; Loss: 0.06221625953912735\n",
      "Step 91 (2133); Episode 60/100; Loss: 0.00560366315767169\n",
      "Step 92 (2134); Episode 60/100; Loss: 0.041008200496435165\n",
      "Step 93 (2135); Episode 60/100; Loss: 0.07157327979803085\n",
      "Step 94 (2136); Episode 60/100; Loss: 0.005878971423953772\n",
      "Step 95 (2137); Episode 60/100; Loss: 0.17067138850688934\n",
      "Step 0 (2138); Episode 61/100; Loss: 0.05661824718117714\n",
      "Step 1 (2139); Episode 61/100; Loss: 0.123236283659935\n",
      "Step 2 (2140); Episode 61/100; Loss: 0.14040665328502655\n",
      "Step 3 (2141); Episode 61/100; Loss: 0.05964667350053787\n",
      "Step 4 (2142); Episode 61/100; Loss: 0.10349277406930923\n",
      "Step 5 (2143); Episode 61/100; Loss: 0.06286997348070145\n",
      "Step 6 (2144); Episode 61/100; Loss: 0.10285401344299316\n",
      "Step 7 (2145); Episode 61/100; Loss: 0.00819645170122385\n",
      "Step 8 (2146); Episode 61/100; Loss: 0.005885382182896137\n",
      "Step 9 (2147); Episode 61/100; Loss: 0.0048737674951553345\n",
      "Step 10 (2148); Episode 61/100; Loss: 0.1810203194618225\n",
      "Step 11 (2149); Episode 61/100; Loss: 0.17114081978797913\n",
      "Step 12 (2150); Episode 61/100; Loss: 0.09894905239343643\n",
      "Step 13 (2151); Episode 61/100; Loss: 0.04999849200248718\n",
      "Step 14 (2152); Episode 61/100; Loss: 0.10623212903738022\n",
      "Step 15 (2153); Episode 61/100; Loss: 0.023567121475934982\n",
      "Step 16 (2154); Episode 61/100; Loss: 0.0425214059650898\n",
      "Step 17 (2155); Episode 61/100; Loss: 0.0028652267064899206\n",
      "Step 18 (2156); Episode 61/100; Loss: 0.0708792582154274\n",
      "Step 19 (2157); Episode 61/100; Loss: 0.1069989874958992\n",
      "Step 20 (2158); Episode 61/100; Loss: 0.1381077617406845\n",
      "Step 21 (2159); Episode 61/100; Loss: 0.08450669795274734\n",
      "Step 22 (2160); Episode 61/100; Loss: 0.04244166612625122\n",
      "Step 23 (2161); Episode 61/100; Loss: 0.08877211809158325\n",
      "Step 24 (2162); Episode 61/100; Loss: 0.1516040861606598\n",
      "Step 25 (2163); Episode 61/100; Loss: 0.054528363049030304\n",
      "Step 26 (2164); Episode 61/100; Loss: 0.059008728712797165\n",
      "Step 27 (2165); Episode 61/100; Loss: 0.03347817808389664\n",
      "Step 28 (2166); Episode 61/100; Loss: 0.006913518067449331\n",
      "Step 29 (2167); Episode 61/100; Loss: 0.031433723866939545\n",
      "Step 30 (2168); Episode 61/100; Loss: 0.1723828762769699\n",
      "Step 31 (2169); Episode 61/100; Loss: 0.0047838869504630566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32 (2170); Episode 61/100; Loss: 0.04606226086616516\n",
      "Step 33 (2171); Episode 61/100; Loss: 0.0940147414803505\n",
      "Step 34 (2172); Episode 61/100; Loss: 0.10262447595596313\n",
      "Step 35 (2173); Episode 61/100; Loss: 0.006946584675461054\n",
      "Step 36 (2174); Episode 61/100; Loss: 0.13548150658607483\n",
      "Step 37 (2175); Episode 61/100; Loss: 0.0513484962284565\n",
      "Step 38 (2176); Episode 61/100; Loss: 0.07652676850557327\n",
      "Step 39 (2177); Episode 61/100; Loss: 0.05384806543588638\n",
      "Step 40 (2178); Episode 61/100; Loss: 0.004878248553723097\n",
      "Step 41 (2179); Episode 61/100; Loss: 0.03648548945784569\n",
      "Step 42 (2180); Episode 61/100; Loss: 0.05617839843034744\n",
      "Step 43 (2181); Episode 61/100; Loss: 0.10672318935394287\n",
      "Step 44 (2182); Episode 61/100; Loss: 0.1712334305047989\n",
      "Step 45 (2183); Episode 61/100; Loss: 0.056775014847517014\n",
      "Step 46 (2184); Episode 61/100; Loss: 0.005419410765171051\n",
      "Step 47 (2185); Episode 61/100; Loss: 0.12682434916496277\n",
      "Step 48 (2186); Episode 61/100; Loss: 0.0490739680826664\n",
      "Step 49 (2187); Episode 61/100; Loss: 0.0030611774418503046\n",
      "Step 50 (2188); Episode 61/100; Loss: 0.0038490237202495337\n",
      "Step 51 (2189); Episode 61/100; Loss: 0.10480715334415436\n",
      "Step 52 (2190); Episode 61/100; Loss: 0.002394848968833685\n",
      "Step 53 (2191); Episode 61/100; Loss: 0.09629539400339127\n",
      "Step 54 (2192); Episode 61/100; Loss: 0.0763363316655159\n",
      "Step 55 (2193); Episode 61/100; Loss: 0.15438462793827057\n",
      "Step 56 (2194); Episode 61/100; Loss: 0.0992417261004448\n",
      "Step 57 (2195); Episode 61/100; Loss: 0.0038889115676283836\n",
      "Step 58 (2196); Episode 61/100; Loss: 0.04412458837032318\n",
      "Step 59 (2197); Episode 61/100; Loss: 0.1481654942035675\n",
      "Step 60 (2198); Episode 61/100; Loss: 0.12369675934314728\n",
      "Step 61 (2199); Episode 61/100; Loss: 0.12464997172355652\n",
      "Step 62 (2200); Episode 61/100; Loss: 0.07221589982509613\n",
      "Step 63 (2201); Episode 61/100; Loss: 0.10323227941989899\n",
      "Step 64 (2202); Episode 61/100; Loss: 0.12659353017807007\n",
      "Step 65 (2203); Episode 61/100; Loss: 0.10158807784318924\n",
      "Step 66 (2204); Episode 61/100; Loss: 0.0513782799243927\n",
      "Step 67 (2205); Episode 61/100; Loss: 0.2253860980272293\n",
      "Step 68 (2206); Episode 61/100; Loss: 0.03881636634469032\n",
      "Step 69 (2207); Episode 61/100; Loss: 0.10103878378868103\n",
      "Step 70 (2208); Episode 61/100; Loss: 0.057952381670475006\n",
      "Step 71 (2209); Episode 61/100; Loss: 0.033932220190763474\n",
      "Step 72 (2210); Episode 61/100; Loss: 0.062225550413131714\n",
      "Step 73 (2211); Episode 61/100; Loss: 0.048820167779922485\n",
      "Step 74 (2212); Episode 61/100; Loss: 0.006537130102515221\n",
      "Step 75 (2213); Episode 61/100; Loss: 0.023017115890979767\n",
      "Step 76 (2214); Episode 61/100; Loss: 0.10055533796548843\n",
      "Step 77 (2215); Episode 61/100; Loss: 0.05080963298678398\n",
      "Step 78 (2216); Episode 61/100; Loss: 0.025516368448734283\n",
      "Step 79 (2217); Episode 61/100; Loss: 0.002321346430107951\n",
      "Step 80 (2218); Episode 61/100; Loss: 0.006797434762120247\n",
      "Step 81 (2219); Episode 61/100; Loss: 0.10144850611686707\n",
      "Step 82 (2220); Episode 61/100; Loss: 0.0080393236130476\n",
      "Step 83 (2221); Episode 61/100; Loss: 0.04209498316049576\n",
      "Step 84 (2222); Episode 61/100; Loss: 0.03599347174167633\n",
      "Step 85 (2223); Episode 61/100; Loss: 0.12507100403308868\n",
      "Step 86 (2224); Episode 61/100; Loss: 0.10392666608095169\n",
      "Step 87 (2225); Episode 61/100; Loss: 0.05479162186384201\n",
      "Step 88 (2226); Episode 61/100; Loss: 0.027916669845581055\n",
      "Step 89 (2227); Episode 61/100; Loss: 0.10912982374429703\n",
      "Step 90 (2228); Episode 61/100; Loss: 0.10538491606712341\n",
      "Step 91 (2229); Episode 61/100; Loss: 0.022029932588338852\n",
      "Step 92 (2230); Episode 61/100; Loss: 0.20460475981235504\n",
      "Step 93 (2231); Episode 61/100; Loss: 0.043150391429662704\n",
      "Step 94 (2232); Episode 61/100; Loss: 0.10316973179578781\n",
      "Step 95 (2233); Episode 61/100; Loss: 0.13506661355495453\n",
      "Step 96 (2234); Episode 61/100; Loss: 0.12128790467977524\n",
      "Step 97 (2235); Episode 61/100; Loss: 0.05903317406773567\n",
      "Step 98 (2236); Episode 61/100; Loss: 0.1932014375925064\n",
      "Step 99 (2237); Episode 61/100; Loss: 0.054210200905799866\n",
      "Step 100 (2238); Episode 61/100; Loss: 0.12313336133956909\n",
      "Step 101 (2239); Episode 61/100; Loss: 0.056090518832206726\n",
      "Step 102 (2240); Episode 61/100; Loss: 0.10045386850833893\n",
      "Step 103 (2241); Episode 61/100; Loss: 0.02910533733665943\n",
      "Step 104 (2242); Episode 61/100; Loss: 0.07924648374319077\n",
      "Step 105 (2243); Episode 61/100; Loss: 0.22059427201747894\n",
      "Step 106 (2244); Episode 61/100; Loss: 0.049197081476449966\n",
      "Step 107 (2245); Episode 61/100; Loss: 0.02504698559641838\n",
      "Step 108 (2246); Episode 61/100; Loss: 0.035871945321559906\n",
      "Step 109 (2247); Episode 61/100; Loss: 0.07270757853984833\n",
      "Step 110 (2248); Episode 61/100; Loss: 0.07627078145742416\n",
      "Step 111 (2249); Episode 61/100; Loss: 0.05087849870324135\n",
      "Step 112 (2250); Episode 61/100; Loss: 0.02612781897187233\n",
      "Step 113 (2251); Episode 61/100; Loss: 0.12289430946111679\n",
      "Step 114 (2252); Episode 61/100; Loss: 0.03456096351146698\n",
      "Step 115 (2253); Episode 61/100; Loss: 0.048724059015512466\n",
      "Step 116 (2254); Episode 61/100; Loss: 0.023718204349279404\n",
      "Step 117 (2255); Episode 61/100; Loss: 0.09402447938919067\n",
      "Step 118 (2256); Episode 61/100; Loss: 0.11108691245317459\n",
      "Step 119 (2257); Episode 61/100; Loss: 0.04641706496477127\n",
      "Step 120 (2258); Episode 61/100; Loss: 0.029263844713568687\n",
      "Step 121 (2259); Episode 61/100; Loss: 0.13261151313781738\n",
      "Step 122 (2260); Episode 61/100; Loss: 0.1575508862733841\n",
      "Step 123 (2261); Episode 61/100; Loss: 0.10055360198020935\n",
      "Step 124 (2262); Episode 61/100; Loss: 0.13539335131645203\n",
      "Step 125 (2263); Episode 61/100; Loss: 0.15273183584213257\n",
      "Step 126 (2264); Episode 61/100; Loss: 0.0053534796461462975\n",
      "Step 127 (2265); Episode 61/100; Loss: 0.2136945277452469\n",
      "Step 128 (2266); Episode 61/100; Loss: 0.060567524284124374\n",
      "Step 129 (2267); Episode 61/100; Loss: 0.005104581359773874\n",
      "Step 130 (2268); Episode 61/100; Loss: 0.09195644408464432\n",
      "Step 131 (2269); Episode 61/100; Loss: 0.007339025381952524\n",
      "Step 132 (2270); Episode 61/100; Loss: 0.05406202748417854\n",
      "Step 133 (2271); Episode 61/100; Loss: 0.0036165565252304077\n",
      "Step 134 (2272); Episode 61/100; Loss: 0.05338393896818161\n",
      "Step 135 (2273); Episode 61/100; Loss: 0.03738270327448845\n",
      "Step 136 (2274); Episode 61/100; Loss: 0.04841770976781845\n",
      "Step 137 (2275); Episode 61/100; Loss: 0.0750032365322113\n",
      "Step 138 (2276); Episode 61/100; Loss: 0.03373917192220688\n",
      "Step 139 (2277); Episode 61/100; Loss: 0.05148676782846451\n",
      "Step 140 (2278); Episode 61/100; Loss: 0.06935620307922363\n",
      "Step 141 (2279); Episode 61/100; Loss: 0.07537815719842911\n",
      "Step 142 (2280); Episode 61/100; Loss: 0.06304581463336945\n",
      "Step 143 (2281); Episode 61/100; Loss: 0.030107641592621803\n",
      "Step 144 (2282); Episode 61/100; Loss: 0.04986221715807915\n",
      "Step 145 (2283); Episode 61/100; Loss: 0.010115238837897778\n",
      "Step 146 (2284); Episode 61/100; Loss: 0.09676770865917206\n",
      "Step 147 (2285); Episode 61/100; Loss: 0.10021377354860306\n",
      "Step 148 (2286); Episode 61/100; Loss: 0.07438454031944275\n",
      "Step 149 (2287); Episode 61/100; Loss: 0.026523767039179802\n",
      "Step 150 (2288); Episode 61/100; Loss: 0.0017510158941149712\n",
      "Step 151 (2289); Episode 61/100; Loss: 0.07374870777130127\n",
      "Step 152 (2290); Episode 61/100; Loss: 0.04235260933637619\n",
      "Step 153 (2291); Episode 61/100; Loss: 0.02502487599849701\n",
      "Step 154 (2292); Episode 61/100; Loss: 0.0799766257405281\n",
      "Step 155 (2293); Episode 61/100; Loss: 0.168191596865654\n",
      "Step 156 (2294); Episode 61/100; Loss: 0.09238805621862411\n",
      "Step 157 (2295); Episode 61/100; Loss: 0.10076659172773361\n",
      "Step 158 (2296); Episode 61/100; Loss: 0.046796075999736786\n",
      "Step 159 (2297); Episode 61/100; Loss: 0.07502913475036621\n",
      "Step 160 (2298); Episode 61/100; Loss: 0.09266920387744904\n",
      "Step 161 (2299); Episode 61/100; Loss: 0.06122937798500061\n",
      "Step 162 (2300); Episode 61/100; Loss: 0.09778251498937607\n",
      "Step 163 (2301); Episode 61/100; Loss: 0.07587631046772003\n",
      "Step 164 (2302); Episode 61/100; Loss: 0.055760156363248825\n",
      "Step 165 (2303); Episode 61/100; Loss: 0.14179471135139465\n",
      "Step 166 (2304); Episode 61/100; Loss: 0.06854470074176788\n",
      "Step 167 (2305); Episode 61/100; Loss: 0.053511153906583786\n",
      "Step 168 (2306); Episode 61/100; Loss: 0.08726534247398376\n",
      "Step 169 (2307); Episode 61/100; Loss: 0.09729675948619843\n",
      "Step 170 (2308); Episode 61/100; Loss: 0.08389582484960556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 171 (2309); Episode 61/100; Loss: 0.11475320905447006\n",
      "Step 172 (2310); Episode 61/100; Loss: 0.04141053929924965\n",
      "Step 173 (2311); Episode 61/100; Loss: 0.11162854731082916\n",
      "Step 174 (2312); Episode 61/100; Loss: 0.06533949077129364\n",
      "Step 175 (2313); Episode 61/100; Loss: 0.14504438638687134\n",
      "Step 176 (2314); Episode 61/100; Loss: 0.08281158655881882\n",
      "Step 177 (2315); Episode 61/100; Loss: 0.0032428603153675795\n",
      "Step 178 (2316); Episode 61/100; Loss: 0.0036077070981264114\n",
      "Step 179 (2317); Episode 61/100; Loss: 0.10942470282316208\n",
      "Step 180 (2318); Episode 61/100; Loss: 0.05474141612648964\n",
      "Step 181 (2319); Episode 61/100; Loss: 0.120457723736763\n",
      "Step 182 (2320); Episode 61/100; Loss: 0.04523826017975807\n",
      "Step 183 (2321); Episode 61/100; Loss: 0.03391248732805252\n",
      "Step 184 (2322); Episode 61/100; Loss: 0.006083034910261631\n",
      "Step 185 (2323); Episode 61/100; Loss: 0.053514063358306885\n",
      "Step 0 (2324); Episode 62/100; Loss: 0.08712883293628693\n",
      "Step 1 (2325); Episode 62/100; Loss: 0.15925590693950653\n",
      "Step 2 (2326); Episode 62/100; Loss: 0.08722605556249619\n",
      "Step 3 (2327); Episode 62/100; Loss: 0.0034951181150972843\n",
      "Step 4 (2328); Episode 62/100; Loss: 0.07591847330331802\n",
      "Step 5 (2329); Episode 62/100; Loss: 0.07177325338125229\n",
      "Step 6 (2330); Episode 62/100; Loss: 0.08575844764709473\n",
      "Step 7 (2331); Episode 62/100; Loss: 0.02826736494898796\n",
      "Step 8 (2332); Episode 62/100; Loss: 0.10106749087572098\n",
      "Step 9 (2333); Episode 62/100; Loss: 0.00978063978254795\n",
      "Step 10 (2334); Episode 62/100; Loss: 0.053056031465530396\n",
      "Step 11 (2335); Episode 62/100; Loss: 0.05264917016029358\n",
      "Step 12 (2336); Episode 62/100; Loss: 0.012219775468111038\n",
      "Step 13 (2337); Episode 62/100; Loss: 0.056324027478694916\n",
      "Step 14 (2338); Episode 62/100; Loss: 0.029860645532608032\n",
      "Step 15 (2339); Episode 62/100; Loss: 0.051220450550317764\n",
      "Step 16 (2340); Episode 62/100; Loss: 0.07811680436134338\n",
      "Step 17 (2341); Episode 62/100; Loss: 0.10028402507305145\n",
      "Step 18 (2342); Episode 62/100; Loss: 0.0053515248000621796\n",
      "Step 19 (2343); Episode 62/100; Loss: 0.0021274061873555183\n",
      "Step 20 (2344); Episode 62/100; Loss: 0.005558317061513662\n",
      "Step 21 (2345); Episode 62/100; Loss: 0.10217555612325668\n",
      "Step 22 (2346); Episode 62/100; Loss: 0.04575322940945625\n",
      "Step 23 (2347); Episode 62/100; Loss: 0.10100186616182327\n",
      "Step 24 (2348); Episode 62/100; Loss: 0.17355620861053467\n",
      "Step 25 (2349); Episode 62/100; Loss: 0.1665644496679306\n",
      "Step 26 (2350); Episode 62/100; Loss: 0.0037357518449425697\n",
      "Step 27 (2351); Episode 62/100; Loss: 0.14658090472221375\n",
      "Step 28 (2352); Episode 62/100; Loss: 0.08617256581783295\n",
      "Step 29 (2353); Episode 62/100; Loss: 0.04744458571076393\n",
      "Step 30 (2354); Episode 62/100; Loss: 0.004051114432513714\n",
      "Step 31 (2355); Episode 62/100; Loss: 0.19808918237686157\n",
      "Step 32 (2356); Episode 62/100; Loss: 0.05067887157201767\n",
      "Step 33 (2357); Episode 62/100; Loss: 0.0033597315195947886\n",
      "Step 34 (2358); Episode 62/100; Loss: 0.03580806031823158\n",
      "Step 35 (2359); Episode 62/100; Loss: 0.02465970441699028\n",
      "Step 36 (2360); Episode 62/100; Loss: 0.051588598638772964\n",
      "Step 37 (2361); Episode 62/100; Loss: 0.19916518032550812\n",
      "Step 38 (2362); Episode 62/100; Loss: 0.08013471961021423\n",
      "Step 39 (2363); Episode 62/100; Loss: 0.03884322941303253\n",
      "Step 40 (2364); Episode 62/100; Loss: 0.1795770674943924\n",
      "Step 41 (2365); Episode 62/100; Loss: 0.08738034963607788\n",
      "Step 42 (2366); Episode 62/100; Loss: 0.0761367455124855\n",
      "Step 43 (2367); Episode 62/100; Loss: 0.009100721217691898\n",
      "Step 44 (2368); Episode 62/100; Loss: 0.051180560141801834\n",
      "Step 45 (2369); Episode 62/100; Loss: 0.04125464707612991\n",
      "Step 46 (2370); Episode 62/100; Loss: 0.12476615607738495\n",
      "Step 47 (2371); Episode 62/100; Loss: 0.05747358128428459\n",
      "Step 48 (2372); Episode 62/100; Loss: 0.006720925681293011\n",
      "Step 49 (2373); Episode 62/100; Loss: 0.07550622522830963\n",
      "Step 50 (2374); Episode 62/100; Loss: 0.031583692878484726\n",
      "Step 51 (2375); Episode 62/100; Loss: 0.08037445694208145\n",
      "Step 52 (2376); Episode 62/100; Loss: 0.04665321856737137\n",
      "Step 53 (2377); Episode 62/100; Loss: 0.02348286472260952\n",
      "Step 54 (2378); Episode 62/100; Loss: 0.1364545226097107\n",
      "Step 55 (2379); Episode 62/100; Loss: 0.09534429013729095\n",
      "Step 56 (2380); Episode 62/100; Loss: 0.009296093136072159\n",
      "Step 57 (2381); Episode 62/100; Loss: 0.0025264928117394447\n",
      "Step 58 (2382); Episode 62/100; Loss: 0.1756158322095871\n",
      "Step 59 (2383); Episode 62/100; Loss: 0.06023925170302391\n",
      "Step 60 (2384); Episode 62/100; Loss: 0.08332248777151108\n",
      "Step 61 (2385); Episode 62/100; Loss: 0.07129846513271332\n",
      "Step 62 (2386); Episode 62/100; Loss: 0.06523208320140839\n",
      "Step 63 (2387); Episode 62/100; Loss: 0.08317922800779343\n",
      "Step 64 (2388); Episode 62/100; Loss: 0.13932356238365173\n",
      "Step 65 (2389); Episode 62/100; Loss: 0.0757201611995697\n",
      "Step 66 (2390); Episode 62/100; Loss: 0.027564898133277893\n",
      "Step 67 (2391); Episode 62/100; Loss: 0.0018858208786696196\n",
      "Step 68 (2392); Episode 62/100; Loss: 0.24641354382038116\n",
      "Step 69 (2393); Episode 62/100; Loss: 0.057406313717365265\n",
      "Step 70 (2394); Episode 62/100; Loss: 0.0031825490295886993\n",
      "Step 71 (2395); Episode 62/100; Loss: 0.07537104934453964\n",
      "Step 72 (2396); Episode 62/100; Loss: 0.03020961582660675\n",
      "Step 73 (2397); Episode 62/100; Loss: 0.08035178482532501\n",
      "Step 74 (2398); Episode 62/100; Loss: 0.09257582575082779\n",
      "Step 75 (2399); Episode 62/100; Loss: 0.049357131123542786\n",
      "Step 76 (2400); Episode 62/100; Loss: 0.05252476781606674\n",
      "Step 77 (2401); Episode 62/100; Loss: 0.1580163687467575\n",
      "Step 78 (2402); Episode 62/100; Loss: 0.12365102022886276\n",
      "Step 79 (2403); Episode 62/100; Loss: 0.06087503954768181\n",
      "Step 80 (2404); Episode 62/100; Loss: 0.05158400535583496\n",
      "Step 81 (2405); Episode 62/100; Loss: 0.11285172402858734\n",
      "Step 82 (2406); Episode 62/100; Loss: 0.0688929334282875\n",
      "Step 83 (2407); Episode 62/100; Loss: 0.12460970133543015\n",
      "Step 84 (2408); Episode 62/100; Loss: 0.008505444973707199\n",
      "Step 85 (2409); Episode 62/100; Loss: 0.04965157061815262\n",
      "Step 86 (2410); Episode 62/100; Loss: 0.0025107148103415966\n",
      "Step 87 (2411); Episode 62/100; Loss: 0.1887778490781784\n",
      "Step 88 (2412); Episode 62/100; Loss: 0.08468960970640182\n",
      "Step 89 (2413); Episode 62/100; Loss: 0.05971461161971092\n",
      "Step 90 (2414); Episode 62/100; Loss: 0.0688387006521225\n",
      "Step 91 (2415); Episode 62/100; Loss: 0.09132253378629684\n",
      "Step 92 (2416); Episode 62/100; Loss: 0.00241440930403769\n",
      "Step 93 (2417); Episode 62/100; Loss: 0.07388310134410858\n",
      "Step 94 (2418); Episode 62/100; Loss: 0.02776462212204933\n",
      "Step 95 (2419); Episode 62/100; Loss: 0.1182115375995636\n",
      "Step 96 (2420); Episode 62/100; Loss: 0.056231435388326645\n",
      "Step 97 (2421); Episode 62/100; Loss: 0.004419523756951094\n",
      "Step 98 (2422); Episode 62/100; Loss: 0.13953346014022827\n",
      "Step 99 (2423); Episode 62/100; Loss: 0.05953116714954376\n",
      "Step 100 (2424); Episode 62/100; Loss: 0.042648229748010635\n",
      "Step 101 (2425); Episode 62/100; Loss: 0.02950255572795868\n",
      "Step 102 (2426); Episode 62/100; Loss: 0.02079596556723118\n",
      "Step 103 (2427); Episode 62/100; Loss: 0.14098617434501648\n",
      "Step 104 (2428); Episode 62/100; Loss: 0.06630434095859528\n",
      "Step 105 (2429); Episode 62/100; Loss: 0.07077889144420624\n",
      "Step 106 (2430); Episode 62/100; Loss: 0.0015236662002280354\n",
      "Step 107 (2431); Episode 62/100; Loss: 0.0072523062117397785\n",
      "Step 108 (2432); Episode 62/100; Loss: 0.0996989756822586\n",
      "Step 109 (2433); Episode 62/100; Loss: 0.005808745510876179\n",
      "Step 110 (2434); Episode 62/100; Loss: 0.08507656306028366\n",
      "Step 111 (2435); Episode 62/100; Loss: 0.047044213861227036\n",
      "Step 112 (2436); Episode 62/100; Loss: 0.1167668029665947\n",
      "Step 113 (2437); Episode 62/100; Loss: 0.04036478325724602\n",
      "Step 114 (2438); Episode 62/100; Loss: 0.0027873399667441845\n",
      "Step 115 (2439); Episode 62/100; Loss: 0.11155454069375992\n",
      "Step 116 (2440); Episode 62/100; Loss: 0.0506378710269928\n",
      "Step 117 (2441); Episode 62/100; Loss: 0.002743750112131238\n",
      "Step 118 (2442); Episode 62/100; Loss: 0.07269170135259628\n",
      "Step 119 (2443); Episode 62/100; Loss: 0.002986947074532509\n",
      "Step 120 (2444); Episode 62/100; Loss: 0.11316342651844025\n",
      "Step 121 (2445); Episode 62/100; Loss: 0.09863351285457611\n",
      "Step 122 (2446); Episode 62/100; Loss: 0.0062547968700528145\n",
      "Step 123 (2447); Episode 62/100; Loss: 0.047183167189359665\n",
      "Step 124 (2448); Episode 62/100; Loss: 0.10121309757232666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 125 (2449); Episode 62/100; Loss: 0.007957867346704006\n",
      "Step 126 (2450); Episode 62/100; Loss: 0.0521966889500618\n",
      "Step 127 (2451); Episode 62/100; Loss: 0.053642041981220245\n",
      "Step 128 (2452); Episode 62/100; Loss: 0.056530509144067764\n",
      "Step 129 (2453); Episode 62/100; Loss: 0.03977048024535179\n",
      "Step 130 (2454); Episode 62/100; Loss: 0.08306454867124557\n",
      "Step 131 (2455); Episode 62/100; Loss: 0.1193418800830841\n",
      "Step 132 (2456); Episode 62/100; Loss: 0.029355665668845177\n",
      "Step 133 (2457); Episode 62/100; Loss: 0.07828743755817413\n",
      "Step 134 (2458); Episode 62/100; Loss: 0.002891512354835868\n",
      "Step 135 (2459); Episode 62/100; Loss: 0.06817799061536789\n",
      "Step 136 (2460); Episode 62/100; Loss: 0.0554950051009655\n",
      "Step 137 (2461); Episode 62/100; Loss: 0.0911211296916008\n",
      "Step 138 (2462); Episode 62/100; Loss: 0.0024926778860390186\n",
      "Step 139 (2463); Episode 62/100; Loss: 0.0864267647266388\n",
      "Step 140 (2464); Episode 62/100; Loss: 0.0038222521543502808\n",
      "Step 141 (2465); Episode 62/100; Loss: 0.04095211252570152\n",
      "Step 142 (2466); Episode 62/100; Loss: 0.03641725331544876\n",
      "Step 143 (2467); Episode 62/100; Loss: 0.0024299907963722944\n",
      "Step 144 (2468); Episode 62/100; Loss: 0.0042914957739412785\n",
      "Step 145 (2469); Episode 62/100; Loss: 0.003756612306460738\n",
      "Step 146 (2470); Episode 62/100; Loss: 0.002286324743181467\n",
      "Step 147 (2471); Episode 62/100; Loss: 0.038376063108444214\n",
      "Step 148 (2472); Episode 62/100; Loss: 0.06152762472629547\n",
      "Step 149 (2473); Episode 62/100; Loss: 0.020637165755033493\n",
      "Step 150 (2474); Episode 62/100; Loss: 0.010551807470619678\n",
      "Step 151 (2475); Episode 62/100; Loss: 0.002329985611140728\n",
      "Step 152 (2476); Episode 62/100; Loss: 0.005192597862333059\n",
      "Step 153 (2477); Episode 62/100; Loss: 0.07551368325948715\n",
      "Step 154 (2478); Episode 62/100; Loss: 0.10899336636066437\n",
      "Step 155 (2479); Episode 62/100; Loss: 0.05931152030825615\n",
      "Step 156 (2480); Episode 62/100; Loss: 0.006869905162602663\n",
      "Step 157 (2481); Episode 62/100; Loss: 0.038417182862758636\n",
      "Step 158 (2482); Episode 62/100; Loss: 0.10387934744358063\n",
      "Step 159 (2483); Episode 62/100; Loss: 0.0030954277608543634\n",
      "Step 160 (2484); Episode 62/100; Loss: 0.05085407570004463\n",
      "Step 161 (2485); Episode 62/100; Loss: 0.07360523194074631\n",
      "Step 162 (2486); Episode 62/100; Loss: 0.052499186247587204\n",
      "Step 163 (2487); Episode 62/100; Loss: 0.047571685165166855\n",
      "Step 164 (2488); Episode 62/100; Loss: 0.0033474527299404144\n",
      "Step 165 (2489); Episode 62/100; Loss: 0.005020904820412397\n",
      "Step 166 (2490); Episode 62/100; Loss: 0.06152414530515671\n",
      "Step 167 (2491); Episode 62/100; Loss: 0.07232600450515747\n",
      "Step 168 (2492); Episode 62/100; Loss: 0.08593463152647018\n",
      "Step 169 (2493); Episode 62/100; Loss: 0.061347778886556625\n",
      "Step 170 (2494); Episode 62/100; Loss: 0.08582247048616409\n",
      "Step 171 (2495); Episode 62/100; Loss: 0.050531599670648575\n",
      "Step 172 (2496); Episode 62/100; Loss: 0.08738437294960022\n",
      "Step 173 (2497); Episode 62/100; Loss: 0.05127648264169693\n",
      "Step 174 (2498); Episode 62/100; Loss: 0.0027376313228160143\n",
      "Step 175 (2499); Episode 62/100; Loss: 0.0025625904090702534\n",
      "Step 176 (2500); Episode 62/100; Loss: 0.1110878586769104\n",
      "Step 177 (2501); Episode 62/100; Loss: 0.1391051560640335\n",
      "Step 178 (2502); Episode 62/100; Loss: 0.08706431835889816\n",
      "Step 179 (2503); Episode 62/100; Loss: 0.09718944877386093\n",
      "Step 180 (2504); Episode 62/100; Loss: 0.005437924526631832\n",
      "Step 181 (2505); Episode 62/100; Loss: 0.030240094289183617\n",
      "Step 182 (2506); Episode 62/100; Loss: 0.07235324382781982\n",
      "Step 183 (2507); Episode 62/100; Loss: 0.09784425050020218\n",
      "Step 184 (2508); Episode 62/100; Loss: 0.08727545291185379\n",
      "Step 185 (2509); Episode 62/100; Loss: 0.0031225825659930706\n",
      "Step 186 (2510); Episode 62/100; Loss: 0.18020279705524445\n",
      "Step 187 (2511); Episode 62/100; Loss: 0.13594968616962433\n",
      "Step 188 (2512); Episode 62/100; Loss: 0.0021731341257691383\n",
      "Step 189 (2513); Episode 62/100; Loss: 0.04698025435209274\n",
      "Step 190 (2514); Episode 62/100; Loss: 0.02346034348011017\n",
      "Step 0 (2515); Episode 63/100; Loss: 0.21527548134326935\n",
      "Step 1 (2516); Episode 63/100; Loss: 0.0498107373714447\n",
      "Step 2 (2517); Episode 63/100; Loss: 0.028222156688570976\n",
      "Step 3 (2518); Episode 63/100; Loss: 0.0034590736031532288\n",
      "Step 4 (2519); Episode 63/100; Loss: 0.11419860273599625\n",
      "Step 5 (2520); Episode 63/100; Loss: 0.039154645055532455\n",
      "Step 6 (2521); Episode 63/100; Loss: 0.008100565522909164\n",
      "Step 7 (2522); Episode 63/100; Loss: 0.09135232120752335\n",
      "Step 8 (2523); Episode 63/100; Loss: 0.03848487138748169\n",
      "Step 9 (2524); Episode 63/100; Loss: 0.08149304986000061\n",
      "Step 10 (2525); Episode 63/100; Loss: 0.06035887449979782\n",
      "Step 11 (2526); Episode 63/100; Loss: 0.028767608106136322\n",
      "Step 12 (2527); Episode 63/100; Loss: 0.034843217581510544\n",
      "Step 13 (2528); Episode 63/100; Loss: 0.08809070289134979\n",
      "Step 14 (2529); Episode 63/100; Loss: 0.006582638248801231\n",
      "Step 15 (2530); Episode 63/100; Loss: 0.08620141446590424\n",
      "Step 16 (2531); Episode 63/100; Loss: 0.10693832486867905\n",
      "Step 17 (2532); Episode 63/100; Loss: 0.047527361661195755\n",
      "Step 18 (2533); Episode 63/100; Loss: 0.003216484561562538\n",
      "Step 19 (2534); Episode 63/100; Loss: 0.04581579193472862\n",
      "Step 20 (2535); Episode 63/100; Loss: 0.09373638033866882\n",
      "Step 21 (2536); Episode 63/100; Loss: 0.07250182330608368\n",
      "Step 22 (2537); Episode 63/100; Loss: 0.1269802749156952\n",
      "Step 23 (2538); Episode 63/100; Loss: 0.12264270335435867\n",
      "Step 24 (2539); Episode 63/100; Loss: 0.001954341772943735\n",
      "Step 25 (2540); Episode 63/100; Loss: 0.09763987362384796\n",
      "Step 26 (2541); Episode 63/100; Loss: 0.03929496183991432\n",
      "Step 27 (2542); Episode 63/100; Loss: 0.004684637766331434\n",
      "Step 28 (2543); Episode 63/100; Loss: 0.08302051573991776\n",
      "Step 29 (2544); Episode 63/100; Loss: 0.0832720547914505\n",
      "Step 30 (2545); Episode 63/100; Loss: 0.006246894132345915\n",
      "Step 31 (2546); Episode 63/100; Loss: 0.046973977237939835\n",
      "Step 32 (2547); Episode 63/100; Loss: 0.03537765145301819\n",
      "Step 33 (2548); Episode 63/100; Loss: 0.03019234724342823\n",
      "Step 34 (2549); Episode 63/100; Loss: 0.048910606652498245\n",
      "Step 35 (2550); Episode 63/100; Loss: 0.09102080762386322\n",
      "Step 36 (2551); Episode 63/100; Loss: 0.03705756366252899\n",
      "Step 37 (2552); Episode 63/100; Loss: 0.12411924451589584\n",
      "Step 38 (2553); Episode 63/100; Loss: 0.008693180046975613\n",
      "Step 39 (2554); Episode 63/100; Loss: 0.04328945651650429\n",
      "Step 40 (2555); Episode 63/100; Loss: 0.022785678505897522\n",
      "Step 41 (2556); Episode 63/100; Loss: 0.05136265605688095\n",
      "Step 42 (2557); Episode 63/100; Loss: 0.0771087110042572\n",
      "Step 43 (2558); Episode 63/100; Loss: 0.1550924926996231\n",
      "Step 44 (2559); Episode 63/100; Loss: 0.006011240184307098\n",
      "Step 45 (2560); Episode 63/100; Loss: 0.08540667593479156\n",
      "Step 46 (2561); Episode 63/100; Loss: 0.007562029175460339\n",
      "Step 47 (2562); Episode 63/100; Loss: 0.03622760623693466\n",
      "Step 48 (2563); Episode 63/100; Loss: 0.003855843795463443\n",
      "Step 49 (2564); Episode 63/100; Loss: 0.0523291751742363\n",
      "Step 50 (2565); Episode 63/100; Loss: 0.03640945628285408\n",
      "Step 51 (2566); Episode 63/100; Loss: 0.05081844702363014\n",
      "Step 52 (2567); Episode 63/100; Loss: 0.048016566783189774\n",
      "Step 53 (2568); Episode 63/100; Loss: 0.055604130029678345\n",
      "Step 54 (2569); Episode 63/100; Loss: 0.10889049619436264\n",
      "Step 55 (2570); Episode 63/100; Loss: 0.03951083868741989\n",
      "Step 56 (2571); Episode 63/100; Loss: 0.09314406663179398\n",
      "Step 57 (2572); Episode 63/100; Loss: 0.147642582654953\n",
      "Step 58 (2573); Episode 63/100; Loss: 0.10040012001991272\n",
      "Step 59 (2574); Episode 63/100; Loss: 0.05716484785079956\n",
      "Step 60 (2575); Episode 63/100; Loss: 0.10096751898527145\n",
      "Step 61 (2576); Episode 63/100; Loss: 0.005884348880499601\n",
      "Step 62 (2577); Episode 63/100; Loss: 0.004647570196539164\n",
      "Step 63 (2578); Episode 63/100; Loss: 0.006721844896674156\n",
      "Step 64 (2579); Episode 63/100; Loss: 0.05260474979877472\n",
      "Step 65 (2580); Episode 63/100; Loss: 0.09496085345745087\n",
      "Step 66 (2581); Episode 63/100; Loss: 0.04308316484093666\n",
      "Step 67 (2582); Episode 63/100; Loss: 0.07639259099960327\n",
      "Step 68 (2583); Episode 63/100; Loss: 0.07036329060792923\n",
      "Step 69 (2584); Episode 63/100; Loss: 0.04730749502778053\n",
      "Step 70 (2585); Episode 63/100; Loss: 0.03553667291998863\n",
      "Step 71 (2586); Episode 63/100; Loss: 0.05720178037881851\n",
      "Step 72 (2587); Episode 63/100; Loss: 0.047118041664361954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 73 (2588); Episode 63/100; Loss: 0.03485005721449852\n",
      "Step 74 (2589); Episode 63/100; Loss: 0.07400108873844147\n",
      "Step 75 (2590); Episode 63/100; Loss: 0.04901997745037079\n",
      "Step 76 (2591); Episode 63/100; Loss: 0.04467935860157013\n",
      "Step 77 (2592); Episode 63/100; Loss: 0.04823235794901848\n",
      "Step 78 (2593); Episode 63/100; Loss: 0.0301642045378685\n",
      "Step 79 (2594); Episode 63/100; Loss: 0.005633378401398659\n",
      "Step 80 (2595); Episode 63/100; Loss: 0.08191481977701187\n",
      "Step 81 (2596); Episode 63/100; Loss: 0.07556189596652985\n",
      "Step 82 (2597); Episode 63/100; Loss: 0.007793079596012831\n",
      "Step 83 (2598); Episode 63/100; Loss: 0.08548110723495483\n",
      "Step 84 (2599); Episode 63/100; Loss: 0.06282611191272736\n",
      "Step 85 (2600); Episode 63/100; Loss: 0.09257208555936813\n",
      "Step 86 (2601); Episode 63/100; Loss: 0.03722851723432541\n",
      "Step 87 (2602); Episode 63/100; Loss: 0.004159015603363514\n",
      "Step 88 (2603); Episode 63/100; Loss: 0.09340381622314453\n",
      "Step 89 (2604); Episode 63/100; Loss: 0.030716298148036003\n",
      "Step 90 (2605); Episode 63/100; Loss: 0.04669053852558136\n",
      "Step 91 (2606); Episode 63/100; Loss: 0.1171966940164566\n",
      "Step 92 (2607); Episode 63/100; Loss: 0.20189762115478516\n",
      "Step 93 (2608); Episode 63/100; Loss: 0.08884155005216599\n",
      "Step 94 (2609); Episode 63/100; Loss: 0.061054203659296036\n",
      "Step 95 (2610); Episode 63/100; Loss: 0.004885765258222818\n",
      "Step 96 (2611); Episode 63/100; Loss: 0.05409153923392296\n",
      "Step 97 (2612); Episode 63/100; Loss: 0.09053287655115128\n",
      "Step 98 (2613); Episode 63/100; Loss: 0.0021430079359561205\n",
      "Step 99 (2614); Episode 63/100; Loss: 0.07756432890892029\n",
      "Step 100 (2615); Episode 63/100; Loss: 0.04606112092733383\n",
      "Step 101 (2616); Episode 63/100; Loss: 0.051255423575639725\n",
      "Step 102 (2617); Episode 63/100; Loss: 0.005113942548632622\n",
      "Step 103 (2618); Episode 63/100; Loss: 0.10838522017002106\n",
      "Step 104 (2619); Episode 63/100; Loss: 0.12923863530158997\n",
      "Step 105 (2620); Episode 63/100; Loss: 0.09118504822254181\n",
      "Step 0 (2621); Episode 64/100; Loss: 0.0030768809374421835\n",
      "Step 1 (2622); Episode 64/100; Loss: 0.09320206195116043\n",
      "Step 2 (2623); Episode 64/100; Loss: 0.16304826736450195\n",
      "Step 3 (2624); Episode 64/100; Loss: 0.10925861448049545\n",
      "Step 4 (2625); Episode 64/100; Loss: 0.07254406064748764\n",
      "Step 5 (2626); Episode 64/100; Loss: 0.12067157030105591\n",
      "Step 6 (2627); Episode 64/100; Loss: 0.11147133260965347\n",
      "Step 7 (2628); Episode 64/100; Loss: 0.022073477506637573\n",
      "Step 8 (2629); Episode 64/100; Loss: 0.1610066443681717\n",
      "Step 9 (2630); Episode 64/100; Loss: 0.044221919029951096\n",
      "Step 10 (2631); Episode 64/100; Loss: 0.0749669075012207\n",
      "Step 11 (2632); Episode 64/100; Loss: 0.003863323014229536\n",
      "Step 12 (2633); Episode 64/100; Loss: 0.0275978185236454\n",
      "Step 13 (2634); Episode 64/100; Loss: 0.09310588240623474\n",
      "Step 14 (2635); Episode 64/100; Loss: 0.09978669136762619\n",
      "Step 15 (2636); Episode 64/100; Loss: 0.04665957763791084\n",
      "Step 16 (2637); Episode 64/100; Loss: 0.09295830130577087\n",
      "Step 17 (2638); Episode 64/100; Loss: 0.009440053254365921\n",
      "Step 18 (2639); Episode 64/100; Loss: 0.03670884296298027\n",
      "Step 19 (2640); Episode 64/100; Loss: 0.09122996032238007\n",
      "Step 20 (2641); Episode 64/100; Loss: 0.03354588896036148\n",
      "Step 21 (2642); Episode 64/100; Loss: 0.04968291148543358\n",
      "Step 22 (2643); Episode 64/100; Loss: 0.11747769266366959\n",
      "Step 23 (2644); Episode 64/100; Loss: 0.06401073187589645\n",
      "Step 24 (2645); Episode 64/100; Loss: 0.03182310238480568\n",
      "Step 25 (2646); Episode 64/100; Loss: 0.0032453357707709074\n",
      "Step 26 (2647); Episode 64/100; Loss: 0.10584912449121475\n",
      "Step 27 (2648); Episode 64/100; Loss: 0.04986793175339699\n",
      "Step 28 (2649); Episode 64/100; Loss: 0.04911455884575844\n",
      "Step 29 (2650); Episode 64/100; Loss: 0.05540168285369873\n",
      "Step 30 (2651); Episode 64/100; Loss: 0.003441749606281519\n",
      "Step 31 (2652); Episode 64/100; Loss: 0.07514023780822754\n",
      "Step 32 (2653); Episode 64/100; Loss: 0.003597111441195011\n",
      "Step 33 (2654); Episode 64/100; Loss: 0.0014796726172789931\n",
      "Step 34 (2655); Episode 64/100; Loss: 0.043799880892038345\n",
      "Step 35 (2656); Episode 64/100; Loss: 0.005478720646351576\n",
      "Step 36 (2657); Episode 64/100; Loss: 0.0788406953215599\n",
      "Step 37 (2658); Episode 64/100; Loss: 0.036347851157188416\n",
      "Step 38 (2659); Episode 64/100; Loss: 0.04371560737490654\n",
      "Step 39 (2660); Episode 64/100; Loss: 0.08709848672151566\n",
      "Step 40 (2661); Episode 64/100; Loss: 0.07891172170639038\n",
      "Step 41 (2662); Episode 64/100; Loss: 0.052791010588407516\n",
      "Step 42 (2663); Episode 64/100; Loss: 0.05103776603937149\n",
      "Step 43 (2664); Episode 64/100; Loss: 0.002415435155853629\n",
      "Step 44 (2665); Episode 64/100; Loss: 0.03619404509663582\n",
      "Step 45 (2666); Episode 64/100; Loss: 0.0035177485551685095\n",
      "Step 46 (2667); Episode 64/100; Loss: 0.06657952070236206\n",
      "Step 47 (2668); Episode 64/100; Loss: 0.004380964674055576\n",
      "Step 48 (2669); Episode 64/100; Loss: 0.057192638516426086\n",
      "Step 49 (2670); Episode 64/100; Loss: 0.00824309978634119\n",
      "Step 50 (2671); Episode 64/100; Loss: 0.004847333766520023\n",
      "Step 51 (2672); Episode 64/100; Loss: 0.08361391723155975\n",
      "Step 52 (2673); Episode 64/100; Loss: 0.08864450454711914\n",
      "Step 53 (2674); Episode 64/100; Loss: 0.13495568931102753\n",
      "Step 54 (2675); Episode 64/100; Loss: 0.043532490730285645\n",
      "Step 55 (2676); Episode 64/100; Loss: 0.05153791978955269\n",
      "Step 56 (2677); Episode 64/100; Loss: 0.05014044791460037\n",
      "Step 57 (2678); Episode 64/100; Loss: 0.0039490386843681335\n",
      "Step 58 (2679); Episode 64/100; Loss: 0.2841469943523407\n",
      "Step 59 (2680); Episode 64/100; Loss: 0.0038804756477475166\n",
      "Step 60 (2681); Episode 64/100; Loss: 0.1297900378704071\n",
      "Step 61 (2682); Episode 64/100; Loss: 0.04893799498677254\n",
      "Step 62 (2683); Episode 64/100; Loss: 0.1714351922273636\n",
      "Step 63 (2684); Episode 64/100; Loss: 0.05426797643303871\n",
      "Step 64 (2685); Episode 64/100; Loss: 0.10439783334732056\n",
      "Step 65 (2686); Episode 64/100; Loss: 0.046445876359939575\n",
      "Step 66 (2687); Episode 64/100; Loss: 0.04685858264565468\n",
      "Step 67 (2688); Episode 64/100; Loss: 0.006229182705283165\n",
      "Step 68 (2689); Episode 64/100; Loss: 0.04397369176149368\n",
      "Step 69 (2690); Episode 64/100; Loss: 0.0019432527478784323\n",
      "Step 70 (2691); Episode 64/100; Loss: 0.08682567626237869\n",
      "Step 71 (2692); Episode 64/100; Loss: 0.12723208963871002\n",
      "Step 72 (2693); Episode 64/100; Loss: 0.07285050302743912\n",
      "Step 73 (2694); Episode 64/100; Loss: 0.09394798427820206\n",
      "Step 74 (2695); Episode 64/100; Loss: 0.052477166056632996\n",
      "Step 75 (2696); Episode 64/100; Loss: 0.004776102490723133\n",
      "Step 76 (2697); Episode 64/100; Loss: 0.04270204156637192\n",
      "Step 77 (2698); Episode 64/100; Loss: 0.04370390623807907\n",
      "Step 78 (2699); Episode 64/100; Loss: 0.00236655306071043\n",
      "Step 79 (2700); Episode 64/100; Loss: 0.0061040897853672504\n",
      "Step 80 (2701); Episode 64/100; Loss: 0.08049370348453522\n",
      "Step 81 (2702); Episode 64/100; Loss: 0.003687690244987607\n",
      "Step 82 (2703); Episode 64/100; Loss: 0.056648533791303635\n",
      "Step 83 (2704); Episode 64/100; Loss: 0.06195754185318947\n",
      "Step 84 (2705); Episode 64/100; Loss: 0.11120262742042542\n",
      "Step 85 (2706); Episode 64/100; Loss: 0.048822179436683655\n",
      "Step 86 (2707); Episode 64/100; Loss: 0.1114729568362236\n",
      "Step 87 (2708); Episode 64/100; Loss: 0.0372416116297245\n",
      "Step 88 (2709); Episode 64/100; Loss: 0.023608265444636345\n",
      "Step 89 (2710); Episode 64/100; Loss: 0.1308642476797104\n",
      "Step 90 (2711); Episode 64/100; Loss: 0.11669294536113739\n",
      "Step 91 (2712); Episode 64/100; Loss: 0.05288660526275635\n",
      "Step 92 (2713); Episode 64/100; Loss: 0.1358526647090912\n",
      "Step 93 (2714); Episode 64/100; Loss: 0.048570189625024796\n",
      "Step 94 (2715); Episode 64/100; Loss: 0.05318703502416611\n",
      "Step 95 (2716); Episode 64/100; Loss: 0.029698166996240616\n",
      "Step 96 (2717); Episode 64/100; Loss: 0.001698204898275435\n",
      "Step 97 (2718); Episode 64/100; Loss: 0.005590294487774372\n",
      "Step 98 (2719); Episode 64/100; Loss: 0.12353775650262833\n",
      "Step 99 (2720); Episode 64/100; Loss: 0.04259033501148224\n",
      "Step 100 (2721); Episode 64/100; Loss: 0.11045965552330017\n",
      "Step 101 (2722); Episode 64/100; Loss: 0.0782504752278328\n",
      "Step 102 (2723); Episode 64/100; Loss: 0.03756171837449074\n",
      "Step 103 (2724); Episode 64/100; Loss: 0.05184352397918701\n",
      "Step 104 (2725); Episode 64/100; Loss: 0.11574558913707733\n",
      "Step 105 (2726); Episode 64/100; Loss: 0.0695195272564888\n",
      "Step 106 (2727); Episode 64/100; Loss: 0.0027874645311385393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 107 (2728); Episode 64/100; Loss: 0.056957244873046875\n",
      "Step 108 (2729); Episode 64/100; Loss: 0.0026098983362317085\n",
      "Step 109 (2730); Episode 64/100; Loss: 0.004638241603970528\n",
      "Step 110 (2731); Episode 64/100; Loss: 0.06977687031030655\n",
      "Step 111 (2732); Episode 64/100; Loss: 0.08821335434913635\n",
      "Step 112 (2733); Episode 64/100; Loss: 0.0027790553867816925\n",
      "Step 113 (2734); Episode 64/100; Loss: 0.1556176096200943\n",
      "Step 114 (2735); Episode 64/100; Loss: 0.11707784235477448\n",
      "Step 115 (2736); Episode 64/100; Loss: 0.03663814440369606\n",
      "Step 116 (2737); Episode 64/100; Loss: 0.10982521623373032\n",
      "Step 117 (2738); Episode 64/100; Loss: 0.08751749247312546\n",
      "Step 118 (2739); Episode 64/100; Loss: 0.06751444190740585\n",
      "Step 119 (2740); Episode 64/100; Loss: 0.08477795124053955\n",
      "Step 120 (2741); Episode 64/100; Loss: 0.1405262053012848\n",
      "Step 121 (2742); Episode 64/100; Loss: 0.07408072799444199\n",
      "Step 122 (2743); Episode 64/100; Loss: 0.15799793601036072\n",
      "Step 123 (2744); Episode 64/100; Loss: 0.08215904235839844\n",
      "Step 124 (2745); Episode 64/100; Loss: 0.054509274661540985\n",
      "Step 125 (2746); Episode 64/100; Loss: 0.006356947124004364\n",
      "Step 126 (2747); Episode 64/100; Loss: 0.08311587572097778\n",
      "Step 127 (2748); Episode 64/100; Loss: 0.06344761699438095\n",
      "Step 128 (2749); Episode 64/100; Loss: 0.007519178092479706\n",
      "Step 129 (2750); Episode 64/100; Loss: 0.007424578536301851\n",
      "Step 130 (2751); Episode 64/100; Loss: 0.00406635133549571\n",
      "Step 131 (2752); Episode 64/100; Loss: 0.05582017824053764\n",
      "Step 132 (2753); Episode 64/100; Loss: 0.07334457337856293\n",
      "Step 133 (2754); Episode 64/100; Loss: 0.05474480614066124\n",
      "Step 134 (2755); Episode 64/100; Loss: 0.05316977947950363\n",
      "Step 135 (2756); Episode 64/100; Loss: 0.043105777353048325\n",
      "Step 136 (2757); Episode 64/100; Loss: 0.0043826051987707615\n",
      "Step 137 (2758); Episode 64/100; Loss: 0.14185628294944763\n",
      "Step 138 (2759); Episode 64/100; Loss: 0.1552189439535141\n",
      "Step 139 (2760); Episode 64/100; Loss: 0.002075611148029566\n",
      "Step 140 (2761); Episode 64/100; Loss: 0.024227166548371315\n",
      "Step 141 (2762); Episode 64/100; Loss: 0.1470239758491516\n",
      "Step 142 (2763); Episode 64/100; Loss: 0.05595774203538895\n",
      "Step 143 (2764); Episode 64/100; Loss: 0.0025083322543650866\n",
      "Step 144 (2765); Episode 64/100; Loss: 0.07854630053043365\n",
      "Step 145 (2766); Episode 64/100; Loss: 0.003940276801586151\n",
      "Step 146 (2767); Episode 64/100; Loss: 0.03675996512174606\n",
      "Step 147 (2768); Episode 64/100; Loss: 0.004119873046875\n",
      "Step 148 (2769); Episode 64/100; Loss: 0.14647658169269562\n",
      "Step 149 (2770); Episode 64/100; Loss: 0.046531252562999725\n",
      "Step 150 (2771); Episode 64/100; Loss: 0.05203541740775108\n",
      "Step 151 (2772); Episode 64/100; Loss: 0.09941165894269943\n",
      "Step 152 (2773); Episode 64/100; Loss: 0.04679008573293686\n",
      "Step 153 (2774); Episode 64/100; Loss: 0.03153708204627037\n",
      "Step 154 (2775); Episode 64/100; Loss: 0.0033922905568033457\n",
      "Step 155 (2776); Episode 64/100; Loss: 0.04915868863463402\n",
      "Step 156 (2777); Episode 64/100; Loss: 0.06274589896202087\n",
      "Step 157 (2778); Episode 64/100; Loss: 0.11350886523723602\n",
      "Step 158 (2779); Episode 64/100; Loss: 0.11085817962884903\n",
      "Step 159 (2780); Episode 64/100; Loss: 0.006227382458746433\n",
      "Step 160 (2781); Episode 64/100; Loss: 0.026076773181557655\n",
      "Step 161 (2782); Episode 64/100; Loss: 0.026673899963498116\n",
      "Step 162 (2783); Episode 64/100; Loss: 0.002090343739837408\n",
      "Step 163 (2784); Episode 64/100; Loss: 0.06793233752250671\n",
      "Step 164 (2785); Episode 64/100; Loss: 0.07420580089092255\n",
      "Step 165 (2786); Episode 64/100; Loss: 0.0789019986987114\n",
      "Step 166 (2787); Episode 64/100; Loss: 0.07231298834085464\n",
      "Step 167 (2788); Episode 64/100; Loss: 0.16646388173103333\n",
      "Step 168 (2789); Episode 64/100; Loss: 0.056944839656353\n",
      "Step 169 (2790); Episode 64/100; Loss: 0.01688336580991745\n",
      "Step 170 (2791); Episode 64/100; Loss: 0.1491256207227707\n",
      "Step 171 (2792); Episode 64/100; Loss: 0.057821016758680344\n",
      "Step 172 (2793); Episode 64/100; Loss: 0.04332724213600159\n",
      "Step 173 (2794); Episode 64/100; Loss: 0.051033828407526016\n",
      "Step 174 (2795); Episode 64/100; Loss: 0.08037806302309036\n",
      "Step 175 (2796); Episode 64/100; Loss: 0.15598079562187195\n",
      "Step 176 (2797); Episode 64/100; Loss: 0.005311714950948954\n",
      "Step 177 (2798); Episode 64/100; Loss: 0.009203904308378696\n",
      "Step 178 (2799); Episode 64/100; Loss: 0.05320833623409271\n",
      "Step 179 (2800); Episode 64/100; Loss: 0.05097687616944313\n",
      "Step 0 (2801); Episode 65/100; Loss: 0.07818850874900818\n",
      "Step 1 (2802); Episode 65/100; Loss: 0.004241753835231066\n",
      "Step 2 (2803); Episode 65/100; Loss: 0.0013508736155927181\n",
      "Step 3 (2804); Episode 65/100; Loss: 0.04019420966506004\n",
      "Step 4 (2805); Episode 65/100; Loss: 0.05732504650950432\n",
      "Step 5 (2806); Episode 65/100; Loss: 0.05537130683660507\n",
      "Step 6 (2807); Episode 65/100; Loss: 0.06495591998100281\n",
      "Step 7 (2808); Episode 65/100; Loss: 0.026132844388484955\n",
      "Step 8 (2809); Episode 65/100; Loss: 0.10572972893714905\n",
      "Step 9 (2810); Episode 65/100; Loss: 0.08560388535261154\n",
      "Step 10 (2811); Episode 65/100; Loss: 0.0025063001085072756\n",
      "Step 11 (2812); Episode 65/100; Loss: 0.025658538565039635\n",
      "Step 12 (2813); Episode 65/100; Loss: 0.029075827449560165\n",
      "Step 13 (2814); Episode 65/100; Loss: 0.0685226172208786\n",
      "Step 14 (2815); Episode 65/100; Loss: 0.002173215616494417\n",
      "Step 15 (2816); Episode 65/100; Loss: 0.03516343608498573\n",
      "Step 16 (2817); Episode 65/100; Loss: 0.003765980713069439\n",
      "Step 17 (2818); Episode 65/100; Loss: 0.02069014124572277\n",
      "Step 18 (2819); Episode 65/100; Loss: 0.0073797754012048244\n",
      "Step 19 (2820); Episode 65/100; Loss: 0.05953202396631241\n",
      "Step 20 (2821); Episode 65/100; Loss: 0.008885422721505165\n",
      "Step 21 (2822); Episode 65/100; Loss: 0.012600732035934925\n",
      "Step 22 (2823); Episode 65/100; Loss: 0.14214985072612762\n",
      "Step 23 (2824); Episode 65/100; Loss: 0.07443325221538544\n",
      "Step 24 (2825); Episode 65/100; Loss: 0.10970460623502731\n",
      "Step 25 (2826); Episode 65/100; Loss: 0.04801705479621887\n",
      "Step 26 (2827); Episode 65/100; Loss: 0.054509203881025314\n",
      "Step 27 (2828); Episode 65/100; Loss: 0.003925763536244631\n",
      "Step 28 (2829); Episode 65/100; Loss: 0.05043647438287735\n",
      "Step 29 (2830); Episode 65/100; Loss: 0.005150080192834139\n",
      "Step 30 (2831); Episode 65/100; Loss: 0.08920436352491379\n",
      "Step 31 (2832); Episode 65/100; Loss: 0.10516808182001114\n",
      "Step 32 (2833); Episode 65/100; Loss: 0.10234414041042328\n",
      "Step 33 (2834); Episode 65/100; Loss: 0.04606863483786583\n",
      "Step 34 (2835); Episode 65/100; Loss: 0.06640325486660004\n",
      "Step 35 (2836); Episode 65/100; Loss: 0.055844083428382874\n",
      "Step 36 (2837); Episode 65/100; Loss: 0.05862593650817871\n",
      "Step 37 (2838); Episode 65/100; Loss: 0.004634895361959934\n",
      "Step 38 (2839); Episode 65/100; Loss: 0.08849067986011505\n",
      "Step 39 (2840); Episode 65/100; Loss: 0.001532442751340568\n",
      "Step 40 (2841); Episode 65/100; Loss: 0.21365579962730408\n",
      "Step 41 (2842); Episode 65/100; Loss: 0.04925550892949104\n",
      "Step 42 (2843); Episode 65/100; Loss: 0.11295158416032791\n",
      "Step 43 (2844); Episode 65/100; Loss: 0.005458957981318235\n",
      "Step 44 (2845); Episode 65/100; Loss: 0.03794928267598152\n",
      "Step 45 (2846); Episode 65/100; Loss: 0.14718545973300934\n",
      "Step 46 (2847); Episode 65/100; Loss: 0.10237152129411697\n",
      "Step 47 (2848); Episode 65/100; Loss: 0.050155721604824066\n",
      "Step 48 (2849); Episode 65/100; Loss: 0.005979899782687426\n",
      "Step 49 (2850); Episode 65/100; Loss: 0.0025366221088916063\n",
      "Step 50 (2851); Episode 65/100; Loss: 0.06315642595291138\n",
      "Step 51 (2852); Episode 65/100; Loss: 0.03172042965888977\n",
      "Step 52 (2853); Episode 65/100; Loss: 0.004225949291139841\n",
      "Step 53 (2854); Episode 65/100; Loss: 0.04075281322002411\n",
      "Step 54 (2855); Episode 65/100; Loss: 0.048632923513650894\n",
      "Step 55 (2856); Episode 65/100; Loss: 0.05275353044271469\n",
      "Step 56 (2857); Episode 65/100; Loss: 0.008452297188341618\n",
      "Step 57 (2858); Episode 65/100; Loss: 0.04793538153171539\n",
      "Step 58 (2859); Episode 65/100; Loss: 0.10268416255712509\n",
      "Step 59 (2860); Episode 65/100; Loss: 0.1112460196018219\n",
      "Step 60 (2861); Episode 65/100; Loss: 0.09702814370393753\n",
      "Step 61 (2862); Episode 65/100; Loss: 0.05557984486222267\n",
      "Step 62 (2863); Episode 65/100; Loss: 0.004563868511468172\n",
      "Step 63 (2864); Episode 65/100; Loss: 0.05358736962080002\n",
      "Step 64 (2865); Episode 65/100; Loss: 0.02641473524272442\n",
      "Step 65 (2866); Episode 65/100; Loss: 0.10532630234956741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 66 (2867); Episode 65/100; Loss: 0.0552392452955246\n",
      "Step 67 (2868); Episode 65/100; Loss: 0.005003092810511589\n",
      "Step 68 (2869); Episode 65/100; Loss: 0.010049892589449883\n",
      "Step 69 (2870); Episode 65/100; Loss: 0.1519981026649475\n",
      "Step 70 (2871); Episode 65/100; Loss: 0.11064448207616806\n",
      "Step 71 (2872); Episode 65/100; Loss: 0.04052657634019852\n",
      "Step 72 (2873); Episode 65/100; Loss: 0.16780532896518707\n",
      "Step 73 (2874); Episode 65/100; Loss: 0.030221877619624138\n",
      "Step 74 (2875); Episode 65/100; Loss: 0.12020869553089142\n",
      "Step 75 (2876); Episode 65/100; Loss: 0.00370544265024364\n",
      "Step 76 (2877); Episode 65/100; Loss: 0.04832861199975014\n",
      "Step 77 (2878); Episode 65/100; Loss: 0.06148633733391762\n",
      "Step 78 (2879); Episode 65/100; Loss: 0.10580794513225555\n",
      "Step 79 (2880); Episode 65/100; Loss: 0.07296023517847061\n",
      "Step 80 (2881); Episode 65/100; Loss: 0.04771667346358299\n",
      "Step 81 (2882); Episode 65/100; Loss: 0.0031581707298755646\n",
      "Step 82 (2883); Episode 65/100; Loss: 0.0032334979623556137\n",
      "Step 83 (2884); Episode 65/100; Loss: 0.15022313594818115\n",
      "Step 84 (2885); Episode 65/100; Loss: 0.10133983194828033\n",
      "Step 85 (2886); Episode 65/100; Loss: 0.08092615008354187\n",
      "Step 86 (2887); Episode 65/100; Loss: 0.03321263566613197\n",
      "Step 87 (2888); Episode 65/100; Loss: 0.005046603735536337\n",
      "Step 88 (2889); Episode 65/100; Loss: 0.05103731527924538\n",
      "Step 89 (2890); Episode 65/100; Loss: 0.002591470256447792\n",
      "Step 90 (2891); Episode 65/100; Loss: 0.002567127114161849\n",
      "Step 91 (2892); Episode 65/100; Loss: 0.16926005482673645\n",
      "Step 92 (2893); Episode 65/100; Loss: 0.002319555962458253\n",
      "Step 93 (2894); Episode 65/100; Loss: 0.004874691367149353\n",
      "Step 94 (2895); Episode 65/100; Loss: 0.04941362142562866\n",
      "Step 95 (2896); Episode 65/100; Loss: 0.04053610563278198\n",
      "Step 96 (2897); Episode 65/100; Loss: 0.11728944629430771\n",
      "Step 97 (2898); Episode 65/100; Loss: 0.00593649921938777\n",
      "Step 98 (2899); Episode 65/100; Loss: 0.008361651562154293\n",
      "Step 99 (2900); Episode 65/100; Loss: 0.1603754758834839\n",
      "Step 100 (2901); Episode 65/100; Loss: 0.10037420690059662\n",
      "Step 101 (2902); Episode 65/100; Loss: 0.05030123144388199\n",
      "Step 102 (2903); Episode 65/100; Loss: 0.0028245982248336077\n",
      "Step 103 (2904); Episode 65/100; Loss: 0.04946356639266014\n",
      "Step 104 (2905); Episode 65/100; Loss: 0.026849839836359024\n",
      "Step 105 (2906); Episode 65/100; Loss: 0.16952507197856903\n",
      "Step 0 (2907); Episode 66/100; Loss: 0.06330162286758423\n",
      "Step 1 (2908); Episode 66/100; Loss: 0.07799598574638367\n",
      "Step 2 (2909); Episode 66/100; Loss: 0.034985363483428955\n",
      "Step 3 (2910); Episode 66/100; Loss: 0.026294304057955742\n",
      "Step 4 (2911); Episode 66/100; Loss: 0.002731059677898884\n",
      "Step 5 (2912); Episode 66/100; Loss: 0.03970008343458176\n",
      "Step 6 (2913); Episode 66/100; Loss: 0.05805651471018791\n",
      "Step 7 (2914); Episode 66/100; Loss: 0.09791421890258789\n",
      "Step 8 (2915); Episode 66/100; Loss: 0.10443156957626343\n",
      "Step 9 (2916); Episode 66/100; Loss: 0.0031791203655302525\n",
      "Step 10 (2917); Episode 66/100; Loss: 0.10713593661785126\n",
      "Step 11 (2918); Episode 66/100; Loss: 0.09662456065416336\n",
      "Step 12 (2919); Episode 66/100; Loss: 0.044102758169174194\n",
      "Step 13 (2920); Episode 66/100; Loss: 0.0519440658390522\n",
      "Step 14 (2921); Episode 66/100; Loss: 0.045648641884326935\n",
      "Step 15 (2922); Episode 66/100; Loss: 0.022021370008587837\n",
      "Step 16 (2923); Episode 66/100; Loss: 0.03586695343255997\n",
      "Step 17 (2924); Episode 66/100; Loss: 0.07336337864398956\n",
      "Step 18 (2925); Episode 66/100; Loss: 0.10370150208473206\n",
      "Step 19 (2926); Episode 66/100; Loss: 0.0036901081912219524\n",
      "Step 20 (2927); Episode 66/100; Loss: 0.02661382220685482\n",
      "Step 21 (2928); Episode 66/100; Loss: 0.04877734184265137\n",
      "Step 22 (2929); Episode 66/100; Loss: 0.05402059853076935\n",
      "Step 23 (2930); Episode 66/100; Loss: 0.011193100363016129\n",
      "Step 24 (2931); Episode 66/100; Loss: 0.0521702840924263\n",
      "Step 25 (2932); Episode 66/100; Loss: 0.10952819138765335\n",
      "Step 26 (2933); Episode 66/100; Loss: 0.0022148569114506245\n",
      "Step 27 (2934); Episode 66/100; Loss: 0.08064129203557968\n",
      "Step 28 (2935); Episode 66/100; Loss: 0.044736187905073166\n",
      "Step 29 (2936); Episode 66/100; Loss: 0.1051570400595665\n",
      "Step 30 (2937); Episode 66/100; Loss: 0.04832048341631889\n",
      "Step 31 (2938); Episode 66/100; Loss: 0.14092430472373962\n",
      "Step 32 (2939); Episode 66/100; Loss: 0.0029938260558992624\n",
      "Step 33 (2940); Episode 66/100; Loss: 0.0036484773736447096\n",
      "Step 34 (2941); Episode 66/100; Loss: 0.05172279477119446\n",
      "Step 35 (2942); Episode 66/100; Loss: 0.03908621519804001\n",
      "Step 36 (2943); Episode 66/100; Loss: 0.09932097792625427\n",
      "Step 37 (2944); Episode 66/100; Loss: 0.010168563574552536\n",
      "Step 38 (2945); Episode 66/100; Loss: 0.035753149539232254\n",
      "Step 39 (2946); Episode 66/100; Loss: 0.05007625371217728\n",
      "Step 40 (2947); Episode 66/100; Loss: 0.0015226746909320354\n",
      "Step 41 (2948); Episode 66/100; Loss: 0.01883631758391857\n",
      "Step 42 (2949); Episode 66/100; Loss: 0.003104729810729623\n",
      "Step 43 (2950); Episode 66/100; Loss: 0.024497712031006813\n",
      "Step 44 (2951); Episode 66/100; Loss: 0.03963755443692207\n",
      "Step 45 (2952); Episode 66/100; Loss: 0.12288360297679901\n",
      "Step 46 (2953); Episode 66/100; Loss: 0.0023463245015591383\n",
      "Step 47 (2954); Episode 66/100; Loss: 0.1062462255358696\n",
      "Step 48 (2955); Episode 66/100; Loss: 0.06470867991447449\n",
      "Step 49 (2956); Episode 66/100; Loss: 0.18364889919757843\n",
      "Step 50 (2957); Episode 66/100; Loss: 0.04883170127868652\n",
      "Step 51 (2958); Episode 66/100; Loss: 0.013043024577200413\n",
      "Step 52 (2959); Episode 66/100; Loss: 0.04448648542165756\n",
      "Step 53 (2960); Episode 66/100; Loss: 0.008274350315332413\n",
      "Step 54 (2961); Episode 66/100; Loss: 0.13257496058940887\n",
      "Step 55 (2962); Episode 66/100; Loss: 0.08405221998691559\n",
      "Step 56 (2963); Episode 66/100; Loss: 0.054364532232284546\n",
      "Step 57 (2964); Episode 66/100; Loss: 0.059596117585897446\n",
      "Step 58 (2965); Episode 66/100; Loss: 0.009082477539777756\n",
      "Step 59 (2966); Episode 66/100; Loss: 0.027562741190195084\n",
      "Step 60 (2967); Episode 66/100; Loss: 0.03769131377339363\n",
      "Step 61 (2968); Episode 66/100; Loss: 0.08587145805358887\n",
      "Step 62 (2969); Episode 66/100; Loss: 0.03614215552806854\n",
      "Step 63 (2970); Episode 66/100; Loss: 0.05508998781442642\n",
      "Step 64 (2971); Episode 66/100; Loss: 0.07040580362081528\n",
      "Step 65 (2972); Episode 66/100; Loss: 0.24250473082065582\n",
      "Step 66 (2973); Episode 66/100; Loss: 0.10297917574644089\n",
      "Step 67 (2974); Episode 66/100; Loss: 0.047656070441007614\n",
      "Step 68 (2975); Episode 66/100; Loss: 0.05321304127573967\n",
      "Step 69 (2976); Episode 66/100; Loss: 0.0464383028447628\n",
      "Step 70 (2977); Episode 66/100; Loss: 0.1000886783003807\n",
      "Step 71 (2978); Episode 66/100; Loss: 0.08995671570301056\n",
      "Step 72 (2979); Episode 66/100; Loss: 0.09263893216848373\n",
      "Step 73 (2980); Episode 66/100; Loss: 0.15331366658210754\n",
      "Step 74 (2981); Episode 66/100; Loss: 0.0016539384378120303\n",
      "Step 75 (2982); Episode 66/100; Loss: 0.002316901460289955\n",
      "Step 76 (2983); Episode 66/100; Loss: 0.0030301683582365513\n",
      "Step 77 (2984); Episode 66/100; Loss: 0.003678657114505768\n",
      "Step 78 (2985); Episode 66/100; Loss: 0.0029163078870624304\n",
      "Step 79 (2986); Episode 66/100; Loss: 0.04322119429707527\n",
      "Step 80 (2987); Episode 66/100; Loss: 0.12972167134284973\n",
      "Step 81 (2988); Episode 66/100; Loss: 0.041129112243652344\n",
      "Step 82 (2989); Episode 66/100; Loss: 0.003550533438101411\n",
      "Step 83 (2990); Episode 66/100; Loss: 0.09004075825214386\n",
      "Step 84 (2991); Episode 66/100; Loss: 0.043905109167099\n",
      "Step 85 (2992); Episode 66/100; Loss: 0.00406095664948225\n",
      "Step 86 (2993); Episode 66/100; Loss: 0.04578622803092003\n",
      "Step 87 (2994); Episode 66/100; Loss: 0.08726193755865097\n",
      "Step 88 (2995); Episode 66/100; Loss: 0.0038258282002061605\n",
      "Step 89 (2996); Episode 66/100; Loss: 0.10161042213439941\n",
      "Step 90 (2997); Episode 66/100; Loss: 0.053503792732954025\n",
      "Step 91 (2998); Episode 66/100; Loss: 0.06199587881565094\n",
      "Step 92 (2999); Episode 66/100; Loss: 0.1536133587360382\n",
      "Step 93 (3000); Episode 66/100; Loss: 0.0028958667535334826\n",
      "Step 94 (3001); Episode 66/100; Loss: 0.1340208351612091\n",
      "Step 95 (3002); Episode 66/100; Loss: 0.01736963540315628\n",
      "Step 96 (3003); Episode 66/100; Loss: 0.10081517696380615\n",
      "Step 97 (3004); Episode 66/100; Loss: 0.006658216472715139\n",
      "Step 98 (3005); Episode 66/100; Loss: 0.0899338349699974\n",
      "Step 99 (3006); Episode 66/100; Loss: 0.019102435559034348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 (3007); Episode 66/100; Loss: 0.013580274768173695\n",
      "Step 101 (3008); Episode 66/100; Loss: 0.04760952666401863\n",
      "Step 102 (3009); Episode 66/100; Loss: 0.0021071063820272684\n",
      "Step 103 (3010); Episode 66/100; Loss: 0.003794034942984581\n",
      "Step 104 (3011); Episode 66/100; Loss: 0.08943156152963638\n",
      "Step 105 (3012); Episode 66/100; Loss: 0.07255391776561737\n",
      "Step 106 (3013); Episode 66/100; Loss: 0.05367608368396759\n",
      "Step 107 (3014); Episode 66/100; Loss: 0.053590890020132065\n",
      "Step 108 (3015); Episode 66/100; Loss: 0.04768165573477745\n",
      "Step 109 (3016); Episode 66/100; Loss: 0.03427110239863396\n",
      "Step 110 (3017); Episode 66/100; Loss: 0.03859347105026245\n",
      "Step 111 (3018); Episode 66/100; Loss: 0.009125995449721813\n",
      "Step 112 (3019); Episode 66/100; Loss: 0.08657579123973846\n",
      "Step 113 (3020); Episode 66/100; Loss: 0.04791676998138428\n",
      "Step 114 (3021); Episode 66/100; Loss: 0.0040032328106462955\n",
      "Step 115 (3022); Episode 66/100; Loss: 0.048504896461963654\n",
      "Step 116 (3023); Episode 66/100; Loss: 0.06919043511152267\n",
      "Step 117 (3024); Episode 66/100; Loss: 0.0043149711564183235\n",
      "Step 118 (3025); Episode 66/100; Loss: 0.07858378440141678\n",
      "Step 119 (3026); Episode 66/100; Loss: 0.07841179519891739\n",
      "Step 120 (3027); Episode 66/100; Loss: 0.04724464565515518\n",
      "Step 121 (3028); Episode 66/100; Loss: 0.03884178027510643\n",
      "Step 122 (3029); Episode 66/100; Loss: 0.01682821847498417\n",
      "Step 0 (3030); Episode 67/100; Loss: 0.005744520109146833\n",
      "Step 1 (3031); Episode 67/100; Loss: 0.010323809459805489\n",
      "Step 2 (3032); Episode 67/100; Loss: 0.002320443280041218\n",
      "Step 3 (3033); Episode 67/100; Loss: 0.045189231634140015\n",
      "Step 4 (3034); Episode 67/100; Loss: 0.035125114023685455\n",
      "Step 5 (3035); Episode 67/100; Loss: 0.052817437797784805\n",
      "Step 6 (3036); Episode 67/100; Loss: 0.042458225041627884\n",
      "Step 7 (3037); Episode 67/100; Loss: 0.04933873191475868\n",
      "Step 8 (3038); Episode 67/100; Loss: 0.04948645830154419\n",
      "Step 9 (3039); Episode 67/100; Loss: 0.1575988233089447\n",
      "Step 10 (3040); Episode 67/100; Loss: 0.09036026149988174\n",
      "Step 11 (3041); Episode 67/100; Loss: 0.03927028924226761\n",
      "Step 12 (3042); Episode 67/100; Loss: 0.046658750623464584\n",
      "Step 13 (3043); Episode 67/100; Loss: 0.03554981201887131\n",
      "Step 14 (3044); Episode 67/100; Loss: 0.09411698579788208\n",
      "Step 15 (3045); Episode 67/100; Loss: 0.0017441147938370705\n",
      "Step 16 (3046); Episode 67/100; Loss: 0.1121031641960144\n",
      "Step 17 (3047); Episode 67/100; Loss: 0.10584196448326111\n",
      "Step 18 (3048); Episode 67/100; Loss: 0.04388754814863205\n",
      "Step 19 (3049); Episode 67/100; Loss: 0.21358610689640045\n",
      "Step 20 (3050); Episode 67/100; Loss: 0.05568370968103409\n",
      "Step 21 (3051); Episode 67/100; Loss: 0.09498339891433716\n",
      "Step 22 (3052); Episode 67/100; Loss: 0.0913153812289238\n",
      "Step 23 (3053); Episode 67/100; Loss: 0.00393070699647069\n",
      "Step 24 (3054); Episode 67/100; Loss: 0.05387093499302864\n",
      "Step 25 (3055); Episode 67/100; Loss: 0.031247340142726898\n",
      "Step 26 (3056); Episode 67/100; Loss: 0.09731077402830124\n",
      "Step 27 (3057); Episode 67/100; Loss: 0.09238225221633911\n",
      "Step 28 (3058); Episode 67/100; Loss: 0.0013604462146759033\n",
      "Step 29 (3059); Episode 67/100; Loss: 0.002970677800476551\n",
      "Step 30 (3060); Episode 67/100; Loss: 0.04770829156041145\n",
      "Step 31 (3061); Episode 67/100; Loss: 0.048381101340055466\n",
      "Step 32 (3062); Episode 67/100; Loss: 0.06893999874591827\n",
      "Step 33 (3063); Episode 67/100; Loss: 0.036118991672992706\n",
      "Step 34 (3064); Episode 67/100; Loss: 0.06271544098854065\n",
      "Step 35 (3065); Episode 67/100; Loss: 0.024775849655270576\n",
      "Step 36 (3066); Episode 67/100; Loss: 0.055299025028944016\n",
      "Step 37 (3067); Episode 67/100; Loss: 0.052641790360212326\n",
      "Step 38 (3068); Episode 67/100; Loss: 0.0599055290222168\n",
      "Step 39 (3069); Episode 67/100; Loss: 0.0011176001280546188\n",
      "Step 40 (3070); Episode 67/100; Loss: 0.023677583783864975\n",
      "Step 41 (3071); Episode 67/100; Loss: 0.003975782543420792\n",
      "Step 42 (3072); Episode 67/100; Loss: 0.002094453200697899\n",
      "Step 43 (3073); Episode 67/100; Loss: 0.13748963177204132\n",
      "Step 44 (3074); Episode 67/100; Loss: 0.05094178766012192\n",
      "Step 45 (3075); Episode 67/100; Loss: 0.15849566459655762\n",
      "Step 46 (3076); Episode 67/100; Loss: 0.04978730529546738\n",
      "Step 47 (3077); Episode 67/100; Loss: 0.004274195525795221\n",
      "Step 48 (3078); Episode 67/100; Loss: 0.0036573708057403564\n",
      "Step 49 (3079); Episode 67/100; Loss: 0.002734470646828413\n",
      "Step 50 (3080); Episode 67/100; Loss: 0.051516637206077576\n",
      "Step 51 (3081); Episode 67/100; Loss: 0.0023790907580405474\n",
      "Step 52 (3082); Episode 67/100; Loss: 0.0016003944911062717\n",
      "Step 53 (3083); Episode 67/100; Loss: 0.08142928034067154\n",
      "Step 54 (3084); Episode 67/100; Loss: 0.08233849704265594\n",
      "Step 55 (3085); Episode 67/100; Loss: 0.18342864513397217\n",
      "Step 56 (3086); Episode 67/100; Loss: 0.042521413415670395\n",
      "Step 57 (3087); Episode 67/100; Loss: 0.004091109149158001\n",
      "Step 58 (3088); Episode 67/100; Loss: 0.12918953597545624\n",
      "Step 59 (3089); Episode 67/100; Loss: 0.0021318697836250067\n",
      "Step 60 (3090); Episode 67/100; Loss: 0.0032891840673983097\n",
      "Step 61 (3091); Episode 67/100; Loss: 0.09634276479482651\n",
      "Step 62 (3092); Episode 67/100; Loss: 0.05170022323727608\n",
      "Step 63 (3093); Episode 67/100; Loss: 0.0012798282550647855\n",
      "Step 64 (3094); Episode 67/100; Loss: 0.10013046860694885\n",
      "Step 65 (3095); Episode 67/100; Loss: 0.005618012975901365\n",
      "Step 66 (3096); Episode 67/100; Loss: 0.10022920370101929\n",
      "Step 67 (3097); Episode 67/100; Loss: 0.042014285922050476\n",
      "Step 68 (3098); Episode 67/100; Loss: 0.004093922208994627\n",
      "Step 69 (3099); Episode 67/100; Loss: 0.08719184249639511\n",
      "Step 70 (3100); Episode 67/100; Loss: 0.004698268603533506\n",
      "Step 71 (3101); Episode 67/100; Loss: 0.05084998160600662\n",
      "Step 72 (3102); Episode 67/100; Loss: 0.04858853667974472\n",
      "Step 73 (3103); Episode 67/100; Loss: 0.0016320239519700408\n",
      "Step 74 (3104); Episode 67/100; Loss: 0.07372848689556122\n",
      "Step 75 (3105); Episode 67/100; Loss: 0.0029749989043921232\n",
      "Step 76 (3106); Episode 67/100; Loss: 0.006017030216753483\n",
      "Step 77 (3107); Episode 67/100; Loss: 0.0024983605835586786\n",
      "Step 78 (3108); Episode 67/100; Loss: 0.0024023493751883507\n",
      "Step 79 (3109); Episode 67/100; Loss: 0.004057377111166716\n",
      "Step 80 (3110); Episode 67/100; Loss: 0.02930290997028351\n",
      "Step 81 (3111); Episode 67/100; Loss: 0.09510732442140579\n",
      "Step 82 (3112); Episode 67/100; Loss: 0.1246047094464302\n",
      "Step 83 (3113); Episode 67/100; Loss: 0.05412307754158974\n",
      "Step 84 (3114); Episode 67/100; Loss: 0.04807571321725845\n",
      "Step 85 (3115); Episode 67/100; Loss: 0.12958809733390808\n",
      "Step 86 (3116); Episode 67/100; Loss: 0.10180217027664185\n",
      "Step 87 (3117); Episode 67/100; Loss: 0.002636547200381756\n",
      "Step 88 (3118); Episode 67/100; Loss: 0.0430714450776577\n",
      "Step 89 (3119); Episode 67/100; Loss: 0.021992351859807968\n",
      "Step 90 (3120); Episode 67/100; Loss: 0.22065381705760956\n",
      "Step 91 (3121); Episode 67/100; Loss: 0.047792769968509674\n",
      "Step 92 (3122); Episode 67/100; Loss: 0.03878084942698479\n",
      "Step 93 (3123); Episode 67/100; Loss: 0.04997670277953148\n",
      "Step 94 (3124); Episode 67/100; Loss: 0.026413219049572945\n",
      "Step 95 (3125); Episode 67/100; Loss: 0.14594997465610504\n",
      "Step 96 (3126); Episode 67/100; Loss: 0.004300025291740894\n",
      "Step 97 (3127); Episode 67/100; Loss: 0.0021783625707030296\n",
      "Step 98 (3128); Episode 67/100; Loss: 0.1019422635436058\n",
      "Step 99 (3129); Episode 67/100; Loss: 0.01571687124669552\n",
      "Step 100 (3130); Episode 67/100; Loss: 0.031168963760137558\n",
      "Step 101 (3131); Episode 67/100; Loss: 0.10098323971033096\n",
      "Step 102 (3132); Episode 67/100; Loss: 0.044760216027498245\n",
      "Step 103 (3133); Episode 67/100; Loss: 0.1335114687681198\n",
      "Step 104 (3134); Episode 67/100; Loss: 0.0769893005490303\n",
      "Step 105 (3135); Episode 67/100; Loss: 0.0012612395221367478\n",
      "Step 106 (3136); Episode 67/100; Loss: 0.04724682494997978\n",
      "Step 107 (3137); Episode 67/100; Loss: 0.12415151298046112\n",
      "Step 108 (3138); Episode 67/100; Loss: 0.039016515016555786\n",
      "Step 0 (3139); Episode 68/100; Loss: 0.05182656645774841\n",
      "Step 1 (3140); Episode 68/100; Loss: 0.09236620366573334\n",
      "Step 2 (3141); Episode 68/100; Loss: 0.0933072566986084\n",
      "Step 3 (3142); Episode 68/100; Loss: 0.04082367196679115\n",
      "Step 4 (3143); Episode 68/100; Loss: 0.05592890456318855\n",
      "Step 5 (3144); Episode 68/100; Loss: 0.003369861515238881\n",
      "Step 6 (3145); Episode 68/100; Loss: 0.04226689785718918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 (3146); Episode 68/100; Loss: 0.0036427893210202456\n",
      "Step 8 (3147); Episode 68/100; Loss: 0.0029929466545581818\n",
      "Step 9 (3148); Episode 68/100; Loss: 0.07401582598686218\n",
      "Step 10 (3149); Episode 68/100; Loss: 0.08186937868595123\n",
      "Step 11 (3150); Episode 68/100; Loss: 0.03846951201558113\n",
      "Step 12 (3151); Episode 68/100; Loss: 0.06934468448162079\n",
      "Step 13 (3152); Episode 68/100; Loss: 0.12333764880895615\n",
      "Step 14 (3153); Episode 68/100; Loss: 0.02972094714641571\n",
      "Step 15 (3154); Episode 68/100; Loss: 0.14430491626262665\n",
      "Step 16 (3155); Episode 68/100; Loss: 0.002473127096891403\n",
      "Step 17 (3156); Episode 68/100; Loss: 0.07105666399002075\n",
      "Step 18 (3157); Episode 68/100; Loss: 0.07396745681762695\n",
      "Step 19 (3158); Episode 68/100; Loss: 0.04527915269136429\n",
      "Step 20 (3159); Episode 68/100; Loss: 0.07286611199378967\n",
      "Step 21 (3160); Episode 68/100; Loss: 0.006027194205671549\n",
      "Step 22 (3161); Episode 68/100; Loss: 0.05574461445212364\n",
      "Step 23 (3162); Episode 68/100; Loss: 0.04103764519095421\n",
      "Step 24 (3163); Episode 68/100; Loss: 0.04114645719528198\n",
      "Step 25 (3164); Episode 68/100; Loss: 0.1079760193824768\n",
      "Step 26 (3165); Episode 68/100; Loss: 0.006098755169659853\n",
      "Step 27 (3166); Episode 68/100; Loss: 0.08591975271701813\n",
      "Step 28 (3167); Episode 68/100; Loss: 0.0024327232968062162\n",
      "Step 29 (3168); Episode 68/100; Loss: 0.11204363405704498\n",
      "Step 30 (3169); Episode 68/100; Loss: 0.010468905791640282\n",
      "Step 31 (3170); Episode 68/100; Loss: 0.011723409406840801\n",
      "Step 32 (3171); Episode 68/100; Loss: 0.024369895458221436\n",
      "Step 33 (3172); Episode 68/100; Loss: 0.05522341653704643\n",
      "Step 34 (3173); Episode 68/100; Loss: 0.04125232249498367\n",
      "Step 35 (3174); Episode 68/100; Loss: 0.13823384046554565\n",
      "Step 36 (3175); Episode 68/100; Loss: 0.17477770149707794\n",
      "Step 37 (3176); Episode 68/100; Loss: 0.10423397272825241\n",
      "Step 38 (3177); Episode 68/100; Loss: 0.052980273962020874\n",
      "Step 39 (3178); Episode 68/100; Loss: 0.05558720976114273\n",
      "Step 40 (3179); Episode 68/100; Loss: 0.003379615256562829\n",
      "Step 41 (3180); Episode 68/100; Loss: 0.05393914505839348\n",
      "Step 42 (3181); Episode 68/100; Loss: 0.03525291755795479\n",
      "Step 43 (3182); Episode 68/100; Loss: 0.03339137136936188\n",
      "Step 44 (3183); Episode 68/100; Loss: 0.09171240031719208\n",
      "Step 45 (3184); Episode 68/100; Loss: 0.005074525251984596\n",
      "Step 46 (3185); Episode 68/100; Loss: 0.07569533586502075\n",
      "Step 47 (3186); Episode 68/100; Loss: 0.14411617815494537\n",
      "Step 48 (3187); Episode 68/100; Loss: 0.043680381029844284\n",
      "Step 49 (3188); Episode 68/100; Loss: 0.04442623630166054\n",
      "Step 50 (3189); Episode 68/100; Loss: 0.06029428541660309\n",
      "Step 51 (3190); Episode 68/100; Loss: 0.029782408848404884\n",
      "Step 52 (3191); Episode 68/100; Loss: 0.05616310238838196\n",
      "Step 53 (3192); Episode 68/100; Loss: 0.0035289202351123095\n",
      "Step 54 (3193); Episode 68/100; Loss: 0.003774966113269329\n",
      "Step 55 (3194); Episode 68/100; Loss: 0.06476569175720215\n",
      "Step 56 (3195); Episode 68/100; Loss: 0.04952499642968178\n",
      "Step 57 (3196); Episode 68/100; Loss: 0.06851960718631744\n",
      "Step 58 (3197); Episode 68/100; Loss: 0.05694987252354622\n",
      "Step 59 (3198); Episode 68/100; Loss: 0.06781238317489624\n",
      "Step 60 (3199); Episode 68/100; Loss: 0.03708181157708168\n",
      "Step 61 (3200); Episode 68/100; Loss: 0.0013972155284136534\n",
      "Step 62 (3201); Episode 68/100; Loss: 0.002925984328612685\n",
      "Step 63 (3202); Episode 68/100; Loss: 0.06522236764431\n",
      "Step 64 (3203); Episode 68/100; Loss: 0.0016372407553717494\n",
      "Step 65 (3204); Episode 68/100; Loss: 0.1555842161178589\n",
      "Step 66 (3205); Episode 68/100; Loss: 0.04988506808876991\n",
      "Step 67 (3206); Episode 68/100; Loss: 0.10132943093776703\n",
      "Step 68 (3207); Episode 68/100; Loss: 0.0023647642228752375\n",
      "Step 69 (3208); Episode 68/100; Loss: 0.028648564592003822\n",
      "Step 70 (3209); Episode 68/100; Loss: 0.05495128780603409\n",
      "Step 71 (3210); Episode 68/100; Loss: 0.08127418160438538\n",
      "Step 72 (3211); Episode 68/100; Loss: 0.1463126391172409\n",
      "Step 73 (3212); Episode 68/100; Loss: 0.042807839810848236\n",
      "Step 74 (3213); Episode 68/100; Loss: 0.036202769726514816\n",
      "Step 75 (3214); Episode 68/100; Loss: 0.0026653625536710024\n",
      "Step 76 (3215); Episode 68/100; Loss: 0.10647410899400711\n",
      "Step 77 (3216); Episode 68/100; Loss: 0.00415960606187582\n",
      "Step 78 (3217); Episode 68/100; Loss: 0.027721183374524117\n",
      "Step 79 (3218); Episode 68/100; Loss: 0.005469103809446096\n",
      "Step 80 (3219); Episode 68/100; Loss: 0.0035373184364289045\n",
      "Step 81 (3220); Episode 68/100; Loss: 0.00326538342051208\n",
      "Step 82 (3221); Episode 68/100; Loss: 0.08470408618450165\n",
      "Step 83 (3222); Episode 68/100; Loss: 0.0411246083676815\n",
      "Step 84 (3223); Episode 68/100; Loss: 0.006346551235765219\n",
      "Step 85 (3224); Episode 68/100; Loss: 0.0012763281119987369\n",
      "Step 86 (3225); Episode 68/100; Loss: 0.0623762421309948\n",
      "Step 87 (3226); Episode 68/100; Loss: 0.2090606391429901\n",
      "Step 88 (3227); Episode 68/100; Loss: 0.16315439343452454\n",
      "Step 89 (3228); Episode 68/100; Loss: 0.054544854909181595\n",
      "Step 90 (3229); Episode 68/100; Loss: 0.09150636941194534\n",
      "Step 91 (3230); Episode 68/100; Loss: 0.06360059976577759\n",
      "Step 92 (3231); Episode 68/100; Loss: 0.053171444684267044\n",
      "Step 93 (3232); Episode 68/100; Loss: 0.0015479271532967687\n",
      "Step 94 (3233); Episode 68/100; Loss: 0.039051808416843414\n",
      "Step 95 (3234); Episode 68/100; Loss: 0.04454105347394943\n",
      "Step 96 (3235); Episode 68/100; Loss: 0.004420137032866478\n",
      "Step 97 (3236); Episode 68/100; Loss: 0.052695877850055695\n",
      "Step 98 (3237); Episode 68/100; Loss: 0.0006988680688664317\n",
      "Step 99 (3238); Episode 68/100; Loss: 0.09212618321180344\n",
      "Step 100 (3239); Episode 68/100; Loss: 0.002480829134583473\n",
      "Step 101 (3240); Episode 68/100; Loss: 0.0022973024751991034\n",
      "Step 102 (3241); Episode 68/100; Loss: 0.04601964354515076\n",
      "Step 103 (3242); Episode 68/100; Loss: 0.033634960651397705\n",
      "Step 104 (3243); Episode 68/100; Loss: 0.002306075068190694\n",
      "Step 105 (3244); Episode 68/100; Loss: 0.0356212742626667\n",
      "Step 106 (3245); Episode 68/100; Loss: 0.05708802491426468\n",
      "Step 107 (3246); Episode 68/100; Loss: 0.04348234459757805\n",
      "Step 108 (3247); Episode 68/100; Loss: 0.22172491252422333\n",
      "Step 109 (3248); Episode 68/100; Loss: 0.016723336651921272\n",
      "Step 110 (3249); Episode 68/100; Loss: 0.045416031032800674\n",
      "Step 111 (3250); Episode 68/100; Loss: 0.0025477430317550898\n",
      "Step 112 (3251); Episode 68/100; Loss: 0.0047096554189920425\n",
      "Step 113 (3252); Episode 68/100; Loss: 0.12992213666439056\n",
      "Step 114 (3253); Episode 68/100; Loss: 0.11006931960582733\n",
      "Step 115 (3254); Episode 68/100; Loss: 0.15938860177993774\n",
      "Step 116 (3255); Episode 68/100; Loss: 0.05163291096687317\n",
      "Step 117 (3256); Episode 68/100; Loss: 0.09224136173725128\n",
      "Step 118 (3257); Episode 68/100; Loss: 0.055978044867515564\n",
      "Step 119 (3258); Episode 68/100; Loss: 0.14116214215755463\n",
      "Step 120 (3259); Episode 68/100; Loss: 0.05907813832163811\n",
      "Step 121 (3260); Episode 68/100; Loss: 0.05947454646229744\n",
      "Step 122 (3261); Episode 68/100; Loss: 0.03696955740451813\n",
      "Step 123 (3262); Episode 68/100; Loss: 0.12797462940216064\n",
      "Step 124 (3263); Episode 68/100; Loss: 0.18647500872612\n",
      "Step 125 (3264); Episode 68/100; Loss: 0.10320229828357697\n",
      "Step 126 (3265); Episode 68/100; Loss: 0.10991469025611877\n",
      "Step 127 (3266); Episode 68/100; Loss: 0.01171071082353592\n",
      "Step 128 (3267); Episode 68/100; Loss: 0.03371340036392212\n",
      "Step 129 (3268); Episode 68/100; Loss: 0.05466235429048538\n",
      "Step 130 (3269); Episode 68/100; Loss: 0.012131252326071262\n",
      "Step 131 (3270); Episode 68/100; Loss: 0.011349324136972427\n",
      "Step 132 (3271); Episode 68/100; Loss: 0.04244498163461685\n",
      "Step 133 (3272); Episode 68/100; Loss: 0.1595960557460785\n",
      "Step 134 (3273); Episode 68/100; Loss: 0.00898929312825203\n",
      "Step 135 (3274); Episode 68/100; Loss: 0.09349667280912399\n",
      "Step 136 (3275); Episode 68/100; Loss: 0.0454847551882267\n",
      "Step 137 (3276); Episode 68/100; Loss: 0.06513402611017227\n",
      "Step 138 (3277); Episode 68/100; Loss: 0.00645593972876668\n",
      "Step 139 (3278); Episode 68/100; Loss: 0.005059408023953438\n",
      "Step 140 (3279); Episode 68/100; Loss: 0.009795772843062878\n",
      "Step 141 (3280); Episode 68/100; Loss: 0.1523555964231491\n",
      "Step 142 (3281); Episode 68/100; Loss: 0.04415055736899376\n",
      "Step 143 (3282); Episode 68/100; Loss: 0.1322454959154129\n",
      "Step 144 (3283); Episode 68/100; Loss: 0.03841438516974449\n",
      "Step 145 (3284); Episode 68/100; Loss: 0.07566971331834793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 146 (3285); Episode 68/100; Loss: 0.035743072628974915\n",
      "Step 147 (3286); Episode 68/100; Loss: 0.08245223015546799\n",
      "Step 148 (3287); Episode 68/100; Loss: 0.01014444325119257\n",
      "Step 149 (3288); Episode 68/100; Loss: 0.037913646548986435\n",
      "Step 150 (3289); Episode 68/100; Loss: 0.05612221360206604\n",
      "Step 151 (3290); Episode 68/100; Loss: 0.05118728429079056\n",
      "Step 152 (3291); Episode 68/100; Loss: 0.012510123662650585\n",
      "Step 153 (3292); Episode 68/100; Loss: 0.05320889502763748\n",
      "Step 154 (3293); Episode 68/100; Loss: 0.01928740367293358\n",
      "Step 155 (3294); Episode 68/100; Loss: 0.04998525232076645\n",
      "Step 156 (3295); Episode 68/100; Loss: 0.07059532403945923\n",
      "Step 157 (3296); Episode 68/100; Loss: 0.004039785359054804\n",
      "Step 158 (3297); Episode 68/100; Loss: 0.03517751768231392\n",
      "Step 159 (3298); Episode 68/100; Loss: 0.0014969409676268697\n",
      "Step 160 (3299); Episode 68/100; Loss: 0.08759278804063797\n",
      "Step 161 (3300); Episode 68/100; Loss: 0.0723482221364975\n",
      "Step 162 (3301); Episode 68/100; Loss: 0.047808535397052765\n",
      "Step 163 (3302); Episode 68/100; Loss: 0.0033310207072645426\n",
      "Step 164 (3303); Episode 68/100; Loss: 0.15140971541404724\n",
      "Step 165 (3304); Episode 68/100; Loss: 0.0022313138470053673\n",
      "Step 166 (3305); Episode 68/100; Loss: 0.0017029708251357079\n",
      "Step 167 (3306); Episode 68/100; Loss: 0.0027418602257966995\n",
      "Step 168 (3307); Episode 68/100; Loss: 0.0029990561306476593\n",
      "Step 169 (3308); Episode 68/100; Loss: 0.0032880206126719713\n",
      "Step 170 (3309); Episode 68/100; Loss: 0.04930655658245087\n",
      "Step 171 (3310); Episode 68/100; Loss: 0.13477879762649536\n",
      "Step 172 (3311); Episode 68/100; Loss: 0.0472332239151001\n",
      "Step 173 (3312); Episode 68/100; Loss: 0.04842938855290413\n",
      "Step 174 (3313); Episode 68/100; Loss: 0.05370740219950676\n",
      "Step 175 (3314); Episode 68/100; Loss: 0.0700228288769722\n",
      "Step 176 (3315); Episode 68/100; Loss: 0.04832811653614044\n",
      "Step 177 (3316); Episode 68/100; Loss: 0.1393553465604782\n",
      "Step 178 (3317); Episode 68/100; Loss: 0.06796154379844666\n",
      "Step 179 (3318); Episode 68/100; Loss: 0.00342210428789258\n",
      "Step 180 (3319); Episode 68/100; Loss: 0.026476725935935974\n",
      "Step 181 (3320); Episode 68/100; Loss: 0.002507388824597001\n",
      "Step 182 (3321); Episode 68/100; Loss: 0.055377159267663956\n",
      "Step 183 (3322); Episode 68/100; Loss: 0.047742221504449844\n",
      "Step 184 (3323); Episode 68/100; Loss: 0.03936756029725075\n",
      "Step 185 (3324); Episode 68/100; Loss: 0.0023352911230176687\n",
      "Step 186 (3325); Episode 68/100; Loss: 0.00272621912881732\n",
      "Step 187 (3326); Episode 68/100; Loss: 0.0020770635455846786\n",
      "Step 188 (3327); Episode 68/100; Loss: 0.04589058831334114\n",
      "Step 189 (3328); Episode 68/100; Loss: 0.047244857996702194\n",
      "Step 190 (3329); Episode 68/100; Loss: 0.0639161467552185\n",
      "Step 191 (3330); Episode 68/100; Loss: 0.09843871742486954\n",
      "Step 192 (3331); Episode 68/100; Loss: 0.039261240512132645\n",
      "Step 193 (3332); Episode 68/100; Loss: 0.05158572643995285\n",
      "Step 194 (3333); Episode 68/100; Loss: 0.11511243879795074\n",
      "Step 195 (3334); Episode 68/100; Loss: 0.051685404032468796\n",
      "Step 196 (3335); Episode 68/100; Loss: 0.09784279763698578\n",
      "Step 197 (3336); Episode 68/100; Loss: 0.0023897893261164427\n",
      "Step 198 (3337); Episode 68/100; Loss: 0.01705898530781269\n",
      "Step 199 (3338); Episode 68/100; Loss: 0.10037895292043686\n",
      "Step 0 (3339); Episode 69/100; Loss: 0.0979795753955841\n",
      "Step 1 (3340); Episode 69/100; Loss: 0.04881788417696953\n",
      "Step 2 (3341); Episode 69/100; Loss: 0.0022570036817342043\n",
      "Step 3 (3342); Episode 69/100; Loss: 0.08390256762504578\n",
      "Step 4 (3343); Episode 69/100; Loss: 0.00402219919487834\n",
      "Step 5 (3344); Episode 69/100; Loss: 0.1429789811372757\n",
      "Step 6 (3345); Episode 69/100; Loss: 0.11078484356403351\n",
      "Step 7 (3346); Episode 69/100; Loss: 0.04942651838064194\n",
      "Step 8 (3347); Episode 69/100; Loss: 0.05642244592308998\n",
      "Step 9 (3348); Episode 69/100; Loss: 0.05148028954863548\n",
      "Step 10 (3349); Episode 69/100; Loss: 0.031814053654670715\n",
      "Step 11 (3350); Episode 69/100; Loss: 0.13896439969539642\n",
      "Step 12 (3351); Episode 69/100; Loss: 0.0028493176214396954\n",
      "Step 13 (3352); Episode 69/100; Loss: 0.009921833872795105\n",
      "Step 14 (3353); Episode 69/100; Loss: 0.1183461993932724\n",
      "Step 15 (3354); Episode 69/100; Loss: 0.04371706396341324\n",
      "Step 16 (3355); Episode 69/100; Loss: 0.13646160066127777\n",
      "Step 17 (3356); Episode 69/100; Loss: 0.04912188649177551\n",
      "Step 18 (3357); Episode 69/100; Loss: 0.03212273120880127\n",
      "Step 19 (3358); Episode 69/100; Loss: 0.08104711771011353\n",
      "Step 20 (3359); Episode 69/100; Loss: 0.05681873485445976\n",
      "Step 21 (3360); Episode 69/100; Loss: 0.0026864639949053526\n",
      "Step 22 (3361); Episode 69/100; Loss: 0.049017250537872314\n",
      "Step 23 (3362); Episode 69/100; Loss: 0.09309263527393341\n",
      "Step 24 (3363); Episode 69/100; Loss: 0.08949538320302963\n",
      "Step 25 (3364); Episode 69/100; Loss: 0.05939112976193428\n",
      "Step 26 (3365); Episode 69/100; Loss: 0.037394169718027115\n",
      "Step 27 (3366); Episode 69/100; Loss: 0.05535648390650749\n",
      "Step 28 (3367); Episode 69/100; Loss: 0.002771154511719942\n",
      "Step 29 (3368); Episode 69/100; Loss: 0.05244644731283188\n",
      "Step 30 (3369); Episode 69/100; Loss: 0.06541868299245834\n",
      "Step 31 (3370); Episode 69/100; Loss: 0.0530218631029129\n",
      "Step 32 (3371); Episode 69/100; Loss: 0.08949533849954605\n",
      "Step 33 (3372); Episode 69/100; Loss: 0.003150775795802474\n",
      "Step 34 (3373); Episode 69/100; Loss: 0.0023367812391370535\n",
      "Step 35 (3374); Episode 69/100; Loss: 0.007691506762057543\n",
      "Step 36 (3375); Episode 69/100; Loss: 0.038349900394678116\n",
      "Step 37 (3376); Episode 69/100; Loss: 0.0024779129307717085\n",
      "Step 38 (3377); Episode 69/100; Loss: 0.056200891733169556\n",
      "Step 39 (3378); Episode 69/100; Loss: 0.09687571972608566\n",
      "Step 40 (3379); Episode 69/100; Loss: 0.12396549433469772\n",
      "Step 41 (3380); Episode 69/100; Loss: 0.0031538568437099457\n",
      "Step 42 (3381); Episode 69/100; Loss: 0.07671546190977097\n",
      "Step 43 (3382); Episode 69/100; Loss: 0.005621337331831455\n",
      "Step 44 (3383); Episode 69/100; Loss: 0.05408674478530884\n",
      "Step 45 (3384); Episode 69/100; Loss: 0.1686667650938034\n",
      "Step 46 (3385); Episode 69/100; Loss: 0.019851505756378174\n",
      "Step 47 (3386); Episode 69/100; Loss: 0.04864738509058952\n",
      "Step 48 (3387); Episode 69/100; Loss: 0.08422688394784927\n",
      "Step 49 (3388); Episode 69/100; Loss: 0.05319086089730263\n",
      "Step 50 (3389); Episode 69/100; Loss: 0.005614341702312231\n",
      "Step 51 (3390); Episode 69/100; Loss: 0.03550587221980095\n",
      "Step 52 (3391); Episode 69/100; Loss: 0.0785021185874939\n",
      "Step 53 (3392); Episode 69/100; Loss: 0.0015533608384430408\n",
      "Step 54 (3393); Episode 69/100; Loss: 0.10588588565587997\n",
      "Step 55 (3394); Episode 69/100; Loss: 0.09128668159246445\n",
      "Step 56 (3395); Episode 69/100; Loss: 0.06519554555416107\n",
      "Step 57 (3396); Episode 69/100; Loss: 0.057202477008104324\n",
      "Step 58 (3397); Episode 69/100; Loss: 0.004966916982084513\n",
      "Step 59 (3398); Episode 69/100; Loss: 0.0016167412977665663\n",
      "Step 60 (3399); Episode 69/100; Loss: 0.07985740154981613\n",
      "Step 61 (3400); Episode 69/100; Loss: 0.005281449295580387\n",
      "Step 62 (3401); Episode 69/100; Loss: 0.0016294377855956554\n",
      "Step 63 (3402); Episode 69/100; Loss: 0.0013783490285277367\n",
      "Step 64 (3403); Episode 69/100; Loss: 0.07603823393583298\n",
      "Step 65 (3404); Episode 69/100; Loss: 0.0014167818007990718\n",
      "Step 66 (3405); Episode 69/100; Loss: 0.10157806426286697\n",
      "Step 67 (3406); Episode 69/100; Loss: 0.0241563580930233\n",
      "Step 68 (3407); Episode 69/100; Loss: 0.05265131965279579\n",
      "Step 69 (3408); Episode 69/100; Loss: 0.0423663891851902\n",
      "Step 70 (3409); Episode 69/100; Loss: 0.07341216504573822\n",
      "Step 71 (3410); Episode 69/100; Loss: 0.09016945958137512\n",
      "Step 72 (3411); Episode 69/100; Loss: 0.0567609965801239\n",
      "Step 73 (3412); Episode 69/100; Loss: 0.09087444841861725\n",
      "Step 74 (3413); Episode 69/100; Loss: 0.0022008223459124565\n",
      "Step 75 (3414); Episode 69/100; Loss: 0.031453024595975876\n",
      "Step 76 (3415); Episode 69/100; Loss: 0.002740289783105254\n",
      "Step 77 (3416); Episode 69/100; Loss: 0.0678701177239418\n",
      "Step 78 (3417); Episode 69/100; Loss: 0.022038578987121582\n",
      "Step 79 (3418); Episode 69/100; Loss: 0.051919493824243546\n",
      "Step 80 (3419); Episode 69/100; Loss: 0.0012776628136634827\n",
      "Step 81 (3420); Episode 69/100; Loss: 0.10988888889551163\n",
      "Step 82 (3421); Episode 69/100; Loss: 0.004139822907745838\n",
      "Step 83 (3422); Episode 69/100; Loss: 0.12481874972581863\n",
      "Step 84 (3423); Episode 69/100; Loss: 0.17862364649772644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85 (3424); Episode 69/100; Loss: 0.04808597266674042\n",
      "Step 86 (3425); Episode 69/100; Loss: 0.08965013176202774\n",
      "Step 87 (3426); Episode 69/100; Loss: 0.06261962652206421\n",
      "Step 88 (3427); Episode 69/100; Loss: 0.002245913026854396\n",
      "Step 89 (3428); Episode 69/100; Loss: 0.038525789976119995\n",
      "Step 90 (3429); Episode 69/100; Loss: 0.0722070038318634\n",
      "Step 91 (3430); Episode 69/100; Loss: 0.02251436375081539\n",
      "Step 92 (3431); Episode 69/100; Loss: 0.05446472391486168\n",
      "Step 93 (3432); Episode 69/100; Loss: 0.04388180747628212\n",
      "Step 94 (3433); Episode 69/100; Loss: 0.0020910478197038174\n",
      "Step 95 (3434); Episode 69/100; Loss: 0.0023144190199673176\n",
      "Step 96 (3435); Episode 69/100; Loss: 0.002053392818197608\n",
      "Step 97 (3436); Episode 69/100; Loss: 0.15035906434059143\n",
      "Step 98 (3437); Episode 69/100; Loss: 0.05532482638955116\n",
      "Step 99 (3438); Episode 69/100; Loss: 0.007869184017181396\n",
      "Step 100 (3439); Episode 69/100; Loss: 0.03338116779923439\n",
      "Step 101 (3440); Episode 69/100; Loss: 0.02891121804714203\n",
      "Step 102 (3441); Episode 69/100; Loss: 0.14529520273208618\n",
      "Step 103 (3442); Episode 69/100; Loss: 0.0024449245538562536\n",
      "Step 104 (3443); Episode 69/100; Loss: 0.06377503275871277\n",
      "Step 105 (3444); Episode 69/100; Loss: 0.019660746678709984\n",
      "Step 106 (3445); Episode 69/100; Loss: 0.048656873404979706\n",
      "Step 107 (3446); Episode 69/100; Loss: 0.1205589771270752\n",
      "Step 108 (3447); Episode 69/100; Loss: 0.09367574006319046\n",
      "Step 109 (3448); Episode 69/100; Loss: 0.020642882212996483\n",
      "Step 110 (3449); Episode 69/100; Loss: 0.0147045673802495\n",
      "Step 111 (3450); Episode 69/100; Loss: 0.01805959828197956\n",
      "Step 112 (3451); Episode 69/100; Loss: 0.0028451504185795784\n",
      "Step 113 (3452); Episode 69/100; Loss: 0.04067515209317207\n",
      "Step 114 (3453); Episode 69/100; Loss: 0.009093096479773521\n",
      "Step 115 (3454); Episode 69/100; Loss: 0.03765379637479782\n",
      "Step 116 (3455); Episode 69/100; Loss: 0.029002025723457336\n",
      "Step 117 (3456); Episode 69/100; Loss: 0.049563080072402954\n",
      "Step 118 (3457); Episode 69/100; Loss: 0.02590164728462696\n",
      "Step 119 (3458); Episode 69/100; Loss: 0.07309376448392868\n",
      "Step 120 (3459); Episode 69/100; Loss: 0.006460933014750481\n",
      "Step 121 (3460); Episode 69/100; Loss: 0.03529205918312073\n",
      "Step 122 (3461); Episode 69/100; Loss: 0.006760755553841591\n",
      "Step 123 (3462); Episode 69/100; Loss: 0.07454454153776169\n",
      "Step 124 (3463); Episode 69/100; Loss: 0.04530830681324005\n",
      "Step 125 (3464); Episode 69/100; Loss: 0.003901798976585269\n",
      "Step 126 (3465); Episode 69/100; Loss: 0.004029547795653343\n",
      "Step 127 (3466); Episode 69/100; Loss: 0.0295872800052166\n",
      "Step 128 (3467); Episode 69/100; Loss: 0.006327977869659662\n",
      "Step 129 (3468); Episode 69/100; Loss: 0.024900494143366814\n",
      "Step 130 (3469); Episode 69/100; Loss: 0.13156697154045105\n",
      "Step 131 (3470); Episode 69/100; Loss: 0.06915903091430664\n",
      "Step 132 (3471); Episode 69/100; Loss: 0.06142295524477959\n",
      "Step 133 (3472); Episode 69/100; Loss: 0.10190947353839874\n",
      "Step 134 (3473); Episode 69/100; Loss: 0.05415685102343559\n",
      "Step 135 (3474); Episode 69/100; Loss: 0.0025598001666367054\n",
      "Step 136 (3475); Episode 69/100; Loss: 0.002324852393940091\n",
      "Step 137 (3476); Episode 69/100; Loss: 0.12606246769428253\n",
      "Step 138 (3477); Episode 69/100; Loss: 0.16781452298164368\n",
      "Step 139 (3478); Episode 69/100; Loss: 0.061662353575229645\n",
      "Step 140 (3479); Episode 69/100; Loss: 0.05119192600250244\n",
      "Step 141 (3480); Episode 69/100; Loss: 0.03964529186487198\n",
      "Step 142 (3481); Episode 69/100; Loss: 0.07405096292495728\n",
      "Step 143 (3482); Episode 69/100; Loss: 0.051412519067525864\n",
      "Step 144 (3483); Episode 69/100; Loss: 0.04248115047812462\n",
      "Step 145 (3484); Episode 69/100; Loss: 0.09899425506591797\n",
      "Step 146 (3485); Episode 69/100; Loss: 0.020442098379135132\n",
      "Step 147 (3486); Episode 69/100; Loss: 0.001493781921453774\n",
      "Step 148 (3487); Episode 69/100; Loss: 0.046579793095588684\n",
      "Step 149 (3488); Episode 69/100; Loss: 0.12029951065778732\n",
      "Step 150 (3489); Episode 69/100; Loss: 0.09662844985723495\n",
      "Step 151 (3490); Episode 69/100; Loss: 0.0859794095158577\n",
      "Step 152 (3491); Episode 69/100; Loss: 0.1364995241165161\n",
      "Step 153 (3492); Episode 69/100; Loss: 0.04857999458909035\n",
      "Step 154 (3493); Episode 69/100; Loss: 0.03352938964962959\n",
      "Step 155 (3494); Episode 69/100; Loss: 0.13900235295295715\n",
      "Step 156 (3495); Episode 69/100; Loss: 0.004847504664212465\n",
      "Step 157 (3496); Episode 69/100; Loss: 0.0040431637316942215\n",
      "Step 158 (3497); Episode 69/100; Loss: 0.08805816620588303\n",
      "Step 159 (3498); Episode 69/100; Loss: 0.03979676961898804\n",
      "Step 160 (3499); Episode 69/100; Loss: 0.03915313631296158\n",
      "Step 161 (3500); Episode 69/100; Loss: 0.0927044153213501\n",
      "Step 162 (3501); Episode 69/100; Loss: 0.008583499118685722\n",
      "Step 163 (3502); Episode 69/100; Loss: 0.05188474431633949\n",
      "Step 164 (3503); Episode 69/100; Loss: 0.04550567641854286\n",
      "Step 165 (3504); Episode 69/100; Loss: 0.09687539935112\n",
      "Step 166 (3505); Episode 69/100; Loss: 0.0024794081691652536\n",
      "Step 167 (3506); Episode 69/100; Loss: 0.0268408190459013\n",
      "Step 168 (3507); Episode 69/100; Loss: 0.043991703540086746\n",
      "Step 169 (3508); Episode 69/100; Loss: 0.04867386445403099\n",
      "Step 170 (3509); Episode 69/100; Loss: 0.04459304362535477\n",
      "Step 171 (3510); Episode 69/100; Loss: 0.002309702103957534\n",
      "Step 172 (3511); Episode 69/100; Loss: 0.06609036028385162\n",
      "Step 173 (3512); Episode 69/100; Loss: 0.15395650267601013\n",
      "Step 174 (3513); Episode 69/100; Loss: 0.06767284125089645\n",
      "Step 175 (3514); Episode 69/100; Loss: 0.0871242880821228\n",
      "Step 176 (3515); Episode 69/100; Loss: 0.058797337114810944\n",
      "Step 177 (3516); Episode 69/100; Loss: 0.001131948083639145\n",
      "Step 178 (3517); Episode 69/100; Loss: 0.09571696817874908\n",
      "Step 179 (3518); Episode 69/100; Loss: 0.002368821296840906\n",
      "Step 180 (3519); Episode 69/100; Loss: 0.11474823206663132\n",
      "Step 181 (3520); Episode 69/100; Loss: 0.07686788588762283\n",
      "Step 182 (3521); Episode 69/100; Loss: 0.08329272270202637\n",
      "Step 183 (3522); Episode 69/100; Loss: 0.06491474062204361\n",
      "Step 184 (3523); Episode 69/100; Loss: 0.0035561425611376762\n",
      "Step 185 (3524); Episode 69/100; Loss: 0.003364247502759099\n",
      "Step 186 (3525); Episode 69/100; Loss: 0.07158824801445007\n",
      "Step 187 (3526); Episode 69/100; Loss: 0.046186283230781555\n",
      "Step 188 (3527); Episode 69/100; Loss: 0.09199279546737671\n",
      "Step 189 (3528); Episode 69/100; Loss: 0.002883950248360634\n",
      "Step 190 (3529); Episode 69/100; Loss: 0.065030537545681\n",
      "Step 191 (3530); Episode 69/100; Loss: 0.05119878426194191\n",
      "Step 192 (3531); Episode 69/100; Loss: 0.0785767063498497\n",
      "Step 193 (3532); Episode 69/100; Loss: 0.010904019698500633\n",
      "Step 194 (3533); Episode 69/100; Loss: 0.04844740405678749\n",
      "Step 195 (3534); Episode 69/100; Loss: 0.09828591346740723\n",
      "Step 196 (3535); Episode 69/100; Loss: 0.11963573843240738\n",
      "Step 197 (3536); Episode 69/100; Loss: 0.08145328611135483\n",
      "Step 198 (3537); Episode 69/100; Loss: 0.02356361784040928\n",
      "Step 199 (3538); Episode 69/100; Loss: 0.0029063597321510315\n",
      "Step 0 (3539); Episode 70/100; Loss: 0.00175876310095191\n",
      "Step 1 (3540); Episode 70/100; Loss: 0.03159833699464798\n",
      "Step 2 (3541); Episode 70/100; Loss: 0.08008840680122375\n",
      "Step 3 (3542); Episode 70/100; Loss: 0.050117164850234985\n",
      "Step 4 (3543); Episode 70/100; Loss: 0.004098495934158564\n",
      "Step 5 (3544); Episode 70/100; Loss: 0.04101540893316269\n",
      "Step 6 (3545); Episode 70/100; Loss: 0.1324826180934906\n",
      "Step 7 (3546); Episode 70/100; Loss: 0.0030347167048603296\n",
      "Step 8 (3547); Episode 70/100; Loss: 0.051624614745378494\n",
      "Step 9 (3548); Episode 70/100; Loss: 0.09962747991085052\n",
      "Step 10 (3549); Episode 70/100; Loss: 0.0027990846429020166\n",
      "Step 11 (3550); Episode 70/100; Loss: 0.0017212528036907315\n",
      "Step 12 (3551); Episode 70/100; Loss: 0.028391676023602486\n",
      "Step 13 (3552); Episode 70/100; Loss: 0.1267312467098236\n",
      "Step 14 (3553); Episode 70/100; Loss: 0.06607916951179504\n",
      "Step 15 (3554); Episode 70/100; Loss: 0.0018168186070397496\n",
      "Step 16 (3555); Episode 70/100; Loss: 0.053061019629240036\n",
      "Step 17 (3556); Episode 70/100; Loss: 0.09586777538061142\n",
      "Step 18 (3557); Episode 70/100; Loss: 0.1026952937245369\n",
      "Step 19 (3558); Episode 70/100; Loss: 0.007846426218748093\n",
      "Step 20 (3559); Episode 70/100; Loss: 0.09263131767511368\n",
      "Step 21 (3560); Episode 70/100; Loss: 0.00168121256865561\n",
      "Step 22 (3561); Episode 70/100; Loss: 0.006970976945012808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23 (3562); Episode 70/100; Loss: 0.049870967864990234\n",
      "Step 24 (3563); Episode 70/100; Loss: 0.14357277750968933\n",
      "Step 25 (3564); Episode 70/100; Loss: 0.012222516350448132\n",
      "Step 26 (3565); Episode 70/100; Loss: 0.13291047513484955\n",
      "Step 27 (3566); Episode 70/100; Loss: 0.03993586078286171\n",
      "Step 28 (3567); Episode 70/100; Loss: 0.049565866589546204\n",
      "Step 29 (3568); Episode 70/100; Loss: 0.08775588124990463\n",
      "Step 30 (3569); Episode 70/100; Loss: 0.04565535485744476\n",
      "Step 31 (3570); Episode 70/100; Loss: 0.1087585911154747\n",
      "Step 32 (3571); Episode 70/100; Loss: 0.04870443418622017\n",
      "Step 33 (3572); Episode 70/100; Loss: 0.003552813548594713\n",
      "Step 34 (3573); Episode 70/100; Loss: 0.0894460529088974\n",
      "Step 35 (3574); Episode 70/100; Loss: 0.002599910832941532\n",
      "Step 36 (3575); Episode 70/100; Loss: 0.09180165082216263\n",
      "Step 37 (3576); Episode 70/100; Loss: 0.001982389483600855\n",
      "Step 38 (3577); Episode 70/100; Loss: 0.001433132798410952\n",
      "Step 39 (3578); Episode 70/100; Loss: 0.06343851238489151\n",
      "Step 40 (3579); Episode 70/100; Loss: 0.0012724113184958696\n",
      "Step 41 (3580); Episode 70/100; Loss: 0.09319494664669037\n",
      "Step 42 (3581); Episode 70/100; Loss: 0.07753424346446991\n",
      "Step 43 (3582); Episode 70/100; Loss: 0.03715157508850098\n",
      "Step 44 (3583); Episode 70/100; Loss: 0.04134848713874817\n",
      "Step 45 (3584); Episode 70/100; Loss: 0.08278603106737137\n",
      "Step 46 (3585); Episode 70/100; Loss: 0.005500134080648422\n",
      "Step 47 (3586); Episode 70/100; Loss: 0.19965612888336182\n",
      "Step 48 (3587); Episode 70/100; Loss: 0.0032018208876252174\n",
      "Step 49 (3588); Episode 70/100; Loss: 0.02436593547463417\n",
      "Step 50 (3589); Episode 70/100; Loss: 0.051789846271276474\n",
      "Step 51 (3590); Episode 70/100; Loss: 0.12794435024261475\n",
      "Step 52 (3591); Episode 70/100; Loss: 0.05089898407459259\n",
      "Step 53 (3592); Episode 70/100; Loss: 0.04152745380997658\n",
      "Step 54 (3593); Episode 70/100; Loss: 0.00444781593978405\n",
      "Step 55 (3594); Episode 70/100; Loss: 0.05359179154038429\n",
      "Step 56 (3595); Episode 70/100; Loss: 0.04098716378211975\n",
      "Step 57 (3596); Episode 70/100; Loss: 0.08696175366640091\n",
      "Step 58 (3597); Episode 70/100; Loss: 0.0029429467394948006\n",
      "Step 59 (3598); Episode 70/100; Loss: 0.001993226818740368\n",
      "Step 60 (3599); Episode 70/100; Loss: 0.003945413511246443\n",
      "Step 61 (3600); Episode 70/100; Loss: 0.0022491365671157837\n",
      "Step 62 (3601); Episode 70/100; Loss: 0.0035379750188440084\n",
      "Step 63 (3602); Episode 70/100; Loss: 0.002282201312482357\n",
      "Step 64 (3603); Episode 70/100; Loss: 0.002648129826411605\n",
      "Step 65 (3604); Episode 70/100; Loss: 0.0793241336941719\n",
      "Step 66 (3605); Episode 70/100; Loss: 0.03173567354679108\n",
      "Step 67 (3606); Episode 70/100; Loss: 0.08890321105718613\n",
      "Step 68 (3607); Episode 70/100; Loss: 0.0030945856124162674\n",
      "Step 69 (3608); Episode 70/100; Loss: 0.07005742937326431\n",
      "Step 70 (3609); Episode 70/100; Loss: 0.03671056032180786\n",
      "Step 71 (3610); Episode 70/100; Loss: 0.03532334417104721\n",
      "Step 72 (3611); Episode 70/100; Loss: 0.0018187779933214188\n",
      "Step 73 (3612); Episode 70/100; Loss: 0.0033207961823791265\n",
      "Step 74 (3613); Episode 70/100; Loss: 0.020786985754966736\n",
      "Step 75 (3614); Episode 70/100; Loss: 0.06016036868095398\n",
      "Step 76 (3615); Episode 70/100; Loss: 0.031057752668857574\n",
      "Step 77 (3616); Episode 70/100; Loss: 0.059570103883743286\n",
      "Step 78 (3617); Episode 70/100; Loss: 0.0017167339101433754\n",
      "Step 79 (3618); Episode 70/100; Loss: 0.0029682733584195375\n",
      "Step 80 (3619); Episode 70/100; Loss: 0.1462109088897705\n",
      "Step 81 (3620); Episode 70/100; Loss: 0.04430036619305611\n",
      "Step 82 (3621); Episode 70/100; Loss: 0.04352613911032677\n",
      "Step 83 (3622); Episode 70/100; Loss: 0.052388422191143036\n",
      "Step 84 (3623); Episode 70/100; Loss: 0.007386543322354555\n",
      "Step 85 (3624); Episode 70/100; Loss: 0.09853046387434006\n",
      "Step 86 (3625); Episode 70/100; Loss: 0.052561428397893906\n",
      "Step 87 (3626); Episode 70/100; Loss: 0.006179827265441418\n",
      "Step 88 (3627); Episode 70/100; Loss: 0.0056290654465556145\n",
      "Step 89 (3628); Episode 70/100; Loss: 0.0019328055204823613\n",
      "Step 90 (3629); Episode 70/100; Loss: 0.04606586694717407\n",
      "Step 91 (3630); Episode 70/100; Loss: 0.0412798635661602\n",
      "Step 92 (3631); Episode 70/100; Loss: 0.055777665227651596\n",
      "Step 93 (3632); Episode 70/100; Loss: 0.035048749297857285\n",
      "Step 94 (3633); Episode 70/100; Loss: 0.07189119607210159\n",
      "Step 95 (3634); Episode 70/100; Loss: 0.051979705691337585\n",
      "Step 96 (3635); Episode 70/100; Loss: 0.11320856958627701\n",
      "Step 97 (3636); Episode 70/100; Loss: 0.04864361509680748\n",
      "Step 98 (3637); Episode 70/100; Loss: 0.005482484586536884\n",
      "Step 99 (3638); Episode 70/100; Loss: 0.03173254057765007\n",
      "Step 100 (3639); Episode 70/100; Loss: 0.0006463954923674464\n",
      "Step 101 (3640); Episode 70/100; Loss: 0.04391296207904816\n",
      "Step 102 (3641); Episode 70/100; Loss: 0.0014850880252197385\n",
      "Step 103 (3642); Episode 70/100; Loss: 0.059769466519355774\n",
      "Step 104 (3643); Episode 70/100; Loss: 0.010759998112916946\n",
      "Step 105 (3644); Episode 70/100; Loss: 0.07993818819522858\n",
      "Step 106 (3645); Episode 70/100; Loss: 0.1275210678577423\n",
      "Step 107 (3646); Episode 70/100; Loss: 0.0029797644820064306\n",
      "Step 108 (3647); Episode 70/100; Loss: 0.04175280034542084\n",
      "Step 109 (3648); Episode 70/100; Loss: 0.0038301637396216393\n",
      "Step 110 (3649); Episode 70/100; Loss: 0.03189561143517494\n",
      "Step 111 (3650); Episode 70/100; Loss: 0.05775462090969086\n",
      "Step 112 (3651); Episode 70/100; Loss: 0.002038945909589529\n",
      "Step 113 (3652); Episode 70/100; Loss: 0.049035247415304184\n",
      "Step 114 (3653); Episode 70/100; Loss: 0.047802817076444626\n",
      "Step 0 (3654); Episode 71/100; Loss: 0.10873234272003174\n",
      "Step 1 (3655); Episode 71/100; Loss: 0.0018815213115885854\n",
      "Step 2 (3656); Episode 71/100; Loss: 0.002889938885346055\n",
      "Step 3 (3657); Episode 71/100; Loss: 0.04325112700462341\n",
      "Step 4 (3658); Episode 71/100; Loss: 0.01419659424573183\n",
      "Step 5 (3659); Episode 71/100; Loss: 0.0989544466137886\n",
      "Step 6 (3660); Episode 71/100; Loss: 0.004380236379802227\n",
      "Step 7 (3661); Episode 71/100; Loss: 0.0552067905664444\n",
      "Step 8 (3662); Episode 71/100; Loss: 0.0398034006357193\n",
      "Step 9 (3663); Episode 71/100; Loss: 0.058309126645326614\n",
      "Step 10 (3664); Episode 71/100; Loss: 0.022470494732260704\n",
      "Step 11 (3665); Episode 71/100; Loss: 0.12780454754829407\n",
      "Step 12 (3666); Episode 71/100; Loss: 0.16584739089012146\n",
      "Step 13 (3667); Episode 71/100; Loss: 0.14807069301605225\n",
      "Step 14 (3668); Episode 71/100; Loss: 0.007183609995990992\n",
      "Step 15 (3669); Episode 71/100; Loss: 0.0016135143814608455\n",
      "Step 16 (3670); Episode 71/100; Loss: 0.0026455691549926996\n",
      "Step 17 (3671); Episode 71/100; Loss: 0.08370999246835709\n",
      "Step 18 (3672); Episode 71/100; Loss: 0.030312471091747284\n",
      "Step 19 (3673); Episode 71/100; Loss: 0.03890581429004669\n",
      "Step 20 (3674); Episode 71/100; Loss: 0.013802938163280487\n",
      "Step 21 (3675); Episode 71/100; Loss: 0.004637838341295719\n",
      "Step 22 (3676); Episode 71/100; Loss: 0.1252809762954712\n",
      "Step 23 (3677); Episode 71/100; Loss: 0.006395318545401096\n",
      "Step 24 (3678); Episode 71/100; Loss: 0.13117338716983795\n",
      "Step 25 (3679); Episode 71/100; Loss: 0.04937616363167763\n",
      "Step 26 (3680); Episode 71/100; Loss: 0.0019679830875247717\n",
      "Step 27 (3681); Episode 71/100; Loss: 0.0526152141392231\n",
      "Step 28 (3682); Episode 71/100; Loss: 0.0014181281439960003\n",
      "Step 29 (3683); Episode 71/100; Loss: 0.0015285906847566366\n",
      "Step 30 (3684); Episode 71/100; Loss: 0.05074853450059891\n",
      "Step 31 (3685); Episode 71/100; Loss: 0.07643894106149673\n",
      "Step 32 (3686); Episode 71/100; Loss: 0.05173645168542862\n",
      "Step 33 (3687); Episode 71/100; Loss: 0.09144216030836105\n",
      "Step 34 (3688); Episode 71/100; Loss: 0.11075083911418915\n",
      "Step 35 (3689); Episode 71/100; Loss: 0.01968942955136299\n",
      "Step 36 (3690); Episode 71/100; Loss: 0.04872581362724304\n",
      "Step 37 (3691); Episode 71/100; Loss: 0.006110857706516981\n",
      "Step 38 (3692); Episode 71/100; Loss: 0.06557659059762955\n",
      "Step 39 (3693); Episode 71/100; Loss: 0.025830071419477463\n",
      "Step 40 (3694); Episode 71/100; Loss: 0.07555455714464188\n",
      "Step 41 (3695); Episode 71/100; Loss: 0.002253846265375614\n",
      "Step 42 (3696); Episode 71/100; Loss: 0.12684738636016846\n",
      "Step 43 (3697); Episode 71/100; Loss: 0.02367333509027958\n",
      "Step 44 (3698); Episode 71/100; Loss: 0.00401051202788949\n",
      "Step 45 (3699); Episode 71/100; Loss: 0.009816939011216164\n",
      "Step 46 (3700); Episode 71/100; Loss: 0.05420664697885513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47 (3701); Episode 71/100; Loss: 0.11041270941495895\n",
      "Step 48 (3702); Episode 71/100; Loss: 0.00296789756976068\n",
      "Step 49 (3703); Episode 71/100; Loss: 0.005760348867624998\n",
      "Step 50 (3704); Episode 71/100; Loss: 0.020356997847557068\n",
      "Step 51 (3705); Episode 71/100; Loss: 0.08835435658693314\n",
      "Step 52 (3706); Episode 71/100; Loss: 0.0393257811665535\n",
      "Step 53 (3707); Episode 71/100; Loss: 0.08727377653121948\n",
      "Step 54 (3708); Episode 71/100; Loss: 0.056394170969724655\n",
      "Step 55 (3709); Episode 71/100; Loss: 0.002661020727828145\n",
      "Step 56 (3710); Episode 71/100; Loss: 0.08645682781934738\n",
      "Step 57 (3711); Episode 71/100; Loss: 0.0453571081161499\n",
      "Step 58 (3712); Episode 71/100; Loss: 0.003345419652760029\n",
      "Step 59 (3713); Episode 71/100; Loss: 0.10991326719522476\n",
      "Step 60 (3714); Episode 71/100; Loss: 0.049546074122190475\n",
      "Step 61 (3715); Episode 71/100; Loss: 0.04919637367129326\n",
      "Step 62 (3716); Episode 71/100; Loss: 0.13123193383216858\n",
      "Step 63 (3717); Episode 71/100; Loss: 0.0012308104196563363\n",
      "Step 64 (3718); Episode 71/100; Loss: 0.1943349540233612\n",
      "Step 65 (3719); Episode 71/100; Loss: 0.003571128938347101\n",
      "Step 66 (3720); Episode 71/100; Loss: 0.007514782715588808\n",
      "Step 67 (3721); Episode 71/100; Loss: 0.07628042995929718\n",
      "Step 68 (3722); Episode 71/100; Loss: 0.054180264472961426\n",
      "Step 69 (3723); Episode 71/100; Loss: 0.0514405183494091\n",
      "Step 70 (3724); Episode 71/100; Loss: 0.1533941626548767\n",
      "Step 71 (3725); Episode 71/100; Loss: 0.05946428328752518\n",
      "Step 72 (3726); Episode 71/100; Loss: 0.040982455015182495\n",
      "Step 73 (3727); Episode 71/100; Loss: 0.05079390108585358\n",
      "Step 74 (3728); Episode 71/100; Loss: 0.09839897602796555\n",
      "Step 75 (3729); Episode 71/100; Loss: 0.06703399866819382\n",
      "Step 76 (3730); Episode 71/100; Loss: 0.0034689856693148613\n",
      "Step 77 (3731); Episode 71/100; Loss: 0.09535941481590271\n",
      "Step 78 (3732); Episode 71/100; Loss: 0.04630473628640175\n",
      "Step 79 (3733); Episode 71/100; Loss: 0.1275458037853241\n",
      "Step 80 (3734); Episode 71/100; Loss: 0.016353504732251167\n",
      "Step 81 (3735); Episode 71/100; Loss: 0.033016566187143326\n",
      "Step 82 (3736); Episode 71/100; Loss: 0.08413757383823395\n",
      "Step 83 (3737); Episode 71/100; Loss: 0.005188636016100645\n",
      "Step 84 (3738); Episode 71/100; Loss: 0.032640907913446426\n",
      "Step 85 (3739); Episode 71/100; Loss: 0.037834763526916504\n",
      "Step 86 (3740); Episode 71/100; Loss: 0.005297621246427298\n",
      "Step 87 (3741); Episode 71/100; Loss: 0.0543416365981102\n",
      "Step 88 (3742); Episode 71/100; Loss: 0.06242062896490097\n",
      "Step 89 (3743); Episode 71/100; Loss: 0.030949508771300316\n",
      "Step 90 (3744); Episode 71/100; Loss: 0.046954602003097534\n",
      "Step 91 (3745); Episode 71/100; Loss: 0.04568697512149811\n",
      "Step 92 (3746); Episode 71/100; Loss: 0.1018563061952591\n",
      "Step 93 (3747); Episode 71/100; Loss: 0.002105721738189459\n",
      "Step 94 (3748); Episode 71/100; Loss: 0.04101698845624924\n",
      "Step 95 (3749); Episode 71/100; Loss: 0.11823269724845886\n",
      "Step 96 (3750); Episode 71/100; Loss: 0.08486180752515793\n",
      "Step 97 (3751); Episode 71/100; Loss: 0.06507787108421326\n",
      "Step 98 (3752); Episode 71/100; Loss: 0.001646506949327886\n",
      "Step 99 (3753); Episode 71/100; Loss: 0.005934583023190498\n",
      "Step 100 (3754); Episode 71/100; Loss: 0.05435580015182495\n",
      "Step 101 (3755); Episode 71/100; Loss: 0.01356916967779398\n",
      "Step 102 (3756); Episode 71/100; Loss: 0.18211491405963898\n",
      "Step 103 (3757); Episode 71/100; Loss: 0.05768955126404762\n",
      "Step 104 (3758); Episode 71/100; Loss: 0.04432603716850281\n",
      "Step 105 (3759); Episode 71/100; Loss: 0.08614030480384827\n",
      "Step 106 (3760); Episode 71/100; Loss: 0.04061112180352211\n",
      "Step 107 (3761); Episode 71/100; Loss: 0.10262380540370941\n",
      "Step 108 (3762); Episode 71/100; Loss: 0.06091776117682457\n",
      "Step 109 (3763); Episode 71/100; Loss: 0.0031727803871035576\n",
      "Step 110 (3764); Episode 71/100; Loss: 0.0022074650041759014\n",
      "Step 111 (3765); Episode 71/100; Loss: 0.04990075156092644\n",
      "Step 112 (3766); Episode 71/100; Loss: 0.033377598971128464\n",
      "Step 113 (3767); Episode 71/100; Loss: 0.0735384076833725\n",
      "Step 114 (3768); Episode 71/100; Loss: 0.14750327169895172\n",
      "Step 115 (3769); Episode 71/100; Loss: 0.012738136574625969\n",
      "Step 116 (3770); Episode 71/100; Loss: 0.06659147888422012\n",
      "Step 117 (3771); Episode 71/100; Loss: 0.05352163314819336\n",
      "Step 118 (3772); Episode 71/100; Loss: 0.0767161101102829\n",
      "Step 119 (3773); Episode 71/100; Loss: 0.10052953660488129\n",
      "Step 120 (3774); Episode 71/100; Loss: 0.07971413433551788\n",
      "Step 121 (3775); Episode 71/100; Loss: 0.05195184424519539\n",
      "Step 122 (3776); Episode 71/100; Loss: 0.002897064434364438\n",
      "Step 123 (3777); Episode 71/100; Loss: 0.04652882367372513\n",
      "Step 124 (3778); Episode 71/100; Loss: 0.07698312401771545\n",
      "Step 125 (3779); Episode 71/100; Loss: 0.0016482500359416008\n",
      "Step 126 (3780); Episode 71/100; Loss: 0.05766243115067482\n",
      "Step 127 (3781); Episode 71/100; Loss: 0.0018745113629847765\n",
      "Step 128 (3782); Episode 71/100; Loss: 0.006703508086502552\n",
      "Step 129 (3783); Episode 71/100; Loss: 0.0429316982626915\n",
      "Step 130 (3784); Episode 71/100; Loss: 0.039146292954683304\n",
      "Step 131 (3785); Episode 71/100; Loss: 0.09629670530557632\n",
      "Step 132 (3786); Episode 71/100; Loss: 0.0021784896962344646\n",
      "Step 133 (3787); Episode 71/100; Loss: 0.019732560962438583\n",
      "Step 134 (3788); Episode 71/100; Loss: 0.005119455512613058\n",
      "Step 135 (3789); Episode 71/100; Loss: 0.06438849866390228\n",
      "Step 136 (3790); Episode 71/100; Loss: 0.037875160574913025\n",
      "Step 137 (3791); Episode 71/100; Loss: 0.02201388217508793\n",
      "Step 0 (3792); Episode 72/100; Loss: 0.04489045962691307\n",
      "Step 1 (3793); Episode 72/100; Loss: 0.041541364043951035\n",
      "Step 2 (3794); Episode 72/100; Loss: 0.023871060460805893\n",
      "Step 3 (3795); Episode 72/100; Loss: 0.08358913660049438\n",
      "Step 4 (3796); Episode 72/100; Loss: 0.023206321522593498\n",
      "Step 5 (3797); Episode 72/100; Loss: 0.05234924331307411\n",
      "Step 6 (3798); Episode 72/100; Loss: 0.0557267889380455\n",
      "Step 7 (3799); Episode 72/100; Loss: 0.09609631448984146\n",
      "Step 8 (3800); Episode 72/100; Loss: 0.0181581974029541\n",
      "Step 9 (3801); Episode 72/100; Loss: 0.0027398420497775078\n",
      "Step 10 (3802); Episode 72/100; Loss: 0.028780965134501457\n",
      "Step 11 (3803); Episode 72/100; Loss: 0.0015876560937613249\n",
      "Step 12 (3804); Episode 72/100; Loss: 0.08670281618833542\n",
      "Step 13 (3805); Episode 72/100; Loss: 0.046526361256837845\n",
      "Step 14 (3806); Episode 72/100; Loss: 0.07342302799224854\n",
      "Step 15 (3807); Episode 72/100; Loss: 0.0027657742612063885\n",
      "Step 16 (3808); Episode 72/100; Loss: 0.05477677285671234\n",
      "Step 17 (3809); Episode 72/100; Loss: 0.004297933541238308\n",
      "Step 18 (3810); Episode 72/100; Loss: 0.055362965911626816\n",
      "Step 19 (3811); Episode 72/100; Loss: 0.06487954407930374\n",
      "Step 20 (3812); Episode 72/100; Loss: 0.12565475702285767\n",
      "Step 21 (3813); Episode 72/100; Loss: 0.1228640154004097\n",
      "Step 22 (3814); Episode 72/100; Loss: 0.004512697923928499\n",
      "Step 23 (3815); Episode 72/100; Loss: 0.05392695963382721\n",
      "Step 24 (3816); Episode 72/100; Loss: 0.06297986954450607\n",
      "Step 25 (3817); Episode 72/100; Loss: 0.0013919613556936383\n",
      "Step 26 (3818); Episode 72/100; Loss: 0.1017998680472374\n",
      "Step 27 (3819); Episode 72/100; Loss: 0.035416919738054276\n",
      "Step 28 (3820); Episode 72/100; Loss: 0.017318345606327057\n",
      "Step 29 (3821); Episode 72/100; Loss: 0.06456537544727325\n",
      "Step 30 (3822); Episode 72/100; Loss: 0.010747780092060566\n",
      "Step 31 (3823); Episode 72/100; Loss: 0.049446310847997665\n",
      "Step 32 (3824); Episode 72/100; Loss: 0.15647996962070465\n",
      "Step 33 (3825); Episode 72/100; Loss: 0.14132912456989288\n",
      "Step 34 (3826); Episode 72/100; Loss: 0.0028670281171798706\n",
      "Step 35 (3827); Episode 72/100; Loss: 0.09394779056310654\n",
      "Step 36 (3828); Episode 72/100; Loss: 0.03473339602351189\n",
      "Step 37 (3829); Episode 72/100; Loss: 0.0013207739684730768\n",
      "Step 38 (3830); Episode 72/100; Loss: 0.004752774722874165\n",
      "Step 39 (3831); Episode 72/100; Loss: 0.03925170376896858\n",
      "Step 40 (3832); Episode 72/100; Loss: 0.027027782052755356\n",
      "Step 41 (3833); Episode 72/100; Loss: 0.0030679693445563316\n",
      "Step 42 (3834); Episode 72/100; Loss: 0.00244922679848969\n",
      "Step 43 (3835); Episode 72/100; Loss: 0.0055275559425354\n",
      "Step 44 (3836); Episode 72/100; Loss: 0.13454048335552216\n",
      "Step 45 (3837); Episode 72/100; Loss: 0.058323509991168976\n",
      "Step 46 (3838); Episode 72/100; Loss: 0.07773887366056442\n",
      "Step 47 (3839); Episode 72/100; Loss: 0.0028947482351213694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48 (3840); Episode 72/100; Loss: 0.05029395595192909\n",
      "Step 49 (3841); Episode 72/100; Loss: 0.0539381168782711\n",
      "Step 50 (3842); Episode 72/100; Loss: 0.050389792770147324\n",
      "Step 51 (3843); Episode 72/100; Loss: 0.08824659138917923\n",
      "Step 52 (3844); Episode 72/100; Loss: 0.12093351036310196\n",
      "Step 53 (3845); Episode 72/100; Loss: 0.12981802225112915\n",
      "Step 54 (3846); Episode 72/100; Loss: 0.010153504088521004\n",
      "Step 55 (3847); Episode 72/100; Loss: 0.04848423972725868\n",
      "Step 56 (3848); Episode 72/100; Loss: 0.0034253131598234177\n",
      "Step 57 (3849); Episode 72/100; Loss: 0.0770188644528389\n",
      "Step 58 (3850); Episode 72/100; Loss: 0.054116517305374146\n",
      "Step 59 (3851); Episode 72/100; Loss: 0.09960410743951797\n",
      "Step 60 (3852); Episode 72/100; Loss: 0.047703299671411514\n",
      "Step 61 (3853); Episode 72/100; Loss: 0.0514010526239872\n",
      "Step 62 (3854); Episode 72/100; Loss: 0.048194270581007004\n",
      "Step 63 (3855); Episode 72/100; Loss: 0.09245877712965012\n",
      "Step 64 (3856); Episode 72/100; Loss: 0.033350177109241486\n",
      "Step 65 (3857); Episode 72/100; Loss: 0.03926805034279823\n",
      "Step 66 (3858); Episode 72/100; Loss: 0.0024366313591599464\n",
      "Step 67 (3859); Episode 72/100; Loss: 0.10345415025949478\n",
      "Step 68 (3860); Episode 72/100; Loss: 0.059814922511577606\n",
      "Step 69 (3861); Episode 72/100; Loss: 0.039613474160432816\n",
      "Step 70 (3862); Episode 72/100; Loss: 0.025479011237621307\n",
      "Step 71 (3863); Episode 72/100; Loss: 0.07563628256320953\n",
      "Step 72 (3864); Episode 72/100; Loss: 0.00962834618985653\n",
      "Step 73 (3865); Episode 72/100; Loss: 0.005408427678048611\n",
      "Step 74 (3866); Episode 72/100; Loss: 0.014955063350498676\n",
      "Step 75 (3867); Episode 72/100; Loss: 0.08598116040229797\n",
      "Step 76 (3868); Episode 72/100; Loss: 0.019738955423235893\n",
      "Step 77 (3869); Episode 72/100; Loss: 0.04042218625545502\n",
      "Step 78 (3870); Episode 72/100; Loss: 0.09750531613826752\n",
      "Step 79 (3871); Episode 72/100; Loss: 0.03887929394841194\n",
      "Step 80 (3872); Episode 72/100; Loss: 0.09315907955169678\n",
      "Step 81 (3873); Episode 72/100; Loss: 0.09197667241096497\n",
      "Step 82 (3874); Episode 72/100; Loss: 0.038138192147016525\n",
      "Step 83 (3875); Episode 72/100; Loss: 0.1382010579109192\n",
      "Step 84 (3876); Episode 72/100; Loss: 0.055412955582141876\n",
      "Step 85 (3877); Episode 72/100; Loss: 0.012129266746342182\n",
      "Step 86 (3878); Episode 72/100; Loss: 0.12523210048675537\n",
      "Step 87 (3879); Episode 72/100; Loss: 0.01185195054858923\n",
      "Step 88 (3880); Episode 72/100; Loss: 0.07211147993803024\n",
      "Step 89 (3881); Episode 72/100; Loss: 0.15077276527881622\n",
      "Step 90 (3882); Episode 72/100; Loss: 0.002013476798310876\n",
      "Step 91 (3883); Episode 72/100; Loss: 0.0018259999342262745\n",
      "Step 92 (3884); Episode 72/100; Loss: 0.0021231991704553366\n",
      "Step 93 (3885); Episode 72/100; Loss: 0.035446442663669586\n",
      "Step 94 (3886); Episode 72/100; Loss: 0.0008237523725256324\n",
      "Step 95 (3887); Episode 72/100; Loss: 0.059783611446619034\n",
      "Step 96 (3888); Episode 72/100; Loss: 0.031174957752227783\n",
      "Step 97 (3889); Episode 72/100; Loss: 0.0478832945227623\n",
      "Step 98 (3890); Episode 72/100; Loss: 0.08783001452684402\n",
      "Step 99 (3891); Episode 72/100; Loss: 0.0013669586041942239\n",
      "Step 100 (3892); Episode 72/100; Loss: 0.0011447247816249728\n",
      "Step 101 (3893); Episode 72/100; Loss: 0.0015086742350831628\n",
      "Step 102 (3894); Episode 72/100; Loss: 0.08979503065347672\n",
      "Step 103 (3895); Episode 72/100; Loss: 0.040994200855493546\n",
      "Step 104 (3896); Episode 72/100; Loss: 0.08962080627679825\n",
      "Step 105 (3897); Episode 72/100; Loss: 0.08044895529747009\n",
      "Step 106 (3898); Episode 72/100; Loss: 0.00177838746458292\n",
      "Step 107 (3899); Episode 72/100; Loss: 0.04102586209774017\n",
      "Step 108 (3900); Episode 72/100; Loss: 0.057341068983078\n",
      "Step 109 (3901); Episode 72/100; Loss: 0.11396551877260208\n",
      "Step 110 (3902); Episode 72/100; Loss: 0.0034367088228464127\n",
      "Step 111 (3903); Episode 72/100; Loss: 0.044817548245191574\n",
      "Step 112 (3904); Episode 72/100; Loss: 0.0111299529671669\n",
      "Step 113 (3905); Episode 72/100; Loss: 0.047937966883182526\n",
      "Step 114 (3906); Episode 72/100; Loss: 0.004320809151977301\n",
      "Step 115 (3907); Episode 72/100; Loss: 0.011828470043838024\n",
      "Step 116 (3908); Episode 72/100; Loss: 0.04974070563912392\n",
      "Step 117 (3909); Episode 72/100; Loss: 0.0387001596391201\n",
      "Step 118 (3910); Episode 72/100; Loss: 0.04720567911863327\n",
      "Step 119 (3911); Episode 72/100; Loss: 0.0018127725925296545\n",
      "Step 120 (3912); Episode 72/100; Loss: 0.0027326077688485384\n",
      "Step 121 (3913); Episode 72/100; Loss: 0.10003992915153503\n",
      "Step 122 (3914); Episode 72/100; Loss: 0.0334734171628952\n",
      "Step 123 (3915); Episode 72/100; Loss: 0.0008354265009984374\n",
      "Step 124 (3916); Episode 72/100; Loss: 0.005336809903383255\n",
      "Step 125 (3917); Episode 72/100; Loss: 0.0008521738927811384\n",
      "Step 126 (3918); Episode 72/100; Loss: 0.0011346664978191257\n",
      "Step 0 (3919); Episode 73/100; Loss: 0.04674544557929039\n",
      "Step 1 (3920); Episode 73/100; Loss: 0.14566850662231445\n",
      "Step 2 (3921); Episode 73/100; Loss: 0.002758365822955966\n",
      "Step 3 (3922); Episode 73/100; Loss: 0.09453687816858292\n",
      "Step 4 (3923); Episode 73/100; Loss: 0.05439280718564987\n",
      "Step 5 (3924); Episode 73/100; Loss: 0.14007358253002167\n",
      "Step 6 (3925); Episode 73/100; Loss: 0.0013435347937047482\n",
      "Step 7 (3926); Episode 73/100; Loss: 0.032734282314777374\n",
      "Step 8 (3927); Episode 73/100; Loss: 0.00943315401673317\n",
      "Step 9 (3928); Episode 73/100; Loss: 0.027399076148867607\n",
      "Step 10 (3929); Episode 73/100; Loss: 0.04726450517773628\n",
      "Step 11 (3930); Episode 73/100; Loss: 0.04187655821442604\n",
      "Step 12 (3931); Episode 73/100; Loss: 0.002533239545300603\n",
      "Step 13 (3932); Episode 73/100; Loss: 0.0015878152335062623\n",
      "Step 14 (3933); Episode 73/100; Loss: 0.04618140682578087\n",
      "Step 15 (3934); Episode 73/100; Loss: 0.022689472883939743\n",
      "Step 16 (3935); Episode 73/100; Loss: 0.04573363438248634\n",
      "Step 17 (3936); Episode 73/100; Loss: 0.005125474650412798\n",
      "Step 18 (3937); Episode 73/100; Loss: 0.050549715757369995\n",
      "Step 19 (3938); Episode 73/100; Loss: 0.001946206553839147\n",
      "Step 20 (3939); Episode 73/100; Loss: 0.0016752742230892181\n",
      "Step 21 (3940); Episode 73/100; Loss: 0.004055831581354141\n",
      "Step 22 (3941); Episode 73/100; Loss: 0.045968517661094666\n",
      "Step 23 (3942); Episode 73/100; Loss: 0.08701139688491821\n",
      "Step 24 (3943); Episode 73/100; Loss: 0.06373316794633865\n",
      "Step 25 (3944); Episode 73/100; Loss: 0.011704741045832634\n",
      "Step 26 (3945); Episode 73/100; Loss: 0.05167381465435028\n",
      "Step 27 (3946); Episode 73/100; Loss: 0.03393903002142906\n",
      "Step 28 (3947); Episode 73/100; Loss: 0.048814527690410614\n",
      "Step 29 (3948); Episode 73/100; Loss: 0.05332939326763153\n",
      "Step 30 (3949); Episode 73/100; Loss: 0.0033455826342105865\n",
      "Step 31 (3950); Episode 73/100; Loss: 0.1274111270904541\n",
      "Step 32 (3951); Episode 73/100; Loss: 0.08449272066354752\n",
      "Step 33 (3952); Episode 73/100; Loss: 0.12917423248291016\n",
      "Step 34 (3953); Episode 73/100; Loss: 0.054419100284576416\n",
      "Step 35 (3954); Episode 73/100; Loss: 0.033410947769880295\n",
      "Step 36 (3955); Episode 73/100; Loss: 0.12902669608592987\n",
      "Step 37 (3956); Episode 73/100; Loss: 0.04654332250356674\n",
      "Step 38 (3957); Episode 73/100; Loss: 0.005512448959052563\n",
      "Step 39 (3958); Episode 73/100; Loss: 0.003212293377146125\n",
      "Step 40 (3959); Episode 73/100; Loss: 0.003862053155899048\n",
      "Step 41 (3960); Episode 73/100; Loss: 0.010540956631302834\n",
      "Step 42 (3961); Episode 73/100; Loss: 0.004086427856236696\n",
      "Step 43 (3962); Episode 73/100; Loss: 0.128299281001091\n",
      "Step 44 (3963); Episode 73/100; Loss: 0.0015231132274493575\n",
      "Step 45 (3964); Episode 73/100; Loss: 0.045372139662504196\n",
      "Step 46 (3965); Episode 73/100; Loss: 0.0028343205340206623\n",
      "Step 47 (3966); Episode 73/100; Loss: 0.0035217537079006433\n",
      "Step 48 (3967); Episode 73/100; Loss: 0.00927850604057312\n",
      "Step 49 (3968); Episode 73/100; Loss: 0.0694732815027237\n",
      "Step 50 (3969); Episode 73/100; Loss: 0.020950084552168846\n",
      "Step 51 (3970); Episode 73/100; Loss: 0.0028449331875890493\n",
      "Step 52 (3971); Episode 73/100; Loss: 0.04461425542831421\n",
      "Step 53 (3972); Episode 73/100; Loss: 0.0021432056091725826\n",
      "Step 54 (3973); Episode 73/100; Loss: 0.05832548439502716\n",
      "Step 55 (3974); Episode 73/100; Loss: 0.08490461111068726\n",
      "Step 56 (3975); Episode 73/100; Loss: 0.04852091893553734\n",
      "Step 57 (3976); Episode 73/100; Loss: 0.0031220060773193836\n",
      "Step 58 (3977); Episode 73/100; Loss: 0.12145295739173889\n",
      "Step 59 (3978); Episode 73/100; Loss: 0.0037967886310070753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60 (3979); Episode 73/100; Loss: 0.05539073795080185\n",
      "Step 61 (3980); Episode 73/100; Loss: 0.045004572719335556\n",
      "Step 62 (3981); Episode 73/100; Loss: 0.18830609321594238\n",
      "Step 63 (3982); Episode 73/100; Loss: 0.04569455236196518\n",
      "Step 64 (3983); Episode 73/100; Loss: 0.009787817485630512\n",
      "Step 65 (3984); Episode 73/100; Loss: 0.009869667701423168\n",
      "Step 66 (3985); Episode 73/100; Loss: 0.03242627903819084\n",
      "Step 67 (3986); Episode 73/100; Loss: 0.0567893460392952\n",
      "Step 68 (3987); Episode 73/100; Loss: 0.058459751307964325\n",
      "Step 69 (3988); Episode 73/100; Loss: 0.08438678830862045\n",
      "Step 70 (3989); Episode 73/100; Loss: 0.05454017221927643\n",
      "Step 71 (3990); Episode 73/100; Loss: 0.0536249503493309\n",
      "Step 72 (3991); Episode 73/100; Loss: 0.001568586565554142\n",
      "Step 73 (3992); Episode 73/100; Loss: 0.042834434658288956\n",
      "Step 74 (3993); Episode 73/100; Loss: 0.1383090615272522\n",
      "Step 75 (3994); Episode 73/100; Loss: 0.03831860423088074\n",
      "Step 76 (3995); Episode 73/100; Loss: 0.041894420981407166\n",
      "Step 77 (3996); Episode 73/100; Loss: 0.016025926917791367\n",
      "Step 78 (3997); Episode 73/100; Loss: 0.002553650178015232\n",
      "Step 79 (3998); Episode 73/100; Loss: 0.0411672368645668\n",
      "Step 80 (3999); Episode 73/100; Loss: 0.1468234658241272\n",
      "Step 81 (4000); Episode 73/100; Loss: 0.115776926279068\n",
      "Step 82 (4001); Episode 73/100; Loss: 0.0012069670483469963\n",
      "Step 83 (4002); Episode 73/100; Loss: 0.0519302673637867\n",
      "Step 84 (4003); Episode 73/100; Loss: 0.0014920763205736876\n",
      "Step 85 (4004); Episode 73/100; Loss: 0.0841294452548027\n",
      "Step 86 (4005); Episode 73/100; Loss: 0.045962437987327576\n",
      "Step 87 (4006); Episode 73/100; Loss: 0.04300224781036377\n",
      "Step 88 (4007); Episode 73/100; Loss: 0.015720566734671593\n",
      "Step 89 (4008); Episode 73/100; Loss: 0.05080532282590866\n",
      "Step 90 (4009); Episode 73/100; Loss: 0.0029036393389105797\n",
      "Step 91 (4010); Episode 73/100; Loss: 0.0013249303447082639\n",
      "Step 92 (4011); Episode 73/100; Loss: 0.03734581544995308\n",
      "Step 93 (4012); Episode 73/100; Loss: 0.0018251306610181928\n",
      "Step 94 (4013); Episode 73/100; Loss: 0.04827794060111046\n",
      "Step 95 (4014); Episode 73/100; Loss: 0.04333782568573952\n",
      "Step 96 (4015); Episode 73/100; Loss: 0.045559678226709366\n",
      "Step 97 (4016); Episode 73/100; Loss: 0.03126664459705353\n",
      "Step 98 (4017); Episode 73/100; Loss: 0.16178473830223083\n",
      "Step 99 (4018); Episode 73/100; Loss: 0.0035890794824808836\n",
      "Step 100 (4019); Episode 73/100; Loss: 0.0499798022210598\n",
      "Step 101 (4020); Episode 73/100; Loss: 0.0029163954313844442\n",
      "Step 102 (4021); Episode 73/100; Loss: 0.005130174569785595\n",
      "Step 103 (4022); Episode 73/100; Loss: 0.0028326530009508133\n",
      "Step 104 (4023); Episode 73/100; Loss: 0.11792633682489395\n",
      "Step 105 (4024); Episode 73/100; Loss: 0.039907265454530716\n",
      "Step 106 (4025); Episode 73/100; Loss: 0.05163482949137688\n",
      "Step 107 (4026); Episode 73/100; Loss: 0.03461210057139397\n",
      "Step 108 (4027); Episode 73/100; Loss: 0.00813564844429493\n",
      "Step 109 (4028); Episode 73/100; Loss: 0.0010563216637820005\n",
      "Step 110 (4029); Episode 73/100; Loss: 0.039587099105119705\n",
      "Step 111 (4030); Episode 73/100; Loss: 0.07099997252225876\n",
      "Step 112 (4031); Episode 73/100; Loss: 0.0949450358748436\n",
      "Step 113 (4032); Episode 73/100; Loss: 0.07736335694789886\n",
      "Step 114 (4033); Episode 73/100; Loss: 0.07049115747213364\n",
      "Step 115 (4034); Episode 73/100; Loss: 0.03423328325152397\n",
      "Step 116 (4035); Episode 73/100; Loss: 0.0785117894411087\n",
      "Step 117 (4036); Episode 73/100; Loss: 0.07964695990085602\n",
      "Step 118 (4037); Episode 73/100; Loss: 0.11789356917142868\n",
      "Step 119 (4038); Episode 73/100; Loss: 0.002202523173764348\n",
      "Step 120 (4039); Episode 73/100; Loss: 0.0030363244004547596\n",
      "Step 121 (4040); Episode 73/100; Loss: 0.03046252392232418\n",
      "Step 122 (4041); Episode 73/100; Loss: 0.0033300144132226706\n",
      "Step 123 (4042); Episode 73/100; Loss: 0.002566056326031685\n",
      "Step 124 (4043); Episode 73/100; Loss: 0.04669543355703354\n",
      "Step 125 (4044); Episode 73/100; Loss: 0.011403282172977924\n",
      "Step 126 (4045); Episode 73/100; Loss: 0.01807512156665325\n",
      "Step 127 (4046); Episode 73/100; Loss: 0.002517871093004942\n",
      "Step 128 (4047); Episode 73/100; Loss: 0.0027411961928009987\n",
      "Step 129 (4048); Episode 73/100; Loss: 0.014424358494579792\n",
      "Step 130 (4049); Episode 73/100; Loss: 0.03994813561439514\n",
      "Step 0 (4050); Episode 74/100; Loss: 0.06886344403028488\n",
      "Step 1 (4051); Episode 74/100; Loss: 0.07631168514490128\n",
      "Step 2 (4052); Episode 74/100; Loss: 0.0028985419776290655\n",
      "Step 3 (4053); Episode 74/100; Loss: 0.001332338317297399\n",
      "Step 4 (4054); Episode 74/100; Loss: 0.1042005717754364\n",
      "Step 5 (4055); Episode 74/100; Loss: 0.03126967325806618\n",
      "Step 6 (4056); Episode 74/100; Loss: 0.1199442446231842\n",
      "Step 7 (4057); Episode 74/100; Loss: 0.03184189647436142\n",
      "Step 8 (4058); Episode 74/100; Loss: 0.16313233971595764\n",
      "Step 9 (4059); Episode 74/100; Loss: 0.05007611960172653\n",
      "Step 10 (4060); Episode 74/100; Loss: 0.004116921219974756\n",
      "Step 11 (4061); Episode 74/100; Loss: 0.044650621712207794\n",
      "Step 12 (4062); Episode 74/100; Loss: 0.0256891418248415\n",
      "Step 13 (4063); Episode 74/100; Loss: 0.06431673467159271\n",
      "Step 14 (4064); Episode 74/100; Loss: 0.0570492148399353\n",
      "Step 15 (4065); Episode 74/100; Loss: 0.09547179192304611\n",
      "Step 16 (4066); Episode 74/100; Loss: 0.030675796791911125\n",
      "Step 17 (4067); Episode 74/100; Loss: 0.09007790684700012\n",
      "Step 18 (4068); Episode 74/100; Loss: 0.0036295729223638773\n",
      "Step 19 (4069); Episode 74/100; Loss: 0.003634540131315589\n",
      "Step 20 (4070); Episode 74/100; Loss: 0.001569468411616981\n",
      "Step 21 (4071); Episode 74/100; Loss: 0.008619973435997963\n",
      "Step 22 (4072); Episode 74/100; Loss: 0.009021411649882793\n",
      "Step 23 (4073); Episode 74/100; Loss: 0.07813890278339386\n",
      "Step 24 (4074); Episode 74/100; Loss: 0.15369004011154175\n",
      "Step 25 (4075); Episode 74/100; Loss: 0.11601219326257706\n",
      "Step 26 (4076); Episode 74/100; Loss: 0.11910127848386765\n",
      "Step 27 (4077); Episode 74/100; Loss: 0.07176119834184647\n",
      "Step 28 (4078); Episode 74/100; Loss: 0.0019251495832577348\n",
      "Step 29 (4079); Episode 74/100; Loss: 0.0030954789835959673\n",
      "Step 30 (4080); Episode 74/100; Loss: 0.052163027226924896\n",
      "Step 31 (4081); Episode 74/100; Loss: 0.003042050404474139\n",
      "Step 32 (4082); Episode 74/100; Loss: 0.0016180037055164576\n",
      "Step 33 (4083); Episode 74/100; Loss: 0.05874034762382507\n",
      "Step 34 (4084); Episode 74/100; Loss: 0.06984691321849823\n",
      "Step 35 (4085); Episode 74/100; Loss: 0.027782779186964035\n",
      "Step 36 (4086); Episode 74/100; Loss: 0.04665951058268547\n",
      "Step 37 (4087); Episode 74/100; Loss: 0.001626260462217033\n",
      "Step 38 (4088); Episode 74/100; Loss: 0.04557943344116211\n",
      "Step 39 (4089); Episode 74/100; Loss: 0.08424974232912064\n",
      "Step 40 (4090); Episode 74/100; Loss: 0.00815325416624546\n",
      "Step 41 (4091); Episode 74/100; Loss: 0.0493876114487648\n",
      "Step 42 (4092); Episode 74/100; Loss: 0.007821755483746529\n",
      "Step 43 (4093); Episode 74/100; Loss: 0.04824274778366089\n",
      "Step 44 (4094); Episode 74/100; Loss: 0.052782367914915085\n",
      "Step 45 (4095); Episode 74/100; Loss: 0.19071631133556366\n",
      "Step 46 (4096); Episode 74/100; Loss: 0.0014794513117522001\n",
      "Step 47 (4097); Episode 74/100; Loss: 0.12377024441957474\n",
      "Step 48 (4098); Episode 74/100; Loss: 0.0025986870750784874\n",
      "Step 49 (4099); Episode 74/100; Loss: 0.0012380380649119616\n",
      "Step 50 (4100); Episode 74/100; Loss: 0.0018194562289863825\n",
      "Step 51 (4101); Episode 74/100; Loss: 0.05088552460074425\n",
      "Step 52 (4102); Episode 74/100; Loss: 0.0011728035751730204\n",
      "Step 53 (4103); Episode 74/100; Loss: 0.003991770092397928\n",
      "Step 54 (4104); Episode 74/100; Loss: 0.0027627351228147745\n",
      "Step 55 (4105); Episode 74/100; Loss: 0.0038621046114712954\n",
      "Step 56 (4106); Episode 74/100; Loss: 0.0019614803604781628\n",
      "Step 57 (4107); Episode 74/100; Loss: 0.004767475184053183\n",
      "Step 58 (4108); Episode 74/100; Loss: 0.0024968369398266077\n",
      "Step 59 (4109); Episode 74/100; Loss: 0.03870353475213051\n",
      "Step 60 (4110); Episode 74/100; Loss: 0.0342099666595459\n",
      "Step 61 (4111); Episode 74/100; Loss: 0.05884686857461929\n",
      "Step 62 (4112); Episode 74/100; Loss: 0.0374663844704628\n",
      "Step 63 (4113); Episode 74/100; Loss: 0.0023587802425026894\n",
      "Step 64 (4114); Episode 74/100; Loss: 0.0022873019333928823\n",
      "Step 65 (4115); Episode 74/100; Loss: 0.03831762820482254\n",
      "Step 66 (4116); Episode 74/100; Loss: 0.036867108196020126\n",
      "Step 67 (4117); Episode 74/100; Loss: 0.05619141086935997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 68 (4118); Episode 74/100; Loss: 0.0831577405333519\n",
      "Step 69 (4119); Episode 74/100; Loss: 0.006999810226261616\n",
      "Step 70 (4120); Episode 74/100; Loss: 0.15547803044319153\n",
      "Step 71 (4121); Episode 74/100; Loss: 0.03753386065363884\n",
      "Step 72 (4122); Episode 74/100; Loss: 0.0025154398754239082\n",
      "Step 73 (4123); Episode 74/100; Loss: 0.041961919516325\n",
      "Step 74 (4124); Episode 74/100; Loss: 0.001593109336681664\n",
      "Step 75 (4125); Episode 74/100; Loss: 0.005293077789247036\n",
      "Step 76 (4126); Episode 74/100; Loss: 0.050592660903930664\n",
      "Step 77 (4127); Episode 74/100; Loss: 0.012165066786110401\n",
      "Step 78 (4128); Episode 74/100; Loss: 0.10708027333021164\n",
      "Step 79 (4129); Episode 74/100; Loss: 0.07654263824224472\n",
      "Step 80 (4130); Episode 74/100; Loss: 0.04608166590332985\n",
      "Step 81 (4131); Episode 74/100; Loss: 0.0014428673312067986\n",
      "Step 82 (4132); Episode 74/100; Loss: 0.06394000351428986\n",
      "Step 83 (4133); Episode 74/100; Loss: 0.08581758290529251\n",
      "Step 84 (4134); Episode 74/100; Loss: 0.07057677209377289\n",
      "Step 85 (4135); Episode 74/100; Loss: 0.06060720607638359\n",
      "Step 86 (4136); Episode 74/100; Loss: 0.003606891492381692\n",
      "Step 87 (4137); Episode 74/100; Loss: 0.04711577296257019\n",
      "Step 88 (4138); Episode 74/100; Loss: 0.037335459142923355\n",
      "Step 89 (4139); Episode 74/100; Loss: 0.0016727725742384791\n",
      "Step 90 (4140); Episode 74/100; Loss: 0.002155453898012638\n",
      "Step 91 (4141); Episode 74/100; Loss: 0.0008921126136556268\n",
      "Step 92 (4142); Episode 74/100; Loss: 0.0028614250477403402\n",
      "Step 93 (4143); Episode 74/100; Loss: 0.001290210522711277\n",
      "Step 94 (4144); Episode 74/100; Loss: 0.001688340911641717\n",
      "Step 95 (4145); Episode 74/100; Loss: 0.12120258063077927\n",
      "Step 96 (4146); Episode 74/100; Loss: 0.06642799079418182\n",
      "Step 97 (4147); Episode 74/100; Loss: 0.029478644952178\n",
      "Step 98 (4148); Episode 74/100; Loss: 0.03971073776483536\n",
      "Step 99 (4149); Episode 74/100; Loss: 0.054902371019124985\n",
      "Step 100 (4150); Episode 74/100; Loss: 0.054544225335121155\n",
      "Step 101 (4151); Episode 74/100; Loss: 0.07664339244365692\n",
      "Step 102 (4152); Episode 74/100; Loss: 0.002328029368072748\n",
      "Step 103 (4153); Episode 74/100; Loss: 0.005729785189032555\n",
      "Step 104 (4154); Episode 74/100; Loss: 0.07047863304615021\n",
      "Step 105 (4155); Episode 74/100; Loss: 0.0350407250225544\n",
      "Step 106 (4156); Episode 74/100; Loss: 0.00268161972053349\n",
      "Step 107 (4157); Episode 74/100; Loss: 0.07010901719331741\n",
      "Step 108 (4158); Episode 74/100; Loss: 0.03918278217315674\n",
      "Step 109 (4159); Episode 74/100; Loss: 0.01678059995174408\n",
      "Step 110 (4160); Episode 74/100; Loss: 0.0020754013676196337\n",
      "Step 111 (4161); Episode 74/100; Loss: 0.003983892034739256\n",
      "Step 112 (4162); Episode 74/100; Loss: 0.02868407778441906\n",
      "Step 113 (4163); Episode 74/100; Loss: 0.03470464050769806\n",
      "Step 114 (4164); Episode 74/100; Loss: 0.0342559851706028\n",
      "Step 115 (4165); Episode 74/100; Loss: 0.0040836879052221775\n",
      "Step 116 (4166); Episode 74/100; Loss: 0.11661124229431152\n",
      "Step 117 (4167); Episode 74/100; Loss: 0.0035793923307210207\n",
      "Step 118 (4168); Episode 74/100; Loss: 0.08130651712417603\n",
      "Step 119 (4169); Episode 74/100; Loss: 0.028972405940294266\n",
      "Step 120 (4170); Episode 74/100; Loss: 0.0025799579452723265\n",
      "Step 121 (4171); Episode 74/100; Loss: 0.037340860813856125\n",
      "Step 122 (4172); Episode 74/100; Loss: 0.0027451745700091124\n",
      "Step 123 (4173); Episode 74/100; Loss: 0.06223071739077568\n",
      "Step 124 (4174); Episode 74/100; Loss: 0.08366487920284271\n",
      "Step 125 (4175); Episode 74/100; Loss: 0.04582180455327034\n",
      "Step 126 (4176); Episode 74/100; Loss: 0.10247392952442169\n",
      "Step 127 (4177); Episode 74/100; Loss: 0.12862925231456757\n",
      "Step 128 (4178); Episode 74/100; Loss: 0.16471050679683685\n",
      "Step 129 (4179); Episode 74/100; Loss: 0.0016705942107364535\n",
      "Step 130 (4180); Episode 74/100; Loss: 0.06202332302927971\n",
      "Step 131 (4181); Episode 74/100; Loss: 0.045683909207582474\n",
      "Step 132 (4182); Episode 74/100; Loss: 0.09016916155815125\n",
      "Step 133 (4183); Episode 74/100; Loss: 0.06281518191099167\n",
      "Step 134 (4184); Episode 74/100; Loss: 0.0011061265831813216\n",
      "Step 135 (4185); Episode 74/100; Loss: 0.0703984871506691\n",
      "Step 136 (4186); Episode 74/100; Loss: 0.050220854580402374\n",
      "Step 137 (4187); Episode 74/100; Loss: 0.07809391617774963\n",
      "Step 138 (4188); Episode 74/100; Loss: 0.047049567103385925\n",
      "Step 139 (4189); Episode 74/100; Loss: 0.004271900746971369\n",
      "Step 140 (4190); Episode 74/100; Loss: 0.08650965988636017\n",
      "Step 141 (4191); Episode 74/100; Loss: 0.002494663931429386\n",
      "Step 142 (4192); Episode 74/100; Loss: 0.004548782482743263\n",
      "Step 143 (4193); Episode 74/100; Loss: 0.0016136851627379656\n",
      "Step 144 (4194); Episode 74/100; Loss: 0.003980673383921385\n",
      "Step 145 (4195); Episode 74/100; Loss: 0.05043715611100197\n",
      "Step 146 (4196); Episode 74/100; Loss: 0.03930608555674553\n",
      "Step 147 (4197); Episode 74/100; Loss: 0.0897764340043068\n",
      "Step 148 (4198); Episode 74/100; Loss: 0.012579192407429218\n",
      "Step 0 (4199); Episode 75/100; Loss: 0.001396799460053444\n",
      "Step 1 (4200); Episode 75/100; Loss: 0.034365955740213394\n",
      "Step 2 (4201); Episode 75/100; Loss: 0.0776665210723877\n",
      "Step 3 (4202); Episode 75/100; Loss: 0.03338569775223732\n",
      "Step 4 (4203); Episode 75/100; Loss: 0.018431857228279114\n",
      "Step 5 (4204); Episode 75/100; Loss: 0.006240993272513151\n",
      "Step 6 (4205); Episode 75/100; Loss: 0.055499982088804245\n",
      "Step 7 (4206); Episode 75/100; Loss: 0.07234620302915573\n",
      "Step 8 (4207); Episode 75/100; Loss: 0.03788534924387932\n",
      "Step 9 (4208); Episode 75/100; Loss: 0.000862574961502105\n",
      "Step 10 (4209); Episode 75/100; Loss: 0.004678195808082819\n",
      "Step 11 (4210); Episode 75/100; Loss: 0.04099094495177269\n",
      "Step 12 (4211); Episode 75/100; Loss: 0.0857403352856636\n",
      "Step 13 (4212); Episode 75/100; Loss: 0.08872067928314209\n",
      "Step 14 (4213); Episode 75/100; Loss: 0.07092631608247757\n",
      "Step 15 (4214); Episode 75/100; Loss: 0.03623265027999878\n",
      "Step 16 (4215); Episode 75/100; Loss: 0.05387682095170021\n",
      "Step 17 (4216); Episode 75/100; Loss: 0.05040009692311287\n",
      "Step 18 (4217); Episode 75/100; Loss: 0.04817354306578636\n",
      "Step 19 (4218); Episode 75/100; Loss: 0.0024736810009926558\n",
      "Step 20 (4219); Episode 75/100; Loss: 0.007537413388490677\n",
      "Step 21 (4220); Episode 75/100; Loss: 0.012222656980156898\n",
      "Step 22 (4221); Episode 75/100; Loss: 0.001958675915375352\n",
      "Step 23 (4222); Episode 75/100; Loss: 0.0011866126442328095\n",
      "Step 24 (4223); Episode 75/100; Loss: 0.0012856785906478763\n",
      "Step 25 (4224); Episode 75/100; Loss: 0.04422442987561226\n",
      "Step 26 (4225); Episode 75/100; Loss: 0.01731707528233528\n",
      "Step 27 (4226); Episode 75/100; Loss: 0.04654639586806297\n",
      "Step 28 (4227); Episode 75/100; Loss: 0.003620406147092581\n",
      "Step 29 (4228); Episode 75/100; Loss: 0.021937882527709007\n",
      "Step 30 (4229); Episode 75/100; Loss: 0.0447966642677784\n",
      "Step 31 (4230); Episode 75/100; Loss: 0.009209894575178623\n",
      "Step 32 (4231); Episode 75/100; Loss: 0.0010491053108125925\n",
      "Step 33 (4232); Episode 75/100; Loss: 0.0026425286196172237\n",
      "Step 34 (4233); Episode 75/100; Loss: 0.04890440031886101\n",
      "Step 35 (4234); Episode 75/100; Loss: 0.08988325297832489\n",
      "Step 36 (4235); Episode 75/100; Loss: 0.03154482692480087\n",
      "Step 37 (4236); Episode 75/100; Loss: 0.02980678901076317\n",
      "Step 38 (4237); Episode 75/100; Loss: 0.04667111486196518\n",
      "Step 39 (4238); Episode 75/100; Loss: 0.1380034238100052\n",
      "Step 40 (4239); Episode 75/100; Loss: 0.08070376515388489\n",
      "Step 41 (4240); Episode 75/100; Loss: 0.07190445065498352\n",
      "Step 42 (4241); Episode 75/100; Loss: 0.04169385880231857\n",
      "Step 43 (4242); Episode 75/100; Loss: 0.01626983843743801\n",
      "Step 44 (4243); Episode 75/100; Loss: 0.02191968448460102\n",
      "Step 45 (4244); Episode 75/100; Loss: 0.006584781222045422\n",
      "Step 46 (4245); Episode 75/100; Loss: 0.061985235661268234\n",
      "Step 47 (4246); Episode 75/100; Loss: 0.046692125499248505\n",
      "Step 48 (4247); Episode 75/100; Loss: 0.08477779477834702\n",
      "Step 49 (4248); Episode 75/100; Loss: 0.04748690128326416\n",
      "Step 50 (4249); Episode 75/100; Loss: 0.07893969863653183\n",
      "Step 51 (4250); Episode 75/100; Loss: 0.015145424753427505\n",
      "Step 52 (4251); Episode 75/100; Loss: 0.087620809674263\n",
      "Step 53 (4252); Episode 75/100; Loss: 0.005014327820390463\n",
      "Step 54 (4253); Episode 75/100; Loss: 0.0826292484998703\n",
      "Step 55 (4254); Episode 75/100; Loss: 0.018313614651560783\n",
      "Step 56 (4255); Episode 75/100; Loss: 0.004338969010859728\n",
      "Step 57 (4256); Episode 75/100; Loss: 0.06302763521671295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58 (4257); Episode 75/100; Loss: 0.027449704706668854\n",
      "Step 59 (4258); Episode 75/100; Loss: 0.0037123507354408503\n",
      "Step 60 (4259); Episode 75/100; Loss: 0.08726800233125687\n",
      "Step 61 (4260); Episode 75/100; Loss: 0.04584500193595886\n",
      "Step 62 (4261); Episode 75/100; Loss: 0.0019145173719152808\n",
      "Step 63 (4262); Episode 75/100; Loss: 0.002788734622299671\n",
      "Step 64 (4263); Episode 75/100; Loss: 0.03025241009891033\n",
      "Step 65 (4264); Episode 75/100; Loss: 0.06606557220220566\n",
      "Step 66 (4265); Episode 75/100; Loss: 0.03936808183789253\n",
      "Step 67 (4266); Episode 75/100; Loss: 0.04317076876759529\n",
      "Step 68 (4267); Episode 75/100; Loss: 0.0017347640823572874\n",
      "Step 69 (4268); Episode 75/100; Loss: 0.005607374012470245\n",
      "Step 70 (4269); Episode 75/100; Loss: 0.10078155994415283\n",
      "Step 71 (4270); Episode 75/100; Loss: 0.021939735859632492\n",
      "Step 72 (4271); Episode 75/100; Loss: 0.03567635267972946\n",
      "Step 73 (4272); Episode 75/100; Loss: 0.05872632563114166\n",
      "Step 74 (4273); Episode 75/100; Loss: 0.011601722799241543\n",
      "Step 75 (4274); Episode 75/100; Loss: 0.0020386637188494205\n",
      "Step 76 (4275); Episode 75/100; Loss: 0.00867402646690607\n",
      "Step 77 (4276); Episode 75/100; Loss: 0.0043368861079216\n",
      "Step 78 (4277); Episode 75/100; Loss: 0.029125716537237167\n",
      "Step 79 (4278); Episode 75/100; Loss: 0.07855990529060364\n",
      "Step 80 (4279); Episode 75/100; Loss: 0.0023973395582288504\n",
      "Step 81 (4280); Episode 75/100; Loss: 0.006920024286955595\n",
      "Step 82 (4281); Episode 75/100; Loss: 0.07813920080661774\n",
      "Step 83 (4282); Episode 75/100; Loss: 0.04525875300168991\n",
      "Step 84 (4283); Episode 75/100; Loss: 0.09149336069822311\n",
      "Step 85 (4284); Episode 75/100; Loss: 0.0019983015954494476\n",
      "Step 86 (4285); Episode 75/100; Loss: 0.05119243264198303\n",
      "Step 87 (4286); Episode 75/100; Loss: 0.04929066821932793\n",
      "Step 88 (4287); Episode 75/100; Loss: 0.04209288954734802\n",
      "Step 89 (4288); Episode 75/100; Loss: 0.05030224099755287\n",
      "Step 90 (4289); Episode 75/100; Loss: 0.006802202668040991\n",
      "Step 91 (4290); Episode 75/100; Loss: 0.002555444370955229\n",
      "Step 92 (4291); Episode 75/100; Loss: 0.0014016064815223217\n",
      "Step 93 (4292); Episode 75/100; Loss: 0.08553740382194519\n",
      "Step 94 (4293); Episode 75/100; Loss: 0.0570603646337986\n",
      "Step 95 (4294); Episode 75/100; Loss: 0.031527359038591385\n",
      "Step 96 (4295); Episode 75/100; Loss: 0.09765280783176422\n",
      "Step 97 (4296); Episode 75/100; Loss: 0.13190129399299622\n",
      "Step 98 (4297); Episode 75/100; Loss: 0.05029892176389694\n",
      "Step 99 (4298); Episode 75/100; Loss: 0.12967254221439362\n",
      "Step 100 (4299); Episode 75/100; Loss: 0.0563923604786396\n",
      "Step 101 (4300); Episode 75/100; Loss: 0.0777878537774086\n",
      "Step 102 (4301); Episode 75/100; Loss: 0.0022942442446947098\n",
      "Step 103 (4302); Episode 75/100; Loss: 0.048198383301496506\n",
      "Step 104 (4303); Episode 75/100; Loss: 0.0025031748227775097\n",
      "Step 105 (4304); Episode 75/100; Loss: 0.0014001346426084638\n",
      "Step 106 (4305); Episode 75/100; Loss: 0.02899942547082901\n",
      "Step 107 (4306); Episode 75/100; Loss: 0.052490562200546265\n",
      "Step 108 (4307); Episode 75/100; Loss: 0.049885112792253494\n",
      "Step 109 (4308); Episode 75/100; Loss: 0.0350811667740345\n",
      "Step 110 (4309); Episode 75/100; Loss: 0.001957262633368373\n",
      "Step 111 (4310); Episode 75/100; Loss: 0.055809590965509415\n",
      "Step 112 (4311); Episode 75/100; Loss: 0.08885997533798218\n",
      "Step 113 (4312); Episode 75/100; Loss: 0.02221405692398548\n",
      "Step 114 (4313); Episode 75/100; Loss: 0.07209104299545288\n",
      "Step 115 (4314); Episode 75/100; Loss: 0.0025044619105756283\n",
      "Step 116 (4315); Episode 75/100; Loss: 0.024721577763557434\n",
      "Step 117 (4316); Episode 75/100; Loss: 0.04366801306605339\n",
      "Step 118 (4317); Episode 75/100; Loss: 0.0738053247332573\n",
      "Step 119 (4318); Episode 75/100; Loss: 0.04149080440402031\n",
      "Step 120 (4319); Episode 75/100; Loss: 0.050680048763751984\n",
      "Step 121 (4320); Episode 75/100; Loss: 0.0796930193901062\n",
      "Step 122 (4321); Episode 75/100; Loss: 0.004598198924213648\n",
      "Step 123 (4322); Episode 75/100; Loss: 0.02808237448334694\n",
      "Step 124 (4323); Episode 75/100; Loss: 0.05542662739753723\n",
      "Step 125 (4324); Episode 75/100; Loss: 0.0038778844755142927\n",
      "Step 126 (4325); Episode 75/100; Loss: 0.05643670633435249\n",
      "Step 127 (4326); Episode 75/100; Loss: 0.03568186238408089\n",
      "Step 0 (4327); Episode 76/100; Loss: 0.029165005311369896\n",
      "Step 1 (4328); Episode 76/100; Loss: 0.036967597901821136\n",
      "Step 2 (4329); Episode 76/100; Loss: 0.06684378534555435\n",
      "Step 3 (4330); Episode 76/100; Loss: 0.0038078308571130037\n",
      "Step 4 (4331); Episode 76/100; Loss: 0.06096871942281723\n",
      "Step 5 (4332); Episode 76/100; Loss: 0.07569244503974915\n",
      "Step 6 (4333); Episode 76/100; Loss: 0.13707436621189117\n",
      "Step 7 (4334); Episode 76/100; Loss: 0.1480933576822281\n",
      "Step 8 (4335); Episode 76/100; Loss: 0.06468446552753448\n",
      "Step 9 (4336); Episode 76/100; Loss: 0.0025289244949817657\n",
      "Step 10 (4337); Episode 76/100; Loss: 0.047335486859083176\n",
      "Step 11 (4338); Episode 76/100; Loss: 0.00728711299598217\n",
      "Step 12 (4339); Episode 76/100; Loss: 0.001126876100897789\n",
      "Step 13 (4340); Episode 76/100; Loss: 0.010806240141391754\n",
      "Step 14 (4341); Episode 76/100; Loss: 0.031532734632492065\n",
      "Step 15 (4342); Episode 76/100; Loss: 0.017680853605270386\n",
      "Step 16 (4343); Episode 76/100; Loss: 0.010001675225794315\n",
      "Step 17 (4344); Episode 76/100; Loss: 0.028672292828559875\n",
      "Step 18 (4345); Episode 76/100; Loss: 0.04279094561934471\n",
      "Step 19 (4346); Episode 76/100; Loss: 0.03174338489770889\n",
      "Step 20 (4347); Episode 76/100; Loss: 0.0020416693296283484\n",
      "Step 21 (4348); Episode 76/100; Loss: 0.005798232275992632\n",
      "Step 22 (4349); Episode 76/100; Loss: 0.11385531723499298\n",
      "Step 23 (4350); Episode 76/100; Loss: 0.006030003074556589\n",
      "Step 24 (4351); Episode 76/100; Loss: 0.09095451235771179\n",
      "Step 25 (4352); Episode 76/100; Loss: 0.0018141638720408082\n",
      "Step 26 (4353); Episode 76/100; Loss: 0.00165180501062423\n",
      "Step 27 (4354); Episode 76/100; Loss: 0.004377642180770636\n",
      "Step 28 (4355); Episode 76/100; Loss: 0.045562513172626495\n",
      "Step 29 (4356); Episode 76/100; Loss: 0.03151535615324974\n",
      "Step 30 (4357); Episode 76/100; Loss: 0.04513934999704361\n",
      "Step 31 (4358); Episode 76/100; Loss: 0.1482977569103241\n",
      "Step 32 (4359); Episode 76/100; Loss: 0.15573401749134064\n",
      "Step 33 (4360); Episode 76/100; Loss: 0.002427840605378151\n",
      "Step 34 (4361); Episode 76/100; Loss: 0.04277629777789116\n",
      "Step 35 (4362); Episode 76/100; Loss: 0.0012767005246132612\n",
      "Step 36 (4363); Episode 76/100; Loss: 0.07997818291187286\n",
      "Step 37 (4364); Episode 76/100; Loss: 0.044452816247940063\n",
      "Step 38 (4365); Episode 76/100; Loss: 0.0045770639553666115\n",
      "Step 39 (4366); Episode 76/100; Loss: 0.004941754508763552\n",
      "Step 40 (4367); Episode 76/100; Loss: 0.0016975798644125462\n",
      "Step 41 (4368); Episode 76/100; Loss: 0.002293821657076478\n",
      "Step 42 (4369); Episode 76/100; Loss: 0.09386046230792999\n",
      "Step 43 (4370); Episode 76/100; Loss: 0.035909462720155716\n",
      "Step 44 (4371); Episode 76/100; Loss: 0.10358713567256927\n",
      "Step 45 (4372); Episode 76/100; Loss: 0.03711917623877525\n",
      "Step 46 (4373); Episode 76/100; Loss: 0.058289915323257446\n",
      "Step 47 (4374); Episode 76/100; Loss: 0.001972744707018137\n",
      "Step 48 (4375); Episode 76/100; Loss: 0.03524065762758255\n",
      "Step 49 (4376); Episode 76/100; Loss: 0.013322652317583561\n",
      "Step 50 (4377); Episode 76/100; Loss: 0.0372186079621315\n",
      "Step 51 (4378); Episode 76/100; Loss: 0.09923651069402695\n",
      "Step 52 (4379); Episode 76/100; Loss: 0.02771916426718235\n",
      "Step 53 (4380); Episode 76/100; Loss: 0.03798051178455353\n",
      "Step 54 (4381); Episode 76/100; Loss: 0.09490881860256195\n",
      "Step 55 (4382); Episode 76/100; Loss: 0.0592520497739315\n",
      "Step 56 (4383); Episode 76/100; Loss: 0.04431432858109474\n",
      "Step 57 (4384); Episode 76/100; Loss: 0.06445258855819702\n",
      "Step 58 (4385); Episode 76/100; Loss: 0.015167104080319405\n",
      "Step 59 (4386); Episode 76/100; Loss: 0.06531761586666107\n",
      "Step 60 (4387); Episode 76/100; Loss: 0.009057441726326942\n",
      "Step 61 (4388); Episode 76/100; Loss: 0.04218082129955292\n",
      "Step 62 (4389); Episode 76/100; Loss: 0.0040984321385622025\n",
      "Step 63 (4390); Episode 76/100; Loss: 0.045019786804914474\n",
      "Step 64 (4391); Episode 76/100; Loss: 0.10318909585475922\n",
      "Step 65 (4392); Episode 76/100; Loss: 0.01968197710812092\n",
      "Step 66 (4393); Episode 76/100; Loss: 0.02390173077583313\n",
      "Step 67 (4394); Episode 76/100; Loss: 0.0030232525896281004\n",
      "Step 68 (4395); Episode 76/100; Loss: 0.09037086367607117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 69 (4396); Episode 76/100; Loss: 0.07707726955413818\n",
      "Step 70 (4397); Episode 76/100; Loss: 0.06521176546812057\n",
      "Step 71 (4398); Episode 76/100; Loss: 0.019383518025279045\n",
      "Step 72 (4399); Episode 76/100; Loss: 0.03968635946512222\n",
      "Step 73 (4400); Episode 76/100; Loss: 0.08847648650407791\n",
      "Step 74 (4401); Episode 76/100; Loss: 0.02482927218079567\n",
      "Step 75 (4402); Episode 76/100; Loss: 0.009230970405042171\n",
      "Step 76 (4403); Episode 76/100; Loss: 0.03727005422115326\n",
      "Step 77 (4404); Episode 76/100; Loss: 0.08066309988498688\n",
      "Step 78 (4405); Episode 76/100; Loss: 0.0038723968900740147\n",
      "Step 79 (4406); Episode 76/100; Loss: 0.04095115140080452\n",
      "Step 80 (4407); Episode 76/100; Loss: 0.004301224369555712\n",
      "Step 81 (4408); Episode 76/100; Loss: 0.005713550373911858\n",
      "Step 82 (4409); Episode 76/100; Loss: 0.05451306700706482\n",
      "Step 83 (4410); Episode 76/100; Loss: 0.040056899189949036\n",
      "Step 84 (4411); Episode 76/100; Loss: 0.11295629292726517\n",
      "Step 85 (4412); Episode 76/100; Loss: 0.002611404750496149\n",
      "Step 86 (4413); Episode 76/100; Loss: 0.0026858740020543337\n",
      "Step 87 (4414); Episode 76/100; Loss: 0.06300715357065201\n",
      "Step 88 (4415); Episode 76/100; Loss: 0.11891995370388031\n",
      "Step 89 (4416); Episode 76/100; Loss: 0.11192972213029861\n",
      "Step 90 (4417); Episode 76/100; Loss: 0.07027210295200348\n",
      "Step 91 (4418); Episode 76/100; Loss: 0.048341378569602966\n",
      "Step 92 (4419); Episode 76/100; Loss: 0.08050086349248886\n",
      "Step 93 (4420); Episode 76/100; Loss: 0.05134611576795578\n",
      "Step 94 (4421); Episode 76/100; Loss: 0.037686750292778015\n",
      "Step 95 (4422); Episode 76/100; Loss: 0.005737804342061281\n",
      "Step 96 (4423); Episode 76/100; Loss: 0.04300113022327423\n",
      "Step 97 (4424); Episode 76/100; Loss: 0.002496521919965744\n",
      "Step 98 (4425); Episode 76/100; Loss: 0.02346774935722351\n",
      "Step 99 (4426); Episode 76/100; Loss: 0.0033761663362383842\n",
      "Step 100 (4427); Episode 76/100; Loss: 0.03497195243835449\n",
      "Step 101 (4428); Episode 76/100; Loss: 0.00206716638058424\n",
      "Step 102 (4429); Episode 76/100; Loss: 0.003797980723902583\n",
      "Step 103 (4430); Episode 76/100; Loss: 0.037946268916130066\n",
      "Step 104 (4431); Episode 76/100; Loss: 0.03841714188456535\n",
      "Step 105 (4432); Episode 76/100; Loss: 0.00202730018645525\n",
      "Step 106 (4433); Episode 76/100; Loss: 0.010689151473343372\n",
      "Step 107 (4434); Episode 76/100; Loss: 0.04954664409160614\n",
      "Step 108 (4435); Episode 76/100; Loss: 0.0028788731433451176\n",
      "Step 109 (4436); Episode 76/100; Loss: 0.05442072078585625\n",
      "Step 110 (4437); Episode 76/100; Loss: 0.042592454701662064\n",
      "Step 111 (4438); Episode 76/100; Loss: 0.0870569720864296\n",
      "Step 112 (4439); Episode 76/100; Loss: 0.030848905444145203\n",
      "Step 113 (4440); Episode 76/100; Loss: 0.14263015985488892\n",
      "Step 114 (4441); Episode 76/100; Loss: 0.09008986502885818\n",
      "Step 115 (4442); Episode 76/100; Loss: 0.002033818745985627\n",
      "Step 116 (4443); Episode 76/100; Loss: 0.0035696392878890038\n",
      "Step 117 (4444); Episode 76/100; Loss: 0.051561854779720306\n",
      "Step 118 (4445); Episode 76/100; Loss: 0.0030053388327360153\n",
      "Step 119 (4446); Episode 76/100; Loss: 0.0038216926623135805\n",
      "Step 120 (4447); Episode 76/100; Loss: 0.0014055800857022405\n",
      "Step 121 (4448); Episode 76/100; Loss: 0.09674067795276642\n",
      "Step 122 (4449); Episode 76/100; Loss: 0.006455691531300545\n",
      "Step 123 (4450); Episode 76/100; Loss: 0.0012819189578294754\n",
      "Step 124 (4451); Episode 76/100; Loss: 0.1465405374765396\n",
      "Step 125 (4452); Episode 76/100; Loss: 0.003956343978643417\n",
      "Step 126 (4453); Episode 76/100; Loss: 0.06669558584690094\n",
      "Step 127 (4454); Episode 76/100; Loss: 0.0016469599213451147\n",
      "Step 128 (4455); Episode 76/100; Loss: 0.032750651240348816\n",
      "Step 129 (4456); Episode 76/100; Loss: 0.019640492275357246\n",
      "Step 130 (4457); Episode 76/100; Loss: 0.004133586306124926\n",
      "Step 131 (4458); Episode 76/100; Loss: 0.007096025627106428\n",
      "Step 132 (4459); Episode 76/100; Loss: 0.006126806605607271\n",
      "Step 133 (4460); Episode 76/100; Loss: 0.07999023050069809\n",
      "Step 134 (4461); Episode 76/100; Loss: 0.011448998004198074\n",
      "Step 135 (4462); Episode 76/100; Loss: 0.043159693479537964\n",
      "Step 136 (4463); Episode 76/100; Loss: 0.001574514782987535\n",
      "Step 137 (4464); Episode 76/100; Loss: 0.04283624514937401\n",
      "Step 138 (4465); Episode 76/100; Loss: 0.0038766919169574976\n",
      "Step 139 (4466); Episode 76/100; Loss: 0.0022272623609751463\n",
      "Step 140 (4467); Episode 76/100; Loss: 0.05063387379050255\n",
      "Step 0 (4468); Episode 77/100; Loss: 0.002138512209057808\n",
      "Step 1 (4469); Episode 77/100; Loss: 0.10984158515930176\n",
      "Step 2 (4470); Episode 77/100; Loss: 0.08489365130662918\n",
      "Step 3 (4471); Episode 77/100; Loss: 0.06962598115205765\n",
      "Step 4 (4472); Episode 77/100; Loss: 0.0312592051923275\n",
      "Step 5 (4473); Episode 77/100; Loss: 0.0033678985200822353\n",
      "Step 6 (4474); Episode 77/100; Loss: 0.00292978435754776\n",
      "Step 7 (4475); Episode 77/100; Loss: 0.003264838131144643\n",
      "Step 8 (4476); Episode 77/100; Loss: 0.09119492769241333\n",
      "Step 9 (4477); Episode 77/100; Loss: 0.005012080539017916\n",
      "Step 10 (4478); Episode 77/100; Loss: 0.0032623945735394955\n",
      "Step 11 (4479); Episode 77/100; Loss: 0.044147852808237076\n",
      "Step 12 (4480); Episode 77/100; Loss: 0.14772389829158783\n",
      "Step 13 (4481); Episode 77/100; Loss: 0.06745251268148422\n",
      "Step 14 (4482); Episode 77/100; Loss: 0.07108033448457718\n",
      "Step 15 (4483); Episode 77/100; Loss: 0.0019821925088763237\n",
      "Step 16 (4484); Episode 77/100; Loss: 0.030325958505272865\n",
      "Step 17 (4485); Episode 77/100; Loss: 0.001528235268779099\n",
      "Step 18 (4486); Episode 77/100; Loss: 0.02960135042667389\n",
      "Step 19 (4487); Episode 77/100; Loss: 0.04609240964055061\n",
      "Step 20 (4488); Episode 77/100; Loss: 0.0462288074195385\n",
      "Step 21 (4489); Episode 77/100; Loss: 0.003183499677106738\n",
      "Step 22 (4490); Episode 77/100; Loss: 0.0897451862692833\n",
      "Step 23 (4491); Episode 77/100; Loss: 0.07121563702821732\n",
      "Step 24 (4492); Episode 77/100; Loss: 0.08701971918344498\n",
      "Step 25 (4493); Episode 77/100; Loss: 0.07897777855396271\n",
      "Step 26 (4494); Episode 77/100; Loss: 0.03959963470697403\n",
      "Step 27 (4495); Episode 77/100; Loss: 0.04891005530953407\n",
      "Step 28 (4496); Episode 77/100; Loss: 0.0024160125758498907\n",
      "Step 29 (4497); Episode 77/100; Loss: 0.002329530194401741\n",
      "Step 30 (4498); Episode 77/100; Loss: 0.025812232866883278\n",
      "Step 31 (4499); Episode 77/100; Loss: 0.08432266861200333\n",
      "Step 32 (4500); Episode 77/100; Loss: 0.04043472930788994\n",
      "Step 33 (4501); Episode 77/100; Loss: 0.0017358398763462901\n",
      "Step 34 (4502); Episode 77/100; Loss: 0.011151070706546307\n",
      "Step 35 (4503); Episode 77/100; Loss: 0.0016350809019058943\n",
      "Step 36 (4504); Episode 77/100; Loss: 0.07706822454929352\n",
      "Step 37 (4505); Episode 77/100; Loss: 0.08377116173505783\n",
      "Step 38 (4506); Episode 77/100; Loss: 0.10113976150751114\n",
      "Step 39 (4507); Episode 77/100; Loss: 0.013892538845539093\n",
      "Step 40 (4508); Episode 77/100; Loss: 0.03995437175035477\n",
      "Step 41 (4509); Episode 77/100; Loss: 0.014504633843898773\n",
      "Step 42 (4510); Episode 77/100; Loss: 0.0009320456301793456\n",
      "Step 43 (4511); Episode 77/100; Loss: 0.0020868878345936537\n",
      "Step 44 (4512); Episode 77/100; Loss: 0.042644090950489044\n",
      "Step 45 (4513); Episode 77/100; Loss: 0.07806860655546188\n",
      "Step 46 (4514); Episode 77/100; Loss: 0.09390799701213837\n",
      "Step 47 (4515); Episode 77/100; Loss: 0.013206681236624718\n",
      "Step 48 (4516); Episode 77/100; Loss: 0.040116626769304276\n",
      "Step 49 (4517); Episode 77/100; Loss: 0.00316444830968976\n",
      "Step 50 (4518); Episode 77/100; Loss: 0.05056369677186012\n",
      "Step 51 (4519); Episode 77/100; Loss: 0.05941927805542946\n",
      "Step 52 (4520); Episode 77/100; Loss: 0.040064822882413864\n",
      "Step 53 (4521); Episode 77/100; Loss: 0.01903744973242283\n",
      "Step 54 (4522); Episode 77/100; Loss: 0.039554595947265625\n",
      "Step 55 (4523); Episode 77/100; Loss: 0.07216072827577591\n",
      "Step 56 (4524); Episode 77/100; Loss: 0.06711958348751068\n",
      "Step 57 (4525); Episode 77/100; Loss: 0.004020553082227707\n",
      "Step 58 (4526); Episode 77/100; Loss: 0.035596344619989395\n",
      "Step 59 (4527); Episode 77/100; Loss: 0.001872920896857977\n",
      "Step 60 (4528); Episode 77/100; Loss: 0.0802435502409935\n",
      "Step 61 (4529); Episode 77/100; Loss: 0.04404158890247345\n",
      "Step 62 (4530); Episode 77/100; Loss: 0.08793625235557556\n",
      "Step 63 (4531); Episode 77/100; Loss: 0.013901579193770885\n",
      "Step 64 (4532); Episode 77/100; Loss: 0.03948536142706871\n",
      "Step 65 (4533); Episode 77/100; Loss: 0.001960729481652379\n",
      "Step 66 (4534); Episode 77/100; Loss: 0.04246562346816063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 67 (4535); Episode 77/100; Loss: 0.04693969711661339\n",
      "Step 68 (4536); Episode 77/100; Loss: 0.09727277606725693\n",
      "Step 69 (4537); Episode 77/100; Loss: 0.03340243920683861\n",
      "Step 70 (4538); Episode 77/100; Loss: 0.048819754272699356\n",
      "Step 71 (4539); Episode 77/100; Loss: 0.0546567440032959\n",
      "Step 72 (4540); Episode 77/100; Loss: 0.004061097279191017\n",
      "Step 73 (4541); Episode 77/100; Loss: 0.03890152648091316\n",
      "Step 74 (4542); Episode 77/100; Loss: 0.10540523380041122\n",
      "Step 75 (4543); Episode 77/100; Loss: 0.002996064256876707\n",
      "Step 76 (4544); Episode 77/100; Loss: 0.0016178698278963566\n",
      "Step 77 (4545); Episode 77/100; Loss: 0.09656688570976257\n",
      "Step 78 (4546); Episode 77/100; Loss: 0.04406886175274849\n",
      "Step 79 (4547); Episode 77/100; Loss: 0.09286725521087646\n",
      "Step 80 (4548); Episode 77/100; Loss: 0.11304578930139542\n",
      "Step 81 (4549); Episode 77/100; Loss: 0.003303930629044771\n",
      "Step 82 (4550); Episode 77/100; Loss: 0.046331796795129776\n",
      "Step 83 (4551); Episode 77/100; Loss: 0.04118574410676956\n",
      "Step 84 (4552); Episode 77/100; Loss: 0.011162773706018925\n",
      "Step 85 (4553); Episode 77/100; Loss: 0.01307134609669447\n",
      "Step 86 (4554); Episode 77/100; Loss: 0.0006698282086290419\n",
      "Step 87 (4555); Episode 77/100; Loss: 0.009375992231070995\n",
      "Step 88 (4556); Episode 77/100; Loss: 0.04468090087175369\n",
      "Step 89 (4557); Episode 77/100; Loss: 0.045297443866729736\n",
      "Step 90 (4558); Episode 77/100; Loss: 0.04331820830702782\n",
      "Step 91 (4559); Episode 77/100; Loss: 0.003074114443734288\n",
      "Step 92 (4560); Episode 77/100; Loss: 0.11033317446708679\n",
      "Step 93 (4561); Episode 77/100; Loss: 0.002254572231322527\n",
      "Step 94 (4562); Episode 77/100; Loss: 0.009268330410122871\n",
      "Step 95 (4563); Episode 77/100; Loss: 0.0014832181623205543\n",
      "Step 96 (4564); Episode 77/100; Loss: 0.011605598963797092\n",
      "Step 97 (4565); Episode 77/100; Loss: 0.00855385884642601\n",
      "Step 98 (4566); Episode 77/100; Loss: 0.001340731862001121\n",
      "Step 99 (4567); Episode 77/100; Loss: 0.11107657104730606\n",
      "Step 100 (4568); Episode 77/100; Loss: 0.05438847839832306\n",
      "Step 101 (4569); Episode 77/100; Loss: 0.07994944602251053\n",
      "Step 102 (4570); Episode 77/100; Loss: 0.025352440774440765\n",
      "Step 103 (4571); Episode 77/100; Loss: 0.13107171654701233\n",
      "Step 104 (4572); Episode 77/100; Loss: 0.08565036952495575\n",
      "Step 105 (4573); Episode 77/100; Loss: 0.03364000841975212\n",
      "Step 106 (4574); Episode 77/100; Loss: 0.06633955240249634\n",
      "Step 107 (4575); Episode 77/100; Loss: 0.05658687651157379\n",
      "Step 108 (4576); Episode 77/100; Loss: 0.0017646887572482228\n",
      "Step 109 (4577); Episode 77/100; Loss: 0.0027711030561476946\n",
      "Step 110 (4578); Episode 77/100; Loss: 0.05076093226671219\n",
      "Step 111 (4579); Episode 77/100; Loss: 0.04214422404766083\n",
      "Step 112 (4580); Episode 77/100; Loss: 0.031337156891822815\n",
      "Step 113 (4581); Episode 77/100; Loss: 0.05000544711947441\n",
      "Step 114 (4582); Episode 77/100; Loss: 0.0024780959356576204\n",
      "Step 115 (4583); Episode 77/100; Loss: 0.04690792039036751\n",
      "Step 116 (4584); Episode 77/100; Loss: 0.007442310452461243\n",
      "Step 117 (4585); Episode 77/100; Loss: 0.004541858099400997\n",
      "Step 118 (4586); Episode 77/100; Loss: 0.002404571045190096\n",
      "Step 119 (4587); Episode 77/100; Loss: 0.004500082228332758\n",
      "Step 0 (4588); Episode 78/100; Loss: 0.04670058190822601\n",
      "Step 1 (4589); Episode 78/100; Loss: 0.0961434617638588\n",
      "Step 2 (4590); Episode 78/100; Loss: 0.009115428663790226\n",
      "Step 3 (4591); Episode 78/100; Loss: 0.03627554699778557\n",
      "Step 4 (4592); Episode 78/100; Loss: 0.0674479678273201\n",
      "Step 5 (4593); Episode 78/100; Loss: 0.005413420032709837\n",
      "Step 6 (4594); Episode 78/100; Loss: 0.0463174469769001\n",
      "Step 7 (4595); Episode 78/100; Loss: 0.04722942039370537\n",
      "Step 8 (4596); Episode 78/100; Loss: 0.03326689079403877\n",
      "Step 9 (4597); Episode 78/100; Loss: 0.03876372426748276\n",
      "Step 10 (4598); Episode 78/100; Loss: 0.13029199838638306\n",
      "Step 11 (4599); Episode 78/100; Loss: 0.0063087064772844315\n",
      "Step 12 (4600); Episode 78/100; Loss: 0.04662493243813515\n",
      "Step 13 (4601); Episode 78/100; Loss: 0.08157212287187576\n",
      "Step 14 (4602); Episode 78/100; Loss: 0.13320660591125488\n",
      "Step 15 (4603); Episode 78/100; Loss: 0.0018542351899668574\n",
      "Step 16 (4604); Episode 78/100; Loss: 0.0031735729426145554\n",
      "Step 17 (4605); Episode 78/100; Loss: 0.011508649215102196\n",
      "Step 18 (4606); Episode 78/100; Loss: 0.036733537912368774\n",
      "Step 19 (4607); Episode 78/100; Loss: 0.0022576639894396067\n",
      "Step 20 (4608); Episode 78/100; Loss: 0.09160710126161575\n",
      "Step 21 (4609); Episode 78/100; Loss: 0.01327456347644329\n",
      "Step 22 (4610); Episode 78/100; Loss: 0.002641420578584075\n",
      "Step 23 (4611); Episode 78/100; Loss: 0.0017331534763798118\n",
      "Step 24 (4612); Episode 78/100; Loss: 0.039538174867630005\n",
      "Step 25 (4613); Episode 78/100; Loss: 0.07905837893486023\n",
      "Step 26 (4614); Episode 78/100; Loss: 0.006180502474308014\n",
      "Step 27 (4615); Episode 78/100; Loss: 0.0014839556533843279\n",
      "Step 28 (4616); Episode 78/100; Loss: 0.0030155545100569725\n",
      "Step 29 (4617); Episode 78/100; Loss: 0.10124228149652481\n",
      "Step 30 (4618); Episode 78/100; Loss: 0.044348761439323425\n",
      "Step 31 (4619); Episode 78/100; Loss: 0.08052653819322586\n",
      "Step 32 (4620); Episode 78/100; Loss: 0.058169759809970856\n",
      "Step 33 (4621); Episode 78/100; Loss: 0.049404606223106384\n",
      "Step 34 (4622); Episode 78/100; Loss: 0.0024051207583397627\n",
      "Step 35 (4623); Episode 78/100; Loss: 0.04001226648688316\n",
      "Step 36 (4624); Episode 78/100; Loss: 0.13006103038787842\n",
      "Step 37 (4625); Episode 78/100; Loss: 0.07746824622154236\n",
      "Step 38 (4626); Episode 78/100; Loss: 0.05272187292575836\n",
      "Step 39 (4627); Episode 78/100; Loss: 0.0014368821866810322\n",
      "Step 40 (4628); Episode 78/100; Loss: 0.06890306621789932\n",
      "Step 41 (4629); Episode 78/100; Loss: 0.14718085527420044\n",
      "Step 42 (4630); Episode 78/100; Loss: 0.0018606017110869288\n",
      "Step 43 (4631); Episode 78/100; Loss: 0.04227300360798836\n",
      "Step 44 (4632); Episode 78/100; Loss: 0.0019716040696948767\n",
      "Step 45 (4633); Episode 78/100; Loss: 0.04829450324177742\n",
      "Step 46 (4634); Episode 78/100; Loss: 0.0066634900867938995\n",
      "Step 47 (4635); Episode 78/100; Loss: 0.0011361875804141164\n",
      "Step 48 (4636); Episode 78/100; Loss: 0.04964769259095192\n",
      "Step 49 (4637); Episode 78/100; Loss: 0.002416333882138133\n",
      "Step 50 (4638); Episode 78/100; Loss: 0.0024338976945728064\n",
      "Step 51 (4639); Episode 78/100; Loss: 0.043418560177087784\n",
      "Step 52 (4640); Episode 78/100; Loss: 0.07871554791927338\n",
      "Step 53 (4641); Episode 78/100; Loss: 0.032235804945230484\n",
      "Step 54 (4642); Episode 78/100; Loss: 0.08888527005910873\n",
      "Step 55 (4643); Episode 78/100; Loss: 0.0999501645565033\n",
      "Step 56 (4644); Episode 78/100; Loss: 0.06867844611406326\n",
      "Step 57 (4645); Episode 78/100; Loss: 0.0032386372331529856\n",
      "Step 58 (4646); Episode 78/100; Loss: 0.0020051158498972654\n",
      "Step 59 (4647); Episode 78/100; Loss: 0.05046572908759117\n",
      "Step 60 (4648); Episode 78/100; Loss: 0.03764934837818146\n",
      "Step 61 (4649); Episode 78/100; Loss: 0.001335142063908279\n",
      "Step 62 (4650); Episode 78/100; Loss: 0.006262457463890314\n",
      "Step 63 (4651); Episode 78/100; Loss: 0.03536513075232506\n",
      "Step 64 (4652); Episode 78/100; Loss: 0.002041212283074856\n",
      "Step 65 (4653); Episode 78/100; Loss: 0.07953113317489624\n",
      "Step 66 (4654); Episode 78/100; Loss: 0.0011471991892904043\n",
      "Step 67 (4655); Episode 78/100; Loss: 0.04818284139037132\n",
      "Step 68 (4656); Episode 78/100; Loss: 0.10425134748220444\n",
      "Step 69 (4657); Episode 78/100; Loss: 0.0030309059657156467\n",
      "Step 70 (4658); Episode 78/100; Loss: 0.04252435639500618\n",
      "Step 71 (4659); Episode 78/100; Loss: 0.043750666081905365\n",
      "Step 72 (4660); Episode 78/100; Loss: 0.08164230734109879\n",
      "Step 73 (4661); Episode 78/100; Loss: 0.05460340529680252\n",
      "Step 74 (4662); Episode 78/100; Loss: 0.20629285275936127\n",
      "Step 75 (4663); Episode 78/100; Loss: 0.0012075802078470588\n",
      "Step 76 (4664); Episode 78/100; Loss: 0.13551858067512512\n",
      "Step 77 (4665); Episode 78/100; Loss: 0.05265204608440399\n",
      "Step 78 (4666); Episode 78/100; Loss: 0.005113806575536728\n",
      "Step 79 (4667); Episode 78/100; Loss: 0.037453263998031616\n",
      "Step 80 (4668); Episode 78/100; Loss: 0.005262965336441994\n",
      "Step 81 (4669); Episode 78/100; Loss: 0.007959476672112942\n",
      "Step 82 (4670); Episode 78/100; Loss: 0.02487749606370926\n",
      "Step 83 (4671); Episode 78/100; Loss: 0.0584806352853775\n",
      "Step 84 (4672); Episode 78/100; Loss: 0.050347622483968735\n",
      "Step 85 (4673); Episode 78/100; Loss: 0.038796208798885345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 86 (4674); Episode 78/100; Loss: 0.002972128801047802\n",
      "Step 87 (4675); Episode 78/100; Loss: 0.049057550728321075\n",
      "Step 88 (4676); Episode 78/100; Loss: 0.0994315966963768\n",
      "Step 89 (4677); Episode 78/100; Loss: 0.037970464676618576\n",
      "Step 90 (4678); Episode 78/100; Loss: 0.036748334765434265\n",
      "Step 91 (4679); Episode 78/100; Loss: 0.00396616430953145\n",
      "Step 92 (4680); Episode 78/100; Loss: 0.048314567655324936\n",
      "Step 93 (4681); Episode 78/100; Loss: 0.0017275046557188034\n",
      "Step 94 (4682); Episode 78/100; Loss: 0.04119274392724037\n",
      "Step 95 (4683); Episode 78/100; Loss: 0.001799489837139845\n",
      "Step 96 (4684); Episode 78/100; Loss: 0.01598586142063141\n",
      "Step 97 (4685); Episode 78/100; Loss: 0.0698162242770195\n",
      "Step 98 (4686); Episode 78/100; Loss: 0.1237800270318985\n",
      "Step 99 (4687); Episode 78/100; Loss: 0.050325509160757065\n",
      "Step 100 (4688); Episode 78/100; Loss: 0.0024865400046110153\n",
      "Step 101 (4689); Episode 78/100; Loss: 0.0012114980490878224\n",
      "Step 102 (4690); Episode 78/100; Loss: 0.0461416132748127\n",
      "Step 103 (4691); Episode 78/100; Loss: 0.002186895813792944\n",
      "Step 104 (4692); Episode 78/100; Loss: 0.00263412413187325\n",
      "Step 105 (4693); Episode 78/100; Loss: 0.05154412239789963\n",
      "Step 106 (4694); Episode 78/100; Loss: 0.0673656016588211\n",
      "Step 107 (4695); Episode 78/100; Loss: 0.1426531970500946\n",
      "Step 108 (4696); Episode 78/100; Loss: 0.13022933900356293\n",
      "Step 109 (4697); Episode 78/100; Loss: 0.07714544981718063\n",
      "Step 110 (4698); Episode 78/100; Loss: 0.032043177634477615\n",
      "Step 111 (4699); Episode 78/100; Loss: 0.13019287586212158\n",
      "Step 112 (4700); Episode 78/100; Loss: 0.04165114462375641\n",
      "Step 113 (4701); Episode 78/100; Loss: 0.04999060183763504\n",
      "Step 114 (4702); Episode 78/100; Loss: 0.016948938369750977\n",
      "Step 115 (4703); Episode 78/100; Loss: 0.0025578278582543135\n",
      "Step 116 (4704); Episode 78/100; Loss: 0.04742487519979477\n",
      "Step 117 (4705); Episode 78/100; Loss: 0.0021984337363392115\n",
      "Step 118 (4706); Episode 78/100; Loss: 0.0014527208404615521\n",
      "Step 119 (4707); Episode 78/100; Loss: 0.04858134686946869\n",
      "Step 120 (4708); Episode 78/100; Loss: 0.07100516557693481\n",
      "Step 121 (4709); Episode 78/100; Loss: 0.051580339670181274\n",
      "Step 122 (4710); Episode 78/100; Loss: 0.007826956920325756\n",
      "Step 123 (4711); Episode 78/100; Loss: 0.0022234302014112473\n",
      "Step 124 (4712); Episode 78/100; Loss: 0.0071855103597044945\n",
      "Step 125 (4713); Episode 78/100; Loss: 0.04779854789376259\n",
      "Step 126 (4714); Episode 78/100; Loss: 0.0417221374809742\n",
      "Step 127 (4715); Episode 78/100; Loss: 0.04753800109028816\n",
      "Step 128 (4716); Episode 78/100; Loss: 0.0455278605222702\n",
      "Step 129 (4717); Episode 78/100; Loss: 0.02635626308619976\n",
      "Step 0 (4718); Episode 79/100; Loss: 0.002556706080213189\n",
      "Step 1 (4719); Episode 79/100; Loss: 0.0581529438495636\n",
      "Step 2 (4720); Episode 79/100; Loss: 0.04505398869514465\n",
      "Step 3 (4721); Episode 79/100; Loss: 0.05666442587971687\n",
      "Step 4 (4722); Episode 79/100; Loss: 0.06769243627786636\n",
      "Step 5 (4723); Episode 79/100; Loss: 0.0011950071202591062\n",
      "Step 6 (4724); Episode 79/100; Loss: 0.03204675018787384\n",
      "Step 7 (4725); Episode 79/100; Loss: 0.004595823585987091\n",
      "Step 8 (4726); Episode 79/100; Loss: 0.0036812564358115196\n",
      "Step 9 (4727); Episode 79/100; Loss: 0.052158012986183167\n",
      "Step 10 (4728); Episode 79/100; Loss: 0.05565387383103371\n",
      "Step 11 (4729); Episode 79/100; Loss: 0.009472960606217384\n",
      "Step 12 (4730); Episode 79/100; Loss: 0.036693036556243896\n",
      "Step 13 (4731); Episode 79/100; Loss: 0.02335711196064949\n",
      "Step 14 (4732); Episode 79/100; Loss: 0.13147442042827606\n",
      "Step 15 (4733); Episode 79/100; Loss: 0.008147124201059341\n",
      "Step 16 (4734); Episode 79/100; Loss: 0.03619576245546341\n",
      "Step 17 (4735); Episode 79/100; Loss: 0.13895852863788605\n",
      "Step 18 (4736); Episode 79/100; Loss: 0.0017808726988732815\n",
      "Step 19 (4737); Episode 79/100; Loss: 0.0054315547458827496\n",
      "Step 20 (4738); Episode 79/100; Loss: 0.03126765415072441\n",
      "Step 21 (4739); Episode 79/100; Loss: 0.03654962405562401\n",
      "Step 22 (4740); Episode 79/100; Loss: 0.0017328968970105052\n",
      "Step 23 (4741); Episode 79/100; Loss: 0.002445432124659419\n",
      "Step 24 (4742); Episode 79/100; Loss: 0.039330337196588516\n",
      "Step 25 (4743); Episode 79/100; Loss: 0.09856250882148743\n",
      "Step 26 (4744); Episode 79/100; Loss: 0.003456989536061883\n",
      "Step 27 (4745); Episode 79/100; Loss: 0.001978323794901371\n",
      "Step 28 (4746); Episode 79/100; Loss: 0.09164438396692276\n",
      "Step 29 (4747); Episode 79/100; Loss: 0.0020797327160835266\n",
      "Step 30 (4748); Episode 79/100; Loss: 0.0022391988895833492\n",
      "Step 31 (4749); Episode 79/100; Loss: 0.03870142623782158\n",
      "Step 32 (4750); Episode 79/100; Loss: 0.006537249311804771\n",
      "Step 33 (4751); Episode 79/100; Loss: 0.03595490753650665\n",
      "Step 34 (4752); Episode 79/100; Loss: 0.0032982612028717995\n",
      "Step 35 (4753); Episode 79/100; Loss: 0.0026324971113353968\n",
      "Step 36 (4754); Episode 79/100; Loss: 0.06268922984600067\n",
      "Step 37 (4755); Episode 79/100; Loss: 0.02633110247552395\n",
      "Step 38 (4756); Episode 79/100; Loss: 0.002909693866968155\n",
      "Step 39 (4757); Episode 79/100; Loss: 0.0021369890309870243\n",
      "Step 40 (4758); Episode 79/100; Loss: 0.04265351966023445\n",
      "Step 41 (4759); Episode 79/100; Loss: 0.035265326499938965\n",
      "Step 42 (4760); Episode 79/100; Loss: 0.00627978565171361\n",
      "Step 43 (4761); Episode 79/100; Loss: 0.05341913551092148\n",
      "Step 44 (4762); Episode 79/100; Loss: 0.0013174592750146985\n",
      "Step 45 (4763); Episode 79/100; Loss: 0.048741258680820465\n",
      "Step 46 (4764); Episode 79/100; Loss: 0.04432172700762749\n",
      "Step 47 (4765); Episode 79/100; Loss: 0.0024469036143273115\n",
      "Step 48 (4766); Episode 79/100; Loss: 0.0013024706859141588\n",
      "Step 49 (4767); Episode 79/100; Loss: 0.006517438683658838\n",
      "Step 50 (4768); Episode 79/100; Loss: 0.0016297369729727507\n",
      "Step 51 (4769); Episode 79/100; Loss: 0.008642688393592834\n",
      "Step 52 (4770); Episode 79/100; Loss: 0.0011621734593063593\n",
      "Step 53 (4771); Episode 79/100; Loss: 0.001723637804389\n",
      "Step 54 (4772); Episode 79/100; Loss: 0.007089790888130665\n",
      "Step 55 (4773); Episode 79/100; Loss: 0.07937153428792953\n",
      "Step 56 (4774); Episode 79/100; Loss: 0.002197801135480404\n",
      "Step 57 (4775); Episode 79/100; Loss: 0.04633517935872078\n",
      "Step 58 (4776); Episode 79/100; Loss: 0.09282709658145905\n",
      "Step 59 (4777); Episode 79/100; Loss: 0.028668498620390892\n",
      "Step 60 (4778); Episode 79/100; Loss: 0.031574640423059464\n",
      "Step 61 (4779); Episode 79/100; Loss: 0.05944758281111717\n",
      "Step 62 (4780); Episode 79/100; Loss: 0.00257779355160892\n",
      "Step 63 (4781); Episode 79/100; Loss: 0.0017541383858770132\n",
      "Step 64 (4782); Episode 79/100; Loss: 0.08232972025871277\n",
      "Step 65 (4783); Episode 79/100; Loss: 0.036106497049331665\n",
      "Step 66 (4784); Episode 79/100; Loss: 0.014189478941261768\n",
      "Step 67 (4785); Episode 79/100; Loss: 0.06466405838727951\n",
      "Step 68 (4786); Episode 79/100; Loss: 0.04698988050222397\n",
      "Step 69 (4787); Episode 79/100; Loss: 0.04925209656357765\n",
      "Step 70 (4788); Episode 79/100; Loss: 0.024461330845952034\n",
      "Step 71 (4789); Episode 79/100; Loss: 0.002145160222426057\n",
      "Step 72 (4790); Episode 79/100; Loss: 0.1628182828426361\n",
      "Step 73 (4791); Episode 79/100; Loss: 0.0034718047827482224\n",
      "Step 74 (4792); Episode 79/100; Loss: 0.002663599094375968\n",
      "Step 75 (4793); Episode 79/100; Loss: 0.07554277777671814\n",
      "Step 76 (4794); Episode 79/100; Loss: 0.0863010361790657\n",
      "Step 77 (4795); Episode 79/100; Loss: 0.07589935511350632\n",
      "Step 78 (4796); Episode 79/100; Loss: 0.00741076935082674\n",
      "Step 79 (4797); Episode 79/100; Loss: 0.06923624873161316\n",
      "Step 80 (4798); Episode 79/100; Loss: 0.05352127552032471\n",
      "Step 81 (4799); Episode 79/100; Loss: 0.08127472549676895\n",
      "Step 82 (4800); Episode 79/100; Loss: 0.051525529474020004\n",
      "Step 83 (4801); Episode 79/100; Loss: 0.0018309061415493488\n",
      "Step 84 (4802); Episode 79/100; Loss: 0.07994306087493896\n",
      "Step 85 (4803); Episode 79/100; Loss: 0.038400799036026\n",
      "Step 86 (4804); Episode 79/100; Loss: 0.0058635203167796135\n",
      "Step 87 (4805); Episode 79/100; Loss: 0.0014815977774560452\n",
      "Step 88 (4806); Episode 79/100; Loss: 0.06524314731359482\n",
      "Step 89 (4807); Episode 79/100; Loss: 0.0008505927980877459\n",
      "Step 90 (4808); Episode 79/100; Loss: 0.05238167941570282\n",
      "Step 91 (4809); Episode 79/100; Loss: 0.04569750651717186\n",
      "Step 92 (4810); Episode 79/100; Loss: 0.0033795444760471582\n",
      "Step 93 (4811); Episode 79/100; Loss: 0.06558322161436081\n",
      "Step 94 (4812); Episode 79/100; Loss: 0.057683538645505905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 95 (4813); Episode 79/100; Loss: 0.07751987129449844\n",
      "Step 96 (4814); Episode 79/100; Loss: 0.0959930419921875\n",
      "Step 97 (4815); Episode 79/100; Loss: 0.031533513218164444\n",
      "Step 98 (4816); Episode 79/100; Loss: 0.04557919129729271\n",
      "Step 99 (4817); Episode 79/100; Loss: 0.0012907081982120872\n",
      "Step 100 (4818); Episode 79/100; Loss: 0.09684336930513382\n",
      "Step 101 (4819); Episode 79/100; Loss: 0.003533822251483798\n",
      "Step 102 (4820); Episode 79/100; Loss: 0.04656430706381798\n",
      "Step 103 (4821); Episode 79/100; Loss: 0.04226341471076012\n",
      "Step 104 (4822); Episode 79/100; Loss: 0.0017667311476543546\n",
      "Step 105 (4823); Episode 79/100; Loss: 0.004886498209089041\n",
      "Step 106 (4824); Episode 79/100; Loss: 0.004419619217514992\n",
      "Step 107 (4825); Episode 79/100; Loss: 0.039956171065568924\n",
      "Step 108 (4826); Episode 79/100; Loss: 0.09742289036512375\n",
      "Step 109 (4827); Episode 79/100; Loss: 0.018315596505999565\n",
      "Step 110 (4828); Episode 79/100; Loss: 0.15309947729110718\n",
      "Step 111 (4829); Episode 79/100; Loss: 0.030589988455176353\n",
      "Step 112 (4830); Episode 79/100; Loss: 0.008290763013064861\n",
      "Step 113 (4831); Episode 79/100; Loss: 0.08048215508460999\n",
      "Step 114 (4832); Episode 79/100; Loss: 0.13340869545936584\n",
      "Step 115 (4833); Episode 79/100; Loss: 0.002350395079702139\n",
      "Step 116 (4834); Episode 79/100; Loss: 0.031134724617004395\n",
      "Step 117 (4835); Episode 79/100; Loss: 0.0024197730235755444\n",
      "Step 118 (4836); Episode 79/100; Loss: 0.0937890037894249\n",
      "Step 119 (4837); Episode 79/100; Loss: 0.036033499985933304\n",
      "Step 120 (4838); Episode 79/100; Loss: 0.0017059789970517159\n",
      "Step 121 (4839); Episode 79/100; Loss: 0.006060815416276455\n",
      "Step 122 (4840); Episode 79/100; Loss: 0.05636480450630188\n",
      "Step 123 (4841); Episode 79/100; Loss: 0.11164984107017517\n",
      "Step 124 (4842); Episode 79/100; Loss: 0.05005049332976341\n",
      "Step 125 (4843); Episode 79/100; Loss: 0.04125295206904411\n",
      "Step 126 (4844); Episode 79/100; Loss: 0.0016245230799540877\n",
      "Step 127 (4845); Episode 79/100; Loss: 0.01934245601296425\n",
      "Step 128 (4846); Episode 79/100; Loss: 0.04702695459127426\n",
      "Step 0 (4847); Episode 80/100; Loss: 0.001227766741067171\n",
      "Step 1 (4848); Episode 80/100; Loss: 0.004043691325932741\n",
      "Step 2 (4849); Episode 80/100; Loss: 0.03177804872393608\n",
      "Step 3 (4850); Episode 80/100; Loss: 0.0008795474423095584\n",
      "Step 4 (4851); Episode 80/100; Loss: 0.1701442301273346\n",
      "Step 5 (4852); Episode 80/100; Loss: 0.03096255101263523\n",
      "Step 6 (4853); Episode 80/100; Loss: 0.0037726217415183783\n",
      "Step 7 (4854); Episode 80/100; Loss: 0.012593972496688366\n",
      "Step 8 (4855); Episode 80/100; Loss: 0.0029000649228692055\n",
      "Step 9 (4856); Episode 80/100; Loss: 0.09852229803800583\n",
      "Step 10 (4857); Episode 80/100; Loss: 0.035757604986429214\n",
      "Step 11 (4858); Episode 80/100; Loss: 0.14458242058753967\n",
      "Step 12 (4859); Episode 80/100; Loss: 0.07146843522787094\n",
      "Step 13 (4860); Episode 80/100; Loss: 0.13875584304332733\n",
      "Step 14 (4861); Episode 80/100; Loss: 0.052623409777879715\n",
      "Step 15 (4862); Episode 80/100; Loss: 0.05353591963648796\n",
      "Step 16 (4863); Episode 80/100; Loss: 0.05922956392168999\n",
      "Step 17 (4864); Episode 80/100; Loss: 0.0407782681286335\n",
      "Step 18 (4865); Episode 80/100; Loss: 0.03935287520289421\n",
      "Step 19 (4866); Episode 80/100; Loss: 0.04196777939796448\n",
      "Step 20 (4867); Episode 80/100; Loss: 0.08514104783535004\n",
      "Step 21 (4868); Episode 80/100; Loss: 0.047345250844955444\n",
      "Step 22 (4869); Episode 80/100; Loss: 0.06492368876934052\n",
      "Step 23 (4870); Episode 80/100; Loss: 0.03570104390382767\n",
      "Step 24 (4871); Episode 80/100; Loss: 0.10345330089330673\n",
      "Step 25 (4872); Episode 80/100; Loss: 0.004723058082163334\n",
      "Step 26 (4873); Episode 80/100; Loss: 0.0296352319419384\n",
      "Step 27 (4874); Episode 80/100; Loss: 0.04289143905043602\n",
      "Step 28 (4875); Episode 80/100; Loss: 0.047643404453992844\n",
      "Step 29 (4876); Episode 80/100; Loss: 0.031942106783390045\n",
      "Step 30 (4877); Episode 80/100; Loss: 0.0012391182826831937\n",
      "Step 31 (4878); Episode 80/100; Loss: 0.037552569061517715\n",
      "Step 32 (4879); Episode 80/100; Loss: 0.03535560145974159\n",
      "Step 33 (4880); Episode 80/100; Loss: 0.10472918301820755\n",
      "Step 34 (4881); Episode 80/100; Loss: 0.0074302563443779945\n",
      "Step 35 (4882); Episode 80/100; Loss: 0.006690808106213808\n",
      "Step 36 (4883); Episode 80/100; Loss: 0.005835589021444321\n",
      "Step 37 (4884); Episode 80/100; Loss: 0.0014306987868621945\n",
      "Step 38 (4885); Episode 80/100; Loss: 0.04042074829339981\n",
      "Step 39 (4886); Episode 80/100; Loss: 0.08053120225667953\n",
      "Step 40 (4887); Episode 80/100; Loss: 0.05092997848987579\n",
      "Step 41 (4888); Episode 80/100; Loss: 0.04616229981184006\n",
      "Step 42 (4889); Episode 80/100; Loss: 0.003670116188004613\n",
      "Step 43 (4890); Episode 80/100; Loss: 0.033957213163375854\n",
      "Step 44 (4891); Episode 80/100; Loss: 0.04383357614278793\n",
      "Step 45 (4892); Episode 80/100; Loss: 0.0270529817789793\n",
      "Step 46 (4893); Episode 80/100; Loss: 0.006952348165214062\n",
      "Step 47 (4894); Episode 80/100; Loss: 0.00322001613676548\n",
      "Step 48 (4895); Episode 80/100; Loss: 0.003143180860206485\n",
      "Step 49 (4896); Episode 80/100; Loss: 0.0038122052792459726\n",
      "Step 50 (4897); Episode 80/100; Loss: 0.0011523509165272117\n",
      "Step 51 (4898); Episode 80/100; Loss: 0.04851235821843147\n",
      "Step 52 (4899); Episode 80/100; Loss: 0.003355538472533226\n",
      "Step 53 (4900); Episode 80/100; Loss: 0.003470279509201646\n",
      "Step 54 (4901); Episode 80/100; Loss: 0.0008648260263726115\n",
      "Step 55 (4902); Episode 80/100; Loss: 0.03484704717993736\n",
      "Step 56 (4903); Episode 80/100; Loss: 0.0014759249752387404\n",
      "Step 57 (4904); Episode 80/100; Loss: 0.0016479252371937037\n",
      "Step 58 (4905); Episode 80/100; Loss: 0.0031710572075098753\n",
      "Step 59 (4906); Episode 80/100; Loss: 0.001484933658502996\n",
      "Step 60 (4907); Episode 80/100; Loss: 0.0512552373111248\n",
      "Step 61 (4908); Episode 80/100; Loss: 0.0014352480648085475\n",
      "Step 62 (4909); Episode 80/100; Loss: 0.001820713747292757\n",
      "Step 63 (4910); Episode 80/100; Loss: 0.051872849464416504\n",
      "Step 64 (4911); Episode 80/100; Loss: 0.018811866641044617\n",
      "Step 65 (4912); Episode 80/100; Loss: 0.0020724753849208355\n",
      "Step 66 (4913); Episode 80/100; Loss: 0.04263957217335701\n",
      "Step 67 (4914); Episode 80/100; Loss: 0.06865183264017105\n",
      "Step 68 (4915); Episode 80/100; Loss: 0.03325265645980835\n",
      "Step 69 (4916); Episode 80/100; Loss: 0.0021184233482927084\n",
      "Step 70 (4917); Episode 80/100; Loss: 0.0017125395825132728\n",
      "Step 71 (4918); Episode 80/100; Loss: 0.03747949004173279\n",
      "Step 72 (4919); Episode 80/100; Loss: 0.19739724695682526\n",
      "Step 73 (4920); Episode 80/100; Loss: 0.029831279069185257\n",
      "Step 74 (4921); Episode 80/100; Loss: 0.0020977521780878305\n",
      "Step 75 (4922); Episode 80/100; Loss: 0.0018835922237485647\n",
      "Step 76 (4923); Episode 80/100; Loss: 0.029292430728673935\n",
      "Step 77 (4924); Episode 80/100; Loss: 0.13238674402236938\n",
      "Step 78 (4925); Episode 80/100; Loss: 0.04956086724996567\n",
      "Step 79 (4926); Episode 80/100; Loss: 0.008140794932842255\n",
      "Step 80 (4927); Episode 80/100; Loss: 0.028094353154301643\n",
      "Step 81 (4928); Episode 80/100; Loss: 0.04446655511856079\n",
      "Step 82 (4929); Episode 80/100; Loss: 0.06128034368157387\n",
      "Step 83 (4930); Episode 80/100; Loss: 0.04696597158908844\n",
      "Step 84 (4931); Episode 80/100; Loss: 0.06096288934350014\n",
      "Step 85 (4932); Episode 80/100; Loss: 0.06550898402929306\n",
      "Step 86 (4933); Episode 80/100; Loss: 0.043154243379831314\n",
      "Step 87 (4934); Episode 80/100; Loss: 0.0025685434229671955\n",
      "Step 88 (4935); Episode 80/100; Loss: 0.001169248716905713\n",
      "Step 89 (4936); Episode 80/100; Loss: 0.004917576443403959\n",
      "Step 90 (4937); Episode 80/100; Loss: 0.004502943716943264\n",
      "Step 91 (4938); Episode 80/100; Loss: 0.0007944740355014801\n",
      "Step 92 (4939); Episode 80/100; Loss: 0.00240849400870502\n",
      "Step 93 (4940); Episode 80/100; Loss: 0.03179795667529106\n",
      "Step 94 (4941); Episode 80/100; Loss: 0.021142730489373207\n",
      "Step 95 (4942); Episode 80/100; Loss: 0.0024245125241577625\n",
      "Step 96 (4943); Episode 80/100; Loss: 0.019218171015381813\n",
      "Step 97 (4944); Episode 80/100; Loss: 0.03998272493481636\n",
      "Step 98 (4945); Episode 80/100; Loss: 0.0013118505012243986\n",
      "Step 99 (4946); Episode 80/100; Loss: 0.008113200776278973\n",
      "Step 100 (4947); Episode 80/100; Loss: 0.007226464804261923\n",
      "Step 101 (4948); Episode 80/100; Loss: 0.009913738816976547\n",
      "Step 102 (4949); Episode 80/100; Loss: 0.0012018673587590456\n",
      "Step 103 (4950); Episode 80/100; Loss: 0.0011516318190842867\n",
      "Step 0 (4951); Episode 81/100; Loss: 0.0029350644908845425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (4952); Episode 81/100; Loss: 0.012061378918588161\n",
      "Step 2 (4953); Episode 81/100; Loss: 0.002417665207758546\n",
      "Step 3 (4954); Episode 81/100; Loss: 0.04930572584271431\n",
      "Step 4 (4955); Episode 81/100; Loss: 0.0017268260708078742\n",
      "Step 5 (4956); Episode 81/100; Loss: 0.026513520628213882\n",
      "Step 6 (4957); Episode 81/100; Loss: 0.003038011258468032\n",
      "Step 7 (4958); Episode 81/100; Loss: 0.04863502085208893\n",
      "Step 8 (4959); Episode 81/100; Loss: 0.09958253055810928\n",
      "Step 9 (4960); Episode 81/100; Loss: 0.08482913672924042\n",
      "Step 10 (4961); Episode 81/100; Loss: 0.0395866297185421\n",
      "Step 11 (4962); Episode 81/100; Loss: 0.09278447180986404\n",
      "Step 12 (4963); Episode 81/100; Loss: 0.0016137934289872646\n",
      "Step 13 (4964); Episode 81/100; Loss: 0.056861475110054016\n",
      "Step 14 (4965); Episode 81/100; Loss: 0.11044107377529144\n",
      "Step 15 (4966); Episode 81/100; Loss: 0.09069966524839401\n",
      "Step 16 (4967); Episode 81/100; Loss: 0.02947474829852581\n",
      "Step 17 (4968); Episode 81/100; Loss: 0.05229071155190468\n",
      "Step 18 (4969); Episode 81/100; Loss: 0.019090821966528893\n",
      "Step 19 (4970); Episode 81/100; Loss: 0.07192021608352661\n",
      "Step 20 (4971); Episode 81/100; Loss: 0.004849761724472046\n",
      "Step 21 (4972); Episode 81/100; Loss: 0.043601978570222855\n",
      "Step 22 (4973); Episode 81/100; Loss: 0.0179570522159338\n",
      "Step 23 (4974); Episode 81/100; Loss: 0.03337850794196129\n",
      "Step 24 (4975); Episode 81/100; Loss: 0.03568852320313454\n",
      "Step 25 (4976); Episode 81/100; Loss: 0.07241996377706528\n",
      "Step 26 (4977); Episode 81/100; Loss: 0.003355027176439762\n",
      "Step 27 (4978); Episode 81/100; Loss: 0.031456802040338516\n",
      "Step 28 (4979); Episode 81/100; Loss: 0.041189003735780716\n",
      "Step 29 (4980); Episode 81/100; Loss: 0.027096087113022804\n",
      "Step 30 (4981); Episode 81/100; Loss: 0.09349998086690903\n",
      "Step 31 (4982); Episode 81/100; Loss: 0.0035413678269833326\n",
      "Step 32 (4983); Episode 81/100; Loss: 0.05444997921586037\n",
      "Step 33 (4984); Episode 81/100; Loss: 0.0044172764755785465\n",
      "Step 34 (4985); Episode 81/100; Loss: 0.06914335489273071\n",
      "Step 35 (4986); Episode 81/100; Loss: 0.060599781572818756\n",
      "Step 36 (4987); Episode 81/100; Loss: 0.006416850723326206\n",
      "Step 37 (4988); Episode 81/100; Loss: 0.0038930086884647608\n",
      "Step 38 (4989); Episode 81/100; Loss: 0.04316085949540138\n",
      "Step 39 (4990); Episode 81/100; Loss: 0.009287548251450062\n",
      "Step 40 (4991); Episode 81/100; Loss: 0.001229980494827032\n",
      "Step 41 (4992); Episode 81/100; Loss: 0.015506433323025703\n",
      "Step 42 (4993); Episode 81/100; Loss: 0.036168139427900314\n",
      "Step 43 (4994); Episode 81/100; Loss: 0.04153990373015404\n",
      "Step 44 (4995); Episode 81/100; Loss: 0.033558182418346405\n",
      "Step 45 (4996); Episode 81/100; Loss: 0.10575033724308014\n",
      "Step 46 (4997); Episode 81/100; Loss: 0.004501168616116047\n",
      "Step 47 (4998); Episode 81/100; Loss: 0.08412802964448929\n",
      "Step 48 (4999); Episode 81/100; Loss: 0.08685246855020523\n",
      "Step 49 (5000); Episode 81/100; Loss: 0.04121626541018486\n",
      "Step 50 (5001); Episode 81/100; Loss: 0.0916331559419632\n",
      "Step 51 (5002); Episode 81/100; Loss: 0.0010745652252808213\n",
      "Step 52 (5003); Episode 81/100; Loss: 0.016455141827464104\n",
      "Step 53 (5004); Episode 81/100; Loss: 0.0019215745851397514\n",
      "Step 54 (5005); Episode 81/100; Loss: 0.04630018398165703\n",
      "Step 55 (5006); Episode 81/100; Loss: 0.062314193695783615\n",
      "Step 56 (5007); Episode 81/100; Loss: 0.0028898133896291256\n",
      "Step 57 (5008); Episode 81/100; Loss: 0.004166030324995518\n",
      "Step 58 (5009); Episode 81/100; Loss: 0.04771525412797928\n",
      "Step 59 (5010); Episode 81/100; Loss: 0.03218815103173256\n",
      "Step 60 (5011); Episode 81/100; Loss: 0.03634059429168701\n",
      "Step 61 (5012); Episode 81/100; Loss: 0.036370597779750824\n",
      "Step 62 (5013); Episode 81/100; Loss: 0.0031997780315577984\n",
      "Step 63 (5014); Episode 81/100; Loss: 0.0551496222615242\n",
      "Step 64 (5015); Episode 81/100; Loss: 0.051700133830308914\n",
      "Step 65 (5016); Episode 81/100; Loss: 0.0016610095044597983\n",
      "Step 66 (5017); Episode 81/100; Loss: 0.08654044568538666\n",
      "Step 67 (5018); Episode 81/100; Loss: 0.004458302166312933\n",
      "Step 68 (5019); Episode 81/100; Loss: 0.038352638483047485\n",
      "Step 69 (5020); Episode 81/100; Loss: 0.002620742656290531\n",
      "Step 70 (5021); Episode 81/100; Loss: 0.031107354909181595\n",
      "Step 71 (5022); Episode 81/100; Loss: 0.04211109131574631\n",
      "Step 72 (5023); Episode 81/100; Loss: 0.0879243016242981\n",
      "Step 73 (5024); Episode 81/100; Loss: 0.12284544855356216\n",
      "Step 74 (5025); Episode 81/100; Loss: 0.0015407459577545524\n",
      "Step 75 (5026); Episode 81/100; Loss: 0.0011147434124723077\n",
      "Step 76 (5027); Episode 81/100; Loss: 0.019359171390533447\n",
      "Step 77 (5028); Episode 81/100; Loss: 0.004274242091923952\n",
      "Step 78 (5029); Episode 81/100; Loss: 0.046173688024282455\n",
      "Step 79 (5030); Episode 81/100; Loss: 0.08423981815576553\n",
      "Step 80 (5031); Episode 81/100; Loss: 0.018760383129119873\n",
      "Step 81 (5032); Episode 81/100; Loss: 0.043635934591293335\n",
      "Step 82 (5033); Episode 81/100; Loss: 0.0037098240572959185\n",
      "Step 83 (5034); Episode 81/100; Loss: 0.11177382618188858\n",
      "Step 84 (5035); Episode 81/100; Loss: 0.0010475522140040994\n",
      "Step 85 (5036); Episode 81/100; Loss: 0.03248443081974983\n",
      "Step 86 (5037); Episode 81/100; Loss: 0.00690135033801198\n",
      "Step 87 (5038); Episode 81/100; Loss: 0.0016585077391937375\n",
      "Step 88 (5039); Episode 81/100; Loss: 0.03275691717863083\n",
      "Step 89 (5040); Episode 81/100; Loss: 0.022869417443871498\n",
      "Step 90 (5041); Episode 81/100; Loss: 0.017446327954530716\n",
      "Step 91 (5042); Episode 81/100; Loss: 0.0014022892573848367\n",
      "Step 92 (5043); Episode 81/100; Loss: 0.011768152937293053\n",
      "Step 93 (5044); Episode 81/100; Loss: 0.019957486540079117\n",
      "Step 94 (5045); Episode 81/100; Loss: 0.0027022412978112698\n",
      "Step 95 (5046); Episode 81/100; Loss: 0.0011411963496357203\n",
      "Step 96 (5047); Episode 81/100; Loss: 0.0013030574191361666\n",
      "Step 97 (5048); Episode 81/100; Loss: 0.05683314800262451\n",
      "Step 98 (5049); Episode 81/100; Loss: 0.037091705948114395\n",
      "Step 99 (5050); Episode 81/100; Loss: 0.08370277285575867\n",
      "Step 100 (5051); Episode 81/100; Loss: 0.01561107486486435\n",
      "Step 101 (5052); Episode 81/100; Loss: 0.016569294035434723\n",
      "Step 102 (5053); Episode 81/100; Loss: 0.0013800400774925947\n",
      "Step 103 (5054); Episode 81/100; Loss: 0.002429185900837183\n",
      "Step 104 (5055); Episode 81/100; Loss: 0.06092984229326248\n",
      "Step 105 (5056); Episode 81/100; Loss: 0.034872137010097504\n",
      "Step 106 (5057); Episode 81/100; Loss: 0.011774551123380661\n",
      "Step 107 (5058); Episode 81/100; Loss: 0.09873515367507935\n",
      "Step 108 (5059); Episode 81/100; Loss: 0.05587431415915489\n",
      "Step 109 (5060); Episode 81/100; Loss: 0.00701542291790247\n",
      "Step 110 (5061); Episode 81/100; Loss: 0.04980853572487831\n",
      "Step 111 (5062); Episode 81/100; Loss: 0.047895483672618866\n",
      "Step 112 (5063); Episode 81/100; Loss: 0.010680901817977428\n",
      "Step 113 (5064); Episode 81/100; Loss: 0.058440934866666794\n",
      "Step 114 (5065); Episode 81/100; Loss: 0.004486010875552893\n",
      "Step 115 (5066); Episode 81/100; Loss: 0.0011272416450083256\n",
      "Step 116 (5067); Episode 81/100; Loss: 0.04441631957888603\n",
      "Step 117 (5068); Episode 81/100; Loss: 0.039907731115818024\n",
      "Step 118 (5069); Episode 81/100; Loss: 0.0032050188165158033\n",
      "Step 119 (5070); Episode 81/100; Loss: 0.05884326249361038\n",
      "Step 0 (5071); Episode 82/100; Loss: 0.04707492142915726\n",
      "Step 1 (5072); Episode 82/100; Loss: 0.04342876002192497\n",
      "Step 2 (5073); Episode 82/100; Loss: 0.08910475671291351\n",
      "Step 3 (5074); Episode 82/100; Loss: 0.12790408730506897\n",
      "Step 4 (5075); Episode 82/100; Loss: 0.0013552046148106456\n",
      "Step 5 (5076); Episode 82/100; Loss: 0.004056185949593782\n",
      "Step 6 (5077); Episode 82/100; Loss: 0.029174748808145523\n",
      "Step 7 (5078); Episode 82/100; Loss: 0.07956214249134064\n",
      "Step 8 (5079); Episode 82/100; Loss: 0.0023234703112393618\n",
      "Step 9 (5080); Episode 82/100; Loss: 0.05181446298956871\n",
      "Step 10 (5081); Episode 82/100; Loss: 0.04777253791689873\n",
      "Step 11 (5082); Episode 82/100; Loss: 0.05766020342707634\n",
      "Step 12 (5083); Episode 82/100; Loss: 0.08977976441383362\n",
      "Step 13 (5084); Episode 82/100; Loss: 0.032906144857406616\n",
      "Step 14 (5085); Episode 82/100; Loss: 0.03804924711585045\n",
      "Step 15 (5086); Episode 82/100; Loss: 0.06577815860509872\n",
      "Step 16 (5087); Episode 82/100; Loss: 0.049909282475709915\n",
      "Step 17 (5088); Episode 82/100; Loss: 0.007190062664449215\n",
      "Step 18 (5089); Episode 82/100; Loss: 0.05332149565219879\n",
      "Step 19 (5090); Episode 82/100; Loss: 0.0027910161297768354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 (5091); Episode 82/100; Loss: 0.0601460225880146\n",
      "Step 21 (5092); Episode 82/100; Loss: 0.11892639845609665\n",
      "Step 22 (5093); Episode 82/100; Loss: 0.03104417771100998\n",
      "Step 23 (5094); Episode 82/100; Loss: 0.00930534116923809\n",
      "Step 24 (5095); Episode 82/100; Loss: 0.02860233187675476\n",
      "Step 25 (5096); Episode 82/100; Loss: 0.030401136726140976\n",
      "Step 26 (5097); Episode 82/100; Loss: 0.0019210464088246226\n",
      "Step 27 (5098); Episode 82/100; Loss: 0.0006373862852342427\n",
      "Step 28 (5099); Episode 82/100; Loss: 0.0019254814833402634\n",
      "Step 29 (5100); Episode 82/100; Loss: 0.08707902580499649\n",
      "Step 30 (5101); Episode 82/100; Loss: 0.002510719234123826\n",
      "Step 31 (5102); Episode 82/100; Loss: 0.030382120981812477\n",
      "Step 32 (5103); Episode 82/100; Loss: 0.039270948618650436\n",
      "Step 33 (5104); Episode 82/100; Loss: 0.02918270416557789\n",
      "Step 34 (5105); Episode 82/100; Loss: 0.05295806750655174\n",
      "Step 35 (5106); Episode 82/100; Loss: 0.05748269706964493\n",
      "Step 36 (5107); Episode 82/100; Loss: 0.004065047483891249\n",
      "Step 37 (5108); Episode 82/100; Loss: 0.034572575241327286\n",
      "Step 38 (5109); Episode 82/100; Loss: 0.004339221399277449\n",
      "Step 39 (5110); Episode 82/100; Loss: 0.0446794331073761\n",
      "Step 40 (5111); Episode 82/100; Loss: 0.04904423654079437\n",
      "Step 41 (5112); Episode 82/100; Loss: 0.0881429985165596\n",
      "Step 42 (5113); Episode 82/100; Loss: 0.0012974463170394301\n",
      "Step 43 (5114); Episode 82/100; Loss: 0.041594766080379486\n",
      "Step 44 (5115); Episode 82/100; Loss: 0.017046339809894562\n",
      "Step 45 (5116); Episode 82/100; Loss: 0.04095315560698509\n",
      "Step 46 (5117); Episode 82/100; Loss: 0.12003707140684128\n",
      "Step 47 (5118); Episode 82/100; Loss: 0.04464567080140114\n",
      "Step 48 (5119); Episode 82/100; Loss: 0.021193446591496468\n",
      "Step 49 (5120); Episode 82/100; Loss: 0.02719445526599884\n",
      "Step 50 (5121); Episode 82/100; Loss: 0.06506627053022385\n",
      "Step 51 (5122); Episode 82/100; Loss: 0.0398809015750885\n",
      "Step 52 (5123); Episode 82/100; Loss: 0.08952353149652481\n",
      "Step 53 (5124); Episode 82/100; Loss: 0.0013857329031452537\n",
      "Step 54 (5125); Episode 82/100; Loss: 0.002148555824533105\n",
      "Step 55 (5126); Episode 82/100; Loss: 0.04083266854286194\n",
      "Step 56 (5127); Episode 82/100; Loss: 0.04342665523290634\n",
      "Step 57 (5128); Episode 82/100; Loss: 0.00103983364533633\n",
      "Step 58 (5129); Episode 82/100; Loss: 0.0021459239069372416\n",
      "Step 59 (5130); Episode 82/100; Loss: 0.002134334994480014\n",
      "Step 60 (5131); Episode 82/100; Loss: 0.04320598393678665\n",
      "Step 61 (5132); Episode 82/100; Loss: 0.08289426565170288\n",
      "Step 62 (5133); Episode 82/100; Loss: 0.003478701924905181\n",
      "Step 63 (5134); Episode 82/100; Loss: 0.02934676595032215\n",
      "Step 64 (5135); Episode 82/100; Loss: 0.005172751843929291\n",
      "Step 65 (5136); Episode 82/100; Loss: 0.04744533821940422\n",
      "Step 66 (5137); Episode 82/100; Loss: 0.0019017561571672559\n",
      "Step 67 (5138); Episode 82/100; Loss: 0.0738140270113945\n",
      "Step 68 (5139); Episode 82/100; Loss: 0.0018256959738209844\n",
      "Step 69 (5140); Episode 82/100; Loss: 0.0010777246206998825\n",
      "Step 70 (5141); Episode 82/100; Loss: 0.0024427573662251234\n",
      "Step 71 (5142); Episode 82/100; Loss: 0.05447455123066902\n",
      "Step 72 (5143); Episode 82/100; Loss: 0.0025381867308169603\n",
      "Step 73 (5144); Episode 82/100; Loss: 0.002096345415338874\n",
      "Step 74 (5145); Episode 82/100; Loss: 0.0042870864272117615\n",
      "Step 75 (5146); Episode 82/100; Loss: 0.0021065431647002697\n",
      "Step 76 (5147); Episode 82/100; Loss: 0.0064332992769777775\n",
      "Step 77 (5148); Episode 82/100; Loss: 0.045046571642160416\n",
      "Step 78 (5149); Episode 82/100; Loss: 0.08770719170570374\n",
      "Step 79 (5150); Episode 82/100; Loss: 0.001480471226386726\n",
      "Step 80 (5151); Episode 82/100; Loss: 0.04683975130319595\n",
      "Step 81 (5152); Episode 82/100; Loss: 0.0030842002015560865\n",
      "Step 82 (5153); Episode 82/100; Loss: 0.003219727659597993\n",
      "Step 83 (5154); Episode 82/100; Loss: 0.044853076338768005\n",
      "Step 84 (5155); Episode 82/100; Loss: 0.0013330754591152072\n",
      "Step 85 (5156); Episode 82/100; Loss: 0.042409900575876236\n",
      "Step 86 (5157); Episode 82/100; Loss: 0.05421806871891022\n",
      "Step 87 (5158); Episode 82/100; Loss: 0.01713714189827442\n",
      "Step 88 (5159); Episode 82/100; Loss: 0.04417251795530319\n",
      "Step 89 (5160); Episode 82/100; Loss: 0.006820492446422577\n",
      "Step 90 (5161); Episode 82/100; Loss: 0.03819563612341881\n",
      "Step 91 (5162); Episode 82/100; Loss: 0.10093606263399124\n",
      "Step 92 (5163); Episode 82/100; Loss: 0.07310325652360916\n",
      "Step 93 (5164); Episode 82/100; Loss: 0.014357609674334526\n",
      "Step 94 (5165); Episode 82/100; Loss: 0.04306494817137718\n",
      "Step 95 (5166); Episode 82/100; Loss: 0.05144379660487175\n",
      "Step 96 (5167); Episode 82/100; Loss: 0.04413556307554245\n",
      "Step 97 (5168); Episode 82/100; Loss: 0.07772865146398544\n",
      "Step 98 (5169); Episode 82/100; Loss: 0.07604341208934784\n",
      "Step 99 (5170); Episode 82/100; Loss: 0.04241247847676277\n",
      "Step 100 (5171); Episode 82/100; Loss: 0.0010820062598213553\n",
      "Step 101 (5172); Episode 82/100; Loss: 0.0035901109222322702\n",
      "Step 102 (5173); Episode 82/100; Loss: 0.08823705464601517\n",
      "Step 103 (5174); Episode 82/100; Loss: 0.0034260889515280724\n",
      "Step 104 (5175); Episode 82/100; Loss: 0.08191308379173279\n",
      "Step 105 (5176); Episode 82/100; Loss: 0.026413213461637497\n",
      "Step 106 (5177); Episode 82/100; Loss: 0.004051234107464552\n",
      "Step 107 (5178); Episode 82/100; Loss: 0.062173277139663696\n",
      "Step 108 (5179); Episode 82/100; Loss: 0.04671168699860573\n",
      "Step 109 (5180); Episode 82/100; Loss: 0.0033647615928202868\n",
      "Step 110 (5181); Episode 82/100; Loss: 0.04801664128899574\n",
      "Step 111 (5182); Episode 82/100; Loss: 0.08493391424417496\n",
      "Step 112 (5183); Episode 82/100; Loss: 0.023369554430246353\n",
      "Step 113 (5184); Episode 82/100; Loss: 0.03763856738805771\n",
      "Step 114 (5185); Episode 82/100; Loss: 0.0041069830767810345\n",
      "Step 115 (5186); Episode 82/100; Loss: 0.004061243962496519\n",
      "Step 116 (5187); Episode 82/100; Loss: 0.002101871417835355\n",
      "Step 117 (5188); Episode 82/100; Loss: 0.005624087993055582\n",
      "Step 118 (5189); Episode 82/100; Loss: 0.041503455489873886\n",
      "Step 119 (5190); Episode 82/100; Loss: 0.0013443896314129233\n",
      "Step 120 (5191); Episode 82/100; Loss: 0.0437081940472126\n",
      "Step 121 (5192); Episode 82/100; Loss: 0.03979361057281494\n",
      "Step 122 (5193); Episode 82/100; Loss: 0.04413824528455734\n",
      "Step 123 (5194); Episode 82/100; Loss: 0.0018210153793916106\n",
      "Step 124 (5195); Episode 82/100; Loss: 0.0020754593424499035\n",
      "Step 125 (5196); Episode 82/100; Loss: 0.04014144465327263\n",
      "Step 126 (5197); Episode 82/100; Loss: 0.007753763813525438\n",
      "Step 127 (5198); Episode 82/100; Loss: 0.0017361922655254602\n",
      "Step 128 (5199); Episode 82/100; Loss: 0.050916098058223724\n",
      "Step 0 (5200); Episode 83/100; Loss: 0.005130428355187178\n",
      "Step 1 (5201); Episode 83/100; Loss: 0.0459490530192852\n",
      "Step 2 (5202); Episode 83/100; Loss: 0.041636161506175995\n",
      "Step 3 (5203); Episode 83/100; Loss: 0.021979352459311485\n",
      "Step 4 (5204); Episode 83/100; Loss: 0.10440473258495331\n",
      "Step 5 (5205); Episode 83/100; Loss: 0.030551759526133537\n",
      "Step 6 (5206); Episode 83/100; Loss: 0.04326507821679115\n",
      "Step 7 (5207); Episode 83/100; Loss: 0.05178379639983177\n",
      "Step 8 (5208); Episode 83/100; Loss: 0.0012937275459989905\n",
      "Step 9 (5209); Episode 83/100; Loss: 0.009350336156785488\n",
      "Step 10 (5210); Episode 83/100; Loss: 0.001144982292316854\n",
      "Step 11 (5211); Episode 83/100; Loss: 0.10123390704393387\n",
      "Step 12 (5212); Episode 83/100; Loss: 0.0397631973028183\n",
      "Step 13 (5213); Episode 83/100; Loss: 0.11973827332258224\n",
      "Step 14 (5214); Episode 83/100; Loss: 0.04153827577829361\n",
      "Step 15 (5215); Episode 83/100; Loss: 0.002293857978656888\n",
      "Step 16 (5216); Episode 83/100; Loss: 0.0020874228794127703\n",
      "Step 17 (5217); Episode 83/100; Loss: 0.008866306394338608\n",
      "Step 18 (5218); Episode 83/100; Loss: 0.07745655626058578\n",
      "Step 19 (5219); Episode 83/100; Loss: 0.05174539238214493\n",
      "Step 20 (5220); Episode 83/100; Loss: 0.0031109079718589783\n",
      "Step 21 (5221); Episode 83/100; Loss: 0.0013417652808129787\n",
      "Step 22 (5222); Episode 83/100; Loss: 0.002912616590037942\n",
      "Step 23 (5223); Episode 83/100; Loss: 0.0018256636103615165\n",
      "Step 24 (5224); Episode 83/100; Loss: 0.09248540550470352\n",
      "Step 25 (5225); Episode 83/100; Loss: 0.0028874315321445465\n",
      "Step 26 (5226); Episode 83/100; Loss: 0.09901261329650879\n",
      "Step 27 (5227); Episode 83/100; Loss: 0.0010791189270094037\n",
      "Step 28 (5228); Episode 83/100; Loss: 0.03687809407711029\n",
      "Step 29 (5229); Episode 83/100; Loss: 0.047017015516757965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30 (5230); Episode 83/100; Loss: 0.03091062419116497\n",
      "Step 31 (5231); Episode 83/100; Loss: 0.05379197746515274\n",
      "Step 32 (5232); Episode 83/100; Loss: 0.025603296235203743\n",
      "Step 33 (5233); Episode 83/100; Loss: 0.12280665338039398\n",
      "Step 34 (5234); Episode 83/100; Loss: 0.00774073600769043\n",
      "Step 35 (5235); Episode 83/100; Loss: 0.0017877265345305204\n",
      "Step 36 (5236); Episode 83/100; Loss: 0.13113617897033691\n",
      "Step 37 (5237); Episode 83/100; Loss: 0.033909156918525696\n",
      "Step 38 (5238); Episode 83/100; Loss: 0.001174188801087439\n",
      "Step 39 (5239); Episode 83/100; Loss: 0.0011935657821595669\n",
      "Step 40 (5240); Episode 83/100; Loss: 0.03803882747888565\n",
      "Step 41 (5241); Episode 83/100; Loss: 0.0016506693791598082\n",
      "Step 42 (5242); Episode 83/100; Loss: 0.06897808611392975\n",
      "Step 43 (5243); Episode 83/100; Loss: 0.001435956684872508\n",
      "Step 44 (5244); Episode 83/100; Loss: 0.056436408311128616\n",
      "Step 45 (5245); Episode 83/100; Loss: 0.017716651782393456\n",
      "Step 46 (5246); Episode 83/100; Loss: 0.0033702931832522154\n",
      "Step 47 (5247); Episode 83/100; Loss: 0.03344182297587395\n",
      "Step 48 (5248); Episode 83/100; Loss: 0.031604450196027756\n",
      "Step 49 (5249); Episode 83/100; Loss: 0.03923217952251434\n",
      "Step 50 (5250); Episode 83/100; Loss: 0.03193400055170059\n",
      "Step 51 (5251); Episode 83/100; Loss: 0.032978229224681854\n",
      "Step 52 (5252); Episode 83/100; Loss: 0.027330519631505013\n",
      "Step 53 (5253); Episode 83/100; Loss: 0.04210355877876282\n",
      "Step 54 (5254); Episode 83/100; Loss: 0.04021207615733147\n",
      "Step 55 (5255); Episode 83/100; Loss: 0.0014307936653494835\n",
      "Step 56 (5256); Episode 83/100; Loss: 0.0016735350945964456\n",
      "Step 57 (5257); Episode 83/100; Loss: 0.015899909660220146\n",
      "Step 58 (5258); Episode 83/100; Loss: 0.10113029927015305\n",
      "Step 59 (5259); Episode 83/100; Loss: 0.037362612783908844\n",
      "Step 60 (5260); Episode 83/100; Loss: 0.0959242582321167\n",
      "Step 61 (5261); Episode 83/100; Loss: 0.04895683377981186\n",
      "Step 62 (5262); Episode 83/100; Loss: 0.01751534268260002\n",
      "Step 63 (5263); Episode 83/100; Loss: 0.04857093095779419\n",
      "Step 64 (5264); Episode 83/100; Loss: 0.06374454498291016\n",
      "Step 65 (5265); Episode 83/100; Loss: 0.0018983930349349976\n",
      "Step 66 (5266); Episode 83/100; Loss: 0.00305275060236454\n",
      "Step 67 (5267); Episode 83/100; Loss: 0.11333908885717392\n",
      "Step 68 (5268); Episode 83/100; Loss: 0.030077457427978516\n",
      "Step 69 (5269); Episode 83/100; Loss: 0.03310545161366463\n",
      "Step 70 (5270); Episode 83/100; Loss: 0.0016221220139414072\n",
      "Step 71 (5271); Episode 83/100; Loss: 0.04493751749396324\n",
      "Step 72 (5272); Episode 83/100; Loss: 0.02859252505004406\n",
      "Step 73 (5273); Episode 83/100; Loss: 0.07962385565042496\n",
      "Step 74 (5274); Episode 83/100; Loss: 0.003230624133720994\n",
      "Step 75 (5275); Episode 83/100; Loss: 0.012309296056628227\n",
      "Step 76 (5276); Episode 83/100; Loss: 0.0013848880771547556\n",
      "Step 77 (5277); Episode 83/100; Loss: 0.03222796693444252\n",
      "Step 78 (5278); Episode 83/100; Loss: 0.005421447567641735\n",
      "Step 79 (5279); Episode 83/100; Loss: 0.10384157299995422\n",
      "Step 80 (5280); Episode 83/100; Loss: 0.03473110496997833\n",
      "Step 81 (5281); Episode 83/100; Loss: 0.02890859544277191\n",
      "Step 82 (5282); Episode 83/100; Loss: 0.06825683265924454\n",
      "Step 83 (5283); Episode 83/100; Loss: 0.012516339309513569\n",
      "Step 84 (5284); Episode 83/100; Loss: 0.0022591608576476574\n",
      "Step 85 (5285); Episode 83/100; Loss: 0.023472115397453308\n",
      "Step 86 (5286); Episode 83/100; Loss: 0.048206303268671036\n",
      "Step 87 (5287); Episode 83/100; Loss: 0.06874706596136093\n",
      "Step 88 (5288); Episode 83/100; Loss: 0.008673080243170261\n",
      "Step 89 (5289); Episode 83/100; Loss: 0.030803583562374115\n",
      "Step 90 (5290); Episode 83/100; Loss: 0.0570736899971962\n",
      "Step 91 (5291); Episode 83/100; Loss: 0.0009007199550978839\n",
      "Step 92 (5292); Episode 83/100; Loss: 0.045068491250276566\n",
      "Step 93 (5293); Episode 83/100; Loss: 0.04461131617426872\n",
      "Step 94 (5294); Episode 83/100; Loss: 0.0018211430869996548\n",
      "Step 95 (5295); Episode 83/100; Loss: 0.024625342339277267\n",
      "Step 96 (5296); Episode 83/100; Loss: 0.049241382628679276\n",
      "Step 97 (5297); Episode 83/100; Loss: 0.08874624222517014\n",
      "Step 98 (5298); Episode 83/100; Loss: 0.003909064922481775\n",
      "Step 99 (5299); Episode 83/100; Loss: 0.03220056742429733\n",
      "Step 100 (5300); Episode 83/100; Loss: 0.042305655777454376\n",
      "Step 101 (5301); Episode 83/100; Loss: 0.002162182005122304\n",
      "Step 102 (5302); Episode 83/100; Loss: 0.07674598693847656\n",
      "Step 103 (5303); Episode 83/100; Loss: 0.07763169705867767\n",
      "Step 104 (5304); Episode 83/100; Loss: 0.0632399395108223\n",
      "Step 105 (5305); Episode 83/100; Loss: 0.0504227876663208\n",
      "Step 106 (5306); Episode 83/100; Loss: 0.0014403595123440027\n",
      "Step 107 (5307); Episode 83/100; Loss: 0.06540116667747498\n",
      "Step 108 (5308); Episode 83/100; Loss: 0.039960552006959915\n",
      "Step 109 (5309); Episode 83/100; Loss: 0.0314229317009449\n",
      "Step 110 (5310); Episode 83/100; Loss: 0.11525282263755798\n",
      "Step 111 (5311); Episode 83/100; Loss: 0.07035328447818756\n",
      "Step 112 (5312); Episode 83/100; Loss: 0.0802302211523056\n",
      "Step 113 (5313); Episode 83/100; Loss: 0.0780918151140213\n",
      "Step 114 (5314); Episode 83/100; Loss: 0.04462497681379318\n",
      "Step 115 (5315); Episode 83/100; Loss: 0.0030683272052556276\n",
      "Step 116 (5316); Episode 83/100; Loss: 0.039063163101673126\n",
      "Step 117 (5317); Episode 83/100; Loss: 0.10031187534332275\n",
      "Step 118 (5318); Episode 83/100; Loss: 0.002816195134073496\n",
      "Step 119 (5319); Episode 83/100; Loss: 0.07903651893138885\n",
      "Step 120 (5320); Episode 83/100; Loss: 0.008210081607103348\n",
      "Step 121 (5321); Episode 83/100; Loss: 0.0021307284478098154\n",
      "Step 122 (5322); Episode 83/100; Loss: 0.0018199614714831114\n",
      "Step 123 (5323); Episode 83/100; Loss: 0.02732795849442482\n",
      "Step 124 (5324); Episode 83/100; Loss: 0.0346762053668499\n",
      "Step 125 (5325); Episode 83/100; Loss: 0.005537500139325857\n",
      "Step 126 (5326); Episode 83/100; Loss: 0.0016791846137493849\n",
      "Step 127 (5327); Episode 83/100; Loss: 0.004917825106531382\n",
      "Step 0 (5328); Episode 84/100; Loss: 0.049808766692876816\n",
      "Step 1 (5329); Episode 84/100; Loss: 0.08993750065565109\n",
      "Step 2 (5330); Episode 84/100; Loss: 0.05507974326610565\n",
      "Step 3 (5331); Episode 84/100; Loss: 0.029612964019179344\n",
      "Step 4 (5332); Episode 84/100; Loss: 0.031207241117954254\n",
      "Step 5 (5333); Episode 84/100; Loss: 0.04553620517253876\n",
      "Step 6 (5334); Episode 84/100; Loss: 0.04249083623290062\n",
      "Step 7 (5335); Episode 84/100; Loss: 0.05547976493835449\n",
      "Step 8 (5336); Episode 84/100; Loss: 0.0009944384219124913\n",
      "Step 9 (5337); Episode 84/100; Loss: 0.003282641526311636\n",
      "Step 10 (5338); Episode 84/100; Loss: 0.07417864352464676\n",
      "Step 11 (5339); Episode 84/100; Loss: 0.0012051810044795275\n",
      "Step 12 (5340); Episode 84/100; Loss: 0.04307762160897255\n",
      "Step 13 (5341); Episode 84/100; Loss: 0.04006802663207054\n",
      "Step 14 (5342); Episode 84/100; Loss: 0.07484772801399231\n",
      "Step 15 (5343); Episode 84/100; Loss: 0.03001835010945797\n",
      "Step 16 (5344); Episode 84/100; Loss: 0.07814344018697739\n",
      "Step 17 (5345); Episode 84/100; Loss: 0.05422140657901764\n",
      "Step 18 (5346); Episode 84/100; Loss: 0.06272073090076447\n",
      "Step 19 (5347); Episode 84/100; Loss: 0.045575086027383804\n",
      "Step 20 (5348); Episode 84/100; Loss: 0.0011969166807830334\n",
      "Step 21 (5349); Episode 84/100; Loss: 0.036824844777584076\n",
      "Step 22 (5350); Episode 84/100; Loss: 0.06462353467941284\n",
      "Step 23 (5351); Episode 84/100; Loss: 0.0029682728927582502\n",
      "Step 24 (5352); Episode 84/100; Loss: 0.0018792388727888465\n",
      "Step 25 (5353); Episode 84/100; Loss: 0.037786729633808136\n",
      "Step 26 (5354); Episode 84/100; Loss: 0.03403180465102196\n",
      "Step 27 (5355); Episode 84/100; Loss: 0.0052214558236300945\n",
      "Step 28 (5356); Episode 84/100; Loss: 0.03612770140171051\n",
      "Step 29 (5357); Episode 84/100; Loss: 0.00359104061499238\n",
      "Step 30 (5358); Episode 84/100; Loss: 0.002511001192033291\n",
      "Step 31 (5359); Episode 84/100; Loss: 0.0026236791163682938\n",
      "Step 32 (5360); Episode 84/100; Loss: 0.0019674694631248713\n",
      "Step 33 (5361); Episode 84/100; Loss: 0.038307417184114456\n",
      "Step 34 (5362); Episode 84/100; Loss: 0.0763539969921112\n",
      "Step 35 (5363); Episode 84/100; Loss: 0.003028728300705552\n",
      "Step 36 (5364); Episode 84/100; Loss: 0.039904117584228516\n",
      "Step 37 (5365); Episode 84/100; Loss: 0.011705190874636173\n",
      "Step 38 (5366); Episode 84/100; Loss: 0.004338008817285299\n",
      "Step 39 (5367); Episode 84/100; Loss: 0.037683963775634766\n",
      "Step 40 (5368); Episode 84/100; Loss: 0.04684211686253548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41 (5369); Episode 84/100; Loss: 0.07191675156354904\n",
      "Step 42 (5370); Episode 84/100; Loss: 0.0576234795153141\n",
      "Step 43 (5371); Episode 84/100; Loss: 0.0450192354619503\n",
      "Step 44 (5372); Episode 84/100; Loss: 0.08171608299016953\n",
      "Step 45 (5373); Episode 84/100; Loss: 0.0017914107302203774\n",
      "Step 46 (5374); Episode 84/100; Loss: 0.03947307541966438\n",
      "Step 47 (5375); Episode 84/100; Loss: 0.03836006298661232\n",
      "Step 48 (5376); Episode 84/100; Loss: 0.10178631544113159\n",
      "Step 49 (5377); Episode 84/100; Loss: 0.022942934185266495\n",
      "Step 50 (5378); Episode 84/100; Loss: 0.0009168412652797997\n",
      "Step 51 (5379); Episode 84/100; Loss: 0.010726524516940117\n",
      "Step 52 (5380); Episode 84/100; Loss: 0.0011098275426775217\n",
      "Step 53 (5381); Episode 84/100; Loss: 0.006552586331963539\n",
      "Step 54 (5382); Episode 84/100; Loss: 0.01757204346358776\n",
      "Step 55 (5383); Episode 84/100; Loss: 0.03750913217663765\n",
      "Step 56 (5384); Episode 84/100; Loss: 0.06810997426509857\n",
      "Step 57 (5385); Episode 84/100; Loss: 0.001011706655845046\n",
      "Step 58 (5386); Episode 84/100; Loss: 0.07037194073200226\n",
      "Step 59 (5387); Episode 84/100; Loss: 0.05546441301703453\n",
      "Step 60 (5388); Episode 84/100; Loss: 0.006339745130389929\n",
      "Step 61 (5389); Episode 84/100; Loss: 0.05808920040726662\n",
      "Step 62 (5390); Episode 84/100; Loss: 0.023790040984749794\n",
      "Step 63 (5391); Episode 84/100; Loss: 0.00294293649494648\n",
      "Step 64 (5392); Episode 84/100; Loss: 0.0027505308389663696\n",
      "Step 65 (5393); Episode 84/100; Loss: 0.04358171299099922\n",
      "Step 66 (5394); Episode 84/100; Loss: 0.0016258105169981718\n",
      "Step 67 (5395); Episode 84/100; Loss: 0.05778762325644493\n",
      "Step 68 (5396); Episode 84/100; Loss: 0.00315947737544775\n",
      "Step 69 (5397); Episode 84/100; Loss: 0.002050577662885189\n",
      "Step 70 (5398); Episode 84/100; Loss: 0.043919116258621216\n",
      "Step 71 (5399); Episode 84/100; Loss: 0.12792345881462097\n",
      "Step 72 (5400); Episode 84/100; Loss: 0.0015333883929997683\n",
      "Step 73 (5401); Episode 84/100; Loss: 0.08639100193977356\n",
      "Step 74 (5402); Episode 84/100; Loss: 0.001671460224315524\n",
      "Step 75 (5403); Episode 84/100; Loss: 0.04876280203461647\n",
      "Step 76 (5404); Episode 84/100; Loss: 0.07679545134305954\n",
      "Step 77 (5405); Episode 84/100; Loss: 0.002385344123467803\n",
      "Step 78 (5406); Episode 84/100; Loss: 0.009710694663226604\n",
      "Step 79 (5407); Episode 84/100; Loss: 0.0016286057652905583\n",
      "Step 80 (5408); Episode 84/100; Loss: 0.09213881194591522\n",
      "Step 81 (5409); Episode 84/100; Loss: 0.026085980236530304\n",
      "Step 82 (5410); Episode 84/100; Loss: 0.001480645383708179\n",
      "Step 83 (5411); Episode 84/100; Loss: 0.03551429137587547\n",
      "Step 84 (5412); Episode 84/100; Loss: 0.0012654189486056566\n",
      "Step 85 (5413); Episode 84/100; Loss: 0.05284813791513443\n",
      "Step 86 (5414); Episode 84/100; Loss: 0.04184611514210701\n",
      "Step 87 (5415); Episode 84/100; Loss: 0.04074058681726456\n",
      "Step 88 (5416); Episode 84/100; Loss: 0.0017543280264362693\n",
      "Step 89 (5417); Episode 84/100; Loss: 0.015738025307655334\n",
      "Step 90 (5418); Episode 84/100; Loss: 0.013483568094670773\n",
      "Step 91 (5419); Episode 84/100; Loss: 0.017009004950523376\n",
      "Step 92 (5420); Episode 84/100; Loss: 0.003999799955636263\n",
      "Step 93 (5421); Episode 84/100; Loss: 0.0012224246747791767\n",
      "Step 94 (5422); Episode 84/100; Loss: 0.0030799307860434055\n",
      "Step 95 (5423); Episode 84/100; Loss: 0.05560680478811264\n",
      "Step 96 (5424); Episode 84/100; Loss: 0.041438594460487366\n",
      "Step 97 (5425); Episode 84/100; Loss: 0.03158228471875191\n",
      "Step 98 (5426); Episode 84/100; Loss: 0.0862959548830986\n",
      "Step 99 (5427); Episode 84/100; Loss: 0.05836621671915054\n",
      "Step 100 (5428); Episode 84/100; Loss: 0.0013208972522988915\n",
      "Step 101 (5429); Episode 84/100; Loss: 0.04763307422399521\n",
      "Step 102 (5430); Episode 84/100; Loss: 0.002838525688275695\n",
      "Step 103 (5431); Episode 84/100; Loss: 0.04666182026267052\n",
      "Step 104 (5432); Episode 84/100; Loss: 0.09599802643060684\n",
      "Step 105 (5433); Episode 84/100; Loss: 0.15265591442584991\n",
      "Step 106 (5434); Episode 84/100; Loss: 0.0014679208397865295\n",
      "Step 107 (5435); Episode 84/100; Loss: 0.03354983404278755\n",
      "Step 108 (5436); Episode 84/100; Loss: 0.047955069690942764\n",
      "Step 109 (5437); Episode 84/100; Loss: 0.048050105571746826\n",
      "Step 110 (5438); Episode 84/100; Loss: 0.06221760809421539\n",
      "Step 111 (5439); Episode 84/100; Loss: 0.05977126955986023\n",
      "Step 112 (5440); Episode 84/100; Loss: 0.015189304947853088\n",
      "Step 113 (5441); Episode 84/100; Loss: 0.037222281098365784\n",
      "Step 114 (5442); Episode 84/100; Loss: 0.026024172082543373\n",
      "Step 115 (5443); Episode 84/100; Loss: 0.00449011754244566\n",
      "Step 116 (5444); Episode 84/100; Loss: 0.12571722269058228\n",
      "Step 117 (5445); Episode 84/100; Loss: 0.053969137370586395\n",
      "Step 118 (5446); Episode 84/100; Loss: 0.09511964023113251\n",
      "Step 119 (5447); Episode 84/100; Loss: 0.10835669189691544\n",
      "Step 120 (5448); Episode 84/100; Loss: 0.04192017391324043\n",
      "Step 121 (5449); Episode 84/100; Loss: 0.0019450230756774545\n",
      "Step 122 (5450); Episode 84/100; Loss: 0.0784708634018898\n",
      "Step 123 (5451); Episode 84/100; Loss: 0.0014498033560812473\n",
      "Step 124 (5452); Episode 84/100; Loss: 0.03648766875267029\n",
      "Step 125 (5453); Episode 84/100; Loss: 0.051117729395627975\n",
      "Step 126 (5454); Episode 84/100; Loss: 0.0798681378364563\n",
      "Step 127 (5455); Episode 84/100; Loss: 0.0030211997218430042\n",
      "Step 128 (5456); Episode 84/100; Loss: 0.04524854198098183\n",
      "Step 129 (5457); Episode 84/100; Loss: 0.002250849734991789\n",
      "Step 130 (5458); Episode 84/100; Loss: 0.009891822002828121\n",
      "Step 131 (5459); Episode 84/100; Loss: 0.0014120147097855806\n",
      "Step 132 (5460); Episode 84/100; Loss: 0.08154945075511932\n",
      "Step 133 (5461); Episode 84/100; Loss: 0.04718107730150223\n",
      "Step 134 (5462); Episode 84/100; Loss: 0.002565947128459811\n",
      "Step 135 (5463); Episode 84/100; Loss: 0.03770630434155464\n",
      "Step 136 (5464); Episode 84/100; Loss: 0.001091029611416161\n",
      "Step 137 (5465); Episode 84/100; Loss: 0.0015896697295829654\n",
      "Step 138 (5466); Episode 84/100; Loss: 0.034079909324645996\n",
      "Step 139 (5467); Episode 84/100; Loss: 0.04133584350347519\n",
      "Step 140 (5468); Episode 84/100; Loss: 0.008737235330045223\n",
      "Step 141 (5469); Episode 84/100; Loss: 0.08501821011304855\n",
      "Step 142 (5470); Episode 84/100; Loss: 0.06499892473220825\n",
      "Step 143 (5471); Episode 84/100; Loss: 0.05015206336975098\n",
      "Step 144 (5472); Episode 84/100; Loss: 0.0011525218142196536\n",
      "Step 145 (5473); Episode 84/100; Loss: 0.061154283583164215\n",
      "Step 146 (5474); Episode 84/100; Loss: 0.04889797046780586\n",
      "Step 147 (5475); Episode 84/100; Loss: 0.0012450168142095208\n",
      "Step 148 (5476); Episode 84/100; Loss: 0.019911130890250206\n",
      "Step 149 (5477); Episode 84/100; Loss: 0.038159001618623734\n",
      "Step 150 (5478); Episode 84/100; Loss: 0.016749676316976547\n",
      "Step 151 (5479); Episode 84/100; Loss: 0.10689738392829895\n",
      "Step 152 (5480); Episode 84/100; Loss: 0.04128051549196243\n",
      "Step 153 (5481); Episode 84/100; Loss: 0.08796410262584686\n",
      "Step 154 (5482); Episode 84/100; Loss: 0.07978291064500809\n",
      "Step 155 (5483); Episode 84/100; Loss: 0.002697030548006296\n",
      "Step 156 (5484); Episode 84/100; Loss: 0.03744712099432945\n",
      "Step 157 (5485); Episode 84/100; Loss: 0.06344636529684067\n",
      "Step 158 (5486); Episode 84/100; Loss: 0.11831508576869965\n",
      "Step 159 (5487); Episode 84/100; Loss: 0.02437584474682808\n",
      "Step 160 (5488); Episode 84/100; Loss: 0.0024037430994212627\n",
      "Step 161 (5489); Episode 84/100; Loss: 0.0013410609681159258\n",
      "Step 162 (5490); Episode 84/100; Loss: 0.002558209700509906\n",
      "Step 163 (5491); Episode 84/100; Loss: 0.015009616501629353\n",
      "Step 164 (5492); Episode 84/100; Loss: 0.07149696350097656\n",
      "Step 165 (5493); Episode 84/100; Loss: 0.0425993874669075\n",
      "Step 166 (5494); Episode 84/100; Loss: 0.04584910720586777\n",
      "Step 167 (5495); Episode 84/100; Loss: 0.040693528950214386\n",
      "Step 168 (5496); Episode 84/100; Loss: 0.00510014733299613\n",
      "Step 169 (5497); Episode 84/100; Loss: 0.03814644366502762\n",
      "Step 170 (5498); Episode 84/100; Loss: 0.003142960136756301\n",
      "Step 171 (5499); Episode 84/100; Loss: 0.04543306306004524\n",
      "Step 172 (5500); Episode 84/100; Loss: 0.054552022367715836\n",
      "Step 173 (5501); Episode 84/100; Loss: 0.052799105644226074\n",
      "Step 174 (5502); Episode 84/100; Loss: 0.001630835235118866\n",
      "Step 175 (5503); Episode 84/100; Loss: 0.0021939040161669254\n",
      "Step 176 (5504); Episode 84/100; Loss: 0.0016828313237056136\n",
      "Step 177 (5505); Episode 84/100; Loss: 0.0023971872869879007\n",
      "Step 178 (5506); Episode 84/100; Loss: 0.04552408680319786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 179 (5507); Episode 84/100; Loss: 0.05276549234986305\n",
      "Step 180 (5508); Episode 84/100; Loss: 0.0018895487301051617\n",
      "Step 181 (5509); Episode 84/100; Loss: 0.052493199706077576\n",
      "Step 182 (5510); Episode 84/100; Loss: 0.0026987253222614527\n",
      "Step 183 (5511); Episode 84/100; Loss: 0.01879674755036831\n",
      "Step 184 (5512); Episode 84/100; Loss: 0.0018989444943144917\n",
      "Step 185 (5513); Episode 84/100; Loss: 0.002106545027345419\n",
      "Step 186 (5514); Episode 84/100; Loss: 0.03365541994571686\n",
      "Step 187 (5515); Episode 84/100; Loss: 0.09454076737165451\n",
      "Step 188 (5516); Episode 84/100; Loss: 0.023731732740998268\n",
      "Step 189 (5517); Episode 84/100; Loss: 0.020865026861429214\n",
      "Step 190 (5518); Episode 84/100; Loss: 0.011732567101716995\n",
      "Step 191 (5519); Episode 84/100; Loss: 0.07041501998901367\n",
      "Step 192 (5520); Episode 84/100; Loss: 0.014915156178176403\n",
      "Step 193 (5521); Episode 84/100; Loss: 0.0018120357999578118\n",
      "Step 194 (5522); Episode 84/100; Loss: 0.004178025759756565\n",
      "Step 195 (5523); Episode 84/100; Loss: 0.0038218721747398376\n",
      "Step 196 (5524); Episode 84/100; Loss: 0.027413159608840942\n",
      "Step 197 (5525); Episode 84/100; Loss: 0.030938347801566124\n",
      "Step 198 (5526); Episode 84/100; Loss: 0.007528003770858049\n",
      "Step 199 (5527); Episode 84/100; Loss: 0.039894554764032364\n",
      "Step 0 (5528); Episode 85/100; Loss: 0.025917556136846542\n",
      "Step 1 (5529); Episode 85/100; Loss: 0.0015750769525766373\n",
      "Step 2 (5530); Episode 85/100; Loss: 0.0028111899737268686\n",
      "Step 3 (5531); Episode 85/100; Loss: 0.0393657423555851\n",
      "Step 4 (5532); Episode 85/100; Loss: 0.05458313599228859\n",
      "Step 5 (5533); Episode 85/100; Loss: 0.00692438380792737\n",
      "Step 6 (5534); Episode 85/100; Loss: 0.0015838297549635172\n",
      "Step 7 (5535); Episode 85/100; Loss: 0.0038129875902086496\n",
      "Step 8 (5536); Episode 85/100; Loss: 0.09579261392354965\n",
      "Step 9 (5537); Episode 85/100; Loss: 0.03767286241054535\n",
      "Step 10 (5538); Episode 85/100; Loss: 0.056736480444669724\n",
      "Step 11 (5539); Episode 85/100; Loss: 0.09265496581792831\n",
      "Step 12 (5540); Episode 85/100; Loss: 0.0030943136662244797\n",
      "Step 13 (5541); Episode 85/100; Loss: 0.003082089591771364\n",
      "Step 14 (5542); Episode 85/100; Loss: 0.0025569971185177565\n",
      "Step 15 (5543); Episode 85/100; Loss: 0.003069340717047453\n",
      "Step 16 (5544); Episode 85/100; Loss: 0.0376632995903492\n",
      "Step 17 (5545); Episode 85/100; Loss: 0.0067368242889642715\n",
      "Step 18 (5546); Episode 85/100; Loss: 0.003277823096141219\n",
      "Step 19 (5547); Episode 85/100; Loss: 0.07727993279695511\n",
      "Step 20 (5548); Episode 85/100; Loss: 0.033258106559515\n",
      "Step 21 (5549); Episode 85/100; Loss: 0.00532032223418355\n",
      "Step 22 (5550); Episode 85/100; Loss: 0.0014404702233150601\n",
      "Step 23 (5551); Episode 85/100; Loss: 0.002204766497015953\n",
      "Step 24 (5552); Episode 85/100; Loss: 0.002451846841722727\n",
      "Step 25 (5553); Episode 85/100; Loss: 0.018809683620929718\n",
      "Step 26 (5554); Episode 85/100; Loss: 0.024875223636627197\n",
      "Step 27 (5555); Episode 85/100; Loss: 0.049959730356931686\n",
      "Step 28 (5556); Episode 85/100; Loss: 0.08119244873523712\n",
      "Step 29 (5557); Episode 85/100; Loss: 0.09284865111112595\n",
      "Step 30 (5558); Episode 85/100; Loss: 0.004508253652602434\n",
      "Step 31 (5559); Episode 85/100; Loss: 0.07549390196800232\n",
      "Step 32 (5560); Episode 85/100; Loss: 0.004688873887062073\n",
      "Step 33 (5561); Episode 85/100; Loss: 0.04177303612232208\n",
      "Step 34 (5562); Episode 85/100; Loss: 0.04362434893846512\n",
      "Step 35 (5563); Episode 85/100; Loss: 0.04462443292140961\n",
      "Step 36 (5564); Episode 85/100; Loss: 0.05830485373735428\n",
      "Step 37 (5565); Episode 85/100; Loss: 0.03448864072561264\n",
      "Step 38 (5566); Episode 85/100; Loss: 0.07919178158044815\n",
      "Step 39 (5567); Episode 85/100; Loss: 0.009145344607532024\n",
      "Step 40 (5568); Episode 85/100; Loss: 0.04887238144874573\n",
      "Step 41 (5569); Episode 85/100; Loss: 0.05571024492383003\n",
      "Step 42 (5570); Episode 85/100; Loss: 0.0023999628610908985\n",
      "Step 43 (5571); Episode 85/100; Loss: 0.0016125221736729145\n",
      "Step 44 (5572); Episode 85/100; Loss: 0.005175200756639242\n",
      "Step 45 (5573); Episode 85/100; Loss: 0.040544938296079636\n",
      "Step 46 (5574); Episode 85/100; Loss: 0.03919873386621475\n",
      "Step 47 (5575); Episode 85/100; Loss: 0.0016626864671707153\n",
      "Step 48 (5576); Episode 85/100; Loss: 0.1297101080417633\n",
      "Step 49 (5577); Episode 85/100; Loss: 0.005836972501128912\n",
      "Step 50 (5578); Episode 85/100; Loss: 0.005648395512253046\n",
      "Step 51 (5579); Episode 85/100; Loss: 0.0023735910654067993\n",
      "Step 52 (5580); Episode 85/100; Loss: 0.06600931286811829\n",
      "Step 53 (5581); Episode 85/100; Loss: 0.044133976101875305\n",
      "Step 54 (5582); Episode 85/100; Loss: 0.039378587156534195\n",
      "Step 55 (5583); Episode 85/100; Loss: 0.046434320509433746\n",
      "Step 56 (5584); Episode 85/100; Loss: 0.04467472806572914\n",
      "Step 57 (5585); Episode 85/100; Loss: 0.0046477592550218105\n",
      "Step 58 (5586); Episode 85/100; Loss: 0.05476820468902588\n",
      "Step 59 (5587); Episode 85/100; Loss: 0.001519412500783801\n",
      "Step 60 (5588); Episode 85/100; Loss: 0.05045466497540474\n",
      "Step 61 (5589); Episode 85/100; Loss: 0.0015173038700595498\n",
      "Step 62 (5590); Episode 85/100; Loss: 0.0020342147909104824\n",
      "Step 63 (5591); Episode 85/100; Loss: 0.042680077254772186\n",
      "Step 64 (5592); Episode 85/100; Loss: 0.001332284533418715\n",
      "Step 65 (5593); Episode 85/100; Loss: 0.001638351590372622\n",
      "Step 66 (5594); Episode 85/100; Loss: 0.06204243749380112\n",
      "Step 67 (5595); Episode 85/100; Loss: 0.019495990127325058\n",
      "Step 68 (5596); Episode 85/100; Loss: 0.006927120499312878\n",
      "Step 69 (5597); Episode 85/100; Loss: 0.002893131459131837\n",
      "Step 70 (5598); Episode 85/100; Loss: 0.07656262069940567\n",
      "Step 71 (5599); Episode 85/100; Loss: 0.07889513671398163\n",
      "Step 72 (5600); Episode 85/100; Loss: 0.0027226083911955357\n",
      "Step 73 (5601); Episode 85/100; Loss: 0.1620863527059555\n",
      "Step 74 (5602); Episode 85/100; Loss: 0.10197527706623077\n",
      "Step 75 (5603); Episode 85/100; Loss: 0.003758865175768733\n",
      "Step 76 (5604); Episode 85/100; Loss: 0.0019311446230858564\n",
      "Step 77 (5605); Episode 85/100; Loss: 0.031043916940689087\n",
      "Step 78 (5606); Episode 85/100; Loss: 0.004379624500870705\n",
      "Step 79 (5607); Episode 85/100; Loss: 0.012528285384178162\n",
      "Step 80 (5608); Episode 85/100; Loss: 0.0783614069223404\n",
      "Step 81 (5609); Episode 85/100; Loss: 0.03578970953822136\n",
      "Step 82 (5610); Episode 85/100; Loss: 0.03841407597064972\n",
      "Step 83 (5611); Episode 85/100; Loss: 0.004935916047543287\n",
      "Step 84 (5612); Episode 85/100; Loss: 0.001253618742339313\n",
      "Step 85 (5613); Episode 85/100; Loss: 0.04511525481939316\n",
      "Step 86 (5614); Episode 85/100; Loss: 0.0007671733619645238\n",
      "Step 87 (5615); Episode 85/100; Loss: 0.05733216926455498\n",
      "Step 88 (5616); Episode 85/100; Loss: 0.0036172177642583847\n",
      "Step 89 (5617); Episode 85/100; Loss: 0.04748247191309929\n",
      "Step 90 (5618); Episode 85/100; Loss: 0.00235702539794147\n",
      "Step 91 (5619); Episode 85/100; Loss: 0.1051076352596283\n",
      "Step 92 (5620); Episode 85/100; Loss: 0.025169668719172478\n",
      "Step 93 (5621); Episode 85/100; Loss: 0.03403312340378761\n",
      "Step 94 (5622); Episode 85/100; Loss: 0.005687721539288759\n",
      "Step 95 (5623); Episode 85/100; Loss: 0.001158926053903997\n",
      "Step 96 (5624); Episode 85/100; Loss: 0.0025092908181250095\n",
      "Step 97 (5625); Episode 85/100; Loss: 0.052979398518800735\n",
      "Step 98 (5626); Episode 85/100; Loss: 0.0018630186095833778\n",
      "Step 99 (5627); Episode 85/100; Loss: 0.018062133342027664\n",
      "Step 100 (5628); Episode 85/100; Loss: 0.05279560759663582\n",
      "Step 101 (5629); Episode 85/100; Loss: 0.002849422162398696\n",
      "Step 102 (5630); Episode 85/100; Loss: 0.1541759967803955\n",
      "Step 103 (5631); Episode 85/100; Loss: 0.0375925749540329\n",
      "Step 104 (5632); Episode 85/100; Loss: 0.09328844398260117\n",
      "Step 105 (5633); Episode 85/100; Loss: 0.0012261327356100082\n",
      "Step 106 (5634); Episode 85/100; Loss: 0.026038486510515213\n",
      "Step 107 (5635); Episode 85/100; Loss: 0.0389915332198143\n",
      "Step 108 (5636); Episode 85/100; Loss: 0.038289859890937805\n",
      "Step 109 (5637); Episode 85/100; Loss: 0.03892001882195473\n",
      "Step 110 (5638); Episode 85/100; Loss: 0.05013374239206314\n",
      "Step 111 (5639); Episode 85/100; Loss: 0.005851627327501774\n",
      "Step 112 (5640); Episode 85/100; Loss: 0.0027709638234227896\n",
      "Step 113 (5641); Episode 85/100; Loss: 0.003618458518758416\n",
      "Step 114 (5642); Episode 85/100; Loss: 0.034985512495040894\n",
      "Step 115 (5643); Episode 85/100; Loss: 0.0067057134583592415\n",
      "Step 116 (5644); Episode 85/100; Loss: 0.03835849091410637\n",
      "Step 117 (5645); Episode 85/100; Loss: 0.12165336310863495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 118 (5646); Episode 85/100; Loss: 0.0012177168391644955\n",
      "Step 119 (5647); Episode 85/100; Loss: 0.023770052939653397\n",
      "Step 120 (5648); Episode 85/100; Loss: 0.003974770661443472\n",
      "Step 121 (5649); Episode 85/100; Loss: 0.05255405604839325\n",
      "Step 122 (5650); Episode 85/100; Loss: 0.10925687104463577\n",
      "Step 123 (5651); Episode 85/100; Loss: 0.0013795564882457256\n",
      "Step 124 (5652); Episode 85/100; Loss: 0.08800214529037476\n",
      "Step 125 (5653); Episode 85/100; Loss: 0.05670229718089104\n",
      "Step 126 (5654); Episode 85/100; Loss: 0.034075308591127396\n",
      "Step 127 (5655); Episode 85/100; Loss: 0.09258488565683365\n",
      "Step 128 (5656); Episode 85/100; Loss: 0.12343239784240723\n",
      "Step 0 (5657); Episode 86/100; Loss: 0.03810290992259979\n",
      "Step 1 (5658); Episode 86/100; Loss: 0.03167548030614853\n",
      "Step 2 (5659); Episode 86/100; Loss: 0.04462876543402672\n",
      "Step 3 (5660); Episode 86/100; Loss: 0.0032037405762821436\n",
      "Step 4 (5661); Episode 86/100; Loss: 0.03608700633049011\n",
      "Step 5 (5662); Episode 86/100; Loss: 0.06346961110830307\n",
      "Step 6 (5663); Episode 86/100; Loss: 0.0857783630490303\n",
      "Step 7 (5664); Episode 86/100; Loss: 0.0578654408454895\n",
      "Step 8 (5665); Episode 86/100; Loss: 0.00580669566988945\n",
      "Step 9 (5666); Episode 86/100; Loss: 0.0170891173183918\n",
      "Step 10 (5667); Episode 86/100; Loss: 0.06642468273639679\n",
      "Step 11 (5668); Episode 86/100; Loss: 0.029407035559415817\n",
      "Step 12 (5669); Episode 86/100; Loss: 0.018358033150434494\n",
      "Step 13 (5670); Episode 86/100; Loss: 0.06228877604007721\n",
      "Step 14 (5671); Episode 86/100; Loss: 0.061722464859485626\n",
      "Step 15 (5672); Episode 86/100; Loss: 0.09119272232055664\n",
      "Step 16 (5673); Episode 86/100; Loss: 0.0006937765865586698\n",
      "Step 17 (5674); Episode 86/100; Loss: 0.019894016906619072\n",
      "Step 18 (5675); Episode 86/100; Loss: 0.03651471063494682\n",
      "Step 19 (5676); Episode 86/100; Loss: 0.0026407483965158463\n",
      "Step 20 (5677); Episode 86/100; Loss: 0.0021842580754309893\n",
      "Step 21 (5678); Episode 86/100; Loss: 0.0026848972775042057\n",
      "Step 22 (5679); Episode 86/100; Loss: 0.08281202614307404\n",
      "Step 23 (5680); Episode 86/100; Loss: 0.12697046995162964\n",
      "Step 24 (5681); Episode 86/100; Loss: 0.012947631068527699\n",
      "Step 25 (5682); Episode 86/100; Loss: 0.04477313160896301\n",
      "Step 26 (5683); Episode 86/100; Loss: 0.0028910974506288767\n",
      "Step 27 (5684); Episode 86/100; Loss: 0.03441191837191582\n",
      "Step 28 (5685); Episode 86/100; Loss: 0.05033019930124283\n",
      "Step 29 (5686); Episode 86/100; Loss: 0.005856184754520655\n",
      "Step 30 (5687); Episode 86/100; Loss: 0.0023209978826344013\n",
      "Step 31 (5688); Episode 86/100; Loss: 0.045423801988363266\n",
      "Step 32 (5689); Episode 86/100; Loss: 0.0023677872959524393\n",
      "Step 33 (5690); Episode 86/100; Loss: 0.05687560886144638\n",
      "Step 34 (5691); Episode 86/100; Loss: 0.028210075572133064\n",
      "Step 35 (5692); Episode 86/100; Loss: 0.07151510566473007\n",
      "Step 36 (5693); Episode 86/100; Loss: 0.07370088994503021\n",
      "Step 37 (5694); Episode 86/100; Loss: 0.08510707318782806\n",
      "Step 38 (5695); Episode 86/100; Loss: 0.008386636152863503\n",
      "Step 39 (5696); Episode 86/100; Loss: 0.0026256097480654716\n",
      "Step 40 (5697); Episode 86/100; Loss: 0.03843852877616882\n",
      "Step 41 (5698); Episode 86/100; Loss: 0.03328843042254448\n",
      "Step 42 (5699); Episode 86/100; Loss: 0.05310183763504028\n",
      "Step 43 (5700); Episode 86/100; Loss: 0.09066307544708252\n",
      "Step 44 (5701); Episode 86/100; Loss: 0.002107593696564436\n",
      "Step 45 (5702); Episode 86/100; Loss: 0.008759110234677792\n",
      "Step 46 (5703); Episode 86/100; Loss: 0.034271448850631714\n",
      "Step 47 (5704); Episode 86/100; Loss: 0.0010914840968325734\n",
      "Step 48 (5705); Episode 86/100; Loss: 0.002422353019937873\n",
      "Step 49 (5706); Episode 86/100; Loss: 0.022964736446738243\n",
      "Step 50 (5707); Episode 86/100; Loss: 0.0017330065602436662\n",
      "Step 51 (5708); Episode 86/100; Loss: 0.037562865763902664\n",
      "Step 52 (5709); Episode 86/100; Loss: 0.07922174781560898\n",
      "Step 53 (5710); Episode 86/100; Loss: 0.043153006583452225\n",
      "Step 54 (5711); Episode 86/100; Loss: 0.0068779271095991135\n",
      "Step 55 (5712); Episode 86/100; Loss: 0.0022408992517739534\n",
      "Step 56 (5713); Episode 86/100; Loss: 0.05840672552585602\n",
      "Step 57 (5714); Episode 86/100; Loss: 0.0035085100680589676\n",
      "Step 58 (5715); Episode 86/100; Loss: 0.03610055893659592\n",
      "Step 59 (5716); Episode 86/100; Loss: 0.0013345348415896297\n",
      "Step 60 (5717); Episode 86/100; Loss: 0.0013698784168809652\n",
      "Step 61 (5718); Episode 86/100; Loss: 0.0015690781874582171\n",
      "Step 62 (5719); Episode 86/100; Loss: 0.03531080484390259\n",
      "Step 63 (5720); Episode 86/100; Loss: 0.04159855842590332\n",
      "Step 64 (5721); Episode 86/100; Loss: 0.0930446907877922\n",
      "Step 65 (5722); Episode 86/100; Loss: 0.03856709226965904\n",
      "Step 66 (5723); Episode 86/100; Loss: 0.004966604523360729\n",
      "Step 67 (5724); Episode 86/100; Loss: 0.0010192716727033257\n",
      "Step 68 (5725); Episode 86/100; Loss: 0.11364957690238953\n",
      "Step 69 (5726); Episode 86/100; Loss: 0.031440380960702896\n",
      "Step 70 (5727); Episode 86/100; Loss: 0.051477570086717606\n",
      "Step 71 (5728); Episode 86/100; Loss: 0.045345939695835114\n",
      "Step 72 (5729); Episode 86/100; Loss: 0.0017516551306471229\n",
      "Step 73 (5730); Episode 86/100; Loss: 0.0014838834758847952\n",
      "Step 74 (5731); Episode 86/100; Loss: 0.039303719997406006\n",
      "Step 75 (5732); Episode 86/100; Loss: 0.0293206125497818\n",
      "Step 76 (5733); Episode 86/100; Loss: 0.08572046458721161\n",
      "Step 77 (5734); Episode 86/100; Loss: 0.0378941111266613\n",
      "Step 78 (5735); Episode 86/100; Loss: 0.04061023145914078\n",
      "Step 79 (5736); Episode 86/100; Loss: 0.001361052505671978\n",
      "Step 80 (5737); Episode 86/100; Loss: 0.0021912718657404184\n",
      "Step 81 (5738); Episode 86/100; Loss: 0.0015432479558512568\n",
      "Step 82 (5739); Episode 86/100; Loss: 0.07235749065876007\n",
      "Step 83 (5740); Episode 86/100; Loss: 0.013227712363004684\n",
      "Step 84 (5741); Episode 86/100; Loss: 0.013315346091985703\n",
      "Step 85 (5742); Episode 86/100; Loss: 0.052983883768320084\n",
      "Step 86 (5743); Episode 86/100; Loss: 0.05488061532378197\n",
      "Step 87 (5744); Episode 86/100; Loss: 0.01992962136864662\n",
      "Step 88 (5745); Episode 86/100; Loss: 0.0479680635035038\n",
      "Step 89 (5746); Episode 86/100; Loss: 0.0021352972835302353\n",
      "Step 90 (5747); Episode 86/100; Loss: 0.003171902848407626\n",
      "Step 91 (5748); Episode 86/100; Loss: 0.05353729799389839\n",
      "Step 92 (5749); Episode 86/100; Loss: 0.04070989042520523\n",
      "Step 93 (5750); Episode 86/100; Loss: 0.027955301105976105\n",
      "Step 94 (5751); Episode 86/100; Loss: 0.001928161014802754\n",
      "Step 95 (5752); Episode 86/100; Loss: 0.037576425820589066\n",
      "Step 96 (5753); Episode 86/100; Loss: 0.03986479714512825\n",
      "Step 97 (5754); Episode 86/100; Loss: 0.005091962870210409\n",
      "Step 98 (5755); Episode 86/100; Loss: 0.004215868189930916\n",
      "Step 99 (5756); Episode 86/100; Loss: 0.0800136923789978\n",
      "Step 100 (5757); Episode 86/100; Loss: 0.001515682670287788\n",
      "Step 101 (5758); Episode 86/100; Loss: 0.0022664510179311037\n",
      "Step 102 (5759); Episode 86/100; Loss: 0.0017244950868189335\n",
      "Step 103 (5760); Episode 86/100; Loss: 0.00568726658821106\n",
      "Step 104 (5761); Episode 86/100; Loss: 0.08224888145923615\n",
      "Step 105 (5762); Episode 86/100; Loss: 0.04978584498167038\n",
      "Step 106 (5763); Episode 86/100; Loss: 0.00187216408085078\n",
      "Step 107 (5764); Episode 86/100; Loss: 0.050943516194820404\n",
      "Step 108 (5765); Episode 86/100; Loss: 0.003092212602496147\n",
      "Step 109 (5766); Episode 86/100; Loss: 0.02274923026561737\n",
      "Step 110 (5767); Episode 86/100; Loss: 0.00936909206211567\n",
      "Step 111 (5768); Episode 86/100; Loss: 0.0006475253612734377\n",
      "Step 112 (5769); Episode 86/100; Loss: 0.0018122673500329256\n",
      "Step 113 (5770); Episode 86/100; Loss: 0.066669762134552\n",
      "Step 114 (5771); Episode 86/100; Loss: 0.001424354501068592\n",
      "Step 115 (5772); Episode 86/100; Loss: 0.027073515579104424\n",
      "Step 116 (5773); Episode 86/100; Loss: 0.03882327675819397\n",
      "Step 117 (5774); Episode 86/100; Loss: 0.048799507319927216\n",
      "Step 118 (5775); Episode 86/100; Loss: 0.04630381241440773\n",
      "Step 119 (5776); Episode 86/100; Loss: 0.02633109875023365\n",
      "Step 120 (5777); Episode 86/100; Loss: 0.10691432654857635\n",
      "Step 121 (5778); Episode 86/100; Loss: 0.001554911839775741\n",
      "Step 122 (5779); Episode 86/100; Loss: 0.0010486324317753315\n",
      "Step 123 (5780); Episode 86/100; Loss: 0.051584966480731964\n",
      "Step 124 (5781); Episode 86/100; Loss: 0.001963115995749831\n",
      "Step 125 (5782); Episode 86/100; Loss: 0.05851255729794502\n",
      "Step 126 (5783); Episode 86/100; Loss: 0.04137250781059265\n",
      "Step 127 (5784); Episode 86/100; Loss: 0.03267250582575798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 128 (5785); Episode 86/100; Loss: 0.028492765501141548\n",
      "Step 129 (5786); Episode 86/100; Loss: 0.05559609830379486\n",
      "Step 130 (5787); Episode 86/100; Loss: 0.05965273827314377\n",
      "Step 131 (5788); Episode 86/100; Loss: 0.001491488772444427\n",
      "Step 132 (5789); Episode 86/100; Loss: 0.04238563030958176\n",
      "Step 133 (5790); Episode 86/100; Loss: 0.013975405134260654\n",
      "Step 134 (5791); Episode 86/100; Loss: 0.09421198070049286\n",
      "Step 135 (5792); Episode 86/100; Loss: 0.03005504235625267\n",
      "Step 136 (5793); Episode 86/100; Loss: 0.1267372965812683\n",
      "Step 137 (5794); Episode 86/100; Loss: 0.007405601907521486\n",
      "Step 138 (5795); Episode 86/100; Loss: 0.06286270916461945\n",
      "Step 139 (5796); Episode 86/100; Loss: 0.0032125748693943024\n",
      "Step 140 (5797); Episode 86/100; Loss: 0.0015825909795239568\n",
      "Step 141 (5798); Episode 86/100; Loss: 0.024948474019765854\n",
      "Step 142 (5799); Episode 86/100; Loss: 0.026781590655446053\n",
      "Step 143 (5800); Episode 86/100; Loss: 0.004283302929252386\n",
      "Step 144 (5801); Episode 86/100; Loss: 0.0015912969829514623\n",
      "Step 145 (5802); Episode 86/100; Loss: 0.0149360541254282\n",
      "Step 146 (5803); Episode 86/100; Loss: 0.0020519907120615244\n",
      "Step 147 (5804); Episode 86/100; Loss: 0.0017470296006649733\n",
      "Step 148 (5805); Episode 86/100; Loss: 0.02948986552655697\n",
      "Step 149 (5806); Episode 86/100; Loss: 0.001292687258683145\n",
      "Step 150 (5807); Episode 86/100; Loss: 0.003923393785953522\n",
      "Step 151 (5808); Episode 86/100; Loss: 0.14665882289409637\n",
      "Step 152 (5809); Episode 86/100; Loss: 0.03278582543134689\n",
      "Step 153 (5810); Episode 86/100; Loss: 0.015117887407541275\n",
      "Step 154 (5811); Episode 86/100; Loss: 0.0935414731502533\n",
      "Step 155 (5812); Episode 86/100; Loss: 0.06577157229185104\n",
      "Step 156 (5813); Episode 86/100; Loss: 0.05162971839308739\n",
      "Step 157 (5814); Episode 86/100; Loss: 0.09694366902112961\n",
      "Step 158 (5815); Episode 86/100; Loss: 0.012544220313429832\n",
      "Step 159 (5816); Episode 86/100; Loss: 0.11845950782299042\n",
      "Step 160 (5817); Episode 86/100; Loss: 0.0008868534932844341\n",
      "Step 161 (5818); Episode 86/100; Loss: 0.0026032812893390656\n",
      "Step 162 (5819); Episode 86/100; Loss: 0.0016320024151355028\n",
      "Step 163 (5820); Episode 86/100; Loss: 0.06422077119350433\n",
      "Step 164 (5821); Episode 86/100; Loss: 0.052499450743198395\n",
      "Step 165 (5822); Episode 86/100; Loss: 0.05372929945588112\n",
      "Step 166 (5823); Episode 86/100; Loss: 0.08439990878105164\n",
      "Step 167 (5824); Episode 86/100; Loss: 0.012924936600029469\n",
      "Step 168 (5825); Episode 86/100; Loss: 0.001337226596660912\n",
      "Step 169 (5826); Episode 86/100; Loss: 0.03916177898645401\n",
      "Step 170 (5827); Episode 86/100; Loss: 0.0033678668551146984\n",
      "Step 171 (5828); Episode 86/100; Loss: 0.0013428014935925603\n",
      "Step 172 (5829); Episode 86/100; Loss: 0.0328032523393631\n",
      "Step 173 (5830); Episode 86/100; Loss: 0.002700762590393424\n",
      "Step 174 (5831); Episode 86/100; Loss: 0.0571032389998436\n",
      "Step 175 (5832); Episode 86/100; Loss: 0.001767920795828104\n",
      "Step 176 (5833); Episode 86/100; Loss: 0.018957212567329407\n",
      "Step 177 (5834); Episode 86/100; Loss: 0.0014928876189514995\n",
      "Step 178 (5835); Episode 86/100; Loss: 0.0017679958837106824\n",
      "Step 179 (5836); Episode 86/100; Loss: 0.03570343554019928\n",
      "Step 180 (5837); Episode 86/100; Loss: 0.04356115683913231\n",
      "Step 181 (5838); Episode 86/100; Loss: 0.005307043436914682\n",
      "Step 182 (5839); Episode 86/100; Loss: 0.0017636881675571203\n",
      "Step 183 (5840); Episode 86/100; Loss: 0.048149291425943375\n",
      "Step 184 (5841); Episode 86/100; Loss: 0.0016397455474361777\n",
      "Step 185 (5842); Episode 86/100; Loss: 0.07733765989542007\n",
      "Step 186 (5843); Episode 86/100; Loss: 0.014403389766812325\n",
      "Step 187 (5844); Episode 86/100; Loss: 0.04646923765540123\n",
      "Step 188 (5845); Episode 86/100; Loss: 0.0012487879721447825\n",
      "Step 189 (5846); Episode 86/100; Loss: 0.036190226674079895\n",
      "Step 190 (5847); Episode 86/100; Loss: 0.0011891655158251524\n",
      "Step 191 (5848); Episode 86/100; Loss: 0.0023516255896538496\n",
      "Step 192 (5849); Episode 86/100; Loss: 0.041835710406303406\n",
      "Step 193 (5850); Episode 86/100; Loss: 0.03559650108218193\n",
      "Step 194 (5851); Episode 86/100; Loss: 0.010285923257470131\n",
      "Step 195 (5852); Episode 86/100; Loss: 0.002895444631576538\n",
      "Step 196 (5853); Episode 86/100; Loss: 0.0010950627038255334\n",
      "Step 197 (5854); Episode 86/100; Loss: 0.086163729429245\n",
      "Step 198 (5855); Episode 86/100; Loss: 0.0018278915667906404\n",
      "Step 199 (5856); Episode 86/100; Loss: 0.00461120018735528\n",
      "Step 0 (5857); Episode 87/100; Loss: 0.13416524231433868\n",
      "Step 1 (5858); Episode 87/100; Loss: 0.084990955889225\n",
      "Step 2 (5859); Episode 87/100; Loss: 0.07132919877767563\n",
      "Step 3 (5860); Episode 87/100; Loss: 0.04554145783185959\n",
      "Step 4 (5861); Episode 87/100; Loss: 0.003345499048009515\n",
      "Step 5 (5862); Episode 87/100; Loss: 0.07780376821756363\n",
      "Step 6 (5863); Episode 87/100; Loss: 0.002453513676300645\n",
      "Step 7 (5864); Episode 87/100; Loss: 0.06411463767290115\n",
      "Step 8 (5865); Episode 87/100; Loss: 0.0013498755870386958\n",
      "Step 9 (5866); Episode 87/100; Loss: 0.006990843918174505\n",
      "Step 10 (5867); Episode 87/100; Loss: 0.012525158934295177\n",
      "Step 11 (5868); Episode 87/100; Loss: 0.11678235977888107\n",
      "Step 12 (5869); Episode 87/100; Loss: 0.03472105413675308\n",
      "Step 13 (5870); Episode 87/100; Loss: 0.0018307017162442207\n",
      "Step 14 (5871); Episode 87/100; Loss: 0.0023010713048279285\n",
      "Step 15 (5872); Episode 87/100; Loss: 0.053131721913814545\n",
      "Step 16 (5873); Episode 87/100; Loss: 0.003902509342879057\n",
      "Step 17 (5874); Episode 87/100; Loss: 0.06856823712587357\n",
      "Step 18 (5875); Episode 87/100; Loss: 0.07937163859605789\n",
      "Step 19 (5876); Episode 87/100; Loss: 0.08174845576286316\n",
      "Step 20 (5877); Episode 87/100; Loss: 0.04832121729850769\n",
      "Step 21 (5878); Episode 87/100; Loss: 0.08066698908805847\n",
      "Step 22 (5879); Episode 87/100; Loss: 0.005347598344087601\n",
      "Step 23 (5880); Episode 87/100; Loss: 0.09322589635848999\n",
      "Step 24 (5881); Episode 87/100; Loss: 0.12438347935676575\n",
      "Step 25 (5882); Episode 87/100; Loss: 0.005548788234591484\n",
      "Step 26 (5883); Episode 87/100; Loss: 0.004585223272442818\n",
      "Step 27 (5884); Episode 87/100; Loss: 0.0021842496935278177\n",
      "Step 28 (5885); Episode 87/100; Loss: 0.0016464045038446784\n",
      "Step 29 (5886); Episode 87/100; Loss: 0.10550424456596375\n",
      "Step 30 (5887); Episode 87/100; Loss: 0.05707802250981331\n",
      "Step 31 (5888); Episode 87/100; Loss: 0.0018424594309180975\n",
      "Step 32 (5889); Episode 87/100; Loss: 0.0023400941863656044\n",
      "Step 33 (5890); Episode 87/100; Loss: 0.046758539974689484\n",
      "Step 34 (5891); Episode 87/100; Loss: 0.015250124968588352\n",
      "Step 35 (5892); Episode 87/100; Loss: 0.05800289288163185\n",
      "Step 36 (5893); Episode 87/100; Loss: 0.0037845466285943985\n",
      "Step 37 (5894); Episode 87/100; Loss: 0.03807586804032326\n",
      "Step 38 (5895); Episode 87/100; Loss: 0.003348385216668248\n",
      "Step 39 (5896); Episode 87/100; Loss: 0.0065727224573493\n",
      "Step 40 (5897); Episode 87/100; Loss: 0.05030466243624687\n",
      "Step 41 (5898); Episode 87/100; Loss: 0.0013526726979762316\n",
      "Step 42 (5899); Episode 87/100; Loss: 0.038057081401348114\n",
      "Step 43 (5900); Episode 87/100; Loss: 0.05595117807388306\n",
      "Step 44 (5901); Episode 87/100; Loss: 0.043379999697208405\n",
      "Step 45 (5902); Episode 87/100; Loss: 0.04564415290951729\n",
      "Step 46 (5903); Episode 87/100; Loss: 0.0017787228571251035\n",
      "Step 47 (5904); Episode 87/100; Loss: 0.039271626621484756\n",
      "Step 48 (5905); Episode 87/100; Loss: 0.006079234182834625\n",
      "Step 49 (5906); Episode 87/100; Loss: 0.0022225813008844852\n",
      "Step 50 (5907); Episode 87/100; Loss: 0.001663870643824339\n",
      "Step 51 (5908); Episode 87/100; Loss: 0.017889779061079025\n",
      "Step 52 (5909); Episode 87/100; Loss: 0.003094564424827695\n",
      "Step 53 (5910); Episode 87/100; Loss: 0.011301854625344276\n",
      "Step 54 (5911); Episode 87/100; Loss: 0.12969368696212769\n",
      "Step 55 (5912); Episode 87/100; Loss: 0.05605844035744667\n",
      "Step 56 (5913); Episode 87/100; Loss: 0.0374910794198513\n",
      "Step 57 (5914); Episode 87/100; Loss: 0.038687754422426224\n",
      "Step 58 (5915); Episode 87/100; Loss: 0.043400850147008896\n",
      "Step 59 (5916); Episode 87/100; Loss: 0.0019224131247028708\n",
      "Step 60 (5917); Episode 87/100; Loss: 0.03781743720173836\n",
      "Step 61 (5918); Episode 87/100; Loss: 0.07201753556728363\n",
      "Step 62 (5919); Episode 87/100; Loss: 0.0514342375099659\n",
      "Step 63 (5920); Episode 87/100; Loss: 0.08155892044305801\n",
      "Step 64 (5921); Episode 87/100; Loss: 0.035580240190029144\n",
      "Step 65 (5922); Episode 87/100; Loss: 0.043236877769231796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 66 (5923); Episode 87/100; Loss: 0.03796980157494545\n",
      "Step 67 (5924); Episode 87/100; Loss: 0.0021726416889578104\n",
      "Step 68 (5925); Episode 87/100; Loss: 0.0014978960389271379\n",
      "Step 69 (5926); Episode 87/100; Loss: 0.002088406356051564\n",
      "Step 70 (5927); Episode 87/100; Loss: 0.013024501502513885\n",
      "Step 71 (5928); Episode 87/100; Loss: 0.001848602551035583\n",
      "Step 72 (5929); Episode 87/100; Loss: 0.0011703033233061433\n",
      "Step 73 (5930); Episode 87/100; Loss: 0.04945452883839607\n",
      "Step 74 (5931); Episode 87/100; Loss: 0.008281192742288113\n",
      "Step 75 (5932); Episode 87/100; Loss: 0.0012005114695057273\n",
      "Step 76 (5933); Episode 87/100; Loss: 0.0025394216645509005\n",
      "Step 77 (5934); Episode 87/100; Loss: 0.005135387647897005\n",
      "Step 78 (5935); Episode 87/100; Loss: 0.050980377942323685\n",
      "Step 79 (5936); Episode 87/100; Loss: 0.07521753013134003\n",
      "Step 80 (5937); Episode 87/100; Loss: 0.006540261674672365\n",
      "Step 81 (5938); Episode 87/100; Loss: 0.0018864128505811095\n",
      "Step 82 (5939); Episode 87/100; Loss: 0.042257219552993774\n",
      "Step 83 (5940); Episode 87/100; Loss: 0.043983928859233856\n",
      "Step 84 (5941); Episode 87/100; Loss: 0.043306220322847366\n",
      "Step 85 (5942); Episode 87/100; Loss: 0.07580162584781647\n",
      "Step 86 (5943); Episode 87/100; Loss: 0.051680631935596466\n",
      "Step 87 (5944); Episode 87/100; Loss: 0.04661640152335167\n",
      "Step 88 (5945); Episode 87/100; Loss: 0.029443128034472466\n",
      "Step 89 (5946); Episode 87/100; Loss: 0.03634399175643921\n",
      "Step 90 (5947); Episode 87/100; Loss: 0.027362829074263573\n",
      "Step 91 (5948); Episode 87/100; Loss: 0.0015520206652581692\n",
      "Step 92 (5949); Episode 87/100; Loss: 0.05632338672876358\n",
      "Step 93 (5950); Episode 87/100; Loss: 0.04110497236251831\n",
      "Step 94 (5951); Episode 87/100; Loss: 0.09640362113714218\n",
      "Step 95 (5952); Episode 87/100; Loss: 0.002921172184869647\n",
      "Step 96 (5953); Episode 87/100; Loss: 0.01628490723669529\n",
      "Step 97 (5954); Episode 87/100; Loss: 0.01807989366352558\n",
      "Step 98 (5955); Episode 87/100; Loss: 0.005879316478967667\n",
      "Step 99 (5956); Episode 87/100; Loss: 0.0316799022257328\n",
      "Step 100 (5957); Episode 87/100; Loss: 0.03380795940756798\n",
      "Step 101 (5958); Episode 87/100; Loss: 0.033193133771419525\n",
      "Step 102 (5959); Episode 87/100; Loss: 0.05650489032268524\n",
      "Step 103 (5960); Episode 87/100; Loss: 0.09059099107980728\n",
      "Step 104 (5961); Episode 87/100; Loss: 0.0018573898123577237\n",
      "Step 105 (5962); Episode 87/100; Loss: 0.04941688850522041\n",
      "Step 106 (5963); Episode 87/100; Loss: 0.05263586342334747\n",
      "Step 107 (5964); Episode 87/100; Loss: 0.003593138884752989\n",
      "Step 108 (5965); Episode 87/100; Loss: 0.031612955033779144\n",
      "Step 109 (5966); Episode 87/100; Loss: 0.13901184499263763\n",
      "Step 110 (5967); Episode 87/100; Loss: 0.01846095733344555\n",
      "Step 111 (5968); Episode 87/100; Loss: 0.09419985115528107\n",
      "Step 112 (5969); Episode 87/100; Loss: 0.0036990416701883078\n",
      "Step 113 (5970); Episode 87/100; Loss: 0.04570881277322769\n",
      "Step 114 (5971); Episode 87/100; Loss: 0.025002898648381233\n",
      "Step 115 (5972); Episode 87/100; Loss: 0.02368193492293358\n",
      "Step 116 (5973); Episode 87/100; Loss: 0.041930947452783585\n",
      "Step 117 (5974); Episode 87/100; Loss: 0.0070943827740848064\n",
      "Step 118 (5975); Episode 87/100; Loss: 0.043290458619594574\n",
      "Step 119 (5976); Episode 87/100; Loss: 0.0036454384680837393\n",
      "Step 120 (5977); Episode 87/100; Loss: 0.004991795402020216\n",
      "Step 121 (5978); Episode 87/100; Loss: 0.1581324338912964\n",
      "Step 122 (5979); Episode 87/100; Loss: 0.05398232117295265\n",
      "Step 123 (5980); Episode 87/100; Loss: 0.0019060756312683225\n",
      "Step 124 (5981); Episode 87/100; Loss: 0.06108073517680168\n",
      "Step 125 (5982); Episode 87/100; Loss: 0.028430812060832977\n",
      "Step 126 (5983); Episode 87/100; Loss: 0.10020224004983902\n",
      "Step 127 (5984); Episode 87/100; Loss: 0.05400471016764641\n",
      "Step 128 (5985); Episode 87/100; Loss: 0.039109501987695694\n",
      "Step 129 (5986); Episode 87/100; Loss: 0.004508497193455696\n",
      "Step 130 (5987); Episode 87/100; Loss: 0.0025039897300302982\n",
      "Step 131 (5988); Episode 87/100; Loss: 0.0008646983187645674\n",
      "Step 132 (5989); Episode 87/100; Loss: 0.0023361039347946644\n",
      "Step 133 (5990); Episode 87/100; Loss: 0.0477382093667984\n",
      "Step 134 (5991); Episode 87/100; Loss: 0.0031860051676630974\n",
      "Step 135 (5992); Episode 87/100; Loss: 0.06952116638422012\n",
      "Step 136 (5993); Episode 87/100; Loss: 0.0027097854763269424\n",
      "Step 137 (5994); Episode 87/100; Loss: 0.08888779580593109\n",
      "Step 138 (5995); Episode 87/100; Loss: 0.08374954760074615\n",
      "Step 139 (5996); Episode 87/100; Loss: 0.17859381437301636\n",
      "Step 140 (5997); Episode 87/100; Loss: 0.002340668113902211\n",
      "Step 141 (5998); Episode 87/100; Loss: 0.03524123504757881\n",
      "Step 142 (5999); Episode 87/100; Loss: 0.06622982770204544\n",
      "Step 143 (6000); Episode 87/100; Loss: 0.08479175716638565\n",
      "Step 144 (6001); Episode 87/100; Loss: 0.00263700052164495\n",
      "Step 145 (6002); Episode 87/100; Loss: 0.0423591174185276\n",
      "Step 146 (6003); Episode 87/100; Loss: 0.001505067222751677\n",
      "Step 147 (6004); Episode 87/100; Loss: 0.08931775391101837\n",
      "Step 148 (6005); Episode 87/100; Loss: 0.047757554799318314\n",
      "Step 149 (6006); Episode 87/100; Loss: 0.02917109616100788\n",
      "Step 150 (6007); Episode 87/100; Loss: 0.08376897126436234\n",
      "Step 151 (6008); Episode 87/100; Loss: 0.04363396763801575\n",
      "Step 152 (6009); Episode 87/100; Loss: 0.00445929542183876\n",
      "Step 153 (6010); Episode 87/100; Loss: 0.05006828531622887\n",
      "Step 154 (6011); Episode 87/100; Loss: 0.08156035095453262\n",
      "Step 155 (6012); Episode 87/100; Loss: 0.0023069949820637703\n",
      "Step 156 (6013); Episode 87/100; Loss: 0.03828877955675125\n",
      "Step 157 (6014); Episode 87/100; Loss: 0.0034489359240978956\n",
      "Step 158 (6015); Episode 87/100; Loss: 0.034554820507764816\n",
      "Step 159 (6016); Episode 87/100; Loss: 0.040274351835250854\n",
      "Step 160 (6017); Episode 87/100; Loss: 0.05184558406472206\n",
      "Step 161 (6018); Episode 87/100; Loss: 0.03189944103360176\n",
      "Step 162 (6019); Episode 87/100; Loss: 0.10137675702571869\n",
      "Step 163 (6020); Episode 87/100; Loss: 0.001444758614525199\n",
      "Step 164 (6021); Episode 87/100; Loss: 0.04235627129673958\n",
      "Step 165 (6022); Episode 87/100; Loss: 0.05717145651578903\n",
      "Step 166 (6023); Episode 87/100; Loss: 0.007311099208891392\n",
      "Step 167 (6024); Episode 87/100; Loss: 0.0015390244079753757\n",
      "Step 168 (6025); Episode 87/100; Loss: 0.042722828686237335\n",
      "Step 169 (6026); Episode 87/100; Loss: 0.06886521726846695\n",
      "Step 170 (6027); Episode 87/100; Loss: 0.005050490144640207\n",
      "Step 171 (6028); Episode 87/100; Loss: 0.0027354874182492495\n",
      "Step 172 (6029); Episode 87/100; Loss: 0.0020635805558413267\n",
      "Step 173 (6030); Episode 87/100; Loss: 0.04632101207971573\n",
      "Step 174 (6031); Episode 87/100; Loss: 0.0521477609872818\n",
      "Step 175 (6032); Episode 87/100; Loss: 0.02570130117237568\n",
      "Step 176 (6033); Episode 87/100; Loss: 0.04515873268246651\n",
      "Step 177 (6034); Episode 87/100; Loss: 0.031010067090392113\n",
      "Step 178 (6035); Episode 87/100; Loss: 0.002463750774040818\n",
      "Step 179 (6036); Episode 87/100; Loss: 0.039125580340623856\n",
      "Step 180 (6037); Episode 87/100; Loss: 0.0016611312748864293\n",
      "Step 181 (6038); Episode 87/100; Loss: 0.077657051384449\n",
      "Step 182 (6039); Episode 87/100; Loss: 0.04432554915547371\n",
      "Step 183 (6040); Episode 87/100; Loss: 0.0029961082618683577\n",
      "Step 184 (6041); Episode 87/100; Loss: 0.1713714301586151\n",
      "Step 185 (6042); Episode 87/100; Loss: 0.05679234862327576\n",
      "Step 186 (6043); Episode 87/100; Loss: 0.026901420205831528\n",
      "Step 187 (6044); Episode 87/100; Loss: 0.02880099043250084\n",
      "Step 188 (6045); Episode 87/100; Loss: 0.07672009617090225\n",
      "Step 189 (6046); Episode 87/100; Loss: 0.006365837529301643\n",
      "Step 190 (6047); Episode 87/100; Loss: 0.040801409631967545\n",
      "Step 191 (6048); Episode 87/100; Loss: 0.042403023689985275\n",
      "Step 192 (6049); Episode 87/100; Loss: 0.04519176483154297\n",
      "Step 193 (6050); Episode 87/100; Loss: 0.04553896561264992\n",
      "Step 194 (6051); Episode 87/100; Loss: 0.043054692447185516\n",
      "Step 195 (6052); Episode 87/100; Loss: 0.03734930604696274\n",
      "Step 196 (6053); Episode 87/100; Loss: 0.005639749579131603\n",
      "Step 197 (6054); Episode 87/100; Loss: 0.06622245162725449\n",
      "Step 198 (6055); Episode 87/100; Loss: 0.003732166951522231\n",
      "Step 199 (6056); Episode 87/100; Loss: 0.0018330542370676994\n",
      "Step 0 (6057); Episode 88/100; Loss: 0.0028283223509788513\n",
      "Step 1 (6058); Episode 88/100; Loss: 0.0019352789968252182\n",
      "Step 2 (6059); Episode 88/100; Loss: 0.0027721424121409655\n",
      "Step 3 (6060); Episode 88/100; Loss: 0.05113483592867851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 (6061); Episode 88/100; Loss: 0.05517948791384697\n",
      "Step 5 (6062); Episode 88/100; Loss: 0.038021210581064224\n",
      "Step 6 (6063); Episode 88/100; Loss: 0.0017435080371797085\n",
      "Step 7 (6064); Episode 88/100; Loss: 0.04945749416947365\n",
      "Step 8 (6065); Episode 88/100; Loss: 0.002312979195266962\n",
      "Step 9 (6066); Episode 88/100; Loss: 0.04579784721136093\n",
      "Step 10 (6067); Episode 88/100; Loss: 0.0021160843316465616\n",
      "Step 11 (6068); Episode 88/100; Loss: 0.0327153205871582\n",
      "Step 12 (6069); Episode 88/100; Loss: 0.03945238143205643\n",
      "Step 13 (6070); Episode 88/100; Loss: 0.07381244748830795\n",
      "Step 14 (6071); Episode 88/100; Loss: 0.0014827268896624446\n",
      "Step 15 (6072); Episode 88/100; Loss: 0.0028802065644413233\n",
      "Step 16 (6073); Episode 88/100; Loss: 0.03927723690867424\n",
      "Step 17 (6074); Episode 88/100; Loss: 0.02430707961320877\n",
      "Step 18 (6075); Episode 88/100; Loss: 0.0010057012550532818\n",
      "Step 19 (6076); Episode 88/100; Loss: 0.04046468064188957\n",
      "Step 20 (6077); Episode 88/100; Loss: 0.08193153142929077\n",
      "Step 21 (6078); Episode 88/100; Loss: 0.0017405791440978646\n",
      "Step 22 (6079); Episode 88/100; Loss: 0.029244691133499146\n",
      "Step 23 (6080); Episode 88/100; Loss: 0.003987105097621679\n",
      "Step 24 (6081); Episode 88/100; Loss: 0.0018471606308594346\n",
      "Step 25 (6082); Episode 88/100; Loss: 0.0026544646825641394\n",
      "Step 26 (6083); Episode 88/100; Loss: 0.001765636494383216\n",
      "Step 27 (6084); Episode 88/100; Loss: 0.055401694029569626\n",
      "Step 28 (6085); Episode 88/100; Loss: 0.00283703557215631\n",
      "Step 29 (6086); Episode 88/100; Loss: 0.001150383148342371\n",
      "Step 30 (6087); Episode 88/100; Loss: 0.050736136734485626\n",
      "Step 31 (6088); Episode 88/100; Loss: 0.08784160017967224\n",
      "Step 32 (6089); Episode 88/100; Loss: 0.0018878091359511018\n",
      "Step 33 (6090); Episode 88/100; Loss: 0.03672497719526291\n",
      "Step 34 (6091); Episode 88/100; Loss: 0.001311116968281567\n",
      "Step 35 (6092); Episode 88/100; Loss: 0.0011654761619865894\n",
      "Step 36 (6093); Episode 88/100; Loss: 0.03763463720679283\n",
      "Step 37 (6094); Episode 88/100; Loss: 0.0036669326946139336\n",
      "Step 38 (6095); Episode 88/100; Loss: 0.053845442831516266\n",
      "Step 39 (6096); Episode 88/100; Loss: 0.048275236040353775\n",
      "Step 40 (6097); Episode 88/100; Loss: 0.0017107621533796191\n",
      "Step 41 (6098); Episode 88/100; Loss: 0.0011813234305009246\n",
      "Step 42 (6099); Episode 88/100; Loss: 0.002583628287538886\n",
      "Step 43 (6100); Episode 88/100; Loss: 0.03875204920768738\n",
      "Step 44 (6101); Episode 88/100; Loss: 0.07808323949575424\n",
      "Step 45 (6102); Episode 88/100; Loss: 0.0015135909197852015\n",
      "Step 46 (6103); Episode 88/100; Loss: 0.02677774615585804\n",
      "Step 47 (6104); Episode 88/100; Loss: 0.0879477933049202\n",
      "Step 48 (6105); Episode 88/100; Loss: 0.032846346497535706\n",
      "Step 49 (6106); Episode 88/100; Loss: 0.0009309961460530758\n",
      "Step 50 (6107); Episode 88/100; Loss: 0.0034103270154446363\n",
      "Step 51 (6108); Episode 88/100; Loss: 0.08048232644796371\n",
      "Step 52 (6109); Episode 88/100; Loss: 0.06721983850002289\n",
      "Step 53 (6110); Episode 88/100; Loss: 0.04984889551997185\n",
      "Step 54 (6111); Episode 88/100; Loss: 0.04886467382311821\n",
      "Step 55 (6112); Episode 88/100; Loss: 0.029465947300195694\n",
      "Step 56 (6113); Episode 88/100; Loss: 0.05096238851547241\n",
      "Step 57 (6114); Episode 88/100; Loss: 0.0647546797990799\n",
      "Step 58 (6115); Episode 88/100; Loss: 0.05724557861685753\n",
      "Step 59 (6116); Episode 88/100; Loss: 0.05421312153339386\n",
      "Step 60 (6117); Episode 88/100; Loss: 0.0016400085296481848\n",
      "Step 61 (6118); Episode 88/100; Loss: 0.0015606482047587633\n",
      "Step 62 (6119); Episode 88/100; Loss: 0.05340941622853279\n",
      "Step 63 (6120); Episode 88/100; Loss: 0.037816278636455536\n",
      "Step 64 (6121); Episode 88/100; Loss: 0.06213740259408951\n",
      "Step 65 (6122); Episode 88/100; Loss: 0.04406116530299187\n",
      "Step 66 (6123); Episode 88/100; Loss: 0.00405268045142293\n",
      "Step 67 (6124); Episode 88/100; Loss: 0.0025193444453179836\n",
      "Step 68 (6125); Episode 88/100; Loss: 0.04677438735961914\n",
      "Step 69 (6126); Episode 88/100; Loss: 0.0030175368301570415\n",
      "Step 70 (6127); Episode 88/100; Loss: 0.003449168987572193\n",
      "Step 71 (6128); Episode 88/100; Loss: 0.05939880758523941\n",
      "Step 72 (6129); Episode 88/100; Loss: 0.001113672973588109\n",
      "Step 73 (6130); Episode 88/100; Loss: 0.0019041268387809396\n",
      "Step 74 (6131); Episode 88/100; Loss: 0.055989958345890045\n",
      "Step 75 (6132); Episode 88/100; Loss: 0.10792361944913864\n",
      "Step 76 (6133); Episode 88/100; Loss: 0.06921722739934921\n",
      "Step 77 (6134); Episode 88/100; Loss: 0.0017641079612076283\n",
      "Step 78 (6135); Episode 88/100; Loss: 0.005063450429588556\n",
      "Step 79 (6136); Episode 88/100; Loss: 0.002062422689050436\n",
      "Step 80 (6137); Episode 88/100; Loss: 0.042976535856723785\n",
      "Step 81 (6138); Episode 88/100; Loss: 0.03503238782286644\n",
      "Step 82 (6139); Episode 88/100; Loss: 0.08031393587589264\n",
      "Step 83 (6140); Episode 88/100; Loss: 0.060640811920166016\n",
      "Step 84 (6141); Episode 88/100; Loss: 0.0025687790475785732\n",
      "Step 85 (6142); Episode 88/100; Loss: 0.0026176359970122576\n",
      "Step 86 (6143); Episode 88/100; Loss: 0.04292256012558937\n",
      "Step 87 (6144); Episode 88/100; Loss: 0.03165401145815849\n",
      "Step 88 (6145); Episode 88/100; Loss: 0.04200807586312294\n",
      "Step 89 (6146); Episode 88/100; Loss: 0.0024129743687808514\n",
      "Step 90 (6147); Episode 88/100; Loss: 0.029110634699463844\n",
      "Step 91 (6148); Episode 88/100; Loss: 0.055594027042388916\n",
      "Step 92 (6149); Episode 88/100; Loss: 0.049443162977695465\n",
      "Step 93 (6150); Episode 88/100; Loss: 0.004870328586548567\n",
      "Step 94 (6151); Episode 88/100; Loss: 0.00510481046512723\n",
      "Step 95 (6152); Episode 88/100; Loss: 0.023769346997141838\n",
      "Step 96 (6153); Episode 88/100; Loss: 0.0018424862064421177\n",
      "Step 97 (6154); Episode 88/100; Loss: 0.04888101667165756\n",
      "Step 98 (6155); Episode 88/100; Loss: 0.0504283681511879\n",
      "Step 99 (6156); Episode 88/100; Loss: 0.1034042239189148\n",
      "Step 100 (6157); Episode 88/100; Loss: 0.0011781173525378108\n",
      "Step 101 (6158); Episode 88/100; Loss: 0.003411016194149852\n",
      "Step 102 (6159); Episode 88/100; Loss: 0.0011425400152802467\n",
      "Step 103 (6160); Episode 88/100; Loss: 0.091001495718956\n",
      "Step 104 (6161); Episode 88/100; Loss: 0.02398049831390381\n",
      "Step 105 (6162); Episode 88/100; Loss: 0.07152710109949112\n",
      "Step 106 (6163); Episode 88/100; Loss: 0.030754482373595238\n",
      "Step 107 (6164); Episode 88/100; Loss: 0.06291879713535309\n",
      "Step 108 (6165); Episode 88/100; Loss: 0.042386189103126526\n",
      "Step 109 (6166); Episode 88/100; Loss: 0.009442007169127464\n",
      "Step 110 (6167); Episode 88/100; Loss: 0.045235056430101395\n",
      "Step 111 (6168); Episode 88/100; Loss: 0.0021272688172757626\n",
      "Step 112 (6169); Episode 88/100; Loss: 0.002440135693177581\n",
      "Step 113 (6170); Episode 88/100; Loss: 0.07365357130765915\n",
      "Step 114 (6171); Episode 88/100; Loss: 0.021058673039078712\n",
      "Step 115 (6172); Episode 88/100; Loss: 0.00240204855799675\n",
      "Step 116 (6173); Episode 88/100; Loss: 0.041600532829761505\n",
      "Step 117 (6174); Episode 88/100; Loss: 0.15280960500240326\n",
      "Step 118 (6175); Episode 88/100; Loss: 0.026453960686922073\n",
      "Step 119 (6176); Episode 88/100; Loss: 0.08957227319478989\n",
      "Step 120 (6177); Episode 88/100; Loss: 0.11045558750629425\n",
      "Step 121 (6178); Episode 88/100; Loss: 0.0028443103656172752\n",
      "Step 122 (6179); Episode 88/100; Loss: 0.001762488391250372\n",
      "Step 123 (6180); Episode 88/100; Loss: 0.08234593272209167\n",
      "Step 124 (6181); Episode 88/100; Loss: 0.002716250717639923\n",
      "Step 125 (6182); Episode 88/100; Loss: 0.00175098713953048\n",
      "Step 126 (6183); Episode 88/100; Loss: 0.0056577809154987335\n",
      "Step 127 (6184); Episode 88/100; Loss: 0.05397086590528488\n",
      "Step 128 (6185); Episode 88/100; Loss: 0.0037732324562966824\n",
      "Step 129 (6186); Episode 88/100; Loss: 0.00820805225521326\n",
      "Step 130 (6187); Episode 88/100; Loss: 0.004620227962732315\n",
      "Step 131 (6188); Episode 88/100; Loss: 0.005511547438800335\n",
      "Step 132 (6189); Episode 88/100; Loss: 0.001466783112846315\n",
      "Step 133 (6190); Episode 88/100; Loss: 0.0008035622886382043\n",
      "Step 134 (6191); Episode 88/100; Loss: 0.048757754266262054\n",
      "Step 135 (6192); Episode 88/100; Loss: 0.04566357657313347\n",
      "Step 136 (6193); Episode 88/100; Loss: 0.04330287128686905\n",
      "Step 137 (6194); Episode 88/100; Loss: 0.005318777170032263\n",
      "Step 0 (6195); Episode 89/100; Loss: 0.0014298529131338\n",
      "Step 1 (6196); Episode 89/100; Loss: 0.0045047178864479065\n",
      "Step 2 (6197); Episode 89/100; Loss: 0.001753176678903401\n",
      "Step 3 (6198); Episode 89/100; Loss: 0.05457466468214989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 (6199); Episode 89/100; Loss: 0.016588004305958748\n",
      "Step 5 (6200); Episode 89/100; Loss: 0.1114896610379219\n",
      "Step 6 (6201); Episode 89/100; Loss: 0.07091046124696732\n",
      "Step 7 (6202); Episode 89/100; Loss: 0.004040982108563185\n",
      "Step 8 (6203); Episode 89/100; Loss: 0.0012644882081076503\n",
      "Step 9 (6204); Episode 89/100; Loss: 0.0006641168147325516\n",
      "Step 10 (6205); Episode 89/100; Loss: 0.005945061799138784\n",
      "Step 11 (6206); Episode 89/100; Loss: 0.000580818101298064\n",
      "Step 12 (6207); Episode 89/100; Loss: 0.08699428290128708\n",
      "Step 13 (6208); Episode 89/100; Loss: 0.002265632152557373\n",
      "Step 14 (6209); Episode 89/100; Loss: 0.0011197201674804091\n",
      "Step 15 (6210); Episode 89/100; Loss: 0.0024794552009552717\n",
      "Step 16 (6211); Episode 89/100; Loss: 0.08547164499759674\n",
      "Step 17 (6212); Episode 89/100; Loss: 0.004122880753129721\n",
      "Step 18 (6213); Episode 89/100; Loss: 0.0021848620381206274\n",
      "Step 19 (6214); Episode 89/100; Loss: 0.035783156752586365\n",
      "Step 20 (6215); Episode 89/100; Loss: 0.06540916115045547\n",
      "Step 21 (6216); Episode 89/100; Loss: 0.015613364055752754\n",
      "Step 22 (6217); Episode 89/100; Loss: 0.039160486310720444\n",
      "Step 23 (6218); Episode 89/100; Loss: 0.0014244727790355682\n",
      "Step 24 (6219); Episode 89/100; Loss: 0.0021315196063369513\n",
      "Step 25 (6220); Episode 89/100; Loss: 0.08894569426774979\n",
      "Step 26 (6221); Episode 89/100; Loss: 0.044671546667814255\n",
      "Step 27 (6222); Episode 89/100; Loss: 0.001059809816069901\n",
      "Step 28 (6223); Episode 89/100; Loss: 0.0058671822771430016\n",
      "Step 29 (6224); Episode 89/100; Loss: 0.0017501492984592915\n",
      "Step 30 (6225); Episode 89/100; Loss: 0.036804020404815674\n",
      "Step 31 (6226); Episode 89/100; Loss: 0.0033475239761173725\n",
      "Step 32 (6227); Episode 89/100; Loss: 0.01765330135822296\n",
      "Step 33 (6228); Episode 89/100; Loss: 0.0016792361857369542\n",
      "Step 34 (6229); Episode 89/100; Loss: 0.010184329003095627\n",
      "Step 35 (6230); Episode 89/100; Loss: 0.0022828676737844944\n",
      "Step 36 (6231); Episode 89/100; Loss: 0.0024889386259019375\n",
      "Step 37 (6232); Episode 89/100; Loss: 0.0026842050720006227\n",
      "Step 38 (6233); Episode 89/100; Loss: 0.0038159345276653767\n",
      "Step 39 (6234); Episode 89/100; Loss: 0.07397715002298355\n",
      "Step 40 (6235); Episode 89/100; Loss: 0.04276204854249954\n",
      "Step 41 (6236); Episode 89/100; Loss: 0.0899803563952446\n",
      "Step 42 (6237); Episode 89/100; Loss: 0.0019980953074991703\n",
      "Step 43 (6238); Episode 89/100; Loss: 0.00442465441301465\n",
      "Step 44 (6239); Episode 89/100; Loss: 0.004250667057931423\n",
      "Step 45 (6240); Episode 89/100; Loss: 0.0017476442735642195\n",
      "Step 46 (6241); Episode 89/100; Loss: 0.0007664779550395906\n",
      "Step 47 (6242); Episode 89/100; Loss: 0.0007687767501920462\n",
      "Step 48 (6243); Episode 89/100; Loss: 0.0008875402272678912\n",
      "Step 49 (6244); Episode 89/100; Loss: 0.05136683210730553\n",
      "Step 50 (6245); Episode 89/100; Loss: 0.018055105581879616\n",
      "Step 51 (6246); Episode 89/100; Loss: 0.04274396598339081\n",
      "Step 52 (6247); Episode 89/100; Loss: 0.04846610501408577\n",
      "Step 53 (6248); Episode 89/100; Loss: 0.005423770286142826\n",
      "Step 54 (6249); Episode 89/100; Loss: 0.037852004170417786\n",
      "Step 55 (6250); Episode 89/100; Loss: 0.04767724126577377\n",
      "Step 56 (6251); Episode 89/100; Loss: 0.036810729652643204\n",
      "Step 57 (6252); Episode 89/100; Loss: 0.009528415277600288\n",
      "Step 58 (6253); Episode 89/100; Loss: 0.0023705901112407446\n",
      "Step 59 (6254); Episode 89/100; Loss: 0.05435579642653465\n",
      "Step 60 (6255); Episode 89/100; Loss: 0.0014318363973870873\n",
      "Step 61 (6256); Episode 89/100; Loss: 0.0008184225298464298\n",
      "Step 62 (6257); Episode 89/100; Loss: 0.007429077755659819\n",
      "Step 63 (6258); Episode 89/100; Loss: 0.006224341690540314\n",
      "Step 64 (6259); Episode 89/100; Loss: 0.048668187111616135\n",
      "Step 65 (6260); Episode 89/100; Loss: 0.0019264544826000929\n",
      "Step 66 (6261); Episode 89/100; Loss: 0.014327405951917171\n",
      "Step 67 (6262); Episode 89/100; Loss: 0.002376826014369726\n",
      "Step 68 (6263); Episode 89/100; Loss: 0.04746771603822708\n",
      "Step 69 (6264); Episode 89/100; Loss: 0.07239733636379242\n",
      "Step 70 (6265); Episode 89/100; Loss: 0.0012269011931493878\n",
      "Step 71 (6266); Episode 89/100; Loss: 0.041508451104164124\n",
      "Step 72 (6267); Episode 89/100; Loss: 0.03880557790398598\n",
      "Step 73 (6268); Episode 89/100; Loss: 0.04518314078450203\n",
      "Step 74 (6269); Episode 89/100; Loss: 0.0009672986343502998\n",
      "Step 75 (6270); Episode 89/100; Loss: 0.041927751153707504\n",
      "Step 76 (6271); Episode 89/100; Loss: 0.0012790535110980272\n",
      "Step 77 (6272); Episode 89/100; Loss: 0.0347132608294487\n",
      "Step 78 (6273); Episode 89/100; Loss: 0.010040447115898132\n",
      "Step 79 (6274); Episode 89/100; Loss: 0.04043462872505188\n",
      "Step 80 (6275); Episode 89/100; Loss: 0.001241430640220642\n",
      "Step 81 (6276); Episode 89/100; Loss: 0.04192965477705002\n",
      "Step 82 (6277); Episode 89/100; Loss: 0.03184882178902626\n",
      "Step 83 (6278); Episode 89/100; Loss: 0.04941292107105255\n",
      "Step 84 (6279); Episode 89/100; Loss: 0.01783904992043972\n",
      "Step 85 (6280); Episode 89/100; Loss: 0.10695431381464005\n",
      "Step 86 (6281); Episode 89/100; Loss: 0.03367840498685837\n",
      "Step 87 (6282); Episode 89/100; Loss: 0.001730719581246376\n",
      "Step 88 (6283); Episode 89/100; Loss: 0.0019618142396211624\n",
      "Step 89 (6284); Episode 89/100; Loss: 0.10000096261501312\n",
      "Step 90 (6285); Episode 89/100; Loss: 0.001132028759457171\n",
      "Step 91 (6286); Episode 89/100; Loss: 0.04822982847690582\n",
      "Step 92 (6287); Episode 89/100; Loss: 0.00915099773555994\n",
      "Step 93 (6288); Episode 89/100; Loss: 0.012805622071027756\n",
      "Step 94 (6289); Episode 89/100; Loss: 0.04445423558354378\n",
      "Step 95 (6290); Episode 89/100; Loss: 0.0719791129231453\n",
      "Step 96 (6291); Episode 89/100; Loss: 0.0014272312400862575\n",
      "Step 97 (6292); Episode 89/100; Loss: 0.07552682608366013\n",
      "Step 98 (6293); Episode 89/100; Loss: 0.02585289254784584\n",
      "Step 99 (6294); Episode 89/100; Loss: 0.04834058880805969\n",
      "Step 100 (6295); Episode 89/100; Loss: 0.0012802635319530964\n",
      "Step 101 (6296); Episode 89/100; Loss: 0.05643954128026962\n",
      "Step 102 (6297); Episode 89/100; Loss: 0.1039644330739975\n",
      "Step 103 (6298); Episode 89/100; Loss: 0.007738503627479076\n",
      "Step 104 (6299); Episode 89/100; Loss: 0.08331787586212158\n",
      "Step 105 (6300); Episode 89/100; Loss: 0.02945331111550331\n",
      "Step 106 (6301); Episode 89/100; Loss: 0.0032534331548959017\n",
      "Step 107 (6302); Episode 89/100; Loss: 0.001108171883970499\n",
      "Step 108 (6303); Episode 89/100; Loss: 0.07394153624773026\n",
      "Step 109 (6304); Episode 89/100; Loss: 0.0010560023365542293\n",
      "Step 110 (6305); Episode 89/100; Loss: 0.09577789902687073\n",
      "Step 111 (6306); Episode 89/100; Loss: 0.000913390249479562\n",
      "Step 112 (6307); Episode 89/100; Loss: 0.0042215026915073395\n",
      "Step 113 (6308); Episode 89/100; Loss: 0.005691985599696636\n",
      "Step 114 (6309); Episode 89/100; Loss: 0.0063436031341552734\n",
      "Step 115 (6310); Episode 89/100; Loss: 0.04585988447070122\n",
      "Step 116 (6311); Episode 89/100; Loss: 0.04075206071138382\n",
      "Step 117 (6312); Episode 89/100; Loss: 0.04428897425532341\n",
      "Step 118 (6313); Episode 89/100; Loss: 0.003836950985714793\n",
      "Step 119 (6314); Episode 89/100; Loss: 0.1422906219959259\n",
      "Step 120 (6315); Episode 89/100; Loss: 0.05620837211608887\n",
      "Step 121 (6316); Episode 89/100; Loss: 0.029077373445034027\n",
      "Step 122 (6317); Episode 89/100; Loss: 0.05507462099194527\n",
      "Step 123 (6318); Episode 89/100; Loss: 0.006535437889397144\n",
      "Step 124 (6319); Episode 89/100; Loss: 0.08092828840017319\n",
      "Step 125 (6320); Episode 89/100; Loss: 0.002110806992277503\n",
      "Step 126 (6321); Episode 89/100; Loss: 0.09144077450037003\n",
      "Step 127 (6322); Episode 89/100; Loss: 0.001125107053667307\n",
      "Step 128 (6323); Episode 89/100; Loss: 0.04494737833738327\n",
      "Step 129 (6324); Episode 89/100; Loss: 0.027248267084360123\n",
      "Step 130 (6325); Episode 89/100; Loss: 0.040208060294389725\n",
      "Step 131 (6326); Episode 89/100; Loss: 0.04349583014845848\n",
      "Step 132 (6327); Episode 89/100; Loss: 0.002968080807477236\n",
      "Step 133 (6328); Episode 89/100; Loss: 0.0027608477976173162\n",
      "Step 134 (6329); Episode 89/100; Loss: 0.04594610631465912\n",
      "Step 135 (6330); Episode 89/100; Loss: 0.0549120157957077\n",
      "Step 136 (6331); Episode 89/100; Loss: 0.03428762033581734\n",
      "Step 137 (6332); Episode 89/100; Loss: 0.09473875164985657\n",
      "Step 138 (6333); Episode 89/100; Loss: 0.15093401074409485\n",
      "Step 139 (6334); Episode 89/100; Loss: 0.0022080191411077976\n",
      "Step 140 (6335); Episode 89/100; Loss: 0.0017405776306986809\n",
      "Step 141 (6336); Episode 89/100; Loss: 0.044100601226091385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 142 (6337); Episode 89/100; Loss: 0.003079348709434271\n",
      "Step 143 (6338); Episode 89/100; Loss: 0.001268708729185164\n",
      "Step 144 (6339); Episode 89/100; Loss: 0.055655188858509064\n",
      "Step 145 (6340); Episode 89/100; Loss: 0.0019927294924855232\n",
      "Step 0 (6341); Episode 90/100; Loss: 0.03489325940608978\n",
      "Step 1 (6342); Episode 90/100; Loss: 0.026877321302890778\n",
      "Step 2 (6343); Episode 90/100; Loss: 0.0018719875952228904\n",
      "Step 3 (6344); Episode 90/100; Loss: 0.004630273208022118\n",
      "Step 4 (6345); Episode 90/100; Loss: 0.07872916013002396\n",
      "Step 5 (6346); Episode 90/100; Loss: 0.09261802583932877\n",
      "Step 6 (6347); Episode 90/100; Loss: 0.04062363877892494\n",
      "Step 7 (6348); Episode 90/100; Loss: 0.0026597760152071714\n",
      "Step 8 (6349); Episode 90/100; Loss: 0.0025325275491923094\n",
      "Step 9 (6350); Episode 90/100; Loss: 0.0646682009100914\n",
      "Step 10 (6351); Episode 90/100; Loss: 0.03766489773988724\n",
      "Step 11 (6352); Episode 90/100; Loss: 0.02833704650402069\n",
      "Step 12 (6353); Episode 90/100; Loss: 0.09566644579172134\n",
      "Step 13 (6354); Episode 90/100; Loss: 0.0053162951953709126\n",
      "Step 14 (6355); Episode 90/100; Loss: 0.04851897433400154\n",
      "Step 15 (6356); Episode 90/100; Loss: 0.056221164762973785\n",
      "Step 16 (6357); Episode 90/100; Loss: 0.047786761075258255\n",
      "Step 17 (6358); Episode 90/100; Loss: 0.0558343231678009\n",
      "Step 18 (6359); Episode 90/100; Loss: 0.0007484945235773921\n",
      "Step 19 (6360); Episode 90/100; Loss: 0.03822620213031769\n",
      "Step 20 (6361); Episode 90/100; Loss: 0.003010447835549712\n",
      "Step 21 (6362); Episode 90/100; Loss: 0.0028111448045819998\n",
      "Step 22 (6363); Episode 90/100; Loss: 0.012051059864461422\n",
      "Step 23 (6364); Episode 90/100; Loss: 0.0346122644841671\n",
      "Step 24 (6365); Episode 90/100; Loss: 0.002539140172302723\n",
      "Step 25 (6366); Episode 90/100; Loss: 0.09990089386701584\n",
      "Step 26 (6367); Episode 90/100; Loss: 0.039307743310928345\n",
      "Step 27 (6368); Episode 90/100; Loss: 0.041439738124608994\n",
      "Step 28 (6369); Episode 90/100; Loss: 0.08149376511573792\n",
      "Step 29 (6370); Episode 90/100; Loss: 0.0019214125350117683\n",
      "Step 30 (6371); Episode 90/100; Loss: 0.051927994936704636\n",
      "Step 31 (6372); Episode 90/100; Loss: 0.0012043244205415249\n",
      "Step 32 (6373); Episode 90/100; Loss: 0.0006846830947324634\n",
      "Step 33 (6374); Episode 90/100; Loss: 0.033961936831474304\n",
      "Step 34 (6375); Episode 90/100; Loss: 0.009236225858330727\n",
      "Step 35 (6376); Episode 90/100; Loss: 0.0011787950061261654\n",
      "Step 36 (6377); Episode 90/100; Loss: 0.05286196246743202\n",
      "Step 37 (6378); Episode 90/100; Loss: 0.04227618873119354\n",
      "Step 38 (6379); Episode 90/100; Loss: 0.004841686692088842\n",
      "Step 39 (6380); Episode 90/100; Loss: 0.0035436733160167933\n",
      "Step 40 (6381); Episode 90/100; Loss: 0.0027755952905863523\n",
      "Step 41 (6382); Episode 90/100; Loss: 0.0015851501375436783\n",
      "Step 42 (6383); Episode 90/100; Loss: 0.09009154886007309\n",
      "Step 43 (6384); Episode 90/100; Loss: 0.08601691573858261\n",
      "Step 44 (6385); Episode 90/100; Loss: 0.0026335183065384626\n",
      "Step 45 (6386); Episode 90/100; Loss: 0.0026693036779761314\n",
      "Step 46 (6387); Episode 90/100; Loss: 0.032036978751420975\n",
      "Step 47 (6388); Episode 90/100; Loss: 0.006350439973175526\n",
      "Step 48 (6389); Episode 90/100; Loss: 0.11114835739135742\n",
      "Step 49 (6390); Episode 90/100; Loss: 0.0562778078019619\n",
      "Step 50 (6391); Episode 90/100; Loss: 0.048909321427345276\n",
      "Step 51 (6392); Episode 90/100; Loss: 0.028054021298885345\n",
      "Step 52 (6393); Episode 90/100; Loss: 0.0024894592352211475\n",
      "Step 53 (6394); Episode 90/100; Loss: 0.0016064323717728257\n",
      "Step 54 (6395); Episode 90/100; Loss: 0.0058281561359763145\n",
      "Step 55 (6396); Episode 90/100; Loss: 0.042149920016527176\n",
      "Step 56 (6397); Episode 90/100; Loss: 0.037399206310510635\n",
      "Step 57 (6398); Episode 90/100; Loss: 0.030883969739079475\n",
      "Step 58 (6399); Episode 90/100; Loss: 0.040987659245729446\n",
      "Step 59 (6400); Episode 90/100; Loss: 0.0013643677812069654\n",
      "Step 60 (6401); Episode 90/100; Loss: 0.1005144938826561\n",
      "Step 61 (6402); Episode 90/100; Loss: 0.002390077104791999\n",
      "Step 62 (6403); Episode 90/100; Loss: 0.001601515687070787\n",
      "Step 63 (6404); Episode 90/100; Loss: 0.005007355473935604\n",
      "Step 64 (6405); Episode 90/100; Loss: 0.001348724588751793\n",
      "Step 65 (6406); Episode 90/100; Loss: 0.04146137461066246\n",
      "Step 66 (6407); Episode 90/100; Loss: 0.0017576072132214904\n",
      "Step 67 (6408); Episode 90/100; Loss: 0.10460178554058075\n",
      "Step 68 (6409); Episode 90/100; Loss: 0.05252528190612793\n",
      "Step 69 (6410); Episode 90/100; Loss: 0.03425096347928047\n",
      "Step 70 (6411); Episode 90/100; Loss: 0.04384302347898483\n",
      "Step 71 (6412); Episode 90/100; Loss: 0.0834457203745842\n",
      "Step 72 (6413); Episode 90/100; Loss: 0.030482344329357147\n",
      "Step 73 (6414); Episode 90/100; Loss: 0.03692810237407684\n",
      "Step 74 (6415); Episode 90/100; Loss: 0.052118442952632904\n",
      "Step 75 (6416); Episode 90/100; Loss: 0.0007950460421852767\n",
      "Step 76 (6417); Episode 90/100; Loss: 0.03936086222529411\n",
      "Step 77 (6418); Episode 90/100; Loss: 0.03928828611969948\n",
      "Step 78 (6419); Episode 90/100; Loss: 0.0022816841956228018\n",
      "Step 79 (6420); Episode 90/100; Loss: 0.06233949586749077\n",
      "Step 80 (6421); Episode 90/100; Loss: 0.00217299722135067\n",
      "Step 81 (6422); Episode 90/100; Loss: 0.0358235165476799\n",
      "Step 82 (6423); Episode 90/100; Loss: 0.07881215959787369\n",
      "Step 83 (6424); Episode 90/100; Loss: 0.04252998158335686\n",
      "Step 84 (6425); Episode 90/100; Loss: 0.0011469522723928094\n",
      "Step 85 (6426); Episode 90/100; Loss: 0.006010784301906824\n",
      "Step 86 (6427); Episode 90/100; Loss: 0.0016157851787284017\n",
      "Step 87 (6428); Episode 90/100; Loss: 0.0019467046950012445\n",
      "Step 88 (6429); Episode 90/100; Loss: 0.0036317238118499517\n",
      "Step 89 (6430); Episode 90/100; Loss: 0.03269614651799202\n",
      "Step 90 (6431); Episode 90/100; Loss: 0.03928650543093681\n",
      "Step 91 (6432); Episode 90/100; Loss: 0.008096657693386078\n",
      "Step 92 (6433); Episode 90/100; Loss: 0.04464077949523926\n",
      "Step 93 (6434); Episode 90/100; Loss: 0.037565622478723526\n",
      "Step 94 (6435); Episode 90/100; Loss: 0.038235731422901154\n",
      "Step 95 (6436); Episode 90/100; Loss: 0.001150234485976398\n",
      "Step 96 (6437); Episode 90/100; Loss: 0.0044825454242527485\n",
      "Step 97 (6438); Episode 90/100; Loss: 0.13792787492275238\n",
      "Step 98 (6439); Episode 90/100; Loss: 0.04501917213201523\n",
      "Step 99 (6440); Episode 90/100; Loss: 0.015867959707975388\n",
      "Step 100 (6441); Episode 90/100; Loss: 0.0023458683863282204\n",
      "Step 101 (6442); Episode 90/100; Loss: 0.041430581361055374\n",
      "Step 102 (6443); Episode 90/100; Loss: 0.0011255290592089295\n",
      "Step 103 (6444); Episode 90/100; Loss: 0.002999153919517994\n",
      "Step 104 (6445); Episode 90/100; Loss: 0.031556177884340286\n",
      "Step 105 (6446); Episode 90/100; Loss: 0.0015925586922094226\n",
      "Step 106 (6447); Episode 90/100; Loss: 0.07783125340938568\n",
      "Step 107 (6448); Episode 90/100; Loss: 0.0016663947608321905\n",
      "Step 108 (6449); Episode 90/100; Loss: 0.00182861159555614\n",
      "Step 109 (6450); Episode 90/100; Loss: 0.0014055429492145777\n",
      "Step 110 (6451); Episode 90/100; Loss: 0.03512992337346077\n",
      "Step 111 (6452); Episode 90/100; Loss: 0.0011445857817307115\n",
      "Step 112 (6453); Episode 90/100; Loss: 0.001512122224085033\n",
      "Step 113 (6454); Episode 90/100; Loss: 0.0370422825217247\n",
      "Step 114 (6455); Episode 90/100; Loss: 0.0009869958739727736\n",
      "Step 115 (6456); Episode 90/100; Loss: 0.04785262420773506\n",
      "Step 116 (6457); Episode 90/100; Loss: 0.001495686243288219\n",
      "Step 117 (6458); Episode 90/100; Loss: 0.04869841784238815\n",
      "Step 118 (6459); Episode 90/100; Loss: 0.003108497243374586\n",
      "Step 119 (6460); Episode 90/100; Loss: 0.0013322369195520878\n",
      "Step 120 (6461); Episode 90/100; Loss: 0.008736446499824524\n",
      "Step 121 (6462); Episode 90/100; Loss: 0.05361734330654144\n",
      "Step 122 (6463); Episode 90/100; Loss: 0.029843896627426147\n",
      "Step 123 (6464); Episode 90/100; Loss: 0.052178170531988144\n",
      "Step 124 (6465); Episode 90/100; Loss: 0.04408182576298714\n",
      "Step 125 (6466); Episode 90/100; Loss: 0.039783917367458344\n",
      "Step 126 (6467); Episode 90/100; Loss: 0.001539859571494162\n",
      "Step 127 (6468); Episode 90/100; Loss: 0.03657015413045883\n",
      "Step 128 (6469); Episode 90/100; Loss: 0.031038407236337662\n",
      "Step 129 (6470); Episode 90/100; Loss: 0.0018708865391090512\n",
      "Step 130 (6471); Episode 90/100; Loss: 0.04199863225221634\n",
      "Step 131 (6472); Episode 90/100; Loss: 0.014271294698119164\n",
      "Step 132 (6473); Episode 90/100; Loss: 0.02601214125752449\n",
      "Step 133 (6474); Episode 90/100; Loss: 0.060052551329135895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 134 (6475); Episode 90/100; Loss: 0.1317511945962906\n",
      "Step 135 (6476); Episode 90/100; Loss: 0.026114461943507195\n",
      "Step 136 (6477); Episode 90/100; Loss: 0.03897830843925476\n",
      "Step 137 (6478); Episode 90/100; Loss: 0.0012890329817309976\n",
      "Step 138 (6479); Episode 90/100; Loss: 0.030478497967123985\n",
      "Step 139 (6480); Episode 90/100; Loss: 0.0022157023195177317\n",
      "Step 140 (6481); Episode 90/100; Loss: 0.03209635615348816\n",
      "Step 141 (6482); Episode 90/100; Loss: 0.004398919176310301\n",
      "Step 142 (6483); Episode 90/100; Loss: 0.07518276572227478\n",
      "Step 143 (6484); Episode 90/100; Loss: 0.011855916120111942\n",
      "Step 144 (6485); Episode 90/100; Loss: 0.1658814400434494\n",
      "Step 145 (6486); Episode 90/100; Loss: 0.03748919814825058\n",
      "Step 146 (6487); Episode 90/100; Loss: 0.07203143835067749\n",
      "Step 147 (6488); Episode 90/100; Loss: 0.0013515364844352007\n",
      "Step 148 (6489); Episode 90/100; Loss: 0.027249444276094437\n",
      "Step 149 (6490); Episode 90/100; Loss: 0.0021058355923742056\n",
      "Step 150 (6491); Episode 90/100; Loss: 0.040409766137599945\n",
      "Step 151 (6492); Episode 90/100; Loss: 0.0019353212555870414\n",
      "Step 152 (6493); Episode 90/100; Loss: 0.06179847940802574\n",
      "Step 153 (6494); Episode 90/100; Loss: 0.0345749631524086\n",
      "Step 154 (6495); Episode 90/100; Loss: 0.03788087144494057\n",
      "Step 155 (6496); Episode 90/100; Loss: 0.002242887392640114\n",
      "Step 156 (6497); Episode 90/100; Loss: 0.002137724542990327\n",
      "Step 157 (6498); Episode 90/100; Loss: 0.03608625754714012\n",
      "Step 158 (6499); Episode 90/100; Loss: 0.0035503050312399864\n",
      "Step 159 (6500); Episode 90/100; Loss: 0.0012819059193134308\n",
      "Step 160 (6501); Episode 90/100; Loss: 0.0025977904442697763\n",
      "Step 161 (6502); Episode 90/100; Loss: 0.002581546548753977\n",
      "Step 162 (6503); Episode 90/100; Loss: 0.002582723740488291\n",
      "Step 163 (6504); Episode 90/100; Loss: 0.006141630467027426\n",
      "Step 164 (6505); Episode 90/100; Loss: 0.041802238672971725\n",
      "Step 0 (6506); Episode 91/100; Loss: 0.017582230269908905\n",
      "Step 1 (6507); Episode 91/100; Loss: 0.047822266817092896\n",
      "Step 2 (6508); Episode 91/100; Loss: 0.05157077684998512\n",
      "Step 3 (6509); Episode 91/100; Loss: 0.05330926924943924\n",
      "Step 4 (6510); Episode 91/100; Loss: 0.004628247115761042\n",
      "Step 5 (6511); Episode 91/100; Loss: 0.002184238750487566\n",
      "Step 6 (6512); Episode 91/100; Loss: 0.04664840176701546\n",
      "Step 7 (6513); Episode 91/100; Loss: 0.02909129299223423\n",
      "Step 8 (6514); Episode 91/100; Loss: 0.001651270198635757\n",
      "Step 9 (6515); Episode 91/100; Loss: 0.005402508191764355\n",
      "Step 10 (6516); Episode 91/100; Loss: 0.0013377865543588996\n",
      "Step 11 (6517); Episode 91/100; Loss: 0.07765578478574753\n",
      "Step 12 (6518); Episode 91/100; Loss: 0.0009630207787267864\n",
      "Step 13 (6519); Episode 91/100; Loss: 0.01214768085628748\n",
      "Step 14 (6520); Episode 91/100; Loss: 0.029090238735079765\n",
      "Step 15 (6521); Episode 91/100; Loss: 0.021212946623563766\n",
      "Step 16 (6522); Episode 91/100; Loss: 0.0014678328298032284\n",
      "Step 17 (6523); Episode 91/100; Loss: 0.002645634114742279\n",
      "Step 18 (6524); Episode 91/100; Loss: 0.02975989505648613\n",
      "Step 19 (6525); Episode 91/100; Loss: 0.04061051830649376\n",
      "Step 20 (6526); Episode 91/100; Loss: 0.03865152597427368\n",
      "Step 21 (6527); Episode 91/100; Loss: 0.049931444227695465\n",
      "Step 22 (6528); Episode 91/100; Loss: 0.06662634015083313\n",
      "Step 23 (6529); Episode 91/100; Loss: 0.03277430683374405\n",
      "Step 24 (6530); Episode 91/100; Loss: 0.03551546856760979\n",
      "Step 25 (6531); Episode 91/100; Loss: 0.044646333903074265\n",
      "Step 26 (6532); Episode 91/100; Loss: 0.042633093893527985\n",
      "Step 27 (6533); Episode 91/100; Loss: 0.0011782654328271747\n",
      "Step 28 (6534); Episode 91/100; Loss: 0.0008486913284286857\n",
      "Step 29 (6535); Episode 91/100; Loss: 0.024739831686019897\n",
      "Step 30 (6536); Episode 91/100; Loss: 0.025326386094093323\n",
      "Step 31 (6537); Episode 91/100; Loss: 0.06273403018712997\n",
      "Step 32 (6538); Episode 91/100; Loss: 0.04647517576813698\n",
      "Step 33 (6539); Episode 91/100; Loss: 0.002410545479506254\n",
      "Step 34 (6540); Episode 91/100; Loss: 0.026975981891155243\n",
      "Step 35 (6541); Episode 91/100; Loss: 0.1435534656047821\n",
      "Step 36 (6542); Episode 91/100; Loss: 0.0015623653307557106\n",
      "Step 37 (6543); Episode 91/100; Loss: 0.0012853305088356137\n",
      "Step 38 (6544); Episode 91/100; Loss: 0.002355769509449601\n",
      "Step 39 (6545); Episode 91/100; Loss: 0.002852227771654725\n",
      "Step 40 (6546); Episode 91/100; Loss: 0.0036812257021665573\n",
      "Step 41 (6547); Episode 91/100; Loss: 0.008493059314787388\n",
      "Step 42 (6548); Episode 91/100; Loss: 0.04566119611263275\n",
      "Step 43 (6549); Episode 91/100; Loss: 0.08242974430322647\n",
      "Step 44 (6550); Episode 91/100; Loss: 0.0894036740064621\n",
      "Step 45 (6551); Episode 91/100; Loss: 0.09892232716083527\n",
      "Step 46 (6552); Episode 91/100; Loss: 0.12461984157562256\n",
      "Step 47 (6553); Episode 91/100; Loss: 0.12397459149360657\n",
      "Step 48 (6554); Episode 91/100; Loss: 0.08268232643604279\n",
      "Step 49 (6555); Episode 91/100; Loss: 0.0032179029658436775\n",
      "Step 50 (6556); Episode 91/100; Loss: 0.0020931593608111143\n",
      "Step 51 (6557); Episode 91/100; Loss: 0.043932121247053146\n",
      "Step 52 (6558); Episode 91/100; Loss: 0.03659893944859505\n",
      "Step 53 (6559); Episode 91/100; Loss: 0.015534283593297005\n",
      "Step 54 (6560); Episode 91/100; Loss: 0.09905549138784409\n",
      "Step 55 (6561); Episode 91/100; Loss: 0.059474363923072815\n",
      "Step 56 (6562); Episode 91/100; Loss: 0.011290069669485092\n",
      "Step 57 (6563); Episode 91/100; Loss: 0.0856257751584053\n",
      "Step 58 (6564); Episode 91/100; Loss: 0.004529389552772045\n",
      "Step 59 (6565); Episode 91/100; Loss: 0.0038516668137162924\n",
      "Step 60 (6566); Episode 91/100; Loss: 0.07075544446706772\n",
      "Step 61 (6567); Episode 91/100; Loss: 0.0036912362556904554\n",
      "Step 62 (6568); Episode 91/100; Loss: 0.05486680194735527\n",
      "Step 63 (6569); Episode 91/100; Loss: 0.0024697380140423775\n",
      "Step 64 (6570); Episode 91/100; Loss: 0.04691064730286598\n",
      "Step 65 (6571); Episode 91/100; Loss: 0.06650862097740173\n",
      "Step 66 (6572); Episode 91/100; Loss: 0.05089321359992027\n",
      "Step 67 (6573); Episode 91/100; Loss: 0.01719352975487709\n",
      "Step 68 (6574); Episode 91/100; Loss: 0.0527547262609005\n",
      "Step 69 (6575); Episode 91/100; Loss: 0.0042182160541415215\n",
      "Step 70 (6576); Episode 91/100; Loss: 0.04232853651046753\n",
      "Step 71 (6577); Episode 91/100; Loss: 0.0017144640441983938\n",
      "Step 72 (6578); Episode 91/100; Loss: 0.02911834418773651\n",
      "Step 73 (6579); Episode 91/100; Loss: 0.039670053869485855\n",
      "Step 74 (6580); Episode 91/100; Loss: 0.03451254963874817\n",
      "Step 75 (6581); Episode 91/100; Loss: 0.06191534548997879\n",
      "Step 76 (6582); Episode 91/100; Loss: 0.0032050679437816143\n",
      "Step 77 (6583); Episode 91/100; Loss: 0.04913964495062828\n",
      "Step 78 (6584); Episode 91/100; Loss: 0.027332421392202377\n",
      "Step 79 (6585); Episode 91/100; Loss: 0.052323706448078156\n",
      "Step 80 (6586); Episode 91/100; Loss: 0.03455899655818939\n",
      "Step 81 (6587); Episode 91/100; Loss: 0.023567115887999535\n",
      "Step 82 (6588); Episode 91/100; Loss: 0.0025575170293450356\n",
      "Step 83 (6589); Episode 91/100; Loss: 0.02385079301893711\n",
      "Step 84 (6590); Episode 91/100; Loss: 0.0016163312830030918\n",
      "Step 85 (6591); Episode 91/100; Loss: 0.0016652054619044065\n",
      "Step 86 (6592); Episode 91/100; Loss: 0.039417900145053864\n",
      "Step 87 (6593); Episode 91/100; Loss: 0.03413555771112442\n",
      "Step 88 (6594); Episode 91/100; Loss: 0.03731447458267212\n",
      "Step 89 (6595); Episode 91/100; Loss: 0.04362276941537857\n",
      "Step 90 (6596); Episode 91/100; Loss: 0.005324963480234146\n",
      "Step 91 (6597); Episode 91/100; Loss: 0.0019108757842332125\n",
      "Step 92 (6598); Episode 91/100; Loss: 0.006080957595258951\n",
      "Step 93 (6599); Episode 91/100; Loss: 0.0013890719274058938\n",
      "Step 94 (6600); Episode 91/100; Loss: 0.001849714433774352\n",
      "Step 95 (6601); Episode 91/100; Loss: 0.04700656607747078\n",
      "Step 96 (6602); Episode 91/100; Loss: 0.0009498806321062148\n",
      "Step 97 (6603); Episode 91/100; Loss: 0.026899784803390503\n",
      "Step 98 (6604); Episode 91/100; Loss: 0.0017720034811645746\n",
      "Step 99 (6605); Episode 91/100; Loss: 0.04957085847854614\n",
      "Step 100 (6606); Episode 91/100; Loss: 0.07159916311502457\n",
      "Step 101 (6607); Episode 91/100; Loss: 0.0012905594194307923\n",
      "Step 102 (6608); Episode 91/100; Loss: 0.0007750728982500732\n",
      "Step 103 (6609); Episode 91/100; Loss: 0.03040837123990059\n",
      "Step 104 (6610); Episode 91/100; Loss: 0.059860944747924805\n",
      "Step 105 (6611); Episode 91/100; Loss: 0.03976316377520561\n",
      "Step 106 (6612); Episode 91/100; Loss: 0.0008190563530661166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 107 (6613); Episode 91/100; Loss: 0.000624137173872441\n",
      "Step 108 (6614); Episode 91/100; Loss: 0.040683403611183167\n",
      "Step 109 (6615); Episode 91/100; Loss: 0.0009183048387058079\n",
      "Step 110 (6616); Episode 91/100; Loss: 0.0024870308116078377\n",
      "Step 111 (6617); Episode 91/100; Loss: 0.044253285974264145\n",
      "Step 112 (6618); Episode 91/100; Loss: 0.08570276200771332\n",
      "Step 113 (6619); Episode 91/100; Loss: 0.03837542608380318\n",
      "Step 114 (6620); Episode 91/100; Loss: 0.004672408103942871\n",
      "Step 115 (6621); Episode 91/100; Loss: 0.04623936116695404\n",
      "Step 116 (6622); Episode 91/100; Loss: 0.08071353286504745\n",
      "Step 117 (6623); Episode 91/100; Loss: 0.002448957646265626\n",
      "Step 118 (6624); Episode 91/100; Loss: 0.04986165836453438\n",
      "Step 119 (6625); Episode 91/100; Loss: 0.03151889890432358\n",
      "Step 120 (6626); Episode 91/100; Loss: 0.03305817022919655\n",
      "Step 121 (6627); Episode 91/100; Loss: 0.0021117243450134993\n",
      "Step 122 (6628); Episode 91/100; Loss: 0.04757514223456383\n",
      "Step 123 (6629); Episode 91/100; Loss: 0.041535984724760056\n",
      "Step 124 (6630); Episode 91/100; Loss: 0.027286285534501076\n",
      "Step 125 (6631); Episode 91/100; Loss: 0.0012903623282909393\n",
      "Step 126 (6632); Episode 91/100; Loss: 0.054665081202983856\n",
      "Step 127 (6633); Episode 91/100; Loss: 0.0021662942599505186\n",
      "Step 128 (6634); Episode 91/100; Loss: 0.03377531096339226\n",
      "Step 129 (6635); Episode 91/100; Loss: 0.10270649939775467\n",
      "Step 130 (6636); Episode 91/100; Loss: 0.007757587358355522\n",
      "Step 131 (6637); Episode 91/100; Loss: 0.0723138153553009\n",
      "Step 132 (6638); Episode 91/100; Loss: 0.033498410135507584\n",
      "Step 133 (6639); Episode 91/100; Loss: 0.026525970548391342\n",
      "Step 134 (6640); Episode 91/100; Loss: 0.0782887190580368\n",
      "Step 135 (6641); Episode 91/100; Loss: 0.0013959022471681237\n",
      "Step 0 (6642); Episode 92/100; Loss: 0.03843475878238678\n",
      "Step 1 (6643); Episode 92/100; Loss: 0.04275275394320488\n",
      "Step 2 (6644); Episode 92/100; Loss: 0.001323685166426003\n",
      "Step 3 (6645); Episode 92/100; Loss: 0.040994446724653244\n",
      "Step 4 (6646); Episode 92/100; Loss: 0.04692180082201958\n",
      "Step 5 (6647); Episode 92/100; Loss: 0.03959118202328682\n",
      "Step 6 (6648); Episode 92/100; Loss: 0.001564846490509808\n",
      "Step 7 (6649); Episode 92/100; Loss: 0.04924887418746948\n",
      "Step 8 (6650); Episode 92/100; Loss: 0.03912600129842758\n",
      "Step 9 (6651); Episode 92/100; Loss: 0.08070796728134155\n",
      "Step 10 (6652); Episode 92/100; Loss: 0.08009753376245499\n",
      "Step 11 (6653); Episode 92/100; Loss: 0.0016517315525561571\n",
      "Step 12 (6654); Episode 92/100; Loss: 0.044277124106884\n",
      "Step 13 (6655); Episode 92/100; Loss: 0.011776021681725979\n",
      "Step 14 (6656); Episode 92/100; Loss: 0.027165604755282402\n",
      "Step 15 (6657); Episode 92/100; Loss: 0.004733417183160782\n",
      "Step 16 (6658); Episode 92/100; Loss: 0.02629503421485424\n",
      "Step 17 (6659); Episode 92/100; Loss: 0.010331882163882256\n",
      "Step 18 (6660); Episode 92/100; Loss: 0.004014917183667421\n",
      "Step 19 (6661); Episode 92/100; Loss: 0.0018009594641625881\n",
      "Step 20 (6662); Episode 92/100; Loss: 0.02835535816848278\n",
      "Step 21 (6663); Episode 92/100; Loss: 0.11588332056999207\n",
      "Step 22 (6664); Episode 92/100; Loss: 0.0023165384773164988\n",
      "Step 23 (6665); Episode 92/100; Loss: 0.039977286010980606\n",
      "Step 24 (6666); Episode 92/100; Loss: 0.03820721432566643\n",
      "Step 25 (6667); Episode 92/100; Loss: 0.00127854582387954\n",
      "Step 26 (6668); Episode 92/100; Loss: 0.0038802616763859987\n",
      "Step 27 (6669); Episode 92/100; Loss: 0.0662245899438858\n",
      "Step 28 (6670); Episode 92/100; Loss: 0.03187550976872444\n",
      "Step 29 (6671); Episode 92/100; Loss: 0.0007438816828653216\n",
      "Step 30 (6672); Episode 92/100; Loss: 0.11612790822982788\n",
      "Step 31 (6673); Episode 92/100; Loss: 0.0024741655215620995\n",
      "Step 32 (6674); Episode 92/100; Loss: 0.024186117574572563\n",
      "Step 33 (6675); Episode 92/100; Loss: 0.0035374576691538095\n",
      "Step 34 (6676); Episode 92/100; Loss: 0.08906149119138718\n",
      "Step 35 (6677); Episode 92/100; Loss: 0.0017649627989158034\n",
      "Step 36 (6678); Episode 92/100; Loss: 0.03267088904976845\n",
      "Step 37 (6679); Episode 92/100; Loss: 0.003768183058127761\n",
      "Step 38 (6680); Episode 92/100; Loss: 0.0022814334370195866\n",
      "Step 39 (6681); Episode 92/100; Loss: 0.04960903897881508\n",
      "Step 40 (6682); Episode 92/100; Loss: 0.05743163451552391\n",
      "Step 41 (6683); Episode 92/100; Loss: 0.055324848741292953\n",
      "Step 42 (6684); Episode 92/100; Loss: 0.0500757172703743\n",
      "Step 43 (6685); Episode 92/100; Loss: 0.0057790460996329784\n",
      "Step 44 (6686); Episode 92/100; Loss: 0.0024232370778918266\n",
      "Step 45 (6687); Episode 92/100; Loss: 0.0017589811468496919\n",
      "Step 46 (6688); Episode 92/100; Loss: 0.002258008113130927\n",
      "Step 47 (6689); Episode 92/100; Loss: 0.005303289741277695\n",
      "Step 48 (6690); Episode 92/100; Loss: 0.01258536335080862\n",
      "Step 49 (6691); Episode 92/100; Loss: 0.018857892602682114\n",
      "Step 50 (6692); Episode 92/100; Loss: 0.03637204319238663\n",
      "Step 51 (6693); Episode 92/100; Loss: 0.04250681772828102\n",
      "Step 52 (6694); Episode 92/100; Loss: 0.023392487317323685\n",
      "Step 53 (6695); Episode 92/100; Loss: 0.034185733646154404\n",
      "Step 54 (6696); Episode 92/100; Loss: 0.05190243571996689\n",
      "Step 55 (6697); Episode 92/100; Loss: 0.05217862129211426\n",
      "Step 56 (6698); Episode 92/100; Loss: 0.001307120081037283\n",
      "Step 57 (6699); Episode 92/100; Loss: 0.0011670353123918176\n",
      "Step 58 (6700); Episode 92/100; Loss: 0.09866365790367126\n",
      "Step 59 (6701); Episode 92/100; Loss: 0.09208481013774872\n",
      "Step 60 (6702); Episode 92/100; Loss: 0.031132182106375694\n",
      "Step 61 (6703); Episode 92/100; Loss: 0.043822433799505234\n",
      "Step 62 (6704); Episode 92/100; Loss: 0.0027021304704248905\n",
      "Step 63 (6705); Episode 92/100; Loss: 0.04295235127210617\n",
      "Step 64 (6706); Episode 92/100; Loss: 0.05151328444480896\n",
      "Step 65 (6707); Episode 92/100; Loss: 0.017648670822381973\n",
      "Step 66 (6708); Episode 92/100; Loss: 0.04760609194636345\n",
      "Step 67 (6709); Episode 92/100; Loss: 0.001224373816512525\n",
      "Step 68 (6710); Episode 92/100; Loss: 0.0013879957841709256\n",
      "Step 69 (6711); Episode 92/100; Loss: 0.05474765598773956\n",
      "Step 70 (6712); Episode 92/100; Loss: 0.0021518818102777004\n",
      "Step 71 (6713); Episode 92/100; Loss: 0.08432628959417343\n",
      "Step 72 (6714); Episode 92/100; Loss: 0.04387383908033371\n",
      "Step 73 (6715); Episode 92/100; Loss: 0.015464498661458492\n",
      "Step 74 (6716); Episode 92/100; Loss: 0.047692980617284775\n",
      "Step 75 (6717); Episode 92/100; Loss: 0.0029490457382053137\n",
      "Step 76 (6718); Episode 92/100; Loss: 0.002052426105365157\n",
      "Step 77 (6719); Episode 92/100; Loss: 0.030654165893793106\n",
      "Step 78 (6720); Episode 92/100; Loss: 0.02562243677675724\n",
      "Step 79 (6721); Episode 92/100; Loss: 0.002101191785186529\n",
      "Step 80 (6722); Episode 92/100; Loss: 0.026485053822398186\n",
      "Step 81 (6723); Episode 92/100; Loss: 0.03173799067735672\n",
      "Step 82 (6724); Episode 92/100; Loss: 0.03151087462902069\n",
      "Step 83 (6725); Episode 92/100; Loss: 0.0015597013989463449\n",
      "Step 84 (6726); Episode 92/100; Loss: 0.0010293415980413556\n",
      "Step 85 (6727); Episode 92/100; Loss: 0.06962139159440994\n",
      "Step 86 (6728); Episode 92/100; Loss: 0.006305559538304806\n",
      "Step 87 (6729); Episode 92/100; Loss: 0.02628137171268463\n",
      "Step 88 (6730); Episode 92/100; Loss: 0.0014375359751284122\n",
      "Step 89 (6731); Episode 92/100; Loss: 0.0017704502679407597\n",
      "Step 90 (6732); Episode 92/100; Loss: 0.0205288827419281\n",
      "Step 91 (6733); Episode 92/100; Loss: 0.001115151564590633\n",
      "Step 92 (6734); Episode 92/100; Loss: 0.0018044284079223871\n",
      "Step 93 (6735); Episode 92/100; Loss: 0.03708796203136444\n",
      "Step 94 (6736); Episode 92/100; Loss: 0.07760628312826157\n",
      "Step 95 (6737); Episode 92/100; Loss: 0.00784227717667818\n",
      "Step 96 (6738); Episode 92/100; Loss: 0.010398059152066708\n",
      "Step 97 (6739); Episode 92/100; Loss: 0.0045006973668932915\n",
      "Step 98 (6740); Episode 92/100; Loss: 0.0020104495342820883\n",
      "Step 99 (6741); Episode 92/100; Loss: 0.0020752365235239267\n",
      "Step 100 (6742); Episode 92/100; Loss: 0.004385639913380146\n",
      "Step 101 (6743); Episode 92/100; Loss: 0.028552008792757988\n",
      "Step 102 (6744); Episode 92/100; Loss: 0.0013301910366863012\n",
      "Step 103 (6745); Episode 92/100; Loss: 0.0069952537305653095\n",
      "Step 104 (6746); Episode 92/100; Loss: 0.03529418632388115\n",
      "Step 0 (6747); Episode 93/100; Loss: 0.0007751511293463409\n",
      "Step 1 (6748); Episode 93/100; Loss: 0.04959074780344963\n",
      "Step 2 (6749); Episode 93/100; Loss: 0.16157159209251404\n",
      "Step 3 (6750); Episode 93/100; Loss: 0.0019642252009361982\n",
      "Step 4 (6751); Episode 93/100; Loss: 0.04463368281722069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 (6752); Episode 93/100; Loss: 0.0014119859552010894\n",
      "Step 6 (6753); Episode 93/100; Loss: 0.044078368693590164\n",
      "Step 7 (6754); Episode 93/100; Loss: 0.001185628934763372\n",
      "Step 8 (6755); Episode 93/100; Loss: 0.06873753666877747\n",
      "Step 9 (6756); Episode 93/100; Loss: 0.00235305679962039\n",
      "Step 10 (6757); Episode 93/100; Loss: 0.004058231599628925\n",
      "Step 11 (6758); Episode 93/100; Loss: 0.09561121463775635\n",
      "Step 12 (6759); Episode 93/100; Loss: 0.0016233577625826001\n",
      "Step 13 (6760); Episode 93/100; Loss: 0.039673060178756714\n",
      "Step 14 (6761); Episode 93/100; Loss: 0.09242895245552063\n",
      "Step 15 (6762); Episode 93/100; Loss: 0.0658067837357521\n",
      "Step 16 (6763); Episode 93/100; Loss: 0.012955383397638798\n",
      "Step 17 (6764); Episode 93/100; Loss: 0.0029845014214515686\n",
      "Step 18 (6765); Episode 93/100; Loss: 0.0012220409698784351\n",
      "Step 19 (6766); Episode 93/100; Loss: 0.026115814223885536\n",
      "Step 20 (6767); Episode 93/100; Loss: 0.002234608866274357\n",
      "Step 21 (6768); Episode 93/100; Loss: 0.10899181663990021\n",
      "Step 22 (6769); Episode 93/100; Loss: 0.011607239954173565\n",
      "Step 23 (6770); Episode 93/100; Loss: 0.07902476191520691\n",
      "Step 24 (6771); Episode 93/100; Loss: 0.043817583471536636\n",
      "Step 25 (6772); Episode 93/100; Loss: 0.02753193862736225\n",
      "Step 26 (6773); Episode 93/100; Loss: 0.02148582600057125\n",
      "Step 27 (6774); Episode 93/100; Loss: 0.027587860822677612\n",
      "Step 28 (6775); Episode 93/100; Loss: 0.0018195046577602625\n",
      "Step 29 (6776); Episode 93/100; Loss: 0.06787711381912231\n",
      "Step 30 (6777); Episode 93/100; Loss: 0.0011380629148334265\n",
      "Step 31 (6778); Episode 93/100; Loss: 0.0007605787832289934\n",
      "Step 32 (6779); Episode 93/100; Loss: 0.024237077683210373\n",
      "Step 33 (6780); Episode 93/100; Loss: 0.005697747226804495\n",
      "Step 34 (6781); Episode 93/100; Loss: 0.044079169631004333\n",
      "Step 35 (6782); Episode 93/100; Loss: 0.0028492561541497707\n",
      "Step 36 (6783); Episode 93/100; Loss: 0.005058941897004843\n",
      "Step 37 (6784); Episode 93/100; Loss: 0.04234280809760094\n",
      "Step 38 (6785); Episode 93/100; Loss: 0.04175994545221329\n",
      "Step 39 (6786); Episode 93/100; Loss: 0.005850157234817743\n",
      "Step 40 (6787); Episode 93/100; Loss: 0.036861781030893326\n",
      "Step 41 (6788); Episode 93/100; Loss: 0.04320279508829117\n",
      "Step 42 (6789); Episode 93/100; Loss: 0.05276830121874809\n",
      "Step 43 (6790); Episode 93/100; Loss: 0.0015893144300207496\n",
      "Step 44 (6791); Episode 93/100; Loss: 0.03347645699977875\n",
      "Step 45 (6792); Episode 93/100; Loss: 0.04197664558887482\n",
      "Step 46 (6793); Episode 93/100; Loss: 0.09741950035095215\n",
      "Step 47 (6794); Episode 93/100; Loss: 0.00235028937458992\n",
      "Step 48 (6795); Episode 93/100; Loss: 0.030478257685899734\n",
      "Step 49 (6796); Episode 93/100; Loss: 0.0010082328226417303\n",
      "Step 50 (6797); Episode 93/100; Loss: 0.06780717521905899\n",
      "Step 51 (6798); Episode 93/100; Loss: 0.0014740360202267766\n",
      "Step 52 (6799); Episode 93/100; Loss: 0.029279375448822975\n",
      "Step 53 (6800); Episode 93/100; Loss: 0.0019696662202477455\n",
      "Step 54 (6801); Episode 93/100; Loss: 0.03549141064286232\n",
      "Step 55 (6802); Episode 93/100; Loss: 0.0021593074779957533\n",
      "Step 56 (6803); Episode 93/100; Loss: 0.04953869432210922\n",
      "Step 57 (6804); Episode 93/100; Loss: 0.0033445393200963736\n",
      "Step 58 (6805); Episode 93/100; Loss: 0.010351475328207016\n",
      "Step 59 (6806); Episode 93/100; Loss: 0.0013234438374638557\n",
      "Step 60 (6807); Episode 93/100; Loss: 0.0008750795386731625\n",
      "Step 61 (6808); Episode 93/100; Loss: 0.001314816763624549\n",
      "Step 62 (6809); Episode 93/100; Loss: 0.0528901107609272\n",
      "Step 63 (6810); Episode 93/100; Loss: 0.0020001668017357588\n",
      "Step 64 (6811); Episode 93/100; Loss: 0.0061214338056743145\n",
      "Step 65 (6812); Episode 93/100; Loss: 0.0017503912094980478\n",
      "Step 66 (6813); Episode 93/100; Loss: 0.001556214177981019\n",
      "Step 67 (6814); Episode 93/100; Loss: 0.04152051731944084\n",
      "Step 68 (6815); Episode 93/100; Loss: 0.10067090392112732\n",
      "Step 69 (6816); Episode 93/100; Loss: 0.023673605173826218\n",
      "Step 70 (6817); Episode 93/100; Loss: 0.05156005918979645\n",
      "Step 71 (6818); Episode 93/100; Loss: 0.0039062341675162315\n",
      "Step 72 (6819); Episode 93/100; Loss: 0.002822680864483118\n",
      "Step 73 (6820); Episode 93/100; Loss: 0.001415726845152676\n",
      "Step 74 (6821); Episode 93/100; Loss: 0.044299401342868805\n",
      "Step 75 (6822); Episode 93/100; Loss: 0.09598428010940552\n",
      "Step 76 (6823); Episode 93/100; Loss: 0.0012146540684625506\n",
      "Step 77 (6824); Episode 93/100; Loss: 0.040952153503894806\n",
      "Step 78 (6825); Episode 93/100; Loss: 0.019493939355015755\n",
      "Step 79 (6826); Episode 93/100; Loss: 0.03301432728767395\n",
      "Step 80 (6827); Episode 93/100; Loss: 0.0012521817116066813\n",
      "Step 81 (6828); Episode 93/100; Loss: 0.07449609041213989\n",
      "Step 82 (6829); Episode 93/100; Loss: 0.002913266886025667\n",
      "Step 83 (6830); Episode 93/100; Loss: 0.002138404408469796\n",
      "Step 84 (6831); Episode 93/100; Loss: 0.051923155784606934\n",
      "Step 85 (6832); Episode 93/100; Loss: 0.000847132527269423\n",
      "Step 86 (6833); Episode 93/100; Loss: 0.0017880680970847607\n",
      "Step 87 (6834); Episode 93/100; Loss: 0.01054836343973875\n",
      "Step 88 (6835); Episode 93/100; Loss: 0.002143832854926586\n",
      "Step 89 (6836); Episode 93/100; Loss: 0.08902857452630997\n",
      "Step 90 (6837); Episode 93/100; Loss: 0.0023441766388714314\n",
      "Step 91 (6838); Episode 93/100; Loss: 0.04538106545805931\n",
      "Step 92 (6839); Episode 93/100; Loss: 0.0015716779744252563\n",
      "Step 93 (6840); Episode 93/100; Loss: 0.0028282843995839357\n",
      "Step 94 (6841); Episode 93/100; Loss: 0.06764671206474304\n",
      "Step 95 (6842); Episode 93/100; Loss: 0.03958222270011902\n",
      "Step 96 (6843); Episode 93/100; Loss: 0.0020238931756466627\n",
      "Step 97 (6844); Episode 93/100; Loss: 0.0008717522141523659\n",
      "Step 98 (6845); Episode 93/100; Loss: 0.03830721974372864\n",
      "Step 99 (6846); Episode 93/100; Loss: 0.00660882331430912\n",
      "Step 100 (6847); Episode 93/100; Loss: 0.03225859999656677\n",
      "Step 101 (6848); Episode 93/100; Loss: 0.18438352644443512\n",
      "Step 102 (6849); Episode 93/100; Loss: 0.05695313960313797\n",
      "Step 103 (6850); Episode 93/100; Loss: 0.054536402225494385\n",
      "Step 104 (6851); Episode 93/100; Loss: 0.07982127368450165\n",
      "Step 105 (6852); Episode 93/100; Loss: 0.04245370998978615\n",
      "Step 106 (6853); Episode 93/100; Loss: 0.006750037427991629\n",
      "Step 107 (6854); Episode 93/100; Loss: 0.03668072447180748\n",
      "Step 108 (6855); Episode 93/100; Loss: 0.0014816172188147902\n",
      "Step 109 (6856); Episode 93/100; Loss: 0.04980509728193283\n",
      "Step 110 (6857); Episode 93/100; Loss: 0.0020898471120744944\n",
      "Step 111 (6858); Episode 93/100; Loss: 0.008775701746344566\n",
      "Step 112 (6859); Episode 93/100; Loss: 0.0022753982339054346\n",
      "Step 113 (6860); Episode 93/100; Loss: 0.004474800080060959\n",
      "Step 114 (6861); Episode 93/100; Loss: 0.0032782787457108498\n",
      "Step 115 (6862); Episode 93/100; Loss: 0.08177201449871063\n",
      "Step 116 (6863); Episode 93/100; Loss: 0.09241841733455658\n",
      "Step 117 (6864); Episode 93/100; Loss: 0.0015135665889829397\n",
      "Step 118 (6865); Episode 93/100; Loss: 0.002175909234210849\n",
      "Step 119 (6866); Episode 93/100; Loss: 0.09190402179956436\n",
      "Step 120 (6867); Episode 93/100; Loss: 0.031285375356674194\n",
      "Step 121 (6868); Episode 93/100; Loss: 0.003059057518839836\n",
      "Step 122 (6869); Episode 93/100; Loss: 0.006306959316134453\n",
      "Step 123 (6870); Episode 93/100; Loss: 0.0007532160962000489\n",
      "Step 124 (6871); Episode 93/100; Loss: 0.0012250508880242705\n",
      "Step 125 (6872); Episode 93/100; Loss: 0.0009247076231986284\n",
      "Step 126 (6873); Episode 93/100; Loss: 0.018216287717223167\n",
      "Step 127 (6874); Episode 93/100; Loss: 0.0011781987268477678\n",
      "Step 128 (6875); Episode 93/100; Loss: 0.07418271899223328\n",
      "Step 129 (6876); Episode 93/100; Loss: 0.0022978789638727903\n",
      "Step 130 (6877); Episode 93/100; Loss: 0.03913417458534241\n",
      "Step 131 (6878); Episode 93/100; Loss: 0.0013602328253909945\n",
      "Step 132 (6879); Episode 93/100; Loss: 0.03020848147571087\n",
      "Step 133 (6880); Episode 93/100; Loss: 0.04800840839743614\n",
      "Step 134 (6881); Episode 93/100; Loss: 0.0033131586387753487\n",
      "Step 135 (6882); Episode 93/100; Loss: 0.06436464190483093\n",
      "Step 136 (6883); Episode 93/100; Loss: 0.0013916445896029472\n",
      "Step 137 (6884); Episode 93/100; Loss: 0.09143997728824615\n",
      "Step 138 (6885); Episode 93/100; Loss: 0.039542920887470245\n",
      "Step 139 (6886); Episode 93/100; Loss: 0.002004660200327635\n",
      "Step 140 (6887); Episode 93/100; Loss: 0.035030122846364975\n",
      "Step 141 (6888); Episode 93/100; Loss: 0.002486876677721739\n",
      "Step 142 (6889); Episode 93/100; Loss: 0.062245000153779984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 143 (6890); Episode 93/100; Loss: 0.050554171204566956\n",
      "Step 144 (6891); Episode 93/100; Loss: 0.08150866627693176\n",
      "Step 145 (6892); Episode 93/100; Loss: 0.0496210940182209\n",
      "Step 146 (6893); Episode 93/100; Loss: 0.04817138984799385\n",
      "Step 147 (6894); Episode 93/100; Loss: 0.04225097596645355\n",
      "Step 148 (6895); Episode 93/100; Loss: 0.08667877316474915\n",
      "Step 149 (6896); Episode 93/100; Loss: 0.0008591070072725415\n",
      "Step 150 (6897); Episode 93/100; Loss: 0.07524135708808899\n",
      "Step 151 (6898); Episode 93/100; Loss: 0.011971266008913517\n",
      "Step 152 (6899); Episode 93/100; Loss: 0.004735042341053486\n",
      "Step 153 (6900); Episode 93/100; Loss: 0.001435825601220131\n",
      "Step 154 (6901); Episode 93/100; Loss: 0.003028193488717079\n",
      "Step 155 (6902); Episode 93/100; Loss: 0.04124749079346657\n",
      "Step 156 (6903); Episode 93/100; Loss: 0.06585053354501724\n",
      "Step 157 (6904); Episode 93/100; Loss: 0.04962598532438278\n",
      "Step 158 (6905); Episode 93/100; Loss: 0.0020438239444047213\n",
      "Step 159 (6906); Episode 93/100; Loss: 0.0013413223205134273\n",
      "Step 160 (6907); Episode 93/100; Loss: 0.039028797298669815\n",
      "Step 0 (6908); Episode 94/100; Loss: 0.1062626764178276\n",
      "Step 1 (6909); Episode 94/100; Loss: 0.04249945655465126\n",
      "Step 2 (6910); Episode 94/100; Loss: 0.0012258131755515933\n",
      "Step 3 (6911); Episode 94/100; Loss: 0.1418704241514206\n",
      "Step 4 (6912); Episode 94/100; Loss: 0.0020286175422370434\n",
      "Step 5 (6913); Episode 94/100; Loss: 0.0008522992138750851\n",
      "Step 6 (6914); Episode 94/100; Loss: 0.015703542158007622\n",
      "Step 7 (6915); Episode 94/100; Loss: 0.0239742211997509\n",
      "Step 8 (6916); Episode 94/100; Loss: 0.0017151553183794022\n",
      "Step 9 (6917); Episode 94/100; Loss: 0.02678152173757553\n",
      "Step 10 (6918); Episode 94/100; Loss: 0.0015165908262133598\n",
      "Step 11 (6919); Episode 94/100; Loss: 0.029374772682785988\n",
      "Step 12 (6920); Episode 94/100; Loss: 0.026541028171777725\n",
      "Step 13 (6921); Episode 94/100; Loss: 0.044150929898023605\n",
      "Step 14 (6922); Episode 94/100; Loss: 0.061326466500759125\n",
      "Step 15 (6923); Episode 94/100; Loss: 0.003387053729966283\n",
      "Step 16 (6924); Episode 94/100; Loss: 0.0038963768165558577\n",
      "Step 17 (6925); Episode 94/100; Loss: 0.0015953363617882133\n",
      "Step 18 (6926); Episode 94/100; Loss: 0.04260843247175217\n",
      "Step 19 (6927); Episode 94/100; Loss: 0.007044939324259758\n",
      "Step 20 (6928); Episode 94/100; Loss: 0.08880139142274857\n",
      "Step 21 (6929); Episode 94/100; Loss: 0.04830720275640488\n",
      "Step 22 (6930); Episode 94/100; Loss: 0.031303245574235916\n",
      "Step 23 (6931); Episode 94/100; Loss: 0.04147243872284889\n",
      "Step 24 (6932); Episode 94/100; Loss: 0.0034410760272294283\n",
      "Step 25 (6933); Episode 94/100; Loss: 0.005753507371991873\n",
      "Step 26 (6934); Episode 94/100; Loss: 0.0017975621158257127\n",
      "Step 27 (6935); Episode 94/100; Loss: 0.031032757833600044\n",
      "Step 28 (6936); Episode 94/100; Loss: 0.001768149551935494\n",
      "Step 29 (6937); Episode 94/100; Loss: 0.042288243770599365\n",
      "Step 30 (6938); Episode 94/100; Loss: 0.11168317496776581\n",
      "Step 31 (6939); Episode 94/100; Loss: 0.05400243401527405\n",
      "Step 32 (6940); Episode 94/100; Loss: 0.0018881696742027998\n",
      "Step 33 (6941); Episode 94/100; Loss: 0.02939128689467907\n",
      "Step 34 (6942); Episode 94/100; Loss: 0.07649537175893784\n",
      "Step 35 (6943); Episode 94/100; Loss: 0.007912447676062584\n",
      "Step 36 (6944); Episode 94/100; Loss: 0.0014020998496562243\n",
      "Step 37 (6945); Episode 94/100; Loss: 0.02890317514538765\n",
      "Step 38 (6946); Episode 94/100; Loss: 0.0007572249742224813\n",
      "Step 39 (6947); Episode 94/100; Loss: 0.09024219959974289\n",
      "Step 40 (6948); Episode 94/100; Loss: 0.03220047801733017\n",
      "Step 41 (6949); Episode 94/100; Loss: 0.07908354699611664\n",
      "Step 42 (6950); Episode 94/100; Loss: 0.004704516381025314\n",
      "Step 43 (6951); Episode 94/100; Loss: 0.003049810416996479\n",
      "Step 44 (6952); Episode 94/100; Loss: 0.02413504384458065\n",
      "Step 45 (6953); Episode 94/100; Loss: 0.00397041579708457\n",
      "Step 46 (6954); Episode 94/100; Loss: 0.001901077339425683\n",
      "Step 47 (6955); Episode 94/100; Loss: 0.003683899762108922\n",
      "Step 48 (6956); Episode 94/100; Loss: 0.006153370253741741\n",
      "Step 49 (6957); Episode 94/100; Loss: 0.0024719140492379665\n",
      "Step 50 (6958); Episode 94/100; Loss: 0.03922976925969124\n",
      "Step 51 (6959); Episode 94/100; Loss: 0.08641659468412399\n",
      "Step 52 (6960); Episode 94/100; Loss: 0.0008757506730034947\n",
      "Step 53 (6961); Episode 94/100; Loss: 0.044799450784921646\n",
      "Step 54 (6962); Episode 94/100; Loss: 0.10329944640398026\n",
      "Step 55 (6963); Episode 94/100; Loss: 0.07355176657438278\n",
      "Step 56 (6964); Episode 94/100; Loss: 0.0041900984942913055\n",
      "Step 57 (6965); Episode 94/100; Loss: 0.0027837862726300955\n",
      "Step 58 (6966); Episode 94/100; Loss: 0.0601629763841629\n",
      "Step 59 (6967); Episode 94/100; Loss: 0.0607273206114769\n",
      "Step 60 (6968); Episode 94/100; Loss: 0.003232418792322278\n",
      "Step 61 (6969); Episode 94/100; Loss: 0.027298089116811752\n",
      "Step 62 (6970); Episode 94/100; Loss: 0.03289702534675598\n",
      "Step 63 (6971); Episode 94/100; Loss: 0.04431796446442604\n",
      "Step 64 (6972); Episode 94/100; Loss: 0.0023968343157321215\n",
      "Step 65 (6973); Episode 94/100; Loss: 0.0774897113442421\n",
      "Step 66 (6974); Episode 94/100; Loss: 0.04311433807015419\n",
      "Step 67 (6975); Episode 94/100; Loss: 0.00165571179240942\n",
      "Step 68 (6976); Episode 94/100; Loss: 0.0018420806154608727\n",
      "Step 69 (6977); Episode 94/100; Loss: 0.0023804381489753723\n",
      "Step 70 (6978); Episode 94/100; Loss: 0.008466270752251148\n",
      "Step 71 (6979); Episode 94/100; Loss: 0.0011738179018720984\n",
      "Step 72 (6980); Episode 94/100; Loss: 0.00101403275039047\n",
      "Step 73 (6981); Episode 94/100; Loss: 0.007484303321689367\n",
      "Step 74 (6982); Episode 94/100; Loss: 0.007440194021910429\n",
      "Step 75 (6983); Episode 94/100; Loss: 0.04446740448474884\n",
      "Step 76 (6984); Episode 94/100; Loss: 0.003403489477932453\n",
      "Step 77 (6985); Episode 94/100; Loss: 0.04741540178656578\n",
      "Step 78 (6986); Episode 94/100; Loss: 0.003930210135877132\n",
      "Step 79 (6987); Episode 94/100; Loss: 0.0013450996484607458\n",
      "Step 80 (6988); Episode 94/100; Loss: 0.007543118670582771\n",
      "Step 81 (6989); Episode 94/100; Loss: 0.051823150366544724\n",
      "Step 82 (6990); Episode 94/100; Loss: 0.0017496957443654537\n",
      "Step 83 (6991); Episode 94/100; Loss: 0.046967875212430954\n",
      "Step 84 (6992); Episode 94/100; Loss: 0.11987274885177612\n",
      "Step 85 (6993); Episode 94/100; Loss: 0.11080803722143173\n",
      "Step 86 (6994); Episode 94/100; Loss: 0.07468808442354202\n",
      "Step 87 (6995); Episode 94/100; Loss: 0.0072633326053619385\n",
      "Step 88 (6996); Episode 94/100; Loss: 0.11323750764131546\n",
      "Step 89 (6997); Episode 94/100; Loss: 0.0020610769279301167\n",
      "Step 90 (6998); Episode 94/100; Loss: 0.0010637774830684066\n",
      "Step 91 (6999); Episode 94/100; Loss: 0.04205494746565819\n",
      "Step 92 (7000); Episode 94/100; Loss: 0.05735686048865318\n",
      "Step 93 (7001); Episode 94/100; Loss: 0.002363786567002535\n",
      "Step 94 (7002); Episode 94/100; Loss: 0.009385206736624241\n",
      "Step 95 (7003); Episode 94/100; Loss: 0.002047219080850482\n",
      "Step 96 (7004); Episode 94/100; Loss: 0.07695279270410538\n",
      "Step 97 (7005); Episode 94/100; Loss: 0.040895070880651474\n",
      "Step 98 (7006); Episode 94/100; Loss: 0.028493769466876984\n",
      "Step 99 (7007); Episode 94/100; Loss: 0.05029187351465225\n",
      "Step 100 (7008); Episode 94/100; Loss: 0.11694969981908798\n",
      "Step 101 (7009); Episode 94/100; Loss: 0.010096381418406963\n",
      "Step 102 (7010); Episode 94/100; Loss: 0.04839161038398743\n",
      "Step 103 (7011); Episode 94/100; Loss: 0.1194615513086319\n",
      "Step 104 (7012); Episode 94/100; Loss: 0.026677628979086876\n",
      "Step 105 (7013); Episode 94/100; Loss: 0.005449834745377302\n",
      "Step 106 (7014); Episode 94/100; Loss: 0.002875971607863903\n",
      "Step 107 (7015); Episode 94/100; Loss: 0.03239278122782707\n",
      "Step 108 (7016); Episode 94/100; Loss: 0.022957010194659233\n",
      "Step 109 (7017); Episode 94/100; Loss: 0.04332461208105087\n",
      "Step 110 (7018); Episode 94/100; Loss: 0.033975694328546524\n",
      "Step 111 (7019); Episode 94/100; Loss: 0.04521286115050316\n",
      "Step 112 (7020); Episode 94/100; Loss: 0.0011153994128108025\n",
      "Step 113 (7021); Episode 94/100; Loss: 0.04137943312525749\n",
      "Step 114 (7022); Episode 94/100; Loss: 0.00356735335662961\n",
      "Step 115 (7023); Episode 94/100; Loss: 0.15232016146183014\n",
      "Step 116 (7024); Episode 94/100; Loss: 0.0035306254867464304\n",
      "Step 117 (7025); Episode 94/100; Loss: 0.00417759595438838\n",
      "Step 118 (7026); Episode 94/100; Loss: 0.0031786547042429447\n",
      "Step 119 (7027); Episode 94/100; Loss: 0.1071225106716156\n",
      "Step 120 (7028); Episode 94/100; Loss: 0.007069174665957689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 121 (7029); Episode 94/100; Loss: 0.040886957198381424\n",
      "Step 122 (7030); Episode 94/100; Loss: 0.04324966296553612\n",
      "Step 123 (7031); Episode 94/100; Loss: 0.002587483962997794\n",
      "Step 0 (7032); Episode 95/100; Loss: 0.031246468424797058\n",
      "Step 1 (7033); Episode 95/100; Loss: 0.02858158014714718\n",
      "Step 2 (7034); Episode 95/100; Loss: 0.027179177850484848\n",
      "Step 3 (7035); Episode 95/100; Loss: 0.0006952162366360426\n",
      "Step 4 (7036); Episode 95/100; Loss: 0.0019514631712809205\n",
      "Step 5 (7037); Episode 95/100; Loss: 0.005382169969379902\n",
      "Step 6 (7038); Episode 95/100; Loss: 0.0018970873206853867\n",
      "Step 7 (7039); Episode 95/100; Loss: 0.058360036462545395\n",
      "Step 8 (7040); Episode 95/100; Loss: 0.0006701084785163403\n",
      "Step 9 (7041); Episode 95/100; Loss: 0.05531102418899536\n",
      "Step 10 (7042); Episode 95/100; Loss: 0.0054473248310387135\n",
      "Step 11 (7043); Episode 95/100; Loss: 0.002301754429936409\n",
      "Step 12 (7044); Episode 95/100; Loss: 0.02518349327147007\n",
      "Step 13 (7045); Episode 95/100; Loss: 0.0024113955441862345\n",
      "Step 14 (7046); Episode 95/100; Loss: 0.0045071812346577644\n",
      "Step 15 (7047); Episode 95/100; Loss: 0.0023172725923359394\n",
      "Step 16 (7048); Episode 95/100; Loss: 0.0009722721297293901\n",
      "Step 17 (7049); Episode 95/100; Loss: 0.0012113925768062472\n",
      "Step 18 (7050); Episode 95/100; Loss: 0.07314106822013855\n",
      "Step 19 (7051); Episode 95/100; Loss: 0.1031675785779953\n",
      "Step 20 (7052); Episode 95/100; Loss: 0.05014033615589142\n",
      "Step 21 (7053); Episode 95/100; Loss: 0.00136100547388196\n",
      "Step 22 (7054); Episode 95/100; Loss: 0.04802696406841278\n",
      "Step 23 (7055); Episode 95/100; Loss: 0.07640960812568665\n",
      "Step 24 (7056); Episode 95/100; Loss: 0.07472912967205048\n",
      "Step 25 (7057); Episode 95/100; Loss: 0.0005927894380874932\n",
      "Step 26 (7058); Episode 95/100; Loss: 0.09794001281261444\n",
      "Step 27 (7059); Episode 95/100; Loss: 0.12085460871458054\n",
      "Step 28 (7060); Episode 95/100; Loss: 0.002814865903928876\n",
      "Step 29 (7061); Episode 95/100; Loss: 0.07319546490907669\n",
      "Step 30 (7062); Episode 95/100; Loss: 0.02888702042400837\n",
      "Step 31 (7063); Episode 95/100; Loss: 0.0383591428399086\n",
      "Step 32 (7064); Episode 95/100; Loss: 0.0025929168332368135\n",
      "Step 33 (7065); Episode 95/100; Loss: 0.009332784451544285\n",
      "Step 34 (7066); Episode 95/100; Loss: 0.02810758911073208\n",
      "Step 35 (7067); Episode 95/100; Loss: 0.03705185651779175\n",
      "Step 36 (7068); Episode 95/100; Loss: 0.0007905975799076259\n",
      "Step 37 (7069); Episode 95/100; Loss: 0.09364920854568481\n",
      "Step 38 (7070); Episode 95/100; Loss: 0.03319855406880379\n",
      "Step 39 (7071); Episode 95/100; Loss: 0.0802689716219902\n",
      "Step 40 (7072); Episode 95/100; Loss: 0.006605065893381834\n",
      "Step 41 (7073); Episode 95/100; Loss: 0.0468858927488327\n",
      "Step 42 (7074); Episode 95/100; Loss: 0.06737567484378815\n",
      "Step 43 (7075); Episode 95/100; Loss: 0.044879619032144547\n",
      "Step 44 (7076); Episode 95/100; Loss: 0.002153378678485751\n",
      "Step 45 (7077); Episode 95/100; Loss: 0.0023373744916170835\n",
      "Step 46 (7078); Episode 95/100; Loss: 0.010960625484585762\n",
      "Step 47 (7079); Episode 95/100; Loss: 0.10625862330198288\n",
      "Step 48 (7080); Episode 95/100; Loss: 0.025932418182492256\n",
      "Step 49 (7081); Episode 95/100; Loss: 0.004027102142572403\n",
      "Step 50 (7082); Episode 95/100; Loss: 0.045091867446899414\n",
      "Step 51 (7083); Episode 95/100; Loss: 0.03891477361321449\n",
      "Step 52 (7084); Episode 95/100; Loss: 0.05734642967581749\n",
      "Step 53 (7085); Episode 95/100; Loss: 0.07750324159860611\n",
      "Step 54 (7086); Episode 95/100; Loss: 0.0022910640109330416\n",
      "Step 55 (7087); Episode 95/100; Loss: 0.040598828345537186\n",
      "Step 56 (7088); Episode 95/100; Loss: 0.013422539457678795\n",
      "Step 57 (7089); Episode 95/100; Loss: 0.05475163832306862\n",
      "Step 58 (7090); Episode 95/100; Loss: 0.0022863794583827257\n",
      "Step 59 (7091); Episode 95/100; Loss: 0.05411568284034729\n",
      "Step 60 (7092); Episode 95/100; Loss: 0.004512093495577574\n",
      "Step 61 (7093); Episode 95/100; Loss: 0.03280065581202507\n",
      "Step 62 (7094); Episode 95/100; Loss: 0.012725869193673134\n",
      "Step 63 (7095); Episode 95/100; Loss: 0.020993009209632874\n",
      "Step 64 (7096); Episode 95/100; Loss: 0.04026120901107788\n",
      "Step 65 (7097); Episode 95/100; Loss: 0.0023116767406463623\n",
      "Step 66 (7098); Episode 95/100; Loss: 0.11205778270959854\n",
      "Step 67 (7099); Episode 95/100; Loss: 0.08760524541139603\n",
      "Step 68 (7100); Episode 95/100; Loss: 0.000956256000790745\n",
      "Step 69 (7101); Episode 95/100; Loss: 0.0693536251783371\n",
      "Step 70 (7102); Episode 95/100; Loss: 0.0025523798540234566\n",
      "Step 71 (7103); Episode 95/100; Loss: 0.0034990350250154734\n",
      "Step 72 (7104); Episode 95/100; Loss: 0.03998163342475891\n",
      "Step 73 (7105); Episode 95/100; Loss: 0.0040900674648582935\n",
      "Step 74 (7106); Episode 95/100; Loss: 0.010022149421274662\n",
      "Step 75 (7107); Episode 95/100; Loss: 0.05096178874373436\n",
      "Step 76 (7108); Episode 95/100; Loss: 0.001326603814959526\n",
      "Step 77 (7109); Episode 95/100; Loss: 0.002053981414064765\n",
      "Step 78 (7110); Episode 95/100; Loss: 0.10923232138156891\n",
      "Step 79 (7111); Episode 95/100; Loss: 0.04211978241801262\n",
      "Step 80 (7112); Episode 95/100; Loss: 0.17150354385375977\n",
      "Step 81 (7113); Episode 95/100; Loss: 0.05607350543141365\n",
      "Step 82 (7114); Episode 95/100; Loss: 0.0026952256448566914\n",
      "Step 83 (7115); Episode 95/100; Loss: 0.0017570382915437222\n",
      "Step 84 (7116); Episode 95/100; Loss: 0.021484073251485825\n",
      "Step 85 (7117); Episode 95/100; Loss: 0.03514855355024338\n",
      "Step 86 (7118); Episode 95/100; Loss: 0.07999683916568756\n",
      "Step 87 (7119); Episode 95/100; Loss: 0.0024844182189553976\n",
      "Step 88 (7120); Episode 95/100; Loss: 0.021481605246663094\n",
      "Step 89 (7121); Episode 95/100; Loss: 0.0018848214531317353\n",
      "Step 90 (7122); Episode 95/100; Loss: 0.01653766818344593\n",
      "Step 91 (7123); Episode 95/100; Loss: 0.006435442715883255\n",
      "Step 92 (7124); Episode 95/100; Loss: 0.05185549706220627\n",
      "Step 93 (7125); Episode 95/100; Loss: 0.006827295757830143\n",
      "Step 94 (7126); Episode 95/100; Loss: 0.004494539927691221\n",
      "Step 95 (7127); Episode 95/100; Loss: 0.0010120987426489592\n",
      "Step 96 (7128); Episode 95/100; Loss: 0.034467119723558426\n",
      "Step 97 (7129); Episode 95/100; Loss: 0.03670373186469078\n",
      "Step 98 (7130); Episode 95/100; Loss: 0.02154930680990219\n",
      "Step 99 (7131); Episode 95/100; Loss: 0.0016355144325643778\n",
      "Step 100 (7132); Episode 95/100; Loss: 0.0016358072170987725\n",
      "Step 101 (7133); Episode 95/100; Loss: 0.023453684523701668\n",
      "Step 102 (7134); Episode 95/100; Loss: 0.0037804492749273777\n",
      "Step 103 (7135); Episode 95/100; Loss: 0.0016431710682809353\n",
      "Step 104 (7136); Episode 95/100; Loss: 0.001702469657175243\n",
      "Step 105 (7137); Episode 95/100; Loss: 0.0012931905221194029\n",
      "Step 106 (7138); Episode 95/100; Loss: 0.006044342648237944\n",
      "Step 107 (7139); Episode 95/100; Loss: 0.08032308518886566\n",
      "Step 108 (7140); Episode 95/100; Loss: 0.058997467160224915\n",
      "Step 109 (7141); Episode 95/100; Loss: 0.0026879755314439535\n",
      "Step 110 (7142); Episode 95/100; Loss: 0.001968573546037078\n",
      "Step 111 (7143); Episode 95/100; Loss: 0.08711213618516922\n",
      "Step 112 (7144); Episode 95/100; Loss: 0.0014448942383751273\n",
      "Step 113 (7145); Episode 95/100; Loss: 0.0018185853259637952\n",
      "Step 114 (7146); Episode 95/100; Loss: 0.043878380209207535\n",
      "Step 115 (7147); Episode 95/100; Loss: 0.010136221535503864\n",
      "Step 116 (7148); Episode 95/100; Loss: 0.018620509654283524\n",
      "Step 117 (7149); Episode 95/100; Loss: 0.1504833847284317\n",
      "Step 118 (7150); Episode 95/100; Loss: 0.0005902503617107868\n",
      "Step 119 (7151); Episode 95/100; Loss: 0.0015050999354571104\n",
      "Step 120 (7152); Episode 95/100; Loss: 0.0026187009643763304\n",
      "Step 121 (7153); Episode 95/100; Loss: 0.0034140904899686575\n",
      "Step 122 (7154); Episode 95/100; Loss: 0.0019033824792131782\n",
      "Step 123 (7155); Episode 95/100; Loss: 0.00140159681905061\n",
      "Step 124 (7156); Episode 95/100; Loss: 0.04351021349430084\n",
      "Step 125 (7157); Episode 95/100; Loss: 0.003915988374501467\n",
      "Step 126 (7158); Episode 95/100; Loss: 0.0012332432670518756\n",
      "Step 127 (7159); Episode 95/100; Loss: 0.0008522356511093676\n",
      "Step 128 (7160); Episode 95/100; Loss: 0.007817224599421024\n",
      "Step 129 (7161); Episode 95/100; Loss: 0.002886162605136633\n",
      "Step 130 (7162); Episode 95/100; Loss: 0.03318043053150177\n",
      "Step 131 (7163); Episode 95/100; Loss: 0.05256251245737076\n",
      "Step 132 (7164); Episode 95/100; Loss: 0.012269415892660618\n",
      "Step 133 (7165); Episode 95/100; Loss: 0.0015016266843304038\n",
      "Step 134 (7166); Episode 95/100; Loss: 0.011666573584079742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 135 (7167); Episode 95/100; Loss: 0.10487650334835052\n",
      "Step 136 (7168); Episode 95/100; Loss: 0.060382284224033356\n",
      "Step 137 (7169); Episode 95/100; Loss: 0.003899022936820984\n",
      "Step 138 (7170); Episode 95/100; Loss: 0.0030401612166315317\n",
      "Step 139 (7171); Episode 95/100; Loss: 0.03159698471426964\n",
      "Step 140 (7172); Episode 95/100; Loss: 0.04015548154711723\n",
      "Step 141 (7173); Episode 95/100; Loss: 0.0011760583147406578\n",
      "Step 142 (7174); Episode 95/100; Loss: 0.04563598334789276\n",
      "Step 143 (7175); Episode 95/100; Loss: 0.0007643381832167506\n",
      "Step 144 (7176); Episode 95/100; Loss: 0.09140945971012115\n",
      "Step 145 (7177); Episode 95/100; Loss: 0.04551500827074051\n",
      "Step 146 (7178); Episode 95/100; Loss: 0.045792557299137115\n",
      "Step 147 (7179); Episode 95/100; Loss: 0.003651703242212534\n",
      "Step 148 (7180); Episode 95/100; Loss: 0.002883059438318014\n",
      "Step 149 (7181); Episode 95/100; Loss: 0.002708073938265443\n",
      "Step 150 (7182); Episode 95/100; Loss: 0.10488209873437881\n",
      "Step 151 (7183); Episode 95/100; Loss: 0.0025455874856561422\n",
      "Step 152 (7184); Episode 95/100; Loss: 0.0016019117319956422\n",
      "Step 153 (7185); Episode 95/100; Loss: 0.0022220357786864042\n",
      "Step 154 (7186); Episode 95/100; Loss: 0.046063221991062164\n",
      "Step 155 (7187); Episode 95/100; Loss: 0.003230511909350753\n",
      "Step 156 (7188); Episode 95/100; Loss: 0.0016907070530578494\n",
      "Step 157 (7189); Episode 95/100; Loss: 0.03899199888110161\n",
      "Step 158 (7190); Episode 95/100; Loss: 0.03913475573062897\n",
      "Step 159 (7191); Episode 95/100; Loss: 0.0008943347493186593\n",
      "Step 160 (7192); Episode 95/100; Loss: 0.0023501974064856768\n",
      "Step 161 (7193); Episode 95/100; Loss: 0.0554497092962265\n",
      "Step 162 (7194); Episode 95/100; Loss: 0.0012902835151180625\n",
      "Step 163 (7195); Episode 95/100; Loss: 0.0032710356172174215\n",
      "Step 164 (7196); Episode 95/100; Loss: 0.026817675679922104\n",
      "Step 165 (7197); Episode 95/100; Loss: 0.04139048233628273\n",
      "Step 166 (7198); Episode 95/100; Loss: 0.02588810585439205\n",
      "Step 167 (7199); Episode 95/100; Loss: 0.001740327919833362\n",
      "Step 168 (7200); Episode 95/100; Loss: 0.056711021810770035\n",
      "Step 169 (7201); Episode 95/100; Loss: 0.02853035368025303\n",
      "Step 170 (7202); Episode 95/100; Loss: 0.08972882479429245\n",
      "Step 171 (7203); Episode 95/100; Loss: 0.08715067058801651\n",
      "Step 172 (7204); Episode 95/100; Loss: 0.00676717609167099\n",
      "Step 173 (7205); Episode 95/100; Loss: 0.03362782299518585\n",
      "Step 174 (7206); Episode 95/100; Loss: 0.053665317595005035\n",
      "Step 175 (7207); Episode 95/100; Loss: 0.0034313718788325787\n",
      "Step 176 (7208); Episode 95/100; Loss: 0.0012938411673530936\n",
      "Step 177 (7209); Episode 95/100; Loss: 0.036115553230047226\n",
      "Step 178 (7210); Episode 95/100; Loss: 0.0021217763423919678\n",
      "Step 179 (7211); Episode 95/100; Loss: 0.0020648690406233072\n",
      "Step 180 (7212); Episode 95/100; Loss: 0.138380229473114\n",
      "Step 181 (7213); Episode 95/100; Loss: 0.041752807796001434\n",
      "Step 182 (7214); Episode 95/100; Loss: 0.026049766689538956\n",
      "Step 183 (7215); Episode 95/100; Loss: 0.09642379730939865\n",
      "Step 184 (7216); Episode 95/100; Loss: 0.07459989935159683\n",
      "Step 185 (7217); Episode 95/100; Loss: 0.009696880355477333\n",
      "Step 0 (7218); Episode 96/100; Loss: 0.002594351302832365\n",
      "Step 1 (7219); Episode 96/100; Loss: 0.049981337040662766\n",
      "Step 2 (7220); Episode 96/100; Loss: 0.06489728391170502\n",
      "Step 3 (7221); Episode 96/100; Loss: 0.04708820953965187\n",
      "Step 4 (7222); Episode 96/100; Loss: 0.004263467155396938\n",
      "Step 5 (7223); Episode 96/100; Loss: 0.013675150461494923\n",
      "Step 6 (7224); Episode 96/100; Loss: 0.036350760608911514\n",
      "Step 7 (7225); Episode 96/100; Loss: 0.043400850147008896\n",
      "Step 8 (7226); Episode 96/100; Loss: 0.03441866859793663\n",
      "Step 9 (7227); Episode 96/100; Loss: 0.002103229286149144\n",
      "Step 10 (7228); Episode 96/100; Loss: 0.0014279308961704373\n",
      "Step 11 (7229); Episode 96/100; Loss: 0.055800944566726685\n",
      "Step 12 (7230); Episode 96/100; Loss: 0.0013179859379306436\n",
      "Step 13 (7231); Episode 96/100; Loss: 0.0012797997333109379\n",
      "Step 14 (7232); Episode 96/100; Loss: 0.0016384838381782174\n",
      "Step 15 (7233); Episode 96/100; Loss: 0.03370876610279083\n",
      "Step 16 (7234); Episode 96/100; Loss: 0.07764015346765518\n",
      "Step 17 (7235); Episode 96/100; Loss: 0.010062148794531822\n",
      "Step 18 (7236); Episode 96/100; Loss: 0.035919222980737686\n",
      "Step 19 (7237); Episode 96/100; Loss: 0.0015181968919932842\n",
      "Step 20 (7238); Episode 96/100; Loss: 0.00238468823954463\n",
      "Step 21 (7239); Episode 96/100; Loss: 0.0024672483559697866\n",
      "Step 22 (7240); Episode 96/100; Loss: 0.003789700334891677\n",
      "Step 23 (7241); Episode 96/100; Loss: 0.001515995361842215\n",
      "Step 24 (7242); Episode 96/100; Loss: 0.0046929409727454185\n",
      "Step 25 (7243); Episode 96/100; Loss: 0.04967597499489784\n",
      "Step 26 (7244); Episode 96/100; Loss: 0.04626621678471565\n",
      "Step 27 (7245); Episode 96/100; Loss: 0.087877556681633\n",
      "Step 28 (7246); Episode 96/100; Loss: 0.004474374931305647\n",
      "Step 29 (7247); Episode 96/100; Loss: 0.06746988743543625\n",
      "Step 30 (7248); Episode 96/100; Loss: 0.05823969841003418\n",
      "Step 31 (7249); Episode 96/100; Loss: 0.001223145634867251\n",
      "Step 32 (7250); Episode 96/100; Loss: 0.04376647248864174\n",
      "Step 33 (7251); Episode 96/100; Loss: 0.0008627185015939176\n",
      "Step 34 (7252); Episode 96/100; Loss: 0.08885297924280167\n",
      "Step 35 (7253); Episode 96/100; Loss: 0.10762321203947067\n",
      "Step 36 (7254); Episode 96/100; Loss: 0.05250510573387146\n",
      "Step 37 (7255); Episode 96/100; Loss: 0.003413415513932705\n",
      "Step 38 (7256); Episode 96/100; Loss: 0.0482402965426445\n",
      "Step 39 (7257); Episode 96/100; Loss: 0.05809786915779114\n",
      "Step 40 (7258); Episode 96/100; Loss: 0.00380260543897748\n",
      "Step 41 (7259); Episode 96/100; Loss: 0.08626837283372879\n",
      "Step 42 (7260); Episode 96/100; Loss: 0.0006030722288414836\n",
      "Step 43 (7261); Episode 96/100; Loss: 0.025905659422278404\n",
      "Step 44 (7262); Episode 96/100; Loss: 0.0031940906774252653\n",
      "Step 45 (7263); Episode 96/100; Loss: 0.08096779137849808\n",
      "Step 46 (7264); Episode 96/100; Loss: 0.06168309226632118\n",
      "Step 47 (7265); Episode 96/100; Loss: 0.031132129952311516\n",
      "Step 48 (7266); Episode 96/100; Loss: 0.04739898443222046\n",
      "Step 49 (7267); Episode 96/100; Loss: 0.016132188960909843\n",
      "Step 50 (7268); Episode 96/100; Loss: 0.004459633491933346\n",
      "Step 51 (7269); Episode 96/100; Loss: 0.0016026837984099984\n",
      "Step 52 (7270); Episode 96/100; Loss: 0.006306936498731375\n",
      "Step 53 (7271); Episode 96/100; Loss: 0.04173367843031883\n",
      "Step 54 (7272); Episode 96/100; Loss: 0.008474931120872498\n",
      "Step 55 (7273); Episode 96/100; Loss: 0.006360598374158144\n",
      "Step 56 (7274); Episode 96/100; Loss: 0.0442633256316185\n",
      "Step 57 (7275); Episode 96/100; Loss: 0.0016867459053173661\n",
      "Step 58 (7276); Episode 96/100; Loss: 0.009651166386902332\n",
      "Step 59 (7277); Episode 96/100; Loss: 0.03804584592580795\n",
      "Step 60 (7278); Episode 96/100; Loss: 0.007767171133309603\n",
      "Step 61 (7279); Episode 96/100; Loss: 0.022827155888080597\n",
      "Step 62 (7280); Episode 96/100; Loss: 0.04664193466305733\n",
      "Step 63 (7281); Episode 96/100; Loss: 0.001995971892029047\n",
      "Step 64 (7282); Episode 96/100; Loss: 0.04304090887308121\n",
      "Step 65 (7283); Episode 96/100; Loss: 0.03181179240345955\n",
      "Step 66 (7284); Episode 96/100; Loss: 0.0034414685796946287\n",
      "Step 67 (7285); Episode 96/100; Loss: 0.007067711092531681\n",
      "Step 68 (7286); Episode 96/100; Loss: 0.004686692729592323\n",
      "Step 69 (7287); Episode 96/100; Loss: 0.07885417342185974\n",
      "Step 70 (7288); Episode 96/100; Loss: 0.002206077566370368\n",
      "Step 71 (7289); Episode 96/100; Loss: 0.04611799493432045\n",
      "Step 72 (7290); Episode 96/100; Loss: 0.047468580305576324\n",
      "Step 73 (7291); Episode 96/100; Loss: 0.0019962096121162176\n",
      "Step 74 (7292); Episode 96/100; Loss: 0.03775548189878464\n",
      "Step 75 (7293); Episode 96/100; Loss: 0.023916024714708328\n",
      "Step 76 (7294); Episode 96/100; Loss: 0.07565759867429733\n",
      "Step 77 (7295); Episode 96/100; Loss: 0.002512869192287326\n",
      "Step 78 (7296); Episode 96/100; Loss: 0.00330749130807817\n",
      "Step 79 (7297); Episode 96/100; Loss: 0.04387340322136879\n",
      "Step 80 (7298); Episode 96/100; Loss: 0.08050712198019028\n",
      "Step 81 (7299); Episode 96/100; Loss: 0.08229038864374161\n",
      "Step 82 (7300); Episode 96/100; Loss: 0.036683984100818634\n",
      "Step 83 (7301); Episode 96/100; Loss: 0.11698491126298904\n",
      "Step 84 (7302); Episode 96/100; Loss: 0.0027313416358083487\n",
      "Step 85 (7303); Episode 96/100; Loss: 0.040913116186857224\n",
      "Step 86 (7304); Episode 96/100; Loss: 0.0028656390495598316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 87 (7305); Episode 96/100; Loss: 0.05508963018655777\n",
      "Step 88 (7306); Episode 96/100; Loss: 0.05320616438984871\n",
      "Step 89 (7307); Episode 96/100; Loss: 0.002739299088716507\n",
      "Step 90 (7308); Episode 96/100; Loss: 0.001744746812619269\n",
      "Step 91 (7309); Episode 96/100; Loss: 0.055197153240442276\n",
      "Step 92 (7310); Episode 96/100; Loss: 0.011346870101988316\n",
      "Step 93 (7311); Episode 96/100; Loss: 0.030641797930002213\n",
      "Step 94 (7312); Episode 96/100; Loss: 0.0019342706073075533\n",
      "Step 95 (7313); Episode 96/100; Loss: 0.003389196004718542\n",
      "Step 96 (7314); Episode 96/100; Loss: 0.059836264699697495\n",
      "Step 97 (7315); Episode 96/100; Loss: 0.03747139498591423\n",
      "Step 98 (7316); Episode 96/100; Loss: 0.0026089102029800415\n",
      "Step 99 (7317); Episode 96/100; Loss: 0.04318404942750931\n",
      "Step 100 (7318); Episode 96/100; Loss: 0.04199821501970291\n",
      "Step 101 (7319); Episode 96/100; Loss: 0.025820929557085037\n",
      "Step 102 (7320); Episode 96/100; Loss: 0.001702277222648263\n",
      "Step 103 (7321); Episode 96/100; Loss: 0.0014634986873716116\n",
      "Step 104 (7322); Episode 96/100; Loss: 0.0016328571364283562\n",
      "Step 105 (7323); Episode 96/100; Loss: 0.0021228280384093523\n",
      "Step 106 (7324); Episode 96/100; Loss: 0.002791276667267084\n",
      "Step 107 (7325); Episode 96/100; Loss: 0.0037244928535073996\n",
      "Step 108 (7326); Episode 96/100; Loss: 0.14047566056251526\n",
      "Step 109 (7327); Episode 96/100; Loss: 0.04072447866201401\n",
      "Step 110 (7328); Episode 96/100; Loss: 0.04376453906297684\n",
      "Step 111 (7329); Episode 96/100; Loss: 0.00265714293345809\n",
      "Step 112 (7330); Episode 96/100; Loss: 0.0021981943864375353\n",
      "Step 113 (7331); Episode 96/100; Loss: 0.0449228510260582\n",
      "Step 114 (7332); Episode 96/100; Loss: 0.0025812627281993628\n",
      "Step 115 (7333); Episode 96/100; Loss: 0.08642169833183289\n",
      "Step 116 (7334); Episode 96/100; Loss: 0.0006798460963182151\n",
      "Step 117 (7335); Episode 96/100; Loss: 0.02344585955142975\n",
      "Step 118 (7336); Episode 96/100; Loss: 0.002278631553053856\n",
      "Step 119 (7337); Episode 96/100; Loss: 0.01320242416113615\n",
      "Step 120 (7338); Episode 96/100; Loss: 0.04104635491967201\n",
      "Step 121 (7339); Episode 96/100; Loss: 0.0035658327396959066\n",
      "Step 122 (7340); Episode 96/100; Loss: 0.0037002312019467354\n",
      "Step 123 (7341); Episode 96/100; Loss: 0.0011450882302597165\n",
      "Step 124 (7342); Episode 96/100; Loss: 0.0012835576198995113\n",
      "Step 125 (7343); Episode 96/100; Loss: 0.03555595874786377\n",
      "Step 126 (7344); Episode 96/100; Loss: 0.0028094060253351927\n",
      "Step 127 (7345); Episode 96/100; Loss: 0.046656493097543716\n",
      "Step 128 (7346); Episode 96/100; Loss: 0.0007499258499592543\n",
      "Step 129 (7347); Episode 96/100; Loss: 0.0019849510863423347\n",
      "Step 130 (7348); Episode 96/100; Loss: 0.03586734086275101\n",
      "Step 131 (7349); Episode 96/100; Loss: 0.0009513747645542026\n",
      "Step 132 (7350); Episode 96/100; Loss: 0.04593680799007416\n",
      "Step 133 (7351); Episode 96/100; Loss: 0.002134384587407112\n",
      "Step 134 (7352); Episode 96/100; Loss: 0.006944780703634024\n",
      "Step 135 (7353); Episode 96/100; Loss: 0.00254924688488245\n",
      "Step 0 (7354); Episode 97/100; Loss: 0.003917948808521032\n",
      "Step 1 (7355); Episode 97/100; Loss: 0.04608682915568352\n",
      "Step 2 (7356); Episode 97/100; Loss: 0.05277993157505989\n",
      "Step 3 (7357); Episode 97/100; Loss: 0.0015964116901159286\n",
      "Step 4 (7358); Episode 97/100; Loss: 0.0924302488565445\n",
      "Step 5 (7359); Episode 97/100; Loss: 0.035656366497278214\n",
      "Step 6 (7360); Episode 97/100; Loss: 0.0018097074935212731\n",
      "Step 7 (7361); Episode 97/100; Loss: 0.13597075641155243\n",
      "Step 8 (7362); Episode 97/100; Loss: 0.009140877053141594\n",
      "Step 9 (7363); Episode 97/100; Loss: 0.0018416836392134428\n",
      "Step 10 (7364); Episode 97/100; Loss: 0.006609067320823669\n",
      "Step 11 (7365); Episode 97/100; Loss: 0.04599056765437126\n",
      "Step 12 (7366); Episode 97/100; Loss: 0.004621068947017193\n",
      "Step 13 (7367); Episode 97/100; Loss: 0.10613732039928436\n",
      "Step 14 (7368); Episode 97/100; Loss: 0.033108290284872055\n",
      "Step 15 (7369); Episode 97/100; Loss: 0.0028731990605592728\n",
      "Step 16 (7370); Episode 97/100; Loss: 0.003251207759603858\n",
      "Step 17 (7371); Episode 97/100; Loss: 0.0014550469350069761\n",
      "Step 18 (7372); Episode 97/100; Loss: 0.049608685076236725\n",
      "Step 19 (7373); Episode 97/100; Loss: 0.08045120537281036\n",
      "Step 20 (7374); Episode 97/100; Loss: 0.02716989256441593\n",
      "Step 21 (7375); Episode 97/100; Loss: 0.0013364625629037619\n",
      "Step 22 (7376); Episode 97/100; Loss: 0.003296535462141037\n",
      "Step 23 (7377); Episode 97/100; Loss: 0.055228348821401596\n",
      "Step 24 (7378); Episode 97/100; Loss: 0.037369824945926666\n",
      "Step 25 (7379); Episode 97/100; Loss: 0.08404676616191864\n",
      "Step 26 (7380); Episode 97/100; Loss: 0.027378305792808533\n",
      "Step 27 (7381); Episode 97/100; Loss: 0.05594685673713684\n",
      "Step 28 (7382); Episode 97/100; Loss: 0.0013023560168221593\n",
      "Step 29 (7383); Episode 97/100; Loss: 0.10866022855043411\n",
      "Step 30 (7384); Episode 97/100; Loss: 0.03703232482075691\n",
      "Step 31 (7385); Episode 97/100; Loss: 0.0030389747116714716\n",
      "Step 32 (7386); Episode 97/100; Loss: 0.001847776467911899\n",
      "Step 33 (7387); Episode 97/100; Loss: 0.00284919748082757\n",
      "Step 34 (7388); Episode 97/100; Loss: 0.0004607783048413694\n",
      "Step 35 (7389); Episode 97/100; Loss: 0.0032285379711538553\n",
      "Step 36 (7390); Episode 97/100; Loss: 0.03565688803792\n",
      "Step 37 (7391); Episode 97/100; Loss: 0.030313722789287567\n",
      "Step 38 (7392); Episode 97/100; Loss: 0.04222768172621727\n",
      "Step 39 (7393); Episode 97/100; Loss: 0.05137934908270836\n",
      "Step 40 (7394); Episode 97/100; Loss: 0.0038231327198445797\n",
      "Step 41 (7395); Episode 97/100; Loss: 0.0014258637093007565\n",
      "Step 42 (7396); Episode 97/100; Loss: 0.04946513473987579\n",
      "Step 43 (7397); Episode 97/100; Loss: 0.07899997383356094\n",
      "Step 44 (7398); Episode 97/100; Loss: 0.05300818756222725\n",
      "Step 45 (7399); Episode 97/100; Loss: 0.0019420241005718708\n",
      "Step 46 (7400); Episode 97/100; Loss: 0.035067010670900345\n",
      "Step 47 (7401); Episode 97/100; Loss: 0.05221954360604286\n",
      "Step 48 (7402); Episode 97/100; Loss: 0.0012761882971972227\n",
      "Step 49 (7403); Episode 97/100; Loss: 0.003999851178377867\n",
      "Step 50 (7404); Episode 97/100; Loss: 0.029410891234874725\n",
      "Step 51 (7405); Episode 97/100; Loss: 0.05244893580675125\n",
      "Step 52 (7406); Episode 97/100; Loss: 0.053400203585624695\n",
      "Step 53 (7407); Episode 97/100; Loss: 0.041667331010103226\n",
      "Step 54 (7408); Episode 97/100; Loss: 0.008076600730419159\n",
      "Step 55 (7409); Episode 97/100; Loss: 0.03975549712777138\n",
      "Step 56 (7410); Episode 97/100; Loss: 0.025208251550793648\n",
      "Step 57 (7411); Episode 97/100; Loss: 0.0009791816119104624\n",
      "Step 58 (7412); Episode 97/100; Loss: 0.0022590458393096924\n",
      "Step 59 (7413); Episode 97/100; Loss: 0.07216246426105499\n",
      "Step 60 (7414); Episode 97/100; Loss: 0.0009589145192876458\n",
      "Step 61 (7415); Episode 97/100; Loss: 0.0519738644361496\n",
      "Step 62 (7416); Episode 97/100; Loss: 0.004694018512964249\n",
      "Step 63 (7417); Episode 97/100; Loss: 0.0012301949318498373\n",
      "Step 64 (7418); Episode 97/100; Loss: 0.0192643441259861\n",
      "Step 65 (7419); Episode 97/100; Loss: 0.05092649161815643\n",
      "Step 66 (7420); Episode 97/100; Loss: 0.00829526036977768\n",
      "Step 67 (7421); Episode 97/100; Loss: 0.0021595018915832043\n",
      "Step 68 (7422); Episode 97/100; Loss: 0.02127055823802948\n",
      "Step 69 (7423); Episode 97/100; Loss: 0.041293539106845856\n",
      "Step 70 (7424); Episode 97/100; Loss: 0.07064439356327057\n",
      "Step 71 (7425); Episode 97/100; Loss: 0.07745665311813354\n",
      "Step 72 (7426); Episode 97/100; Loss: 0.024556579068303108\n",
      "Step 73 (7427); Episode 97/100; Loss: 0.03767481818795204\n",
      "Step 74 (7428); Episode 97/100; Loss: 0.0016078797634691\n",
      "Step 75 (7429); Episode 97/100; Loss: 0.05406705290079117\n",
      "Step 76 (7430); Episode 97/100; Loss: 0.038534168154001236\n",
      "Step 77 (7431); Episode 97/100; Loss: 0.001963191432878375\n",
      "Step 78 (7432); Episode 97/100; Loss: 0.005269650835543871\n",
      "Step 79 (7433); Episode 97/100; Loss: 0.0023009460419416428\n",
      "Step 80 (7434); Episode 97/100; Loss: 0.026340823620557785\n",
      "Step 81 (7435); Episode 97/100; Loss: 0.03508242964744568\n",
      "Step 82 (7436); Episode 97/100; Loss: 0.07483329623937607\n",
      "Step 83 (7437); Episode 97/100; Loss: 0.0011383122764527798\n",
      "Step 84 (7438); Episode 97/100; Loss: 0.0014234607806429267\n",
      "Step 85 (7439); Episode 97/100; Loss: 0.003001441014930606\n",
      "Step 86 (7440); Episode 97/100; Loss: 0.09644167125225067\n",
      "Step 87 (7441); Episode 97/100; Loss: 0.0012156522134318948\n",
      "Step 88 (7442); Episode 97/100; Loss: 0.0033381462562829256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 89 (7443); Episode 97/100; Loss: 0.0033857952803373337\n",
      "Step 90 (7444); Episode 97/100; Loss: 0.036439426243305206\n",
      "Step 91 (7445); Episode 97/100; Loss: 0.0013510739663615823\n",
      "Step 92 (7446); Episode 97/100; Loss: 0.07815656065940857\n",
      "Step 93 (7447); Episode 97/100; Loss: 0.001924647018313408\n",
      "Step 94 (7448); Episode 97/100; Loss: 0.061351872980594635\n",
      "Step 95 (7449); Episode 97/100; Loss: 0.09492658078670502\n",
      "Step 96 (7450); Episode 97/100; Loss: 0.0717771127820015\n",
      "Step 97 (7451); Episode 97/100; Loss: 0.0014717323938384652\n",
      "Step 98 (7452); Episode 97/100; Loss: 0.005235002841800451\n",
      "Step 99 (7453); Episode 97/100; Loss: 0.0024101503659039736\n",
      "Step 100 (7454); Episode 97/100; Loss: 0.001408384763635695\n",
      "Step 101 (7455); Episode 97/100; Loss: 0.04614874720573425\n",
      "Step 102 (7456); Episode 97/100; Loss: 0.003416286315768957\n",
      "Step 103 (7457); Episode 97/100; Loss: 0.021273743361234665\n",
      "Step 104 (7458); Episode 97/100; Loss: 0.010785426013171673\n",
      "Step 105 (7459); Episode 97/100; Loss: 0.0022176827769726515\n",
      "Step 106 (7460); Episode 97/100; Loss: 0.1010788232088089\n",
      "Step 107 (7461); Episode 97/100; Loss: 0.08640141785144806\n",
      "Step 108 (7462); Episode 97/100; Loss: 0.0022117546759545803\n",
      "Step 109 (7463); Episode 97/100; Loss: 0.04642855003476143\n",
      "Step 110 (7464); Episode 97/100; Loss: 0.04941160976886749\n",
      "Step 111 (7465); Episode 97/100; Loss: 0.003155752783641219\n",
      "Step 112 (7466); Episode 97/100; Loss: 0.0038960149977356195\n",
      "Step 113 (7467); Episode 97/100; Loss: 0.0025748424232006073\n",
      "Step 114 (7468); Episode 97/100; Loss: 0.0396399088203907\n",
      "Step 115 (7469); Episode 97/100; Loss: 0.0012854925589635968\n",
      "Step 116 (7470); Episode 97/100; Loss: 0.08421941101551056\n",
      "Step 117 (7471); Episode 97/100; Loss: 0.002515739994123578\n",
      "Step 118 (7472); Episode 97/100; Loss: 0.0050201620906591415\n",
      "Step 119 (7473); Episode 97/100; Loss: 0.0015188539400696754\n",
      "Step 120 (7474); Episode 97/100; Loss: 0.04626694321632385\n",
      "Step 121 (7475); Episode 97/100; Loss: 0.0019822840113192797\n",
      "Step 122 (7476); Episode 97/100; Loss: 0.02890082076191902\n",
      "Step 123 (7477); Episode 97/100; Loss: 0.029578888788819313\n",
      "Step 124 (7478); Episode 97/100; Loss: 0.005614954046905041\n",
      "Step 125 (7479); Episode 97/100; Loss: 0.0008600191213190556\n",
      "Step 126 (7480); Episode 97/100; Loss: 0.0947212502360344\n",
      "Step 127 (7481); Episode 97/100; Loss: 0.0015577422454953194\n",
      "Step 128 (7482); Episode 97/100; Loss: 0.0009455455583520234\n",
      "Step 129 (7483); Episode 97/100; Loss: 0.04079022258520126\n",
      "Step 130 (7484); Episode 97/100; Loss: 0.09948750585317612\n",
      "Step 131 (7485); Episode 97/100; Loss: 0.05229587480425835\n",
      "Step 0 (7486); Episode 98/100; Loss: 0.03297901153564453\n",
      "Step 1 (7487); Episode 98/100; Loss: 0.0019045082153752446\n",
      "Step 2 (7488); Episode 98/100; Loss: 0.1343497335910797\n",
      "Step 3 (7489); Episode 98/100; Loss: 0.001714378478936851\n",
      "Step 4 (7490); Episode 98/100; Loss: 0.0023848318960517645\n",
      "Step 5 (7491); Episode 98/100; Loss: 0.02483324334025383\n",
      "Step 6 (7492); Episode 98/100; Loss: 0.00931809563189745\n",
      "Step 7 (7493); Episode 98/100; Loss: 0.03338100016117096\n",
      "Step 8 (7494); Episode 98/100; Loss: 0.004520682152360678\n",
      "Step 9 (7495); Episode 98/100; Loss: 0.0020785422530025244\n",
      "Step 10 (7496); Episode 98/100; Loss: 0.04124351218342781\n",
      "Step 11 (7497); Episode 98/100; Loss: 0.0032021948136389256\n",
      "Step 12 (7498); Episode 98/100; Loss: 0.05364488065242767\n",
      "Step 13 (7499); Episode 98/100; Loss: 0.0041338857263326645\n",
      "Step 14 (7500); Episode 98/100; Loss: 0.08300204575061798\n",
      "Step 15 (7501); Episode 98/100; Loss: 0.027157627046108246\n",
      "Step 16 (7502); Episode 98/100; Loss: 0.04055429622530937\n",
      "Step 17 (7503); Episode 98/100; Loss: 0.04011474549770355\n",
      "Step 18 (7504); Episode 98/100; Loss: 0.1516048163175583\n",
      "Step 19 (7505); Episode 98/100; Loss: 0.037317100912332535\n",
      "Step 20 (7506); Episode 98/100; Loss: 0.03086911141872406\n",
      "Step 21 (7507); Episode 98/100; Loss: 0.0021962146274745464\n",
      "Step 22 (7508); Episode 98/100; Loss: 0.001023240969516337\n",
      "Step 23 (7509); Episode 98/100; Loss: 0.0012318526860326529\n",
      "Step 24 (7510); Episode 98/100; Loss: 0.046332284808158875\n",
      "Step 25 (7511); Episode 98/100; Loss: 0.1037096381187439\n",
      "Step 26 (7512); Episode 98/100; Loss: 0.0020486225839704275\n",
      "Step 27 (7513); Episode 98/100; Loss: 0.002244203118607402\n",
      "Step 28 (7514); Episode 98/100; Loss: 0.022794853895902634\n",
      "Step 29 (7515); Episode 98/100; Loss: 0.0023295851424336433\n",
      "Step 30 (7516); Episode 98/100; Loss: 0.0006839330308139324\n",
      "Step 31 (7517); Episode 98/100; Loss: 0.00211482890881598\n",
      "Step 32 (7518); Episode 98/100; Loss: 0.042098578065633774\n",
      "Step 33 (7519); Episode 98/100; Loss: 0.005964493378996849\n",
      "Step 34 (7520); Episode 98/100; Loss: 0.002119170967489481\n",
      "Step 35 (7521); Episode 98/100; Loss: 0.0021037275437265635\n",
      "Step 36 (7522); Episode 98/100; Loss: 0.07666804641485214\n",
      "Step 37 (7523); Episode 98/100; Loss: 0.040658846497535706\n",
      "Step 38 (7524); Episode 98/100; Loss: 0.0027643772773444653\n",
      "Step 39 (7525); Episode 98/100; Loss: 0.04307936131954193\n",
      "Step 40 (7526); Episode 98/100; Loss: 0.0009905582992359996\n",
      "Step 41 (7527); Episode 98/100; Loss: 0.0028720470145344734\n",
      "Step 42 (7528); Episode 98/100; Loss: 0.0457175113260746\n",
      "Step 43 (7529); Episode 98/100; Loss: 0.0030905783642083406\n",
      "Step 44 (7530); Episode 98/100; Loss: 0.0007425124640576541\n",
      "Step 45 (7531); Episode 98/100; Loss: 0.0967838242650032\n",
      "Step 46 (7532); Episode 98/100; Loss: 0.0010437481105327606\n",
      "Step 47 (7533); Episode 98/100; Loss: 0.0008894558995962143\n",
      "Step 48 (7534); Episode 98/100; Loss: 0.05196309834718704\n",
      "Step 49 (7535); Episode 98/100; Loss: 0.04351259022951126\n",
      "Step 50 (7536); Episode 98/100; Loss: 0.002027265727519989\n",
      "Step 51 (7537); Episode 98/100; Loss: 0.06634367257356644\n",
      "Step 52 (7538); Episode 98/100; Loss: 0.0008805133402347565\n",
      "Step 53 (7539); Episode 98/100; Loss: 0.03530113026499748\n",
      "Step 54 (7540); Episode 98/100; Loss: 0.004096580669283867\n",
      "Step 55 (7541); Episode 98/100; Loss: 0.09158123284578323\n",
      "Step 56 (7542); Episode 98/100; Loss: 0.000595312740188092\n",
      "Step 57 (7543); Episode 98/100; Loss: 0.005167393479496241\n",
      "Step 58 (7544); Episode 98/100; Loss: 0.0034415111877024174\n",
      "Step 59 (7545); Episode 98/100; Loss: 0.002801643218845129\n",
      "Step 60 (7546); Episode 98/100; Loss: 0.004939437378197908\n",
      "Step 61 (7547); Episode 98/100; Loss: 0.006550467107445002\n",
      "Step 62 (7548); Episode 98/100; Loss: 0.01645195484161377\n",
      "Step 63 (7549); Episode 98/100; Loss: 0.0022367050405591726\n",
      "Step 64 (7550); Episode 98/100; Loss: 0.008629210293293\n",
      "Step 65 (7551); Episode 98/100; Loss: 0.002216050401329994\n",
      "Step 66 (7552); Episode 98/100; Loss: 0.00891239196062088\n",
      "Step 67 (7553); Episode 98/100; Loss: 0.00284369895234704\n",
      "Step 68 (7554); Episode 98/100; Loss: 0.015733258798718452\n",
      "Step 69 (7555); Episode 98/100; Loss: 0.0356198288500309\n",
      "Step 70 (7556); Episode 98/100; Loss: 0.007317934650927782\n",
      "Step 71 (7557); Episode 98/100; Loss: 0.001932100742124021\n",
      "Step 72 (7558); Episode 98/100; Loss: 0.04873254895210266\n",
      "Step 73 (7559); Episode 98/100; Loss: 0.011514483951032162\n",
      "Step 74 (7560); Episode 98/100; Loss: 0.0937211662530899\n",
      "Step 75 (7561); Episode 98/100; Loss: 0.008822157979011536\n",
      "Step 76 (7562); Episode 98/100; Loss: 0.0013625340070575476\n",
      "Step 77 (7563); Episode 98/100; Loss: 0.1366031914949417\n",
      "Step 78 (7564); Episode 98/100; Loss: 0.08808345347642899\n",
      "Step 79 (7565); Episode 98/100; Loss: 0.03124033287167549\n",
      "Step 80 (7566); Episode 98/100; Loss: 0.00246379803866148\n",
      "Step 81 (7567); Episode 98/100; Loss: 0.006392262876033783\n",
      "Step 82 (7568); Episode 98/100; Loss: 0.00509209930896759\n",
      "Step 83 (7569); Episode 98/100; Loss: 0.0018159281462430954\n",
      "Step 84 (7570); Episode 98/100; Loss: 0.0028686129953712225\n",
      "Step 85 (7571); Episode 98/100; Loss: 0.023930329829454422\n",
      "Step 86 (7572); Episode 98/100; Loss: 0.14733822643756866\n",
      "Step 87 (7573); Episode 98/100; Loss: 0.0009171785786747932\n",
      "Step 88 (7574); Episode 98/100; Loss: 0.039379194378852844\n",
      "Step 89 (7575); Episode 98/100; Loss: 0.0017459967639297247\n",
      "Step 90 (7576); Episode 98/100; Loss: 0.032606299966573715\n",
      "Step 91 (7577); Episode 98/100; Loss: 0.0341230146586895\n",
      "Step 92 (7578); Episode 98/100; Loss: 0.0032555360812693834\n",
      "Step 93 (7579); Episode 98/100; Loss: 0.05714898183941841\n",
      "Step 94 (7580); Episode 98/100; Loss: 0.046481888741254807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 95 (7581); Episode 98/100; Loss: 0.044592566788196564\n",
      "Step 96 (7582); Episode 98/100; Loss: 0.03369191288948059\n",
      "Step 97 (7583); Episode 98/100; Loss: 0.0022374005056917667\n",
      "Step 98 (7584); Episode 98/100; Loss: 0.03385808318853378\n",
      "Step 99 (7585); Episode 98/100; Loss: 0.04002619907259941\n",
      "Step 100 (7586); Episode 98/100; Loss: 0.005021529737859964\n",
      "Step 101 (7587); Episode 98/100; Loss: 0.0032933619804680347\n",
      "Step 102 (7588); Episode 98/100; Loss: 0.011600241996347904\n",
      "Step 103 (7589); Episode 98/100; Loss: 0.0013223271816968918\n",
      "Step 104 (7590); Episode 98/100; Loss: 0.0018985470524057746\n",
      "Step 105 (7591); Episode 98/100; Loss: 0.0009650139254517853\n",
      "Step 106 (7592); Episode 98/100; Loss: 0.002693921560421586\n",
      "Step 107 (7593); Episode 98/100; Loss: 0.05055539682507515\n",
      "Step 108 (7594); Episode 98/100; Loss: 0.008589456789195538\n",
      "Step 109 (7595); Episode 98/100; Loss: 0.0015211883001029491\n",
      "Step 110 (7596); Episode 98/100; Loss: 0.0014366392279043794\n",
      "Step 111 (7597); Episode 98/100; Loss: 0.0016525350511074066\n",
      "Step 112 (7598); Episode 98/100; Loss: 0.05779767781496048\n",
      "Step 113 (7599); Episode 98/100; Loss: 0.07988487184047699\n",
      "Step 114 (7600); Episode 98/100; Loss: 0.004154017195105553\n",
      "Step 115 (7601); Episode 98/100; Loss: 0.0019394307164475322\n",
      "Step 116 (7602); Episode 98/100; Loss: 0.00219375710003078\n",
      "Step 117 (7603); Episode 98/100; Loss: 0.045357491821050644\n",
      "Step 118 (7604); Episode 98/100; Loss: 0.045435719192028046\n",
      "Step 119 (7605); Episode 98/100; Loss: 0.08887961506843567\n",
      "Step 120 (7606); Episode 98/100; Loss: 0.002568082418292761\n",
      "Step 121 (7607); Episode 98/100; Loss: 0.08886462450027466\n",
      "Step 122 (7608); Episode 98/100; Loss: 0.03166640177369118\n",
      "Step 123 (7609); Episode 98/100; Loss: 0.01790034770965576\n",
      "Step 124 (7610); Episode 98/100; Loss: 0.06984339654445648\n",
      "Step 125 (7611); Episode 98/100; Loss: 0.048530463129282\n",
      "Step 126 (7612); Episode 98/100; Loss: 0.04257073998451233\n",
      "Step 127 (7613); Episode 98/100; Loss: 0.0018649804405868053\n",
      "Step 128 (7614); Episode 98/100; Loss: 0.0840439647436142\n",
      "Step 129 (7615); Episode 98/100; Loss: 0.0027114166878163815\n",
      "Step 130 (7616); Episode 98/100; Loss: 0.04124017059803009\n",
      "Step 131 (7617); Episode 98/100; Loss: 0.005285962950438261\n",
      "Step 132 (7618); Episode 98/100; Loss: 0.019531764090061188\n",
      "Step 133 (7619); Episode 98/100; Loss: 0.04768054932355881\n",
      "Step 134 (7620); Episode 98/100; Loss: 0.036414191126823425\n",
      "Step 135 (7621); Episode 98/100; Loss: 0.002945115091279149\n",
      "Step 136 (7622); Episode 98/100; Loss: 0.0018711347365751863\n",
      "Step 137 (7623); Episode 98/100; Loss: 0.03243035823106766\n",
      "Step 138 (7624); Episode 98/100; Loss: 0.000746294972486794\n",
      "Step 139 (7625); Episode 98/100; Loss: 0.0019026398658752441\n",
      "Step 140 (7626); Episode 98/100; Loss: 0.0014827734557911754\n",
      "Step 141 (7627); Episode 98/100; Loss: 0.029736870899796486\n",
      "Step 142 (7628); Episode 98/100; Loss: 0.0012226346880197525\n",
      "Step 143 (7629); Episode 98/100; Loss: 0.0746440589427948\n",
      "Step 144 (7630); Episode 98/100; Loss: 0.0032049864530563354\n",
      "Step 145 (7631); Episode 98/100; Loss: 0.0009118462330661714\n",
      "Step 146 (7632); Episode 98/100; Loss: 0.04439959675073624\n",
      "Step 147 (7633); Episode 98/100; Loss: 0.029474878683686256\n",
      "Step 148 (7634); Episode 98/100; Loss: 0.08189766854047775\n",
      "Step 149 (7635); Episode 98/100; Loss: 0.0005720613989979029\n",
      "Step 150 (7636); Episode 98/100; Loss: 0.0012758400989696383\n",
      "Step 151 (7637); Episode 98/100; Loss: 0.04179802164435387\n",
      "Step 152 (7638); Episode 98/100; Loss: 0.00107295298948884\n",
      "Step 153 (7639); Episode 98/100; Loss: 0.0006802038988098502\n",
      "Step 154 (7640); Episode 98/100; Loss: 0.0009782201377674937\n",
      "Step 155 (7641); Episode 98/100; Loss: 0.06138065457344055\n",
      "Step 156 (7642); Episode 98/100; Loss: 0.025001641362905502\n",
      "Step 157 (7643); Episode 98/100; Loss: 0.0012614734005182981\n",
      "Step 158 (7644); Episode 98/100; Loss: 0.003253448288887739\n",
      "Step 159 (7645); Episode 98/100; Loss: 0.002845722483471036\n",
      "Step 160 (7646); Episode 98/100; Loss: 0.0394054539501667\n",
      "Step 161 (7647); Episode 98/100; Loss: 0.020201079547405243\n",
      "Step 162 (7648); Episode 98/100; Loss: 0.044266823679208755\n",
      "Step 163 (7649); Episode 98/100; Loss: 0.08693242818117142\n",
      "Step 164 (7650); Episode 98/100; Loss: 0.04449453949928284\n",
      "Step 165 (7651); Episode 98/100; Loss: 0.04963761568069458\n",
      "Step 166 (7652); Episode 98/100; Loss: 0.0020236754789948463\n",
      "Step 167 (7653); Episode 98/100; Loss: 0.0018768419977277517\n",
      "Step 168 (7654); Episode 98/100; Loss: 0.0022023653145879507\n",
      "Step 169 (7655); Episode 98/100; Loss: 0.035878270864486694\n",
      "Step 170 (7656); Episode 98/100; Loss: 0.010617624968290329\n",
      "Step 171 (7657); Episode 98/100; Loss: 0.04249851405620575\n",
      "Step 172 (7658); Episode 98/100; Loss: 0.038000233471393585\n",
      "Step 173 (7659); Episode 98/100; Loss: 0.000519329565577209\n",
      "Step 174 (7660); Episode 98/100; Loss: 0.04287892207503319\n",
      "Step 175 (7661); Episode 98/100; Loss: 0.042225759476423264\n",
      "Step 176 (7662); Episode 98/100; Loss: 0.018526921048760414\n",
      "Step 177 (7663); Episode 98/100; Loss: 0.0014922787668183446\n",
      "Step 178 (7664); Episode 98/100; Loss: 0.034486398100852966\n",
      "Step 179 (7665); Episode 98/100; Loss: 0.04679684340953827\n",
      "Step 180 (7666); Episode 98/100; Loss: 0.03686587139964104\n",
      "Step 181 (7667); Episode 98/100; Loss: 0.03637886047363281\n",
      "Step 182 (7668); Episode 98/100; Loss: 0.026255913078784943\n",
      "Step 183 (7669); Episode 98/100; Loss: 0.0015174209838733077\n",
      "Step 184 (7670); Episode 98/100; Loss: 0.0036298963241279125\n",
      "Step 185 (7671); Episode 98/100; Loss: 0.0489908829331398\n",
      "Step 186 (7672); Episode 98/100; Loss: 0.029389631003141403\n",
      "Step 187 (7673); Episode 98/100; Loss: 0.0024710206780582666\n",
      "Step 188 (7674); Episode 98/100; Loss: 0.0017431068699806929\n",
      "Step 189 (7675); Episode 98/100; Loss: 0.052957020699977875\n",
      "Step 190 (7676); Episode 98/100; Loss: 0.043456945568323135\n",
      "Step 191 (7677); Episode 98/100; Loss: 0.041894394904375076\n",
      "Step 192 (7678); Episode 98/100; Loss: 0.032360054552555084\n",
      "Step 193 (7679); Episode 98/100; Loss: 0.10115564614534378\n",
      "Step 194 (7680); Episode 98/100; Loss: 0.0017059926176443696\n",
      "Step 195 (7681); Episode 98/100; Loss: 0.043333616107702255\n",
      "Step 196 (7682); Episode 98/100; Loss: 0.0014262706972658634\n",
      "Step 197 (7683); Episode 98/100; Loss: 0.002475589979439974\n",
      "Step 198 (7684); Episode 98/100; Loss: 0.0013819305459037423\n",
      "Step 199 (7685); Episode 98/100; Loss: 0.002971063368022442\n",
      "Step 0 (7686); Episode 99/100; Loss: 0.023708069697022438\n",
      "Step 1 (7687); Episode 99/100; Loss: 0.015926441177725792\n",
      "Step 2 (7688); Episode 99/100; Loss: 0.0551849864423275\n",
      "Step 3 (7689); Episode 99/100; Loss: 0.04471581429243088\n",
      "Step 4 (7690); Episode 99/100; Loss: 0.0738632008433342\n",
      "Step 5 (7691); Episode 99/100; Loss: 0.0007373598637059331\n",
      "Step 6 (7692); Episode 99/100; Loss: 0.008862762711942196\n",
      "Step 7 (7693); Episode 99/100; Loss: 0.03051900491118431\n",
      "Step 8 (7694); Episode 99/100; Loss: 0.002950236899778247\n",
      "Step 9 (7695); Episode 99/100; Loss: 0.002745481440797448\n",
      "Step 10 (7696); Episode 99/100; Loss: 0.005039464216679335\n",
      "Step 11 (7697); Episode 99/100; Loss: 0.0029556595254689455\n",
      "Step 12 (7698); Episode 99/100; Loss: 0.0010652747005224228\n",
      "Step 13 (7699); Episode 99/100; Loss: 0.004170835018157959\n",
      "Step 14 (7700); Episode 99/100; Loss: 0.055920954793691635\n",
      "Step 15 (7701); Episode 99/100; Loss: 0.0015565806534141302\n",
      "Step 16 (7702); Episode 99/100; Loss: 0.008310635574162006\n",
      "Step 17 (7703); Episode 99/100; Loss: 0.038432177156209946\n",
      "Step 18 (7704); Episode 99/100; Loss: 0.02332201600074768\n",
      "Step 19 (7705); Episode 99/100; Loss: 0.0012854010565206409\n",
      "Step 20 (7706); Episode 99/100; Loss: 0.0018459268612787127\n",
      "Step 21 (7707); Episode 99/100; Loss: 0.0880700945854187\n",
      "Step 22 (7708); Episode 99/100; Loss: 0.0030583390034735203\n",
      "Step 23 (7709); Episode 99/100; Loss: 0.0012212976580485702\n",
      "Step 24 (7710); Episode 99/100; Loss: 0.0564713329076767\n",
      "Step 25 (7711); Episode 99/100; Loss: 0.008367342874407768\n",
      "Step 26 (7712); Episode 99/100; Loss: 0.11774379014968872\n",
      "Step 27 (7713); Episode 99/100; Loss: 0.003706858027726412\n",
      "Step 28 (7714); Episode 99/100; Loss: 0.023538589477539062\n",
      "Step 29 (7715); Episode 99/100; Loss: 0.0012296797940507531\n",
      "Step 30 (7716); Episode 99/100; Loss: 0.0015392786590382457\n",
      "Step 31 (7717); Episode 99/100; Loss: 0.04936996474862099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32 (7718); Episode 99/100; Loss: 0.0610634982585907\n",
      "Step 33 (7719); Episode 99/100; Loss: 0.001787516986951232\n",
      "Step 34 (7720); Episode 99/100; Loss: 0.0407174713909626\n",
      "Step 35 (7721); Episode 99/100; Loss: 0.0020722404588013887\n",
      "Step 36 (7722); Episode 99/100; Loss: 0.03327874839305878\n",
      "Step 37 (7723); Episode 99/100; Loss: 0.0326211042702198\n",
      "Step 38 (7724); Episode 99/100; Loss: 0.0021283645182847977\n",
      "Step 39 (7725); Episode 99/100; Loss: 0.12864483892917633\n",
      "Step 40 (7726); Episode 99/100; Loss: 0.049651630222797394\n",
      "Step 41 (7727); Episode 99/100; Loss: 0.0026889685541391373\n",
      "Step 42 (7728); Episode 99/100; Loss: 0.0028590084984898567\n",
      "Step 43 (7729); Episode 99/100; Loss: 0.04207397624850273\n",
      "Step 44 (7730); Episode 99/100; Loss: 0.024389179423451424\n",
      "Step 45 (7731); Episode 99/100; Loss: 0.0009663801756687462\n",
      "Step 46 (7732); Episode 99/100; Loss: 0.03845254331827164\n",
      "Step 47 (7733); Episode 99/100; Loss: 0.0014194432878866792\n",
      "Step 48 (7734); Episode 99/100; Loss: 0.0017747507663443685\n",
      "Step 49 (7735); Episode 99/100; Loss: 0.02655312605202198\n",
      "Step 50 (7736); Episode 99/100; Loss: 0.0023385286331176758\n",
      "Step 51 (7737); Episode 99/100; Loss: 0.001169961062259972\n",
      "Step 52 (7738); Episode 99/100; Loss: 0.002053655683994293\n",
      "Step 53 (7739); Episode 99/100; Loss: 0.05781424045562744\n",
      "Step 54 (7740); Episode 99/100; Loss: 0.037235528230667114\n",
      "Step 55 (7741); Episode 99/100; Loss: 0.0025394433178007603\n",
      "Step 56 (7742); Episode 99/100; Loss: 0.003474688157439232\n",
      "Step 57 (7743); Episode 99/100; Loss: 0.0022113595623522997\n",
      "Step 58 (7744); Episode 99/100; Loss: 0.10317900031805038\n",
      "Step 59 (7745); Episode 99/100; Loss: 0.043365154415369034\n",
      "Step 60 (7746); Episode 99/100; Loss: 0.05170893669128418\n",
      "Step 61 (7747); Episode 99/100; Loss: 0.03958246856927872\n",
      "Step 62 (7748); Episode 99/100; Loss: 0.02304685302078724\n",
      "Step 63 (7749); Episode 99/100; Loss: 0.0045116962864995\n",
      "Step 64 (7750); Episode 99/100; Loss: 0.03935832157731056\n",
      "Step 65 (7751); Episode 99/100; Loss: 0.04755115881562233\n",
      "Step 66 (7752); Episode 99/100; Loss: 0.0038365561049431562\n",
      "Step 67 (7753); Episode 99/100; Loss: 0.0019939797930419445\n",
      "Step 68 (7754); Episode 99/100; Loss: 0.034215692430734634\n",
      "Step 69 (7755); Episode 99/100; Loss: 0.002001053187996149\n",
      "Step 70 (7756); Episode 99/100; Loss: 0.028745610266923904\n",
      "Step 71 (7757); Episode 99/100; Loss: 0.06076621264219284\n",
      "Step 72 (7758); Episode 99/100; Loss: 0.16790544986724854\n",
      "Step 73 (7759); Episode 99/100; Loss: 0.0026294728741049767\n",
      "Step 74 (7760); Episode 99/100; Loss: 0.0009297921787947416\n",
      "Step 75 (7761); Episode 99/100; Loss: 0.0033161777537316084\n",
      "Step 76 (7762); Episode 99/100; Loss: 0.0013813236728310585\n",
      "Step 77 (7763); Episode 99/100; Loss: 0.04320986196398735\n",
      "Step 78 (7764); Episode 99/100; Loss: 0.0011828680289909244\n",
      "Step 79 (7765); Episode 99/100; Loss: 0.027060015127062798\n",
      "Step 80 (7766); Episode 99/100; Loss: 0.0026990880724042654\n",
      "Step 81 (7767); Episode 99/100; Loss: 0.052763283252716064\n",
      "Step 82 (7768); Episode 99/100; Loss: 0.05094219371676445\n",
      "Step 83 (7769); Episode 99/100; Loss: 0.07650645077228546\n",
      "Step 84 (7770); Episode 99/100; Loss: 0.002254500752314925\n",
      "Step 85 (7771); Episode 99/100; Loss: 0.0026780853513628244\n",
      "Step 86 (7772); Episode 99/100; Loss: 0.03926803916692734\n",
      "Step 87 (7773); Episode 99/100; Loss: 0.004676985554397106\n",
      "Step 88 (7774); Episode 99/100; Loss: 0.11663059145212173\n",
      "Step 89 (7775); Episode 99/100; Loss: 0.001466106390580535\n",
      "Step 90 (7776); Episode 99/100; Loss: 0.0016462948406115174\n",
      "Step 91 (7777); Episode 99/100; Loss: 0.003497147234156728\n",
      "Step 92 (7778); Episode 99/100; Loss: 0.05940105393528938\n",
      "Step 93 (7779); Episode 99/100; Loss: 0.001935930224135518\n",
      "Step 94 (7780); Episode 99/100; Loss: 0.0015287745045498013\n",
      "Step 95 (7781); Episode 99/100; Loss: 0.09766297042369843\n",
      "Step 96 (7782); Episode 99/100; Loss: 0.0016926234820857644\n",
      "Step 97 (7783); Episode 99/100; Loss: 0.16134266555309296\n",
      "Step 98 (7784); Episode 99/100; Loss: 0.03028399683535099\n",
      "Step 99 (7785); Episode 99/100; Loss: 0.004312771372497082\n",
      "Step 100 (7786); Episode 99/100; Loss: 0.0425470769405365\n",
      "Step 101 (7787); Episode 99/100; Loss: 0.006478964351117611\n",
      "Step 102 (7788); Episode 99/100; Loss: 0.03548521175980568\n",
      "Step 103 (7789); Episode 99/100; Loss: 0.0023788169492036104\n",
      "Step 104 (7790); Episode 99/100; Loss: 0.03425981104373932\n",
      "Step 105 (7791); Episode 99/100; Loss: 0.001657578512094915\n",
      "Step 106 (7792); Episode 99/100; Loss: 0.0011535778176039457\n",
      "Step 107 (7793); Episode 99/100; Loss: 0.004567243158817291\n",
      "Step 108 (7794); Episode 99/100; Loss: 0.001767840120010078\n",
      "Step 109 (7795); Episode 99/100; Loss: 0.038479726761579514\n",
      "Step 110 (7796); Episode 99/100; Loss: 0.04295549541711807\n",
      "Step 111 (7797); Episode 99/100; Loss: 0.004681506659835577\n",
      "Step 112 (7798); Episode 99/100; Loss: 0.06904991716146469\n",
      "Step 113 (7799); Episode 99/100; Loss: 0.002080867998301983\n",
      "Step 114 (7800); Episode 99/100; Loss: 0.0010047215037047863\n",
      "Step 115 (7801); Episode 99/100; Loss: 0.05871810391545296\n",
      "Step 116 (7802); Episode 99/100; Loss: 0.0010854508727788925\n",
      "Step 117 (7803); Episode 99/100; Loss: 0.0039892434142529964\n",
      "Step 118 (7804); Episode 99/100; Loss: 0.06613920629024506\n",
      "Step 119 (7805); Episode 99/100; Loss: 0.014944947324693203\n",
      "Step 120 (7806); Episode 99/100; Loss: 0.049316294491291046\n",
      "Step 121 (7807); Episode 99/100; Loss: 0.0018517953576520085\n",
      "Step 122 (7808); Episode 99/100; Loss: 0.0012106127105653286\n",
      "Step 123 (7809); Episode 99/100; Loss: 0.002589930547401309\n",
      "Step 124 (7810); Episode 99/100; Loss: 0.001525130937807262\n",
      "Step 125 (7811); Episode 99/100; Loss: 0.08746694773435593\n",
      "Step 126 (7812); Episode 99/100; Loss: 0.0015044192550703883\n",
      "Step 127 (7813); Episode 99/100; Loss: 0.0036601501051336527\n",
      "Step 128 (7814); Episode 99/100; Loss: 0.001168891554698348\n",
      "Step 129 (7815); Episode 99/100; Loss: 0.0032349098473787308\n",
      "Step 130 (7816); Episode 99/100; Loss: 0.007359220180660486\n",
      "Step 131 (7817); Episode 99/100; Loss: 0.07383092492818832\n",
      "Step 132 (7818); Episode 99/100; Loss: 0.022982027381658554\n",
      "Step 133 (7819); Episode 99/100; Loss: 0.042146820574998856\n",
      "Step 134 (7820); Episode 99/100; Loss: 0.03661959245800972\n",
      "Step 135 (7821); Episode 99/100; Loss: 0.0049881949089467525\n",
      "Step 136 (7822); Episode 99/100; Loss: 0.03692436218261719\n",
      "Step 137 (7823); Episode 99/100; Loss: 0.0010776802664622664\n",
      "Step 138 (7824); Episode 99/100; Loss: 0.02395518124103546\n",
      "Step 139 (7825); Episode 99/100; Loss: 0.0009096052963286638\n",
      "Step 140 (7826); Episode 99/100; Loss: 0.03814925625920296\n",
      "Step 141 (7827); Episode 99/100; Loss: 0.00198287027888\n",
      "Step 142 (7828); Episode 99/100; Loss: 0.0012011131038889289\n",
      "Step 143 (7829); Episode 99/100; Loss: 0.002524072304368019\n",
      "Step 144 (7830); Episode 99/100; Loss: 0.0014096306404098868\n",
      "Step 145 (7831); Episode 99/100; Loss: 0.040459729731082916\n",
      "Step 146 (7832); Episode 99/100; Loss: 0.004910092335194349\n",
      "Step 0 (7833); Episode 100/100; Loss: 0.06667102128267288\n",
      "Step 1 (7834); Episode 100/100; Loss: 0.04446595907211304\n",
      "Step 2 (7835); Episode 100/100; Loss: 0.002391635440289974\n",
      "Step 3 (7836); Episode 100/100; Loss: 0.05427036061882973\n",
      "Step 4 (7837); Episode 100/100; Loss: 0.010775678791105747\n",
      "Step 5 (7838); Episode 100/100; Loss: 0.0015393191715702415\n",
      "Step 6 (7839); Episode 100/100; Loss: 0.02557423897087574\n",
      "Step 7 (7840); Episode 100/100; Loss: 0.11885596066713333\n",
      "Step 8 (7841); Episode 100/100; Loss: 0.003220013342797756\n",
      "Step 9 (7842); Episode 100/100; Loss: 0.04933369532227516\n",
      "Step 10 (7843); Episode 100/100; Loss: 0.08147810399532318\n",
      "Step 11 (7844); Episode 100/100; Loss: 0.0014077756786718965\n",
      "Step 12 (7845); Episode 100/100; Loss: 0.025277068838477135\n",
      "Step 13 (7846); Episode 100/100; Loss: 0.04146058112382889\n",
      "Step 14 (7847); Episode 100/100; Loss: 0.035938043147325516\n",
      "Step 15 (7848); Episode 100/100; Loss: 0.0884016752243042\n",
      "Step 16 (7849); Episode 100/100; Loss: 0.028196433559060097\n",
      "Step 17 (7850); Episode 100/100; Loss: 0.0007798600709065795\n",
      "Step 18 (7851); Episode 100/100; Loss: 0.0752696692943573\n",
      "Step 19 (7852); Episode 100/100; Loss: 0.02068164199590683\n",
      "Step 20 (7853); Episode 100/100; Loss: 0.0520547479391098\n",
      "Step 21 (7854); Episode 100/100; Loss: 0.07076963782310486\n",
      "Step 22 (7855); Episode 100/100; Loss: 0.0024881327990442514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23 (7856); Episode 100/100; Loss: 0.07234720140695572\n",
      "Step 24 (7857); Episode 100/100; Loss: 0.035810686647892\n",
      "Step 25 (7858); Episode 100/100; Loss: 0.03484472632408142\n",
      "Step 26 (7859); Episode 100/100; Loss: 0.0023818679619580507\n",
      "Step 27 (7860); Episode 100/100; Loss: 0.0016978128114715219\n",
      "Step 28 (7861); Episode 100/100; Loss: 0.04244667664170265\n",
      "Step 29 (7862); Episode 100/100; Loss: 0.03053847700357437\n",
      "Step 30 (7863); Episode 100/100; Loss: 0.03953789174556732\n",
      "Step 31 (7864); Episode 100/100; Loss: 0.10049447417259216\n",
      "Step 32 (7865); Episode 100/100; Loss: 0.1092921793460846\n",
      "Step 33 (7866); Episode 100/100; Loss: 0.10947369039058685\n",
      "Step 34 (7867); Episode 100/100; Loss: 0.057768236845731735\n",
      "Step 35 (7868); Episode 100/100; Loss: 0.005385757889598608\n",
      "Step 36 (7869); Episode 100/100; Loss: 0.013342886231839657\n",
      "Step 37 (7870); Episode 100/100; Loss: 0.0022010246757417917\n",
      "Step 38 (7871); Episode 100/100; Loss: 0.026870286092162132\n",
      "Step 39 (7872); Episode 100/100; Loss: 0.002876081271097064\n",
      "Step 40 (7873); Episode 100/100; Loss: 0.0713595598936081\n",
      "Step 41 (7874); Episode 100/100; Loss: 0.06759641319513321\n",
      "Step 42 (7875); Episode 100/100; Loss: 0.0016072550788521767\n",
      "Step 43 (7876); Episode 100/100; Loss: 0.13028979301452637\n",
      "Step 44 (7877); Episode 100/100; Loss: 0.0069814263842999935\n",
      "Step 45 (7878); Episode 100/100; Loss: 0.004134423565119505\n",
      "Step 46 (7879); Episode 100/100; Loss: 0.09143529087305069\n",
      "Step 47 (7880); Episode 100/100; Loss: 0.01498790830373764\n",
      "Step 48 (7881); Episode 100/100; Loss: 0.029200375080108643\n",
      "Step 49 (7882); Episode 100/100; Loss: 0.002011579228565097\n",
      "Step 50 (7883); Episode 100/100; Loss: 0.003078358480706811\n",
      "Step 51 (7884); Episode 100/100; Loss: 0.037721335887908936\n",
      "Step 52 (7885); Episode 100/100; Loss: 0.03451291471719742\n",
      "Step 53 (7886); Episode 100/100; Loss: 0.002550860634073615\n",
      "Step 54 (7887); Episode 100/100; Loss: 0.03289567306637764\n",
      "Step 55 (7888); Episode 100/100; Loss: 0.06590769439935684\n",
      "Step 56 (7889); Episode 100/100; Loss: 0.06562645733356476\n",
      "Step 57 (7890); Episode 100/100; Loss: 0.0015212646685540676\n",
      "Step 58 (7891); Episode 100/100; Loss: 0.020911836996674538\n",
      "Step 59 (7892); Episode 100/100; Loss: 0.0034980273339897394\n",
      "Step 60 (7893); Episode 100/100; Loss: 0.0015572073170915246\n",
      "Step 61 (7894); Episode 100/100; Loss: 0.03754903003573418\n",
      "Step 62 (7895); Episode 100/100; Loss: 0.08500687777996063\n",
      "Step 63 (7896); Episode 100/100; Loss: 0.03223532438278198\n",
      "Step 64 (7897); Episode 100/100; Loss: 0.038608819246292114\n",
      "Step 65 (7898); Episode 100/100; Loss: 0.025163616985082626\n",
      "Step 66 (7899); Episode 100/100; Loss: 0.005851414054632187\n",
      "Step 67 (7900); Episode 100/100; Loss: 0.0036894790828227997\n",
      "Step 68 (7901); Episode 100/100; Loss: 0.0024787515867501497\n",
      "Step 69 (7902); Episode 100/100; Loss: 0.023057343438267708\n",
      "Step 70 (7903); Episode 100/100; Loss: 0.04359714314341545\n",
      "Step 71 (7904); Episode 100/100; Loss: 0.001677554682828486\n",
      "Step 72 (7905); Episode 100/100; Loss: 0.001676978194154799\n",
      "Step 73 (7906); Episode 100/100; Loss: 0.0446813739836216\n",
      "Step 74 (7907); Episode 100/100; Loss: 0.041200414299964905\n",
      "Step 75 (7908); Episode 100/100; Loss: 0.0015717349015176296\n",
      "Step 76 (7909); Episode 100/100; Loss: 0.0021108328364789486\n",
      "Step 77 (7910); Episode 100/100; Loss: 0.04981262981891632\n",
      "Step 78 (7911); Episode 100/100; Loss: 0.0627484917640686\n",
      "Step 79 (7912); Episode 100/100; Loss: 0.0009866459295153618\n",
      "Step 80 (7913); Episode 100/100; Loss: 0.0013234487269073725\n",
      "Step 81 (7914); Episode 100/100; Loss: 0.10929567366838455\n",
      "Step 82 (7915); Episode 100/100; Loss: 0.052011456340551376\n",
      "Step 83 (7916); Episode 100/100; Loss: 0.03549924120306969\n",
      "Step 84 (7917); Episode 100/100; Loss: 0.04494921863079071\n",
      "Step 85 (7918); Episode 100/100; Loss: 0.1128428727388382\n",
      "Step 86 (7919); Episode 100/100; Loss: 0.06554340571165085\n",
      "Step 87 (7920); Episode 100/100; Loss: 0.04996943101286888\n",
      "Step 88 (7921); Episode 100/100; Loss: 0.002701051067560911\n",
      "Step 89 (7922); Episode 100/100; Loss: 0.03523930907249451\n",
      "Step 90 (7923); Episode 100/100; Loss: 0.004223152529448271\n",
      "Step 91 (7924); Episode 100/100; Loss: 0.0012718162033706903\n",
      "Step 92 (7925); Episode 100/100; Loss: 0.0024455408565700054\n",
      "Step 93 (7926); Episode 100/100; Loss: 0.048966970294713974\n",
      "Step 94 (7927); Episode 100/100; Loss: 0.0018760005477815866\n",
      "Step 95 (7928); Episode 100/100; Loss: 0.0035620778799057007\n",
      "Step 96 (7929); Episode 100/100; Loss: 0.0018274718895554543\n",
      "Step 97 (7930); Episode 100/100; Loss: 0.04989345371723175\n",
      "Step 98 (7931); Episode 100/100; Loss: 0.004089401103556156\n",
      "Step 99 (7932); Episode 100/100; Loss: 0.05813971534371376\n",
      "Step 100 (7933); Episode 100/100; Loss: 0.005428498610854149\n",
      "Step 101 (7934); Episode 100/100; Loss: 0.00161726213991642\n",
      "Step 102 (7935); Episode 100/100; Loss: 0.11884653568267822\n",
      "Step 103 (7936); Episode 100/100; Loss: 0.001540220226161182\n",
      "Step 104 (7937); Episode 100/100; Loss: 0.05029025673866272\n",
      "Step 105 (7938); Episode 100/100; Loss: 0.0019126955885440111\n",
      "Step 106 (7939); Episode 100/100; Loss: 0.04283319041132927\n",
      "Step 107 (7940); Episode 100/100; Loss: 0.04281827434897423\n",
      "Step 108 (7941); Episode 100/100; Loss: 0.06994334608316422\n",
      "Step 109 (7942); Episode 100/100; Loss: 0.07892958074808121\n",
      "Step 110 (7943); Episode 100/100; Loss: 0.0012282420648261905\n",
      "Step 111 (7944); Episode 100/100; Loss: 0.0014054590137675405\n",
      "Step 112 (7945); Episode 100/100; Loss: 0.06390233337879181\n",
      "Step 113 (7946); Episode 100/100; Loss: 0.0021813956554979086\n",
      "Step 114 (7947); Episode 100/100; Loss: 0.05641333758831024\n",
      "Step 115 (7948); Episode 100/100; Loss: 0.047049976885318756\n",
      "Step 116 (7949); Episode 100/100; Loss: 0.0035186870954930782\n",
      "Step 117 (7950); Episode 100/100; Loss: 0.03842576965689659\n",
      "Step 118 (7951); Episode 100/100; Loss: 0.03164636716246605\n",
      "Step 119 (7952); Episode 100/100; Loss: 0.041695330291986465\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations = run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70d16eb61eae34605e8d7813a70a604a",
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f5e85e8aa15e9cb9117b17265435eae",
     "grade": false,
     "grade_id": "cell-6607b79e73a101a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Policy Gradient (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "951b88e9cd8396d088d3f80e6da9690c",
     "grade": false,
     "grade_id": "cell-083fe71da94aa7aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So we have spent a lot of time working on *value based* methods. We will now switch to *policy based* methods, i.e. learn a policy directly rather than learn a value function from which the policy follows. Mention two advantages of using a policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5c1f505cb22eca6eb3b8213ff23e60f",
     "grade": true,
     "grade_id": "cell-134510705650d5ac",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "- Policy based methods learn action specific probabilities, which is an advantage over value based methods, as the latter only allows the introduction of an $\\epsilon$ or plain greedy policy. This attributes to acting randomly or greedy. Policy based methods on the other hand are more flexible and therefore easen the trade-off between exploration and exploitation.\n",
    "\n",
    "- Policy based methods allow to take advantage of prior knowledge in the form of policy parametrization. This is helpful, if knowledge about a possible optimal policy exists, which then easens the estimation, as it can be improved directly. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "233ca94abc32f0e510c5d8a164206d05",
     "grade": false,
     "grade_id": "cell-76a10fe31897025f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3.1 Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2bc16b45e6145226b8a6f5117003b7f5",
     "grade": false,
     "grade_id": "cell-34f0712f792bbcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to do so, we will implement a Policy network. Although in general this does not have to be the case, we will use an architecture very similar to the Q-network (two layers with ReLU activation for the hidden layer). Since we have discrete actions, our model will output one value per action, where each value represents the (normalized!) log-probability of selecting that action. *Use the (log-)softmax activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "155baf230fd6deb5f6ccf93138fa3419",
     "grade": false,
     "grade_id": "cell-6a31440f9477f963",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.l2((torch.relu(self.l1(x))))\n",
    "            x = F.log_softmax(x, dim=x.ndim - 1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3cb94e04b03fa4b663bcf38a96ef656d",
     "grade": true,
     "grade_id": "cell-9d280fe6520edc91",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4578, 0.5422],\n",
      "        [0.4657, 0.5343],\n",
      "        [0.4563, 0.5437],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.4564, 0.5436],\n",
      "        [0.4725, 0.5275],\n",
      "        [0.4769, 0.5231],\n",
      "        [0.4834, 0.5166],\n",
      "        [0.4797, 0.5203],\n",
      "        [0.4618, 0.5382]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "log_p = model(x)\n",
    "\n",
    "# Does the outcome make sense?\n",
    "print(log_p.exp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b0ff099a335c248a91df00e975494d0",
     "grade": false,
     "grade_id": "cell-35294ca4eda15b11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3.2 Monte Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93ed9cbcf70541f5a04709ee89a16e78",
     "grade": false,
     "grade_id": "cell-44f33e587542974d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the *Monte Carlo* policy gradient algorithm. Remember from lab 1 that this means that we will estimate returns for states by sample episodes. Compared to DQN, this means that we do *not* perform an update step at every environment step, but only at the end of each episode. This means that we should generate an episode of data, compute the REINFORCE loss (which requires computing the returns) and then perform a gradient step.\n",
    "\n",
    "To help you, we already implemented a few functions that you can (but do not have to) use.\n",
    "\n",
    "* You can use `torch.multinomial` to sample from a categorical distribution.\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$. Make sure that you do this in **linear time**, otherwise your algorithm will be very slow! Note the - (minus) since you want to maximize return while you want to minimize the loss.\n",
    "* Importantly, you should **normalize the returns** (not the rewards!, e.g. subtract mean and divide by standard deviation within the episode) before computing the loss, or your estimator will have very high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b2c75181678fed25fcc7c8b39bb7de3",
     "grade": true,
     "grade_id": "cell-3f6e32c4931392bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    log_p = model(torch.FloatTensor(state))\n",
    "    action = torch.multinomial(log_p.exp(), 1).item()\n",
    "    \n",
    "    return action, log_p[action]\n",
    "\n",
    "def run_episode(env, model):\n",
    "    # save episode\n",
    "    episode = []\n",
    "    # get initial state\n",
    "    s = env.reset()\n",
    "    # termination condition\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # get action and log_p\n",
    "        a, log_p = select_action(model, s)\n",
    "        # take action and get next state, terminal condition\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        # append rewards and log_p as tuple to episode list\n",
    "        episode.append((r, log_p))\n",
    "        # approach next state if not terminal\n",
    "        s = s_next\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def compute_reinforce_loss(episode, discount_factor):\n",
    "    # Compute the reinforce loss\n",
    "    # Make sure that your function runs in LINEAR TIME\n",
    "    # Don't forget to normalize your RETURNS (not rewards)\n",
    "    # Note that the rewards/returns should be maximized \n",
    "    # while the loss should be minimized so you need a - somewhere\n",
    "    \n",
    "    # get log_ps out of episode list \n",
    "    _, log_ps = zip(*episode)\n",
    "\n",
    "    # initialize   \n",
    "    log_ps = torch.stack(log_ps)\n",
    "    returns = []\n",
    "    tmp_return = 0.0\n",
    "    \n",
    "    # get return by reversely iterating through episodes \n",
    "    for r, _ in reversed(episode):\n",
    "        # new return is reward + discount*previous return\n",
    "        tmp_return = r + discount_factor * tmp_return\n",
    "        returns.append(tmp_return)\n",
    "                       \n",
    "    # reverse to correct order\n",
    "    returns = torch.FloatTensor(returns[::-1])\n",
    "    # normalize returns\n",
    "    returns = (returns - returns.mean())/ returns.std()\n",
    "    \n",
    "    return - torch.sum(returns * log_ps)\n",
    "    \n",
    "    \n",
    "def run_episodes_policy_gradient(model, env, num_episodes, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        episode = run_episode(env, model)\n",
    "        loss = compute_reinforce_loss(episode, discount_factor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                           \n",
    "        if i % 10 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(i, len(episode), '\\033[92m' if len(episode) >= 195 else '\\033[99m'))\n",
    "        episode_durations.append(len(episode))\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0 finished after 19 steps\n",
      " Episode 10 finished after 17 steps\n",
      " Episode 20 finished after 27 steps\n",
      " Episode 30 finished after 34 steps\n",
      " Episode 40 finished after 200 steps\n",
      " Episode 50 finished after 71 steps\n",
      " Episode 60 finished after 57 steps\n",
      " Episode 70 finished after 33 steps\n",
      " Episode 80 finished after 41 steps\n",
      " Episode 90 finished after 56 steps\n",
      " Episode 100 finished after 132 steps\n",
      " Episode 110 finished after 35 steps\n",
      " Episode 120 finished after 66 steps\n",
      " Episode 130 finished after 38 steps\n",
      " Episode 140 finished after 73 steps\n",
      " Episode 150 finished after 140 steps\n",
      " Episode 160 finished after 200 steps\n",
      " Episode 170 finished after 200 steps\n",
      " Episode 180 finished after 200 steps\n",
      " Episode 190 finished after 200 steps\n",
      " Episode 200 finished after 200 steps\n",
      " Episode 210 finished after 200 steps\n",
      " Episode 220 finished after 200 steps\n",
      " Episode 230 finished after 100 steps\n",
      " Episode 240 finished after 70 steps\n",
      " Episode 250 finished after 41 steps\n",
      " Episode 260 finished after 53 steps\n",
      " Episode 270 finished after 40 steps\n",
      " Episode 280 finished after 51 steps\n",
      " Episode 290 finished after 56 steps\n",
      " Episode 300 finished after 52 steps\n",
      " Episode 310 finished after 113 steps\n",
      " Episode 320 finished after 173 steps\n",
      " Episode 330 finished after 200 steps\n",
      " Episode 340 finished after 200 steps\n",
      " Episode 350 finished after 200 steps\n",
      " Episode 360 finished after 200 steps\n",
      " Episode 370 finished after 200 steps\n",
      " Episode 380 finished after 200 steps\n",
      " Episode 390 finished after 200 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x158d8abed68>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feel free to play around with the parameters!\n",
    "num_episodes = 400\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.01\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(\n",
    "    model, env, num_episodes, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "454f1fb392b88af636d085896efb2aad",
     "grade": false,
     "grade_id": "cell-ad1138b69e6728a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4. Deep Reinforcement Learning (5 bonus points)\n",
    "Note that so far we used the state variables as input. However, the true power of Deep Learning is that we can directly learn from raw inputs, e.g. we can learn to balance the cart pole *by just looking at the screen*. This probably means that you need a deep(er) (convolutional) network, as well as tweaking some parameters, running for more iterations (perhaps on GPU) and do other tricks to stabilize learning. Can you get this to work? This will earn you bonus points!\n",
    "\n",
    "Hints:\n",
    "* You may want to use [Google Colab](https://colab.research.google.com/) such that you can benefit from GPU acceleration.\n",
    "* Even if you don't use Colab, save the weights of your final model and load it in the code here (see example below). Hand in the model file with the .ipynb in a .zip. We likely won't be able to run your training code during grading!\n",
    "* Preprocessing is already done for you, and the observation is the difference between two consequtive frames such that the model can 'see' (angular) speed from a single image. Now do you see why we (sometimes) use the word observation (and not state)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f660e1484fe2bf60d66467326eacb1ba",
     "grade": false,
     "grade_id": "cell-9c9dfa80827c5680",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class CartPoleRawEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('CartPole-v0', *args, **kwargs)  #.unwrapped\n",
    "        self.action_space = self._env.action_space\n",
    "        screen_height, screen_width = 40, 80  # TODO\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        return self._env.seed(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        self.prev_screen = self.screen = self.get_screen()\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        s, r, done, info = self._env.step(action)\n",
    "        self.prev_screen = self.screen\n",
    "        self.screen = self.get_screen()\n",
    "        return self._get_observation(), r, done, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.screen - self.prev_screen\n",
    "    \n",
    "    def _get_cart_location(self, screen_width):\n",
    "        _env = self._env.unwrapped\n",
    "        world_width = _env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(_env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self._env.unwrapped.render(mode='rgb_array').transpose(\n",
    "            (2, 0, 1))  # transpose into torch order (CHW)\n",
    "        # Strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, screen_height * 4 // 10:screen_height * 8 // 10]\n",
    "        view_width = screen_height * 8 // 10\n",
    "        cart_location = self._get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescare, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        #return screen.unsqueeze(0).to(device)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        return self._env.close()\n",
    "\n",
    "raw_env = CartPoleRawEnv()\n",
    "s = raw_env.reset()\n",
    "\n",
    "# \n",
    "s, r, done, _ = raw_env.step(env.action_space.sample())\n",
    "\n",
    "raw_env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(raw_env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n",
    "# Observations are (-1, 1) while we need to plot (0, 1) so show (rgb + 1) / 2\n",
    "plt.figure()\n",
    "plt.imshow((s.cpu().squeeze(0).permute(1, 2, 0).numpy() + 1) / 2,\n",
    "           interpolation='none')\n",
    "plt.title('Example observation')\n",
    "plt.show()\n",
    "raw_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from weights.pt\n"
     ]
    }
   ],
   "source": [
    "# Maybe you should make it a bit deeper?\n",
    "class DeepPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(40 * 80 * 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        return F.log_softmax(self.l1(x.view(x.size(0), -1)), -1)\n",
    "    \n",
    "policy = DeepPolicy()\n",
    "filename = 'weights.pt'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(f\"Loading weights from {filename}\")\n",
    "    weights = torch.load(filename, map_location='cpu')\n",
    "    \n",
    "    policy.load_state_dict(weights['policy'])\n",
    "    \n",
    "else:\n",
    "    # Train\n",
    "    \n",
    "    ### TODO some training here, maybe? Or run this on a different machine?\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    print(f\"Saving weights to {filename}\")\n",
    "    torch.save({\n",
    "        # You can add more here if you need, e.g. critic\n",
    "        'policy': policy.state_dict()  # Always save weights rather than objects\n",
    "    },\n",
    "    filename)\n",
    "    \n",
    "def bonus_get_action(x):\n",
    "    return policy(x).exp().multinomial(1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4502e425cdd9d5db2ec0e9e8e972fa0b",
     "grade": true,
     "grade_id": "cell-0d7bd58a23fdfabb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO29eXxbd5X3/znaLHmR5N2yHSexszlNk6ZN2yS0aUsXSgtDGXYolH1Y5jUwDM/AMM8zP2Z4mAF+zMrMwHQGhtKylIEBOtBCS1e6JG2aNKmzOLud2JY32ZIsWZIlfZ8/7r2OokiWLN17da9y3q+XX5Z0r3SPruWPzj3fs5AQAgzDMIz5sFTaAIZhGKY0WMAZhmFMCgs4wzCMSWEBZxiGMSks4AzDMCaFBZxhGMaksIAzukJEjxDRPSq/5heJ6AGVXuu7RPR/1XitIo/3HiJ6VK/jMdUFCzizbIjoDBHNE9Fcxs8/F/NcIcTrhRD3aW2jESGiVUQkiMimPCaE+L4Q4rZK2sWYF1vhXRgmJ28UQvy20kYYCSKyCiFSlbaDuXRgD5xRFSJ6PxE9R0TfIKIgER0lopsztj9FRB+Wb68hoqfl/aaI6MGM/XYS0UvytpeIaGfGttXy88JE9BiAliwbthPR80Q0S0QHiOjGJezdSkT75Nd6EIAz6708m7W/IKI18u3vEtE3iehhIooAuImI7iSi/UQUIqKzRPTFjKc/I/+ela9admQfo8D7foqIviSf3zARPUpELfI2JxE9QETT8vt+iYjal/hTMVUACzijBdcCOAVJWP8/AP9NRE059vsSgEcBNALoBvANAJD3/RWAfwLQDODvAPyKiJrl5/0AwMvy638JwGJMnYi65Of+XwBNAD4L4KdE1Jp9cCJyAPg5gPvlff8LwFuW+V7fDeDLABoAPAsgAuB9ALwA7gTwcSK6S953l/zbK4SoF0K8kGVPofetHO8DANoAOOT3B/kceACskJ/7MQDzy3wvjMlgAWdK5eeyp6f8fCRj2wSAfxBCLAghHgQwCEnMslkAsBJApxAiJoRQPNE7ARwXQtwvhEgKIX4I4CiANxJRD4CrAfwfIURcCPEMgP/JeM27ATwshHhYCJEWQjwGYC+AO3IcfzsAe4atPwHw0jLPwy+EEM/Jx4oJIZ4SQrwq3z8I4IcAbijytfK+74x9/lMIcUwIMQ/gxwCukB9fgCTca4QQKSHEy0KI0DLfC2MyWMCZUrlLCOHN+Pn3jG0j4sIuaUMAOnO8xp8CIAAvEtEhIvqg/Hin/JxMhgB0ydtmhBCRrG0KKwG8LfPLBcB1AHw5jt+Zx9blcDbzDhFdS0RPEtEkEQUhecItuZ+a055871vBn3E7CqBevn0/gN8A+BERjRLR14jIXuybYMwJCzijBV1ERBn3ewCMZu8khPALIT4ihOgE8AcA/lWOL49CEmJkvcYIgDEAjURUl7VN4SyA+7O+XOqEEF/JYedYHlsVIgBqlTtE1JHjNbLbef4AwEMAVgghPAC+BelLKte+2Sz1vpdEvoL4SyHERgA7AbwBUiiHqWJYwBktaAPwR0RkJ6K3AegH8HD2TkT0NiLqlu/OQBK4lLzvOiJ6NxHZiOgdADYC+KUQYghSSOQvichBRNfhwhDDA5BCLa8jIqu8uHdjxnEyeQFAUrbVRkS/D+CajO0HAFxGRFcQkRPAF4t47w0AAkKIGBFdAylmrTAJIA2gN89z877vQgclopuI6HIisgIIQQqpcEZMlcMCzpTK/9CFeeA/y9i2B8BaAFOQFvjeKoSYzvEaVwPYQ0RzkLzWTwkhTsv7vgHAnwCYhhRqeYMQYkp+3rshLZQGIC2Sfk95QSHEWQBvAvAFSIJ5FsD/Qo7PuhAiAeD3Abwf0hfIOwD8d8b2YwD+CsBvARyHtEhZiE8A+CsiCgP4C0hxauX1ovL5eE4O72zPsqfQ+16KDgA/gSTeRwA8DenLjKliiAc6MGpCRO8H8GEhxHWVtoVhqh32wBmGYUwKCzjDMIxJ4RAKwzCMSWEPnGEYxqTo2syqpaVFrFq1Ss9DMgzDmJ6XX355SghxUTsIXQV81apV2Lt3r56HZBiGMT1ElLNCuCgBJ6IzAMKQCgOSQohtcuOdBwGsAnAGwNuFEDNqGMswDMMUZjkx8JuEEFcIIbbJ9z8P4HEhxFoAj8v3GYZhGJ0oZxHzTQCUySr3AbhriX0ZhmEYlSlWwAWAR4noZSL6qPxYuxBiDADk3225nkhEHyWivUS0d3JysnyLGYZhGADFL2K+RggxSkRtAB4joqPFHkAIcS+AewFg27ZtnHTOMAyjEkV54EKIUfn3BICfQerYNk5EPgCQf09oZSTDMAxzMQUFnIjqiKhBuQ3gNgADkLrHKaOs7gHwC62MZBiGYS6mmBBKO4CfyT3vbQB+IIT4NRG9BODHRPQhAMMA3qadmeVxeDSESCKJq1flGsvIMAxjTgoKuBDiFIAtOR6fBnDzxc8wHl9/dBCnJufw1P+6qdKmMAzDqMYl0Qtlai6O4UAU8SQPKGEYpnq4JAR8ei6BtADOBqKVNoVhGEY1LgkBD0QSAICTk5ECezIMw5iHqhfw+UQK8wtS6OQUCzjDMFVE1Qt4IJpYvH16aq6CljAMw6hL9Qv43HkBZw+cYZhqouoFfDoSBwCsa6/HqSkWcIZhqoeqF/AZOYRy1comBCIJzGaEVBiGYcxM1Qv4tBxCuXpVIwCwF84wTNVQ9QIeiCRgsxC2rPAC4Dg4wzDVwyUh4I11DvQ01cJmIc5EYRimargkBLyp1gG71YKeplr2wBmGqRouDQGvcwAAelvrWMAZhqkaLg0Br1cEvB6npyNIp3kwEMMw5qfqBXw6kkCz7IGvbqlDIpnGyOx8ha1iGIYpn6oW8GQqjeD8AhprZQ+8pQ4ApxIyDFMdVLWAz0QXAADNGSEUADg9yZkoDMOYn6oWcKWNrLKI2VLvQEONjT1whmGKRggBfzBWaTNyUtUCrvRBUQSciDgThWGYZfHo4XFc99UnDCniVS3gMxEphKIIOCBnorAHzjBMkZyYmEMyLQypG1Ut4IEsDxyQMlFGZucxn+D5mAzDFEbxvP0h42WvVbWAT8sxcCULBZCKeQAY8tuUYRjj4Q9JAj7GIRR9CUQS8LjssFvPv83eFjkThQWcYZgiUDzwsVkWcF3JLKNXWNVSCwA4xamEDMMUAXvgFSKXgNc6bOj0ODmVkGGYgiyk0piak9bSOAauM7kEHABWt9axgDMMU5CJcBxCAA6bhUMoepPZByWT3pZ6nJqcgxDc1IphmPz4g5LXvanTjelIArEFY2WvVa2ACyEwIw9zyKa3tQ7hWBJTczwfk2GY/PiDUvhka480knEiFK+kORdRtQIeiiWRTIucHvjqFk4lZBimMGOyB761RxrJOBo0Vhy8agU8uw9KJn1yUyvORGEYZinGQzHU2CzY0NEAAIYrp69iAb+4ClOh0+uCw2bhhUyGYZbEH4rD53HC53EBMF4qYRUL+MV9UBSsFsKqZp6PyTDM0viD82h3O1FXY4PbaVsMqRiFKhbw/B44IGei8IR6hmGWwB+KwedxAgB8Hhd74Hqh9EFprqvJub23tQ7D01EspNJ6msUwjEkQQmA8GEe7LOAdHid74HoRmEvAZbfC5bDm3L66pQ7JtMC5GWP9QRiGMQaBSAKJVBodbknAO71OXsTUi0A0dxWmQi9nojAMswRKuEQJoXS4XZiaSyCeNE4xT/UKeJ4yeoU+ua0sL2QyDJOLcbmJVbvsgfu80u/xoHGKeS5ZAffWOtBYa+dUQoZhcqJ0IVRSCBVP3Ehx8KoV8Om53H1QMultrecQCsMwOfEHY7CQNAwdOC/girAbgaIFnIisRLSfiH4p319NRHuI6DgRPUhES6ulzsxEc/dByaS3hbsSMgyTG38whrYGJ2zyQJgO2RMfNVBXwuV44J8CcCTj/lcB/L0QYi2AGQAfUtOwcogtpBBNpJYMoQBSW9nJcBzh2IJOljEMYxb8odhiCiEA1NfY0OC0LXYoNAJFCTgRdQO4E8B/yPcJwGsB/ETe5T4Ad2lhYCmczwEv5IHzeLViSKeFoVbeGUYP/MEYOtwX1pH4PE5DFfMU64H/A4A/BaBUvTQDmBVCJOX75wB0qWxbyQTm8jeyyoQzUYrj648O4ta/e4ZFnLmk8AdjiwuYCkarxiwo4ET0BgATQoiXMx/OsWvO6QhE9FEi2ktEeycnJ0s0c3kEosUJeE9zLSwEjoMX4IVT0xgORPHTl0cqbQrD6MJcPIlwPLmYQqhgRg/8NQB+j4jOAPgRpNDJPwDwEpFN3qcbwGiuJwsh7hVCbBNCbGttbVXB5MIU6oOiUGOzoruxljNRliCZSuPIWAgA8K2nTyLJrQeYSwB/VhGPQofHiam5OBJJY/wfFBRwIcSfCSG6hRCrALwTwBNCiPcAeBLAW+Xd7gHwC82sXCbTc0v3Qcmkt7WOQyhLcGoqgthCGrdf1oHhQBS/enWs0iYxjOZkF/EodMohlXGDpBKWkwf+OQCfIaITkGLi31bHpPIJRBKwWghul63gvqtb6nB6KsLzMfMwMBIEAPzxreuwpq0e33zqJJ8rpupZygMHjNMXfFkCLoR4SgjxBvn2KSHENUKINUKItwkhDFNfOhNNoLHWASlZZml6W+sxv5AyVHK+kRgYCcFpt2BNWz0+fkMfjvrDeOLoRKXNYhhNUfSgw3NxDBwwTjVmVVZiFlOFqdDXwpkoSzEwGsRGnxtWC+H3ruhEl9eFf3nyBHvhTFXjD8bgrbXDab+wm6nPa6zJPFUp4IX6oGSyWkkl5EyUi0inBQ6PhrCpywMAsFst+IMberFveBZ7TgcqbB3DaMdYMLbYRjaT+hobGmpshmkrW70CXl+cgHe4nah1WDkTJQdDgSjm4kls6vQsPvb2bSvQUu/Avzx5ooKWMYy2jIdiFy1gKhhpsEN1Cng0gaba4gSciLC6hTNRcqEsYF7W5V58zGm34oPXrcbvjk/h1XPBSpnGMJoyFoxdtICp4PMap5in6gQ8mUpjNrpQdAgFOJ+JwlzIwGgQDqsFa9saLnj87u0r0eC04V+fYi+cqT4WUmlMR+J5PXCf2zjFPFUn4DNRqTFVc5EhFEDKRDk3E+VS8SwOjYSwvqMBDtuFHxO304737ViJXx/y48QEh56Y6mIiHIcQF6cQKvi8xinmqToBD0SKK6PPpK+1DmkBDE1HtTLLdAghMDAaxKaM8EkmH3jNatTYLPjW0yd1toxhtEXpNtieT8A9TghhjGKe6hXwImPggBRCATiVMJOR2XnMRhdwWcYCZiYt9TV459U9+Pn+EYzMGmNBh2HUwC+PTMvngSt9wY1QO1K9Ar6MEMqigE9xOEBhYETqf6KkEObiI7t6AQD//swpXWxiGD1QMkxypRECQKcs7KMGcFyqUMCLa2SVSYPTjraGGvbAMzg0GoTVQtjQ0ZB3ny6vC3dt7cKPXhrG1JxhCnEZpizGQzHU2CzwuOw5tyvVmUbIBa86AVeGOTQuI4QCcCZKNgMjQaxtq7+oEi2bj93Qh3gyjf987rROljGMtigphPlacTQ47aivsRkiE6XqBHwmkoDbaYPdury3xgOOL2RgNJQ3/p3JmrZ63H5ZB773whBCPJqOqQKWKuJR8BmkmKfqBHw6kkBzfeE2stn0ttRhJrqAGdmDv5SZCMUwGY7nzUDJ5hM3rkE4lsQDu4c0toxhtMcfyl/Eo9DhcXIIRQuW0wclk17uibLIq3IF5lILmJlc3u3B9Wtb8J1nTyO2wLn0jHkRQmA8GM+bQqhglMk8LOAyva3SgGMOo0gZKERAv684DxwAPnnTGkzNJfDjvWc1tIxhtCUQSSCRSsNXMITiwqQBinmqU8CXuYAJAN2NLtgsxB44pBL61S11qK8pPBBD4drVTbiyx4t/e/oUFnjsGmNSFK86uw94Nkoxz0S4sl54VQm4EAIz0eI7EWZit1rQ01yL05xKiEMjwQs6EBYDEeGTN63ByOw8Hnol53hUhjE844uDHFxL7meUVMKqEvBQLImFlCh6mEM2vS31l3wxz/RcHKPBWNELmJm8dkMbNnQ04JtPn0Q6zQMfGPOx6IEXCKF0yoMdRlnA1SNQYg64Qm9rHc5MR5G6hMXn0KhcgblMDxyQvPCP39iHExNzePTwuNqmMYzmjIdisBDQUuAq/rwHXtlUwqoU8FJCKICUSphIpg1RIlspBkblHuAlCDgA3Hm5Dyuba/HNp3jsGmM+xoIxtDU4YStQR9JQY0Odw1rxTJSqFPCSQyhyJsrJSzgT5dBICCuaXPDU5i4jLoTNasEf7OrDgXNBPHdiWmXrGEZbxkOxgimEgHS16fO6MDbLAq4apfRByYS7Ekoe+OVF5n/n4y1XdaGtoYYHPjCmwx+MFUwhVPB5nBircEfCqhLw6RJ6gWfSUu9Ag9N2yfZECc4vYGg6WnL4RKHGZsVHru/F8yensX94RiXrGEZ7/MFYwRRCBZ/HibEKh1urSsBnIgk47RbUOorPX86EiKSeKJdoJsrh0cItZIvl3df2wOOy41+f4oEPjDmYiycRjieLFvAOuZinknUPVSXg05EEmuuW3wclk95LeMDxocUFzOWnEGZTV2PD+3euwmOHxzHoD5f9egyjNf4iUwgVzhfzVK6VclUJeKll9Jn0ttRhLBhDNJFUySrzMDAShM/jREsJzcBy8f6dq1DrsPLYNcYUKEU8hToRKigNryoZRqk6AW8sV8DlTJRLMQ5ebAvZYmmsc+Dd1/TgoQOjOBvgeaOMsVFSAgt1IlTwydWalUwlrDoBLzWFUOFSzUSJJpI4OTlXUgXmUnz4+l5YifBvz7AXzhib82X0xcbAK19OX3UCXm4IRRHwS80DPzIWghClVWAuRYfHibu2duKnL49UvHMbwyyFPxiDt9ZecAqVgtspFfOMVrAas2oEPLaQQjSRKlvAXQ4ruryuS66tbDFDjEvltRvaMb+QwoFzs6q/NsOoxVgwVvQCJiBlrVV6sEPVCHi5OeCZrG6pu+Tayg6MBNFS70C7W50FzEy29zaBCHjhJFdmMsZlPFR8DriCz+PiGLgazKgo4L2tdTg9GbmkenkoC5j5BrmWg7fWgY0+N54/OaX6azOMWizXAwcqPxuzagR8usw+KJn0ttQhHE9icq5y+Z16EltI4fh4WPUFzEx29jVj3/Asj1xjDEkimcZ0JF6CB+7ERLhyxTxVI+Dl9kHJZPXieLVLI4xybDyMZFqovoCZyY6+ZiSSaewb4tJ6xnhMhGMQovgiHoUOjwtCAJMVKuapGgGfnlMxhHKJZaJouYCpcPWqJlgthBdOcRycMR6LRTzL9cC9cjFPhcIoVSPgM9EErBaC21laG9RMurwuOGyWSyYTZWA0CLfThu7GpcdIlUOD047Luzy8kMkYkuUW8SgsVmNWaCGzagQ8EEmgsdYBi6X8RTiLhbC6+dLpiXJoJIhNXdosYGayo68Zr5ydRSR+6bUpYIzNcvugKPjcrguerzdVI+DTc+VXYWbS21p3SYRQFlJpHPGHNQ2fKOzsa0YyLbCX4+CMwRgPxeC0W+BxLe8K3u2yodZhxWiFBjtUjYBLfVDKD58o9LbWYTgQrWirSD04MTGHRDKtSgfCQmxb2QS7lTidkDEcSgrhcq9CF4t5QhwDL4tAtPxWspmsbqlHMi0wXOVNmAZGpBayenjgLocVW1c0YjfHwRmDUUoRj4LP4zSuB05ETiJ6kYgOENEhIvpL+fHVRLSHiI4T0YNEpF78ogTU6IOSSW+rnIlS5XHwQ6Mh1DmsWN1cp8vxtvc149WRIEKxBV2OxzDFUEoRj4LP4zJ0DDwO4LVCiC0ArgBwOxFtB/BVAH8vhFgLYAbAh7Qzc2mSqTRmowvqCrjSlbDKp/MMjASxsdOtyuJvMezsa0ZaAC+eCuhyPIYpRDotMBGKLzuFUEEq5okhWYFwa0EBFxKKitnlHwHgtQB+Ij9+H4C7NLGwCGaikjenpoB7ax1oqnNUdSZKKi1weEzdHuCF2NrjRY3NwvngjGEIRBNIpNJFDzPOxudxIV2hyTxFxcCJyEpErwCYAPAYgJMAZoUQSj7YOQBdeZ77USLaS0R7Jycn1bD5Imai6hXxZNJb5U2tTk/NIZpI6RL/VqixWXHVykY8z3FwxiAsphCW4YEDlckFL0rAhRApIcQVALoBXAOgP9dueZ57rxBimxBiW2tra+mWLoFShalmGiEgxcGr2QM/X4GpfQZKJjv7mnFkLLTYgIxhKsn5QQ6lFbJVcrDDsrJQhBCzAJ4CsB2Al4iU8e/dAEbVNa14Akonwnp1BXx1Sz2m5uJVu+A2MBJEjc2CNXLvF73Y0dcMANjNYRTGAIyVWMSj0Lk4Wk3/VMJislBaicgr33YBuAXAEQBPAnirvNs9AH6hlZGFWGxkVau+Bw5UbybKwGgQG3xu2Kz6ZpNu7vai1mHlOHiRCCEuqdbGejMeisFqIbQ2lJaG7HbZ4LJbDRtC8QF4kogOAngJwGNCiF8C+ByAzxDRCQDNAL6tnZlLE4hIHnK5A42zqeZMlHRa4NBICJt0KODJxm614OpVTRwHL5L/evkcrv3rxxFPciteLRgLxtBaXwNriZlYRARfhSbz2ArtIIQ4CGBrjsdPQYqHV5xAJA630wa7yp5kT3MtLFSdbWXPzkQRjid1XcDMZGdfM/7mkaOYCMfQ1lDapeulwgsnpzERjuPExJyuGUOXCuUU8Sj4vM6KzMasikrM6UgCzfXqjwKrsVmxoqm2KjNRFhcwKyQI5+PgnA9eiKP+MADg8GiowpZUJ+UU8Sh0uCtTzFMVAi51IlSvD0omq1uqMxNlYDQIu5WwrkPfBUyFyzo9aHDa8AL3RVmSZCqNkxNSCO/IWLjC1lQn40EVPHB5Mo/exTxVI+BNKvZByaS3pR6np+aQTlfXItLASBDr2htQY7NW5PhWC+Ha1c3cH7wAZ6YjSMiicGSMPXC1mYsnEY4nVQmhpNJC9zGMVSPgaueAK/S21iG2kIY/VLnJ02ojhMCh0VDFwicKO/qacWY6itHZyg2FNTpK+GRLtwdH/CHORlGZUvuAZ1OpYh7TC7gQAjPRhOo54AqLmShVFEYZC8YQiCR0L+DJZqccB2cvPD+D/jCsFsIbt3RiNrpQsckv1cr5Ip7yY+CA/sU8phfwUCyJhZRQPQdcoVcZcFxFqYRKC9nLKpSBorC+vQGNtXZOJ1yCo/4wVjXX4ooVXgAcRlGbcot4FDrl2Zh6X02aXsCVcmy1+6AotLtrUOuwVpUHPjAagoWA/o7KeuAWC2FHXzN2n5rm0EAejo2HsaHDjQ0+6W/FAq4uanngHpcdTruFPfDlMq1RGb0CEUmZKFWUSnhoJIg1bfVwOSqzgJnJjt5mjMzO42yA4+DZRBNJDAeiWN/RgPoaG3qaanHYhAIeji3giaPjlTYjJ2PBeXhr7XDay/tfkIp5XBjTea3M9AKu9EHRahETkHKl954JYCJcHfHHgdFgxRcwFZR8cB6zdjHHxucgBLCuvQEA0O9rMGUq4fdeGMIHv7sXJyaMZ7s/GC87fKLg8zgxxiGU5aH0QWnUKAYOAB+7sQ+JZBp//9hxzY6hFxPhGMZD8YrHvxX6WuvR2lDDfVFyMOiXvO0NHZKAb/R5cGY6gmgiudTTDMfL8hDrp48Z70vaH5ovO3yi0FGBcvoqEHCpD0qzRiEUQCrmuXv7Sjz40jCOjRvPi1gOh0aVCszKxr8ViAg7epvx/EmOg2cz6J+Dy25FT1MtAMkDF+J8aqEZEEJg/7Ak4M8c02YeQDmo6YF3elwYD8eR0rFmpAoEPA6n3YJaR8G2LmXxRzevRV2NDV955Kimx9GaQ3IGykaDCDggpRNOhuM4WUULxWowOB7Cuvb6xXF3/fJCpplK6k9PRTAjjzvcc3oasQXjNORKJNOYjsRV9cBTaYFJHSfzmF7ApyPqTqPPR1OdA3940xo8cXQCz50w3qVgsQyMhLC6pQ4NTm1aD5TCjsV8cGOe18OjIXz72dO6H3fQH16MfwNAd6MLDU6bqTJR9g3PAgA+uqsXsYU0XjpjnN43E+EYhCg/hVDhfDGPfnFw0wt4IJJAY50+YnTPzlXo8rrw5V8dMW1p/cBoEJcZyPsGgJ6mWnR6nIaMg6fSAp/58Sv40i8PY1rHMumpuTim5hJY33FewIkI/T63qQR8//AMGmpsuHv7SjisFkOFUdRKIVTwLQ520C8ObnoBn9GwD0o2TrsVf3r7ehweC+Fn+0d0OaaazEYTODczX7EWsvkgIuzoa8HuUwHDfTH+9OVzizHng3L4SQ+OycfckJWrv9HnxlF/2HDnKR/7hmdxRY8X9TU2bFvViGcMtJA5VuYszGwqUU5vegGf1rAPSi7euLkTm7s9+Pqjg4aK5xXD+QVMYwk4IIVRApEEBg20SBxNJPH1RwdxWacbRMDBs/oJuPKlkemBA9JCZjSRwlAgqpstpTIXT2LQH8LWnkYAwK51rRgcDy96vpVGyRjxuUubhZmNt9aOGpsFfg6hFI/UiVA/AbdYCF+4ox9jwVhF4qLlsFhCb7AQCpAZBzdOGOU/fncaE+E4/upNl6GvtR4Hz83qduxBfxjNdY6Lxnz1m6gi8+C5WaQFcGWP1AZg11ppqLlRwij+YAxOuwVulzoJEESETq8Lo+yBF0dsIYVoIqWrgAPA9t5m3NLfjm8+dRJTOrePLIeB0RC6vC7VR8+pQZfXhZXNtYbpizIRjuFbT5/E6zd14KqVTdjc7cGBc0HdUh2Pjl+4gKmwrr0BVguZQsD3ywuYW1dIHviGjga01NfgmePGCKP4Q9IgB6LSRqnlosOtby64qQU8oHEflKX4/Os3YH4hhX/8rXmKew6NBCvegXApdvY1Y8/paV3zaPPx948dRyKZxudu3wAA2NLtxdRcXJf4ZjotcHw8fFH4BJDWYXpb6kwh4PuGZtDXWgePPGzFYiHsWtuCZ49PGuJvrMYotWz0no3JAl4ia9rq8e5revCDF4dxctL4nQrDsQWcmooYMv6tsL23GeFYEodG9Ys15+LYeBgPvjSM9+5YibfQ6tsAACAASURBVFVyO+HN3dJ5O3BW+zDKuZl5RBOpxQrMbPp9bsPnggshsP/sLK6U498Ku9a1Yia6sBjOqyRqjFLLxud1wh+K6fYFZWoBn9ahD8pSfOqWtXDZraYo7lF6aBgtAyWTHb3GiIN/5ZGjqKux4Y9eu3bxsX6fGzYL4cA57YXnqFxCn8sDV2wZDcYwG01obkupDE1HEYgkFhcwFa5b2wKg8nHwdFpgIhRHh0edBUyFDo8LqbTQLbRqagFf7INSIQFvqa/Bx2/sw2OHx7HbgDnMmZzvAW7cEEqb24k1bfUVzQd/7sQUnjg6gT+8ac0Fnyun3Yp+n1uXhcxBOQNlbY4YOHC+itbIja32yeXzV670XvB4S30NNnW58czxygp4IJpAIpVGh1vdFGSfW99UQpMLuNwHpYKLch+6bjV8Hif++mFjF/cMjAbR1lCDtgZ1LxnVZkdvM148HcCCzsNhAckr+/KvjqDL68I9O1ddtH1ztwevngtq/nc+Oh7GiiYX6mtyZ0f0+yRhN3IcfN/wDOprbFjbdvGX0K61rdg3PItQbKEClkksjlJT2QP3yYMd9OpKaHIBj8NqIbgrWBbutFvx2dvW4+C5IP7n4GjF7CjEoZGQocMnCjv7mhFNpHBQh1BFNj/bP4LDYyH86e3rc/aH3tLtRTiexOlpbXu2HPOHsb49/5VSW4MTLfUOQ/cG3z88iy0rPLBaLs7w2LWuFam0qGiozK9yEY+C3tWYJhfwBBprHYvNfirFm7d2YaPPja/92pjFPfOJFI5PhA3TgXApru2tTF+U2EIKX390EJu7PXjj5s6c+2xeIX0BahlGiSdTODUVybuAqWDkkvpoIomj/vBFC5gKV/Y0os5hrWgcXBlSrvYiZqNSzKNTsZKpBXx6LoEmnfqgLIXFQvjfd/ZjZHYe9z1/ptLmXMQRfwhpUfkZmMXQVOdAv8+texz828+exlgwhi/c0Z/XIVjTWg+X3YoDGlZknpyIIJUWWFdAwDf63Dg+PleRUFMhDpwNIpUWeQXcYbNgR18znjk+WbEWwv5gDFYLXVQoVS7SZB6nbrMxTS3gM1F9qzCXYueaFty0vhX//OSJxfRGo6C0kDVDCAWQ4uB7z8wgntTnamZqLo5vPnUSt25sx3b5CiAXNqsFm7rcOKChBz44fuEQh3z0+9xIpNKGnNW6/6y0gKkMYs7FrnWtOBuYx5npyrQE8IdiaK2vyRniKRc9BzuYWsD1aiVbLH92Rz8i8ST+6XFjFfe8eGYGTXUOdKoc79OKHX3NiCfTi5V8WvOPvz2O+YUUPv/6DQX33dLtxeHRkGae76B/DnarNId1KRZ7g49VPp86m31Ds+htqVsyO6zSZfVaFPEodHpcHAMvBr37oBRiXXsD3nF1Dx7YPYTTBhmCPD0Xx28G/HjjZp+qJcNacs3qJlhIn3zwk5Nz+MGLw3j3NT3oa60vuP/mFV7Ek+nFVD+1GfSH0NdaD7t16X/N3tY6OKwWw6USKhN4svO/s1nVUoeeptqKCbgWRTwKHR4nxnUq5jGtgCdTacxGFwzX1+OPb10Lh82Cr/3aGMU9P957DolUGndvX1lpU4rG47JjU5dHFwH/yiNH4bJb8alb1hbeGcCWbmUhUxvPd9AfLhg+AQC71YJ1HfWGW8g8G5jHdCRxUf53Lnata8ELp6aRSOofxx8PaueB+zxOJNNCl/7xphXw2fnK54Dnoq3BiY/d0IdHBvzYW+HpI6m0wAO7h7CjtzlvUYhR2dHXjP1nZzCf0C4OvvvUNB47PI6P39iHlvriQnE9TbXw1to1yUQJzi9gNBgruICp0N8hldQbaZaoUsCjNLBail1rWxFNpBaHHuvFXDyJcDypoYBLqYR6dCU0rYBXsg9KIT58/Wq0NdTgyw8fqeg/11ODExiZncd7d5jH+1bY0duMhZTA3iFtvgTTaYG/fvgIfB4nPnTd6qKfR0S4vMujSUn98XFliEORAu5zYzqS0HUGYyH2Dc+gzmHN2wYgkx19zbBZSPeqzMU+4BoJuPLFoEdfcNMK+PRcZfugLEWtw4bP3rYe+4dn8fCr/orZcf/uIbS7a3DrxvaK2VAqV69qgs1CmoVR/ufgKA6eC+Kzt+Uu2lmKLd1eHBsPq351cH6IQ3H5+ucXMo0TRtk3PIMtK7xFZXc0OO24cmWj7nFwRcDbNYqB6zmZx7QCrnjgRouBK7zlqm5s6GjAV399VLd0uEyGpiN4+tgk3nVNT8EFMSNSV2PDlhVeTfqDxxZS+NqvB7HR58abt3Yt+/lbVniRSgvVuyYO+sNoqLEVnS200WesnijziRSOjIWxtadw/Fth19oWHBoN6XoVoVURj0JTnQMOm4UFfCkCUeN64ABglSf3DAeiuP+FId2P//09w7AQ4V3X9Oh+bLXY0duMV0eCmIsnVX3d+54/g5HZefzvO/MX7SyFspCpdhhl0B/Guo6GorOFPLV2dHldhlnIPHhudskCnlzsWielEz57Qj8vXO1hxtkoxTws4EsQmDO2Bw5IH87r17bgG0+cQDCqX+Oe2EIKP957Fq+7rF2zy0Q92NnXjFRa4KXT6sXBZyIJ/POTJ/DaDW3YuaalpNdoczvR4XaqupAphMBRf6io2HEm/b4Gw4RQ9ikTeJYh4Js6PWiqc+g67HgsOA9vrX3ZobPlIE3m4Rh4XgKRONxOm+HDA1+4ox+h2AK+8YR+xT2/PDiG2egC3rt9lW7H1IIrVzbCYbXgeRX7ovzTE8cRiSfxZ0UU7SzF5m6PqqmE46E4QrFk0QuYCv0+N05NzhmiB8/+4RmsbqlbVmKBxUK4bk0Lfnd8Urdunv5gXLPwiUKn14XRWfbA8zJtsCKefPT73HjbVd343gtDGNapbPj+F85gTVs9tvc26XI8rXDarbhypVe1viinpyK4/4UhvOPqnrLTKres8OL0VES1K6vFIQ7LtGujz420kKYIVRIhBPYNz2LrEuXz+di1rhVTcwndriT8oXnNwicKSjGP1l9KphVwI/VBKcRnbl0Pq4Xwtd9oX9xz4OwsDpwL4r3bV5qm8nIpdvRKi1xqTJ/52q+PwmGz4I9vLa5oZymUEWsHR9QJowwuZqAs3wMHKt8b/NzMPKbm4ti6svjwicIueUrP73QaduwPxjVLIVRQinmmItouzhYUcCJaQURPEtERIjpERJ+SH28ioseI6Lj8e/l/uTKQOhEapw/KUnR4nPjI9avxy4Nj2D+sbdHCA7uHUOuw4vevXH52hRHZuaYZQgB7yoyD7z0TwCMDfnzshj5Vhlps7pI8TbXCKIP+MNrdNfDWLs8p6WmqRZ3DWvEZmYsTeJaRgaLQ5nZiQ0eDLumEiWQaU3NxzdeGFvuCaxxGKcYDTwL4EyFEP4DtAD5JRBsBfB7A40KItQAel+/rRiCSMGwGSi4+eoNU7ffXGhb3zEQSeOjAKN68tQsNFRxyoSZbur1w2a1l5YMLIfDlh4+graEGH76++KKdpfDU2rG6pU61IceD4+Gi878zsVgI6zsaKp5KuH94FrUO67JDQAo3rGvF3qEAIipnHGUzEdY2hVBBr1zw3DObMhBCjAEYk2+HiegIgC4AbwJwo7zbfQCeAvA5Tay82CbMRBOGzkDJpr7Ghs/cug5f+Nmr+M2hcdy+qUP1Y/zk5XOIJ9OmrLzMh8NmwbZVjXjowChGSuyxHE0ksX94Fl97y2bUOgp+5Itmc7cHe06VnyGTTKVxfGIOrykxK2Zjpxu/eGUUQoiKhc32Dc9gc7cHthKTCnata8W/PXMKu09N4+Z+7QrPtJrEk41e1ZjLOttEtArAVgB7ALTL4q6IfFue53yUiPYS0d7JSXUukcLxJBZSwlQeOAC8fVs31rbV4yuPHFG9gU86LfDAniFcs6oJG0rw5IzMe65diQ63E+dm5kv6CUQW8NaruvGWq7pVtWtztxf+UAwTZU5fOTMdRSKZLtl77fe5EY4lcW5GnyEC2cQWUjg8GlpW/nc2V61shNNu0TyM4tc4B1yhuc4Bh1X7Yp6i3REiqgfwUwCfFkKEiv2mF0LcC+BeANi2bZsqsQMlB9wsi5gKNqsFX7ijHx/47kv4wZ4hvP816lzOA8AzxycxNB3Fn9y2XrXXNAq3b+rQ5IqlXDILem7dWLoglLqAqZBZUr+iqbZkO0rl1ZEgksss4MnGabdie28zntF4IXOxD4pb3WHG2RAROnQo5inKAyciOyTx/r4Q4r/lh8eJyCdv9wGY0MbEi5lWGlnVm0vAAeDG9a3Y2deMf3z8uKpTuR/YPYSWegduv8x4QletXNYpDe0tNw4+OB6GhYA1bYX7kediQ0cDiCqXibJP7iZ4RQkLmJnsWtuK01MRnA1ol27rD8bgtFvgdqkXSsuHVI1Z4RAKSa72twEcEUL8XcamhwDcI9++B8Av1DcvN4udCJe5Ym8EiKQS+9n5BfzrkydVec2zgSgePzqBd17dA4fNtJmhpsPlsGJde0PZI9YG/SGsaqkruTKw1mHD6ua6ygn48AxWNtcW3ZI3H0pZ/dMahlH8oRh8HpcuawV6lNMX89/+GgDvBfBaInpF/rkDwFcA3EpExwHcKt/XhRkDt5Ithk1dHrx5axe+89xpnJsp39v4wYvDIADvvta8fU/MypZuD14dCZaVWVTsEIel6Pe5K1JSrxTwlBM+UehrrUOX14Xfadhe1h+Mod2tT/pxh8eleTFPQQEXQjwrhCAhxGYhxBXyz8NCiGkhxM1CiLXyb92mFyghlGYThlAUPnvbehCAr/9msKzXiSdTePCls7ilvx2dXm3jeszFbO72Yja6gOESL/ujiSSGAlGsK7MytN/XgLOBeYRVDMsVw8jsPCbD8WV1IMwHEWHXuhY8f2Jas5mjigeuB51eJxZS2hbzmPJ6OxCJw2m3qJoSpjedXhc+dN1q/PyVUbxaRjHII6/6EYgkqip10ExsLrMz4fHxOQhR/BCHfCgLmUc1mtWZD6WBlRoeOCDFwcPxJF5RKb8+k3RaYDwU063Bm5JrruWEelMK+HQkYcr4dzYfv7EPzXUOfPnhwyVfgn/vhTPobanDa/pKyyFmymN9RwNqbBYcLFFwBseXN8QhHxs7K1NSv394Bk67pewvIIWda1pgIW2m1QeiCSykBDp0CqEsVmOygF/ITCRhygyUbBqcdnz6lrXYfSqAx48sP4lnYCSIfcOzeM/2lSX1tWbKx261YGOnu+SFzEF/GE67BT1lpv91uJ3w1tp1L6nfNzyLzd3ekgt4svG47LhihVcTAT9fxKNPCMXnlasxSyxAKwZTCnggYp4+KIV45zU96G2tw988cgTJZcb9vr9nCE67BW+9Ut0CFWZ5bOn2YmAktOy/HyAJ+Nq2hqJGkC0FEaG/w62rBy4V8ARVC58o7FrXioMjwcVsM7XQqwpToalWLuYps9BrKUwp4NMm64OyFHarBZ+/fQNOTkbwo5fOFv284PwCfr5/FHdd0QVPbXX0PTErW1Z4ML+QwonJuWU/96g/XHIBTzb9PjcGx8NI6dRX+9BoEAspUVIDq6XYta4VQgDPnlC3qEepwtS6E6GCxUJo99RwDDybQCSBxiqIgSvcurEd16xuwj/89ljRWQQ/ffkc5hdSuHs7L15Wms3dcmfCs8tbyJyei2NqLq5a/HhjpxuxhTROT0VUeb1C7Bta/gSeYtjS7YXHZVc9jOIPxmC1UNn56svB53Fp2pHQdAIeW0ghmkiZOoUwGyLCn9/Rj6m5BP7t6VMF9xdC4IHdQ9ja48WmLo8OFjJLsbq5Dg01tmXHwc8vYKrlgUuvo1c++L7hGaxocqG1QV1BtGZM6VGzc6c/FENbQ03Z4arl4PM4MRbiGPgiAZMX8eRjywov3nRFJ/79d6cKlt8+d2Iap6YieB+nDhoCi4VweQkj1srtgZLNmrZ62CykSxxcKuCZUT3+rbBrXQvGQ3EcG19+WCofUhGPvjNiOzxOjAfjmhXzsIAbiM/eth5CAH/76LEl97t/9xk01Tnw+k0+nSxjCrG524sjY6FlzaYc9IfRWGtHq0qX9DU2K9a01esi4GPBGMZDcQ0FXCqrVzOM4g/FNO8Dnk2nx4VEKr1YfKg2phPw6SoW8BVNtfjAa1bhp/vO4dBobm9uLDiPxw6P4+3bVmg6VZtZHles8CCZFssST2mIQ4OqfTk2+vTJRFEm8KhRgZkLn8eFtW31eEbFsnp/MKZbBorC+b7g2sTBTSfgZu+DUohP3LQGHpc97+SeH+wZhgDwHu57YigWFzKLDKOk0wLH/GHVe7f3+9wYD8UxPaftLMZ9Q7Nw2i2LFaBacP3aVuw5HcB8ovirmnzMxZOYiyd1F/Dzk3m0iYObTsAX+6BUqYB7XHZ86ua1eO7ENJ7KunxMJNP44Ytn8dr1bRXp+8zkx+dxoqW+puiFzJHZeUQSKdXi3wrnhxxrW1K//+wMNnd5YVepgCcXu9a1IJFMY8/p0sfpKSz2AdddwLWtxjSdgAcicVgtBHeVzHzMxXuuXYlVzbX4m4cvLO75zSE/pubiuJsXLw0HEWHLMhYylZ4l5TaxykbJRNEyjBJPpnBoJKRZ+ETh2tXNcNgseOZY+fngioDrvYjZXOeA3Uos4ApSDri9qkvHHTYLPnf7Bhwbn8NPXj63+Pj9Lwyhp6kWN6xtraB1TD42d3txcnKuqFz+YyqnECo019eg3V2jqYAPjISQSKVVz//OxuWw4trVTaq0l9W7iEfBYiG0u7Ub7GBKAa/W+Hcmt2/qwFUrG/G3jx1DJJ7EUX8IL54J4O7tPVX95WVmNq/wQAhpxFghjvrD6G50ob5G/Y6aWvcG3y8vYF65UlsPHJC6Ex6fmMNomf1ElOHCenvggJSJwh64zKUi4ESEP7+zH5PhOO595hQe2D2EGpsFb7tqRaVNY/KwZRkLmYP+kGoVmNn0+9w4MTGHeLL8xb9c7B+eRXejC20N2ouhkk5YrhfuD8XQWGuvSOZWh8fJWSgKUh+U6mhkVYgrexpx5+U+3PvMKfxs3wjeuKUTjZfAl5dZaapzYEWTCwcLLGQmkmmcmoyoHv9W2OhzI5kWODGhXhFMJvuGZzQPnyisa69Hh9tZdhy8EkU8Cj6vJOBaFPOYTsADkQQa66p3ATObP719PZLpNCKJFN7LfU8Mz+ZuLw4U6IlycnIOybRQPf6toGUmylhwHmPBmOoNrPJBRLh+rVRW/7vjk0gkS5vU4w/pnwOu4HM7kUilEYiqX8xjKgFPptIIzi9UTSvZYljZXIdP37IOd17uw5YV+vzTMKWzpduDkdn5JfOwlQVMtXPAFVa31MFpt2jSG3y/yhN4iuEdV6/AQkrgvd9+EVd+6TF84vsv46cvn1tWu1l/MK77AqaCz+tCS70DsxoIuKlmks3OL0CI6s0Bz8cnb1pTaROYIsks6LlpQ1vOfY76w7BbCb2tdZrYYLUQ1rc3aJKJsm9oBjU2bQt4stm2qgn7/s+teP7kFH57ZAJPHB3Hw6/6YSHpi+Tm/nbc0t+GNW31OataE8k0pubiFQuhvO6yDrzusg5NXttUAl7NfVCY6mBTlwdEwCtnZ/MK+KA/jL7Wek2LYDZ2uvHIgB9CCFVL9fcNz+DyLg8cNn0v3l0OK27ub8fN/e1Ipzfh0GgIjx0Zx+NHxvHVXx/FV399FD1Ntbi5vw239EvtmZXzOxGuTAqhHphKwKfnWMAZY1NfY8PatvolFzIH/WFctVLbEES/z40fvnhW1Sns8WQKA6MhvH/nKlVer1SU7o+Xd3vwmVvXYSw4j8ePTODxI+P4/p5h/OdzZ9BQY8Ou9a24pb8NDTXSmlmlPHAtMZWAz0RZwBnjs7nbiyePTuT0fsOxBYzMzuPdGveyUUIch0dDqgn44dEQEsm0bguYxeLzuHD39pW4e/tKRBNJPHt8ShL0oxP41cGxC/arNkwl4NXeB4WpDrZ0e/CTl89hZHYe3Y0X9qw5v4CpTQaKgvL6R8ZCuLm/XZXX3DeszQQeNal12HDbZR247bIOpNMCB0eCePzIOPzBmGZrDpXEVAIekEMonAvNGJnMhcxsAT+q8hCHfDQ47ehpqlU1lXD/8Ay6vC7ThCIsFsIVK7y4ooqzt0yVRhiIxNHgtGm6+MMw5bLB1wC7lXJ2Jhz0h1FfY0OXV/vL+X6fupko+4dnNW9gxSwPUylhILrA4RPG8NTYrOj3uXHgbG4BX9eeO91Nbfp9bpyejiCaSJb9WuOhGEZm5w0dPrkUMZeAR+K8gMmYgi3dXgyMhC4onxZCyFN49Mmh7ve5IcT5sE05LDawYg/cUJhKwKfnEpdUFSZjXjZ3ezAXT+LU1Pl+JBPhOGajC5ovYCpsXCypLz+Msm94Fg6bBZd1esp+LUY9TCXgUifCS6cPCmNelLYHmX1RtBrikI/uRhcanDZ1BHxoBps63boX8DBLY5q/hhACM1H2wBlz0Ndaj1qH9YKCnkG/JKR6eeBEhP4Od9k9URLJNA6OBHXtf8IUh2kEPBxPYiEleBGTMQVWC2FTlwcHMnqDD/rn0NZQo2sabL+vAUf94bJamR4Zkwt4NK4eZZaPafLAA1xGz5iMLd0e3Pf8EBLJNBw2CwbHQ5rnf2ezsdON6AspDAeiWNVSfCFLdkWj0jiKMRamEfBpbmTFmIwtK7xIpE5j0B/Gxk43jo/P4X06D6Tuz1jILCTgmT1Fnjs5jUQyjQanDTesa8Vbr+quWD9tJj+mEfAZFnDGZCgj1g6cm0VtjRXxZFq3BUyFde0NsBBweCyE11/uu2BbOi0wMBrEb2XRPiTHylc21+Lua1filv42XJ3R1Y8xHqYRcG4ly5iN7kYXGmvtOHhudnHtRqshDvlw2q3oba1fzESZT6Tw3IkpPH50HI8fmcBEOL4YHvnc7RuW7KvNGA/TCPhiI6t6FnDGHBARNnd7cfBcED6PCxYC1rbX627HRp8bvzs+iQ999yU8e2IK8WQa9TU27FrXgps3tOOmDW3sGJkU0wh4IBJHjc0CVwWmSjNMqWzp9uCfn5xEm9uJVc11FZmKftXKRjx0YBSD42G865oe3NzfhmtXN3NOdxVgIgGX+qDwpR1jJjZ3e5EWwLPHJ3HbRm3GahXiPddKot3ldfH/T5VR8CuYiL5DRBNENJDxWBMRPUZEx+XfmucXBSJxNHH4hDEZm1dIpedpoX0L2XzYrBZ0N9ayeFchxVxDfRfA7VmPfR7A40KItQAel+9rilRGz1WYjLloa3CiU06/06sCk7l0KCjgQohnAASyHn4TgPvk2/cBuEtluy5iOpJAUy33QWHMhzLgoVIeOFO9lLqK0S6EGAMA+Xfu8dsAiOijRLSXiPZOTk6WeDgpD5w9cMaM3LKxHWvb6rGyufpGejGVRfNFTCHEvQDuBYBt27aV1JAhtpBCJJHiFELGlLz1qm689aruSpvBVCGleuDjROQDAPn3hHomXQwX8TAMw1xMqQL+EIB75Nv3APiFOubkRhHwxloWcIZhGIVi0gh/COAFAOuJ6BwRfQjAVwDcSkTHAdwq39eMAFdhMgzDXETBGLgQ4l15Nt2ssi154RAKwzDMxZiilnaxDwoLOMMwzCKmEPBAJA6rheB2ch44wzCMgkkEfAGNtXZYLFwKzDAMo2ASAY9z/JthGCYLU3Qj3NztRW+r/n2UGYZhjIwpBPyTN62ptAkMwzCGwxQhFIZhGOZiWMAZhmFMCgs4wzCMSWEBZxiGMSks4AzDMCaFBZxhGMaksIAzDMOYFBZwhmEYk0JClDTlrLSDEU0CGCrx6S0AplQ0R23YvvJg+8qD7SsPo9u3UgjRmv2grgJeDkS0VwixrdJ25IPtKw+2rzzYvvIwun354BAKwzCMSWEBZxiGMSlmEvB7K21AAdi+8mD7yoPtKw+j25cT08TAGYZhmAsxkwfOMAzDZMACzjAMY1IMJ+BEdDsRDRLRCSL6fI7tNUT0oLx9DxGt0tG2FUT0JBEdIaJDRPSpHPvcSERBInpF/vkLveyTj3+GiF6Vj703x3Yion+Sz99BIrpSR9vWZ5yXV4goRESfztpH1/NHRN8hogkiGsh4rImIHiOi4/LvxjzPvUfe5zgR3aOjff8/ER2V/34/IyJvnucu+VnQ0L4vEtFIxt/wjjzPXfJ/XUP7Hsyw7QwRvZLnuZqfv7IRQhjmB4AVwEkAvQAcAA4A2Ji1zycAfEu+/U4AD+ponw/AlfLtBgDHcth3I4BfVvAcngHQssT2OwA8AoAAbAewp4J/az+kAoWKnT8AuwBcCWAg47GvAfi8fPvzAL6a43lNAE7Jvxvl24062XcbAJt8+6u57Cvms6ChfV8E8Nki/v5L/q9rZV/W9r8F8BeVOn/l/hjNA78GwAkhxCkhRALAjwC8KWufNwG4T779EwA3E5Eu4+qFEGNCiH3y7TCAIwC69Di2irwJwPeExG4AXiLyVcCOmwGcFEKUWpmrCkKIZwAEsh7O/IzdB+CuHE99HYDHhBABIcQMgMcA3K6HfUKIR4UQSfnubgDdah+3WPKcv2Io5n+9bJayT9aNtwP4odrH1QujCXgXgLMZ98/hYoFc3Ef+EAcBNOtiXQZy6GYrgD05Nu8gogNE9AgRXaarYYAA8CgRvUxEH82xvZhzrAfvRP5/nEqePwBoF0KMAdKXNoC2HPsY5Tx+ENIVVS4KfRa05A/lEM938oSgjHD+rgcwLoQ4nmd7Jc9fURhNwHN50tl5jsXsoylEVA/gpwA+LYQIZW3eBykssAXANwD8XE/bALxGCHElgNcD+CQR7craboTz5wDwewD+K8fmSp+/YjHCefxzAEkA38+zS6HPglZ8E0AfgCsAjEEKU2RT8fMH4F1Y2vuu1PkrGqMJ+DkAKzLudwMYzbcPEdkAeFDaJVxJEJEdknh/Xwjx39nbhRAhIcScfPthAHYiatHLPiHEqPx7AsDPIF2qZyWO9AAAAd5JREFUZlLMOdaa1wPYJ4QYz95Q6fMnM66EleTfEzn2qeh5lBdN3wDgPUIO2GZTxGdBE4QQ40KIlBAiDeDf8xy30ufPBuD3ATyYb59Knb/lYDQBfwnAWiJaLXtp7wTwUNY+DwFQVvzfCuCJfB9gtZFjZt8GcEQI8Xd59ulQYvJEdA2kczytk311RNSg3Ia02DWQtdtDAN4nZ6NsBxBUwgU6ktfzqeT5yyDzM3YPgF/k2Oc3AG4jokY5RHCb/JjmENHtAD4H4PeEENE8+xTzWdDKvsw1lTfnOW4x/+tacguAo0KIc7k2VvL8LYtKr6Jm/0DKkjgGaYX6z+XH/grShxUAnJAuvU8AeBFAr462XQfpMu8ggFfknzsAfAzAx+R9/hDAIUir6rsB7NTRvl75uAdkG5Tzl2kfAfgX+fy+CmCbzn/fWkiC7Ml4rGLnD9IXyRiABUhe4Ycgrak8DuC4/LtJ3ncbgP/IeO4H5c/hCQAf0NG+E5Dix8pnUMnK6gTw8FKfBZ3su1/+bB2EJMq+bPvk+xf9r+thn/z4d5XPXMa+up+/cn+4lJ5hGMakGC2EwjAMwxQJCzjDMIxJYQFnGIYxKSzgDMMwJoUFnGEYxqSwgDMMw5gUFnCGYRiT8v8A+CDfJaDSQ34AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "episode_durations = []\n",
    "for i in range(20):  # Not too many since it may take forever to render\n",
    "    test_env = CartPoleRawEnv()\n",
    "    test_env.seed(seed + i)\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        with torch.no_grad():\n",
    "            action = bonus_get_action(state).item()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "    episode_durations.append(steps)\n",
    "    test_env.close()\n",
    "    \n",
    "plt.plot(episode_durations)\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
