{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or tim|elise|david|qi, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"Philipp Lintl, Bogdan Floris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0fd6bc65a6759a8899e024459ccb28ef",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "039c8296578b2834a9a858a1a19a43bd",
     "grade": false,
     "grade_id": "cell-eecfd6fb626abfae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Temporal Difference (TD) learning (8 points)\n",
    "Mention one advantage and one disadvantage of Monte Carlo methods. Mention an example where you would prefer to use TD learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4b81bcd51404511164971c110ffa838f",
     "grade": true,
     "grade_id": "cell-cac4639044ba9074",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "One advantage of Monte Carlo methods is that it does not require a full knowledge of the environment (MDP), but the disadvatange is that it does not learn until the episode ends (and thus only works for episodic tasks). TD learning is prefered when the task is not episodic (we do not need to store sample trajectories) and the model is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e61bd7837d3b364741b4c3aa43597a10",
     "grade": false,
     "grade_id": "cell-21ca38ffcbe1c3ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the TD algorithms, we will skip the prediction algorithm and go straight for the control setting where we optimize the policy that we are using. In other words: implement SARSA. To keep it dynamic, we will use the windy gridworld environment (Example 6.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "609d0f1e1ef6ad89c8dcd96dd43aa798",
     "grade": false,
     "grade_id": "cell-c046fd0377cee46d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        return int(np.random.rand() * nA) if np.random.rand() < epsilon else np.argmax(Q[observation])\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "42b89f13768d1cd3b41fb52cddef0d97",
     "grade": true,
     "grade_id": "cell-6b662771f3762bb1",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3512.45it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5gdZd3/8ff37NmWTbKbTa+kh4RAKEvo0gIJTRClqRgQ5dGfBcRGeXgQEMFygaLIIyo+IiKgAkYMRJBioaRQE1LYEEg2nfS69fv7Y2ZPTttks7snu9n5vK7rXDnnnjlz7jmTnc/c9z0zx9wdERERgFh7V0BERDoOhYKIiCQoFEREJEGhICIiCQoFERFJUCiIiEiCQkE6BDN7ysymtvEyv2NmD7blMkU6O4WCtBkze9/MdpjZ1qTHz5rzXnc/w91/m+s65kpHCyAzu97MloTboMrMHskyz/+ZWZ2ZDUgr/46Z1Ybv3WhmL5nZMW21fOnYFArS1s5x965Jjy+3d4Vay8zi+9NnhC2uS4FJ7t4VqAD+kTZPCfBxYBPwqSyLeSR8by/geeCPbbx86aAUCrJPmNllZvYfM/upmW0yswVmdmrS9BfM7HPh85Fm9mI434fJR6FmdqyZzQqnzTKzY5OmDQvft8XMniHYoSXX4ejwqHejmb1pZiftpr7vm9m3zewtYJuZxc1sgJn92czWhkfJXw3nnQJcD1wUHjm/mbSMSUnLTLQmzGyombmZXWFmS4HnksqmmtnScN1vSHr/RDObbWabzWy1md3ZRPWPBGa4+2IAd1/l7velzfNxYCNwC9Bkt5271wG/BwaaWe+2Xr50PAoF2ZeOAt4j2FnfBDxmZuVZ5rsV+DvQAxgE/BQgnPdvwN1AT+BO4G9m1jN830PAnHD5t5K0MzKzgeF7vwuUA98A/py0o8vmEuAsoAxoAP4KvAkMBE4Frjazye7+NPA9wqNrd5+wF9/JicBYYHJS2fHAmPAz/sfMxoblPwF+4u7dgRHAo00s8xXgM2b2TTOrMLO8LPNMBf4APAwcaGaHZ1uQmRUAnwHWARvaevnS8SgUpK09ER6JNz4+nzRtDfBjd69190eAhQQ73XS1wAHAAHff6e7/DsvPAt5199+5e527/wFYAJxjZkMIjmBvdPdqd/8nwU680aeB6e4+3d0b3P0ZYDZw5m7W5W53X+buO8Jl93b3W9y9xt3fA34JXLyX30+677j7tvAzGt3s7jvc/U2CEGoMmVpgpJn1cvet7v5KtgW6+4PAVwiC5kVgjZld2zg9/K5OBh5y99UEXT/pR/MXmtlGYAfweeATYauhrZYvHZRCQdraee5elvT4ZdK05Z56B8YPgGyDkN8CDJhpZvPM7LNh+YDwPck+IDhyHwBscPdtadMaHQBckBxYBEfk/XezLsvS3j8g7f3XA3138/7mWJalbFXS8+1A1/D5FcBoYEHYdXZ2Uwt199+7+ySCVs4XgFvMrLE1cikw393fCF//HvikmeUnLeJRdy8jWL+5wBFtvHzpoHI+gCaSZKCZWVIwDAGmpc/k7qsIjk4xs+OBZ83sn8AKgp1zsiHA08BKoIeZlSQFwxCg8bOWAb9z98/TfMkBtgxY4u6jmjFvo21Al6TX/Zr5vuwf4P4ucImZxYDzgT+ZWc+0IEx/Ty3wRzP7NjAemEHQHTTEzBrDJ07QHXcGadvD3T80s/8CZpnZQ+6+si2XLx2PWgqyL/UBvmpm+WZ2AUFf+vT0mczsAjMbFL7cQLDjrA/nHW1mnwwHfi8CxgFPuvsHBN1BN5tZQRgm5yQt9kGCbqbJZpZnZkVmdlLS5+zJTGBzOPhcHC5jvJkdGU5fDQwNd9iN3gAuDte3AvhEMz8rKzP7tJn1dvcGgkFcCL6X9PkuM7OzzKybmcXM7AzgIOBVC04tHQFMBA4NH+MJxmOydvG4+wKCnf23crF86VgUCtLW/mqp1yk8njTtVWAU8CFwG0E/9bosyziSYAezleDI8ip3XxLOezbwdYKBz28BZ7v7h+H7PkkwmL2eYCD7gcYFuvsy4FyCLp+1BEf+36SZfwPuXk8QMocCS8J1+BVQGs7SeMrmOjN7LXx+I8EOcgNwM8GOsTWmAPPC7+UnwMXuvjPLfJsJ1nMpQXj8APhiODYzFfiLu78dnjW0KmyZ/QQ4u4mBf4AfAleaWZ8cLV86CNOP7Mi+YGaXAZ9z9+Pbuy4i0jS1FEREJEGhICIiCeo+EhGRBLUUREQkYb+/TqFXr14+dOjQ9q6GiMh+Zc6cOR+6e8ZtXvb7UBg6dCizZ89u72qIiOxXzCz97gCAuo9ERCSJQkFERBI6XCiY2RQzW2hmlcl3XhQRkdzrUKEQ3pf9HoIbZ40juPnXuPatlYhIdHSoUCC4iValu7/n7jUEP9BxbjvXSUQkMjpaKAwk9f7yVWFZCjO7MvxZwtlr167dZ5UTEensOlooWJayjEuu3f0+d69w94revXf3a4oiIrI3Otp1ClXA4KTXgwh+WKVTaGhw6hqcgnj2LHZ3auobKIznsXF7DZt31LFy0w66FMQ5eFBwh+a3qjby0uJ1HDq4jOUbdjCsdwmHDS7jiTeWc9yIXvTpXsSWnbX8fd5qzj98IGbZcnaXnbXB7fjjMSOe1/xjhJq6BvLzjJcWr6O8pIAxfbuxaUct8TwjPy9GYTzG0vXbKSmM070on4J4jPoGp7a+gfoGpzAeY/22GvLzYvQoKcioU219A/FYjOKCXT//+/6H23ht6QZ21jYwpl83epYUUFIYp3e3Qtyd6roGqmsbeHXJOkb37ca6bdVs3lnHMcN7Ut/g5MWMovy8xHe9o7aeeCzGtuo6uhXFs65/5ZqtzPlgPRVDyxlQWozjdCnI/LNxd3bWNqTUF2DFxh38aU4VG7bXUBjPS6x/96I4Q3qW0LUwj5WbdrJo9VYG9SjmxNG9GT+wNGUZT89dSU29c9rYvqzavJOYwSOzllGUn8e2mjqGlHdhxcYdbKuuZ8LgUnp0KWD8wFKeeH05m3fU0qOkgMkH9aMoP4/NO2qpa3BWbtrBjpp6Vm+p5oDyLhw0oDsvLV7HxGHllBbn0+DBetbVNwCwfnsNpcX5FMZ3rd/O2nry82LkxQx3Z+2Wap54Yzk1dQ0cP6o3f3trBVUbdnDMiJ6cPKYPa7ZUs2j1FpZ8uI2ieIzykgKWfLiN/LwYx47sSdWGHYzq043igjxeWvwhfbsVMbx3CcN6lfDCwrW8sHANnzr6AEoK4sxZuoGl67ZRWpzPGQf3Z84HG6ipa2Bs/+6Udcmnf2kRzy1YwyEDy+jVrYDFa7ZhBi8vXsfg8i5sra7jwH7dyM+L8cw7qxg/sJSCeIwVG3cyrFcXHn99OccM78W/3l1LfYNz2JAelBbns2VnLacc2Ife3QrD72DXNl+8diuvfbCBkX268sG67azavJOjhpVT3+Bs2F5LXX0DBw8qpa7eiZnRq1sB9Q1Ot6J8ttfUUVsf/F1s3llLSUGcksI4O2vr2byzlvxY8HfSuI/Ij8VYuHoLY/t3b/bfbHN1qHsfmVkcWETwg+XLgVnAJ919XlPvqaio8NZevLa1uo6Sgrw97kABHn+9it++9AGP/79jE/Nv2lHLL15czOadtXz3vIObfO81j7zBY68vZ8ntZ7Klug5vgDuens+njjqA8QNL+cPMpVz32Nu8+M2T+Pi9L/Ph1urEeyeN7cuz81fvsX5XHD+Mt6s2MfP99QA89Lmj2Fpdx23T53PZsUO55cl36F6Uz5FDyxlQVsQDLwfXr5QU5DGyT1dOGtOHgWXF/PDvCxk/oDsNDtV19WzaUYe7c9q4vjz06lLWbathYFkxyzcGPy08sKyYlZt20BD+d7qoYjCPzN7VE9i9KM7mnXVZ63zj2eO49cl3EstpXGajAaVFrNq8M7HsZCP7dOVrk0bzvenzWb5xBwNKi1ixKdtPDOxSWpxPdV09O2sbUsq/OXkM8Zhx+1MLdvv+xvUryo8xfmAppxzYhz/OqeKOpxbQv7SIPt2LmL9iM4PLi1m7pbrJ9d6dgwZ0pyAe46AB3XnwlaXA7r/DfalLQR7ba4KDiQGlReTHY3ywbns712rfGlxezLL1wf/TicPK2bi9hkWrt7ZoWQV5MWrqGzLKuxXG2VKdub0L4zFq6xsoiMd4/hsn0b+0uEWfa2Zz3L0io7wjhQKAmZ0J/BjIA+5399t2N39rQ2HN5p1M/N4/6NEln2NG9GTqMUP505wqrj5tNBu21WQctQ299m8AXHXqKM6ZMIBNO2r4+L0vJ6a/duNplIdHvnc+s4jBPYo5sF93igvymHTni8CuHWbXwjhbw43+ueOH8at/L2nxekTJiN4lLF7b5C9QtrmrTh3FT/7xbovff9vHxnPQgFJeXryOnz9fyf2XH8kF//tyyjx3nH8w1z72drOXed6hA3jijaARfeyInozu243NO2p57PXlWee/4vhh/Drt/1dbhUx+nlFbv2s/0q0ozpadddx10QRu/us7bNxeC8AZ4/vx1NzgFzqfveYjfObXM1mxaSdnHdyfBas2860pB/Jfv5sDwOXHDeXhmcvYUZvxw3IpJg4rZ+aS9YnXpxzYh+cWrNljnT9++CD+/FoVAF+bNJqYQa9uhfy78kOWrtvO28s3AUE4nzNhABMGlXHJL1/Zi2+lZUb37UpJYZzXl25MlJ02ri/PvJN5QPjVU0fxtUmjmnUwm81+Ewp7q7Wh8K9313Lpr2c2Ob3xj3Xhd6cwa8kGPv3rV1Om33reeG58Ym5K2cVHDuaa00cz8bZ/tLhee3Li6N588aQRXHxf8B/1398+meO//3yLlzfzhlO5ffoCHk/aqVwycTB/mJntd+UDewqy5NZC/9IifvmZCvLzgubxBf/7Mt84fTQ/+vsiAMb07cat543np8+9y7/eDX5I7cKKQYzo3ZW+3YvYWl3Hfz8xl4Flxfzn2lNYtHoL35s+nxcWBicaTD6oLzPmBX84t59/MJdMHMIjs5by0Mxl3HXhBIb37so9z1fywxkL+fjhg4gZnHVIf6a/vZKvnDKKE37Q9Hf3/h1nsXrzTnbW1tOnWxGvLlnHZb+ZlXXeQweX8eOLDuVHf19IaXE+Fx05mEMGlWXMd+MTc/ndK7vuMrDg1ikceOPTiddHDy9n/sotbNpRmyg7bEgZry/dyP2XVXDKgX3ZvLOWboXxxE7h6bmr+MKDcxLzJ38n799xFmu3VPPU3JXEzDj7kP6UFAbdYPl5MTZtr+Xl9z7k3hcWc8NZ41iwajP/85ddDfSbzhnH468v562qTRlHsM99/UQG9ijmuflrmLdiM9+YPCYxbdn67Zzwg+c5aUxv/u/yiTwyayk9SwqZNK4vNXUNNLgnuvQAfvD0An7+wmIqbzsjozvv6bmr+O1L7/PLqRVs2lGLAQPKihMHaktuPxMz450Vmznz7n9lfOfzbp6c6EJ0d+54egEnju7NsSN6pczX0OC8vmwjRxzQI6V8Z209MTP+OGcZNzye+jff6LUbT+OFhWu45tE3+eklh3HOhAE88PL7lHUp4OQxvSkpiHPaXS+yeO02vnrKSO5+rhII9hknH9iHyQft+hnvGfNWUV5SwJFDy3lh4Rq+/ee3+OKJI+hXWsw9z1fy+88fRfei/Kz1aA6FQhMef72Krz3yZpPT82JGfYPz8JVHJ3bA2fz0ksP4yh9eT7wuLc5P+YNuqcJ4jH6lRZw2ti8XHjmY0+/6JxD8kbs7w66bnnh93WNvJXbiT111Ag+8/AH9S4u485lFu/2MmdefSp/uRcCultCi755BQTxGdV09Y/776Yz3/OLSIzhtbF/ufXEx81duJh4zLj1mKNtr6nj2ndUUxGNcd8ZYhl8f1G/S2D78auqRife/s2IzY/t3Y8yNT1NT18Di751JXsxS6vD2d06nW/ifvqaugct+M5OrTh3FUcN7AkG33YSb/w7As9ecyKQ7X2R47xJmXP0R8rOMD7g781duYdyAzH7Ylxev48FXP+BvbwW/S3/eoQPYvLOOzx43jONH9cpYzl3Pvkvf7oXc8Phcvn7aaL5w0oisn9kUd2fj9loOu/UZPn30EL573sG8t3Yr3YvzKSvOJ54X4/jvP0fVhh3cddEEjhxazqAeXaitb9jt57yzYjOj+nblhzMWMvXYoVz/2Nu8v24bL37z5GbXrdGz76zmcw/MTuzQIdjJdy/Kp7RLPl/43Ryenrcq6w482ZNvreCEkb0p7bLnHZi7U9/gezW+Nea/n6K6roH37zgrUfbqe+u4KPx7ve1j4ykrLuCsQ/o3e5m7s2VnLV988DWOHl7OlPH9+PNry7n3hcUAiTrsbjttr6njhzMWcvWk0Yn/v3/76vEcNKA06/y50lQodLSB5n1u3daalNcF8Rg1dbv69+rDjuzdBQLAR0anngXVFoFQnJ/H3ZccxqkH9iEW7jCnffk4uoZHeGbGwLJiPjI62Gndfv4hVBxQTl1DMOB2+/nB+MZXThnJxu21XP3IG3zv/INZtGoLvbsV8trSDUwYVJYIBAiOUPPzYonB8OSBxUbPfO0jjOrbDYAvnTwyY/oJo3Z9F+/fcRYvLlrLhEGp/+Ebd8wzrz+VBicRCLBr/KRb0lFQQTzGQ58/OmUZjd8DBGMLL3zjJA7o2aXJ5rSZZQ0EgGNG9KS0OD8RCseO6MWFRw7OOq+Zcc1po4P3De/JsF4le92ENzN6lBTw72+fTN/w+x/eu2vWefuXFjOoRxeAPQZP4/pdf+ZYAH772Yl7Va9kw3qXAEGrtNHg8i6J5z+++FA2bK/Z4w787EMGNPszzYx43t59lzNvmJT4O23UeOAA8KmjDtir5e1Jt6J8HvzcUYnXV506KhEKjXa3nboUxLnpnINSylpzxN/WIh8K1XWpAzzF+XkpodAcZx7cj+L8zJ0nwNj+3Zm/cnPi9YUVg3h0dlWzlvvOLZMzdjbpXRH/ufaUlNcfP2JQxnIad0CNO4iBZcHAVPp4CcDDVx6TUfazTx7GkPIufPRn/wFIBEJznTi66dOGy7oUZJTd86nD2Fa9+75kSA0SgKG9SvaqXumCQIH/OXscF1Rkfo/ZNLUjb67Gnf3uJIffvjSid1devf5U+oRn2qQrys9r8SBnWyotzr5D/cWlRzBvxeas09pSPNayPn0AM3APxmE6io5Tk3bg7vxwxsKUsqL8GJt2NPGGJpdDk6eZFqQd9Zw8pk+zQmHCoNIWDyC1tb050msLhfG8rC2UppyQ1r3TUiWFcZbcftaeZ9xHGnt2S9opFIBEK2Z/NPmgfil99LmSfnCyN644LhiXa6/gz6bj1KQdpDc5gZRBr7bQ2IzMixlfPHEEY/rtOso+dkRPXlq8DoAvnzySy48byo5wMLMV/88iZdF3z2jVH2VH1nj0mL+X3Smyb7Xm4O36M8fyzSlj9moMJdciHQrZhtizBUW6Yb1KWPLhrlMiD+iZ2m3Rq2sB1bUNbKmuS7Qg7jj/YC6oCPqof3HpEUwcWk5Zl3wWrd5KdV191jNUOpqvnDKSWAdpvTRqqoXWGdx3aQWPv7480d0nnU8sZhTG2vZAtLUiHQrZAqBqw577jgrSUr1x0HEXSwwMNx7FJg88JTdpk1sOHd3XTx+z55mkzQzp2YWrJo1q72pIxHTew6xmaOnZuIX5u762sw/pn/Vo9cazx1EQj/H/TgrOzjk66WwIEZGOKtIthYYWpsKhg8t4q2oTv7tiYsrpl3deOIFrHg2uefjEEYP4RHgmUPL50yIiHVlkQ2HR6i2JC8H21mXHDuXCisEZp3ROHFYOwMcO27dn64iItJXIhkLjrRR257iRPflP5bqM8oJ4LOv56YN6dGHezZPpUtCxBo5ERJorsmMKzbng5MErjkp53atrcKHV7k6BLEm6F42IyP4msqGwp3PbH/rcURk798bL5dvzYiIRkVyK7N5tdxcE/fSSwzh2ZOZVsldPGsV/nTg864+siEh0/etbJ1PYSa6ZiezeLS/W9AY8Z0L2gWIzUyCISIbkGwXu7zpHtLVAa25iJSLSWUU3FHQ/GRGRDJENhTydISQikiGyodCM+96JiEROZEOhrmHvfkhHRCQKIhsKzb3v0dNXn5DjmoiIdByRDYX6ZjYUDuwX/ObtuP7Zf9tXRKQziexJ9w17Magw/5YpnfbXvUREkkU2FOr34rbZxbrBnYhERIS7j3T6kYhIusiGQkt/YEdEpDOLbCiopSAikkmhICIiCQoFERFJiG4oaExBRCRDzkLBzH5oZgvM7C0ze9zMypKmXWdmlWa20MwmJ5VPCcsqzezaXNUN9u46BRGRqMhlS+EZYLy7HwIsAq4DMLNxwMXAQcAU4OdmlmdmecA9wBnAOOCScN6caO4VzSIiUZKzUHD3v7t7XfjyFWBQ+Pxc4GF3r3b3JUAlMDF8VLr7e+5eAzwczpsTyd1HBXkxPnbYwFx9lIjIfmNfjSl8FngqfD4QWJY0rSosa6o8g5ldaWazzWz22rVrW1Sh5O6juy46lNs+Nr5FyxER6UxadZsLM3sW6Jdl0g3u/pdwnhuAOuD3jW/LMr+TPaCydvy7+33AfQAVFRUtGhxIbinUu2NZqyUiEi2tCgV3n7S76WY2FTgbONU9sReuAgYnzTYIWBE+b6q8zSWffNTQ4OiH2EREcnv20RTg28BH3X170qRpwMVmVmhmw4BRwExgFjDKzIaZWQHBYPS0XNXPkxoh9Q1OTKkgIpLTu6T+DCgEnrFgh/uKu3/B3eeZ2aPAOwTdSl9y93oAM/syMAPIA+5393k5rF/C4PIuaimIiJDDUHD3kbuZdhtwW5by6cD0XNUp9cOgMB7jr185ntF9u+kKZxERInxFM4AZjO7bLXjeznUREekIIhsK6e0CdR+JiEQ4FICU01BNqSAiEt1QcN0QT0QkQ2RDAdRlJCKSLrKhoIaCiEimyIYC6IwjEZF0kQ6FbLoX5fJ6PhGRji2ye8BsvUe/ufzIxHULIiJRFNlQgMzTUE8e06edaiIi0jFEtvtIA80iIpkiGwqggWYRkXSRDQXP/vs9IiKRFtlQANRUEBFJE9lQ0JiCiEimyIYCqKEgIpIu0qEgIiKpIh0Kul22iEiqyIaCbp0tIpIpsqEAunW2iEi6yIaC2gkiIpkiGwqgs49ERNJFNhQ0pCAikimyoQA6+0hEJF1kQ0H3PhIRyRTZUACNKYiIpItsKGhMQUQkU2RDAXSdgohIusiGghoKIiKZch4KZvYNM3Mz6xW+NjO728wqzewtMzs8ad6pZvZu+Jia67ppVEFEJFU8lws3s8HAacDSpOIzgFHh4yjgXuAoMysHbgIqCA7k55jZNHffkIu6aUxBRCRTrlsKdwHfIrW35lzgAQ+8ApSZWX9gMvCMu68Pg+AZYEouK6cxBRGRVDkLBTP7KLDc3d9MmzQQWJb0uiosa6o827KvNLPZZjZ77dq1LayhmgoiIula1X1kZs8C/bJMugG4Hjg929uylPluyjML3e8D7gOoqKho8d5dDQURkVStCgV3n5St3MwOBoYBb4a3khgEvGZmEwlaAIOTZh8ErAjLT0orf6E19dsdjSmIiGTKSfeRu7/t7n3cfai7DyXY4R/u7quAacBnwrOQjgY2uftKYAZwupn1MLMeBK2MGbmoXyONKYiIpMrp2UdNmA6cCVQC24HLAdx9vZndCswK57vF3dfnqhJqKYiIZNonoRC2FhqfO/ClJua7H7h/X9QJwDSqICKSIrJXNIuISKbIhoJunS0ikimyoQAaaBYRSRfZUNBAs4hIpsiGAujiNRGRdJENBTUUREQyRTYUAEyDCiIiKSIbChpTEBHJFNlQEBGRTJENBV2nICKSKbKhALpOQUQkXXRDQQ0FEZEM0Q0F1FIQEUkX2VBQQ0FEJFNkQwF062wRkXSRDQXXhQoiIhkiGwqgMQURkXSRDQW1E0REMkU2FEB3SRURSRfZUNCQgohIpsiGAuguqSIi6SIbCmooiIhkimwogMYURETSRTYUdJ2CiEimyIYCoKaCiEiayIaC2gkiIpkiGwqghoKISLrohoKaCiIiGaIbCug6BRGRdDkNBTP7ipktNLN5ZvaDpPLrzKwynDY5qXxKWFZpZtfmsm76jWYRkUzxXC3YzE4GzgUOcfdqM+sTlo8DLgYOAgYAz5rZ6PBt9wCnAVXALDOb5u7v5KyOuVqwiMh+KmehAHwRuMPdqwHcfU1Yfi7wcFi+xMwqgYnhtEp3fw/AzB4O581JKOgyBRGRTLnsPhoNnGBmr5rZi2Z2ZFg+EFiWNF9VWNZUec5oSEFEJFWrWgpm9izQL8ukG8Jl9wCOBo4EHjWz4WTvtXGyB1TW43kzuxK4EmDIkCF7X3EREcmqVaHg7pOammZmXwQe8+B+EjPNrAHoRdACGJw06yBgRfi8qfL0z70PuA+goqKiRR1B6j4SEcmUy+6jJ4BTAMKB5ALgQ2AacLGZFZrZMGAUMBOYBYwys2FmVkAwGD0th/XDNNQsIpIilwPN9wP3m9lcoAaYGrYa5pnZowQDyHXAl9y9HsDMvgzMAPKA+919Xq4qp1NSRUQy5SwU3L0G+HQT024DbstSPh2Ynqs6pdNAs4hIqshe0awxBRGRTJENBRERyRTZUFBDQUQkU2RDAXRDPBGRdJENBY0piIhkimwogG6IJyKSLsKhoKaCiEi6CIeCrlMQEUkX2VDQmIKISKbIhgKopSAiki6yoaCGgohIpsiGAuguqSIi6SIbCq5BBRGRDJENBdCYgohIusiGgtoJIiKZIhsKoCuaRUTSRTYUNKQgIpIpsqEAaFBBRCRNZENBDQURkUyRDQXQmIKISLrIhoKuUxARyRTZUAANKYiIpIt0KIiISKpIh4IaCiIiqSIbChpSEBHJFNlQADANKoiIpIhsKLiuVBARyRDZUBARkUyRDgV1HomIpIpsKGigWUQkU85CwcwONbNXzOwNM5ttZhPDcjOzu82s0szeMrPDk94z1czeDR9Tc1W3XZ+X608QEdm/xHO47B8AN7v7U2Z2Zvj6JOAMYFT4OAq4FzjKzMqBm4AKgvvVzTGzae6+IReVU0tBRCRTLruPHOgePi8FVoTPz+VkZbgAAAiTSURBVAUe8MArQJmZ9QcmA8+4+/owCJ4BpuSwfphGFUREUuSypXA1MMPMfkQQPseG5QOBZUnzVYVlTZVnMLMrgSsBhgwZ0qLK6ZRUEZFMrQoFM3sW6Jdl0g3AqcDX3P3PZnYh8GtgEtlP+vHdlGcWut8H3AdQUVHR8r27GgoiIilaFQruPqmpaWb2AHBV+PKPwK/C51XA4KRZBxF0LVURjDkkl7/QmvrtjsYUREQy5XJMYQVwYvj8FODd8Pk04DPhWUhHA5vcfSUwAzjdzHqYWQ/g9LAsZ9RQEBFJlcsxhc8DPzGzOLCTcAwAmA6cCVQC24HLAdx9vZndCswK57vF3dfnqnJN9VeJiERZzkLB3f8NHJGl3IEvNfGe+4H7c1WndLpOQUQkVWSvaNbJRyIimaIbCug6BRGRdJENBV2nICKSKbKhABpTEBFJF9lQ0HUKIiKZIhsKoJaCiEi6yIaCGgoiIpkiGwqgs49ERNJFNhRcgwoiIhkiGwqgMQURkXSRDQW1E0REMkU2FEREJFNkQ0FDCiIimSIbCgCmQQURkRSRDQU1FEREMkU2FEA/siMiki66oaBBBRGRDNENBXSdgohIusiGgtoJIiKZIhsKoDEFEZF0kQ4FERFJFdlQ0DiziEimyIYC6OI1EZF0kQ0F11CziEiGyIYCaKBZRCRdZENBYwoiIpkiGwqgi9dERNJFNhTUUhARyRTZUAioqSAikqxVoWBmF5jZPDNrMLOKtGnXmVmlmS00s8lJ5VPCskozuzapfJiZvWpm75rZI2ZW0Jq67YkaCiIimVrbUpgLnA/8M7nQzMYBFwMHAVOAn5tZnpnlAfcAZwDjgEvCeQG+D9zl7qOADcAVrazbHmlMQUQkVatCwd3nu/vCLJPOBR5292p3XwJUAhPDR6W7v+fuNcDDwLkWXEV2CvCn8P2/Bc5rTd2aUfdcLl5EZL+UqzGFgcCypNdVYVlT5T2Bje5el1aelZldaWazzWz22rVrW1xJNRRERFLF9zSDmT0L9Msy6QZ3/0tTb8tS5mQPId/N/Fm5+33AfQAVFRU65BcRaSN7DAV3n9SC5VYBg5NeDwJWhM+zlX8IlJlZPGwtJM+fMxpTEBFJlavuo2nAxWZWaGbDgFHATGAWMCo806iAYDB6mgcd/M8DnwjfPxVoqhXSJjSkICKSqbWnpH7MzKqAY4C/mdkMAHefBzwKvAM8DXzJ3evDVsCXgRnAfODRcF6AbwPXmFklwRjDr1tTt2bVX6MKIiIp9th9tDvu/jjweBPTbgNuy1I+HZiepfw9grOT9gndJVVEJFOkr2jWmIKISKrIhoLGFEREMkU2FEAtBRGRdK0aU9iffWR0b/qXFrV3NUREOpTIhsKNZ4/b80wiIhET6e4jERFJpVAQEZEEhYKIiCQoFEREJEGhICIiCQoFERFJUCiIiEiCQkFERBJsf/+tYjNbC3zQwrf3IviBnyjROkeD1jkaWrPOB7h77/TC/T4UWsPMZrt7RXvXY1/SOkeD1jkacrHO6j4SEZEEhYKIiCREPRTua+8KtAOtczRonaOhzdc50mMKIiKSKuotBRERSaJQEBGRhEiGgplNMbOFZlZpZte2d33aipkNNrPnzWy+mc0zs6vC8nIze8bM3g3/7RGWm5ndHX4Pb5nZ4e27Bi1nZnlm9rqZPRm+HmZmr4br/IiZFYTlheHrynD60Pasd0uZWZmZ/cnMFoTb+5jOvp3N7Gvh/+u5ZvYHMyvqbNvZzO43szVmNjepbK+3q5lNDed/18ym7k0dIhcKZpYH3AOcAYwDLjGzzvIzbHXA1919LHA08KVw3a4F/uHuo4B/hK8h+A5GhY8rgXv3fZXbzFXA/KTX3wfuCtd5A3BFWH4FsMHdRwJ3hfPtj34CPO3uBwITCNa9025nMxsIfBWocPfxQB5wMZ1vO/8fMCWtbK+2q5mVAzcBRwETgZsag6RZ3D1SD+AYYEbS6+uA69q7Xjla178ApwELgf5hWX9gYfj8F8AlSfMn5tufHsCg8I/lFOBJwAiu8oynb3NgBnBM+DwezmftvQ57ub7dgSXp9e7M2xkYCCwDysPt9iQwuTNuZ2AoMLel2xW4BPhFUnnKfHt6RK6lwK7/XI2qwrJOJWwuHwa8CvR195UA4b99wtk6y3fxY+BbQEP4uiew0d3rwtfJ65VY53D6pnD+/clwYC3wm7DL7FdmVkIn3s7uvhz4EbAUWEmw3ebQubdzo73drq3a3lEMBctS1qnOyzWzrsCfgavdffPuZs1Stl99F2Z2NrDG3eckF2eZ1ZsxbX8RBw4H7nX3w4Bt7OpSyGa/X+ew++NcYBgwACgh6D5J15m28540tY6tWvcohkIVMDjp9SBgRTvVpc2ZWT5BIPze3R8Li1ebWf9wen9gTVjeGb6L44CPmtn7wMMEXUg/BsrMLB7Ok7xeiXUOp5cC6/dlhdtAFVDl7q+Gr/9EEBKdeTtPApa4+1p3rwUeA46lc2/nRnu7XVu1vaMYCrOAUeFZCwUEg1XT2rlObcLMDPg1MN/d70yaNA1oPANhKsFYQ2P5Z8KzGI4GNjU2U/cX7n6duw9y96EE2/I5d/8U8DzwiXC29HVu/C4+Ec6/Xx1BuvsqYJmZjQmLTgXeoRNvZ4Juo6PNrEv4/7xxnTvtdk6yt9t1BnC6mfUIW1inh2XN096DKu00kHMmsAhYDNzQ3vVpw/U6nqCZ+BbwRvg4k6Av9R/Au+G/5eH8RnAm1mLgbYIzO9p9PVqx/icBT4bPhwMzgUrgj0BhWF4Uvq4Mpw9v73q3cF0PBWaH2/oJoEdn387AzcACYC7wO6Cws21n4A8EYya1BEf8V7RkuwKfDde9Erh8b+qg21yIiEhCFLuPRESkCQoFERFJUCiIiEiCQkFERBIUCiIikqBQEBGRBIWCiIgk/H+yvHKpq9g4JwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, Q=None):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Probability to sample a random action. Float between 0 and 1.\n",
    "        Q: hot-start the algorithm with a Q value function (optional)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is a list of tuples giving the episode lengths and rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    if Q is None:\n",
    "        Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize state\n",
    "        state = env.reset()\n",
    "        # choose action from state based on the policy\n",
    "        action = policy(state)\n",
    "        # loop for each step in the episode\n",
    "        for step in itertools.count():\n",
    "            # take action and observe the next state, reward, and if we are done\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # choose an action from next_state using the policy\n",
    "            next_action = policy(next_state)\n",
    "            # update the total episode reward\n",
    "            episode_reward += reward\n",
    "            # sarsa Q update\n",
    "            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        stats.append((step, episode_reward))\n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)\n",
    "\n",
    "Q_sarsa, (episode_lengths_sarsa, episode_returns_sarsa) = sarsa(env, 1000)\n",
    "\n",
    "# We will help you with plotting this time\n",
    "plt.plot(episode_lengths_sarsa)\n",
    "plt.title('Episode lengths SARSA')\n",
    "plt.show()\n",
    "plt.plot(episode_returns_sarsa)\n",
    "plt.title('Episode returns SARSA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e8df3908ce548708b64f69e11a34896",
     "grade": false,
     "grade_id": "cell-0eaf4b925ab3ea34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We learn the optimal (non-exploring) policy while using another policy to do exploration, which is where we arrive at _off-policy_ learning. In the simplest variant, we learn our own value by bootstrapping based on the action value corresponding to the best action we could take, while the exploration policy actual follows the $\\epsilon$-greedy strategy. This is known as Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "954556134388a34f8d4b9a07834180c5",
     "grade": true,
     "grade_id": "cell-a87637d2e582fec0",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3681.92it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, Q=None):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Probability to sample a random action. Float between 0 and 1.\n",
    "        Q: hot-start the algorithm with a Q value function (optional)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is a list of tuples giving the episode lengths and rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    if Q is None:\n",
    "        Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize the state\n",
    "        state = env.reset()\n",
    "        # loop for each step in the episode\n",
    "        for t in itertools.count():\n",
    "            # choose action from state based on the policy\n",
    "            action = policy(state)\n",
    "            # take a step in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # update statistics\n",
    "            episode_reward += reward\n",
    "            # update Q\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] += alpha * (reward + discount_factor *\n",
    "                Q[next_state][best_next_action] - Q[state][action])\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state\n",
    "            state = next_state\n",
    "\n",
    "        stats.append((t, episode_reward))\n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)\n",
    "\n",
    "Q_q_learning, (episode_lengths_q_learning, episode_returns_q_learning) = q_learning(env, 1000)\n",
    "\n",
    "# We will help you with plotting this time\n",
    "plt.plot(episode_lengths_q_learning)\n",
    "plt.title('Episode lengths Q-learning')\n",
    "plt.show()\n",
    "plt.plot(episode_returns_q_learning)\n",
    "plt.title('Episode returns Q-learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f695c6e9d66afd4fc7a49b565419ba5d",
     "grade": false,
     "grade_id": "cell-9f1fcee44ba712c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now compare the episode returns while learning for Q-learning and Sarsa (maybe run some more iterations?), by plotting the returns for both algorithms in a single plot, like in the book, Example 6.6. In order to be able to compare them, you may want to zoom in on the y-axis and smooth the returns (e.g. plotting the $n$ episode average instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c1a110fe85c38220afed145a8cf09bc",
     "grade": true,
     "grade_id": "cell-69ed62a52a44dd78",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3731.82it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3988.65it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4100.92it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4067.27it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3832.12it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3990.17it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3866.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3840.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3984.54it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3932.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4016.41it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3980.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4155.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4008.22it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3993.13it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3949.59it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4011.57it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4016.86it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3892.07it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3907.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4076.96it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3941.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3966.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4058.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4011.73it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3288.22it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3736.44it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3423.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3896.30it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3923.86it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3906.89it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3821.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3568.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3703.12it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3944.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3954.82it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3921.09it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4182.66it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3944.24it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3855.79it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4095.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3988.67it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3459.62it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3788.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3916.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3305.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3929.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3981.61it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4027.95it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4012.41it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3818.43it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4169.33it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3993.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4079.69it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3993.30it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3972.76it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4113.67it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4057.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3984.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3837.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4013.08it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4184.44it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4076.43it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4003.65it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3977.52it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4103.20it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4010.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4027.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4098.13it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3992.08it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3687.07it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3656.76it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4132.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3898.74it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3978.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4025.15it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3991.27it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4041.04it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4117.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4193.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3822.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3962.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4013.30it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4041.20it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4072.18it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4075.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3923.69it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4034.53it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4005.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4049.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4036.49it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3393.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4028.36it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4107.01it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4056.56it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3859.17it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3980.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3948.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4035.92it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4023.81it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4125.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3946.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4237.63it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4242.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4057.01it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4089.89it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4179.31it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4278.80it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3790.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3879.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4056.27it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3500.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3725.88it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3861.54it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4063.59it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3664.87it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4226.56it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4003.78it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3878.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3595.27it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3873.54it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4175.75it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4053.13it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3683.05it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3911.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4278.23it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4213.69it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3994.42it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3635.04it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3943.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3637.99it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4085.10it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4093.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4048.63it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3943.31it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3968.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3904.24it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4151.08it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4158.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4179.77it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4170.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3748.10it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3921.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4125.86it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4134.06it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4275.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4317.36it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4298.75it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4244.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4044.88it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3939.95it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3609.99it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3961.01it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3723.43it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3811.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4234.38it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4152.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3569.15it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4185.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3557.06it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4212.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4169.50it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4268.92it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3916.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3601.76it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4116.57it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3927.06it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3458.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4214.89it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4169.93it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4215.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4233.22it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4208.64it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4225.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4254.02it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4243.10it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4230.80it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4238.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4253.25it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4250.44it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4248.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4191.64it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3900.01it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3600.23it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3936.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3944.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3899.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4108.13it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3534.84it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3923.09it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4128.48it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4294.22it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4283.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4216.49it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4209.07it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4258.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4222.68it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4230.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4318.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4275.99it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_episode_rewards_average(algo, n=100):\n",
    "    episode_rewards = np.zeros(1000)\n",
    "    for i in range(n):\n",
    "        _, (_, episode_returns) = algo(env, 1000)\n",
    "        episode_rewards += episode_returns\n",
    "    return episode_rewards / n\n",
    "\n",
    "# Compute episode rewards for SARSA\n",
    "episode_rewards_sarsa = get_episode_rewards_average(sarsa)\n",
    "# COmpute episode rewards for Q-Learning\n",
    "episode_rewards_q_learning = get_episode_rewards_average(q_learning)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_rewards_sarsa[100:], label=\"SARSA\")\n",
    "plt.plot(episode_rewards_q_learning[100:], label=\"Q-Learning\")\n",
    "plt.title(\"Averaged episode rewards for SARSA and Q-Learning from episode 100 to 1000\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed2526b0c0f17f055f520f67072c59ac",
     "grade": false,
     "grade_id": "cell-7ef9de74c57a4f0c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which algorithm achieves higher return during learning? How does this compare to Example 6.6 from the book? Try to explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3357293c326223f2a02cae0f38ca24a",
     "grade": true,
     "grade_id": "cell-7acf9de8c94a171f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For the Windy Gridworld environment, Q-Learning achieves the higher returns during learning, which is opposed to the Cliff Walking environment. Since there is no risk in running of a cliff in this example, Q-Learning can learn the optiomal policy and progress through it even with an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2f954f745662334010f6fb0fcfd9896",
     "grade": false,
     "grade_id": "cell-316d3cfd35d55387",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After we have learned the policy, we do not care about exploration any more and we may switch to a deterministic (greedy) policy instead. If we evaluate this for both Sarsa and Q-learning (actually, for Q-learning the learned policy is already deterministic), which policy would you expect to perform better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "011f8038ac100bfdc5e40b78c1bdc2f8",
     "grade": true,
     "grade_id": "cell-ea5058e6f352d717",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Since Q-Learning seems to have learned the better policy, it should perform slightly better than SARSA even in a deterministic setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57ab54058d433e24421d1e1224a9bc87",
     "grade": false,
     "grade_id": "cell-8bcc6f5839a36860",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Please run the experiments to test your hypothesis (print or plot your results). How many runs do you need to evaluate the policy? Note: without learning, the order of the episodes is not relevant so a normal `plt.plot` may not be the most appropriate choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the policies are deterministic, we only need one run to determine the rewards for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "149c39efef43f1807d2b06e6bc50bf95",
     "grade": true,
     "grade_id": "cell-55f9d1767bb7c011",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 622.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 666.29it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Get the deterministic policies\n",
    "policy_sarsa = make_epsilon_greedy_policy(Q_sarsa, 0.0, env.action_space.n)\n",
    "policy_q_learning= make_epsilon_greedy_policy(Q_q_learning, 0.0, env.action_space.n)\n",
    "\n",
    "def get_rewards(env, policy, num_episodes=1):\n",
    "    rewards = []\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        episode_reward = 0\n",
    "        # initialize the state\n",
    "        state = env.reset()\n",
    "        # loop for each step in the episode\n",
    "        for t in itertools.count():\n",
    "            # choose action from state based on the policy\n",
    "            action = policy(state)\n",
    "            # take a step in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # update statistics\n",
    "            episode_reward += reward\n",
    "            # check for finished episode\n",
    "            if done:\n",
    "                break\n",
    "            # otherwise update state\n",
    "            state = next_state\n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "determ_rewards_sarsa = get_rewards(env, policy_sarsa)\n",
    "determ_rewards_q_learning = get_rewards(env, policy_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic rewards SARSA: -16.0\n",
      "Deterministic rewards Q-Learning: -15.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Deterministic rewards SARSA: {}\".format(np.mean(determ_rewards_sarsa)))\n",
    "print(\"Deterministic rewards Q-Learning: {}\".format(np.mean(determ_rewards_q_learning)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e70351edfa59760104962f08d541557b",
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Deep Q-Network (DQN) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e27fe8f72a248bbcf1f7a21e5550e657",
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bogdanfloris/miniconda3/envs/rl2019/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11a9c014ee5fbe790ce999428cc22658",
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9692b7acb09d018d9f80ce95685b81d5",
     "grade": false,
     "grade_id": "cell-bf2ac21267daffbb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you think of a way in which we can still use a tabular approach? Why would this work and can you think of an example problem where this would not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ffce6fca4071a1b543186db1b74cc98",
     "grade": true,
     "grade_id": "cell-b0fa2cb0c2cd2a63",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "One way in which we can still use a tabular approach is to aggregate the values into bins. All the values that make up a state in CartPole have a minimum and a maximum. This approach fails for problems where we cannot determine a minimum or a maximum value and thus we cannot determine the ammount of bins to use for aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c5bddd080e12cb076c845d093a70ed7",
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84b9c38718c952ef8e62486fc9bf5e4a",
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ef7d14363dc2aa4beb638856c57a58c",
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(torch.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b9a48f9aee9ebc46da01c6f11cd789a",
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7227d52671b410864319222a98e27d1",
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b3265bef151a12fe6969c378af76be2",
     "grade": false,
     "grade_id": "cell-b5b012e42dd2029e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What could be a problem with doing gradient updates on a sequence of state, action pairs $((s_t, a_t), (s_{t+1}, a_{t+1}) ...)$ observed while interacting with the environment? How will using *experience replay* help to overcome this (potential problem)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75e1a8b00b2bfa9b7dd8805b371c6a4e",
     "grade": true,
     "grade_id": "cell-70a2e59541668a25",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Doing gradient updates on a sequence of state, action pairs clearly violates the independence assumption of the data. Since the states come in a sequence, they are very much dependent on each other. Experience replay helps overcome this problem by randomly sampling from a big pool of state, action pairs. The data is still dependent but clearly not as much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b3bbd8aaf3aade515736d0d07917a61",
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the `push` function that adds a transition to the replay buffer, and the sample function that returns a batch of samples. It should keep at most the maximum number of transitions. Also implement the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c64677cbc7efad32a949783b7c9b53b7",
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            raise ValueError('Not enough memory to sample batch.')\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6865749b3a8810bdaaf1604a9cea42e7",
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.01641128, -0.01327852, -0.00861558, -0.04991406]), 0, 1.0, array([-0.01667685, -0.20827588, -0.00961386,  0.24003818]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "354743bd76d6ba43d95b5b177443a202",
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61d26d0dec0133f2aa737ed4711d6e08",
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "270ab31d4bb29dc9a05223c16a4967a7",
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(1.0, 0.05, 1000)\n",
    "\n",
    "def get_epsilon(it):\n",
    "    if it >= 1000:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return epsilons[it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1a81dd07e1b7a98d2cd06ebc171ebdd",
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a27ddd898>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84685c23e4eb899d7fed3a87b7f8915e",
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function that takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon (which we will pass later). Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Unlike numpy, PyTorch has no argmax function, but Google is your friend... Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "882f51819100c850120e73340aec387d",
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return int(np.random.rand() * 2)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = model(torch.Tensor(state))\n",
    "            return torch.argmax(output).item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21f939075cb0c8dde152dabf47568a9d",
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = select_action(model, s, 0.05)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e66ac58d65710439ddf7cdf19a50cd8c",
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4839aac72a80552046ebecc40c1615cf",
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c466ee49add35cb1ec6a3e4a85f733c9",
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    q_val = model(state)\n",
    "    return q_val[np.arange(action.size()[0]), action]\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    q_val_next = model(next_state)\n",
    "    targets = reward + discount_factor * (1 - done).float() * torch.max(q_val_next, dim=1)[0]\n",
    "    return targets\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "877c400001292b619e6871c1366524b9",
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5715031027793884\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2841924b22cdf411348a0eb6080502",
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06dd71aae5c3c699f2b707b348a88107",
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3f61b2ca270d84ab9b28d989dd65d4c",
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        # Get the initial state\n",
    "        state = env.reset()\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            # Get the epsilon\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            \n",
    "            # Get an action and take a step\n",
    "            action = select_action(model, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Save to memory\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Train\n",
    "            loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "            # Print the step\n",
    "            print(\"Step {} ({}); Episode {}/{}; Loss: {}\".format(t, global_steps, i + 1, num_episodes, loss))\n",
    "            \n",
    "            # Increase the global steps\n",
    "            global_steps += 1         \n",
    "            # Check for done\n",
    "            if done:\n",
    "                episode_durations.append(t)\n",
    "                break\n",
    "            \n",
    "            # Otherwise continue\n",
    "            state = next_state\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (0); Episode 1/100; Loss: None\n",
      "Step 1 (1); Episode 1/100; Loss: None\n",
      "Step 2 (2); Episode 1/100; Loss: None\n",
      "Step 3 (3); Episode 1/100; Loss: None\n",
      "Step 4 (4); Episode 1/100; Loss: None\n",
      "Step 5 (5); Episode 1/100; Loss: None\n",
      "Step 6 (6); Episode 1/100; Loss: None\n",
      "Step 7 (7); Episode 1/100; Loss: None\n",
      "Step 8 (8); Episode 1/100; Loss: None\n",
      "Step 9 (9); Episode 1/100; Loss: None\n",
      "Step 10 (10); Episode 1/100; Loss: None\n",
      "Step 11 (11); Episode 1/100; Loss: None\n",
      "Step 12 (12); Episode 1/100; Loss: None\n",
      "Step 13 (13); Episode 1/100; Loss: None\n",
      "Step 14 (14); Episode 1/100; Loss: None\n",
      "Step 15 (15); Episode 1/100; Loss: None\n",
      "Step 16 (16); Episode 1/100; Loss: None\n",
      "Step 17 (17); Episode 1/100; Loss: None\n",
      "Step 18 (18); Episode 1/100; Loss: None\n",
      "Step 19 (19); Episode 1/100; Loss: None\n",
      "Step 20 (20); Episode 1/100; Loss: None\n",
      "Step 21 (21); Episode 1/100; Loss: None\n",
      "Step 22 (22); Episode 1/100; Loss: None\n",
      "Step 23 (23); Episode 1/100; Loss: None\n",
      "Step 24 (24); Episode 1/100; Loss: None\n",
      "Step 25 (25); Episode 1/100; Loss: None\n",
      "Step 26 (26); Episode 1/100; Loss: None\n",
      "Step 27 (27); Episode 1/100; Loss: None\n",
      "Step 28 (28); Episode 1/100; Loss: None\n",
      "Step 29 (29); Episode 1/100; Loss: None\n",
      "Step 30 (30); Episode 1/100; Loss: None\n",
      "Step 31 (31); Episode 1/100; Loss: None\n",
      "Step 32 (32); Episode 1/100; Loss: None\n",
      "Step 33 (33); Episode 1/100; Loss: None\n",
      "Step 34 (34); Episode 1/100; Loss: None\n",
      "Step 35 (35); Episode 1/100; Loss: None\n",
      "Step 36 (36); Episode 1/100; Loss: None\n",
      "Step 37 (37); Episode 1/100; Loss: None\n",
      "Step 38 (38); Episode 1/100; Loss: None\n",
      "Step 39 (39); Episode 1/100; Loss: None\n",
      "Step 40 (40); Episode 1/100; Loss: None\n",
      "Step 41 (41); Episode 1/100; Loss: None\n",
      "Step 42 (42); Episode 1/100; Loss: None\n",
      "Step 43 (43); Episode 1/100; Loss: None\n",
      "Step 44 (44); Episode 1/100; Loss: None\n",
      "Step 45 (45); Episode 1/100; Loss: None\n",
      "Step 46 (46); Episode 1/100; Loss: None\n",
      "Step 47 (47); Episode 1/100; Loss: None\n",
      "Step 48 (48); Episode 1/100; Loss: None\n",
      "Step 49 (49); Episode 1/100; Loss: None\n",
      "Step 50 (50); Episode 1/100; Loss: None\n",
      "Step 51 (51); Episode 1/100; Loss: None\n",
      "Step 52 (52); Episode 1/100; Loss: None\n",
      "Step 53 (53); Episode 1/100; Loss: None\n",
      "Step 54 (54); Episode 1/100; Loss: None\n",
      "Step 55 (55); Episode 1/100; Loss: None\n",
      "Step 56 (56); Episode 1/100; Loss: None\n",
      "Step 57 (57); Episode 1/100; Loss: None\n",
      "Step 58 (58); Episode 1/100; Loss: None\n",
      "Step 59 (59); Episode 1/100; Loss: None\n",
      "Step 60 (60); Episode 1/100; Loss: None\n",
      "Step 61 (61); Episode 1/100; Loss: None\n",
      "Step 62 (62); Episode 1/100; Loss: None\n",
      "Step 63 (63); Episode 1/100; Loss: 0.641791045665741\n",
      "Step 0 (64); Episode 2/100; Loss: 0.6372376680374146\n",
      "Step 1 (65); Episode 2/100; Loss: 0.6330956220626831\n",
      "Step 2 (66); Episode 2/100; Loss: 0.6275447607040405\n",
      "Step 3 (67); Episode 2/100; Loss: 0.6246970295906067\n",
      "Step 4 (68); Episode 2/100; Loss: 0.6178159713745117\n",
      "Step 5 (69); Episode 2/100; Loss: 0.6107783913612366\n",
      "Step 6 (70); Episode 2/100; Loss: 0.6056423187255859\n",
      "Step 7 (71); Episode 2/100; Loss: 0.5998889803886414\n",
      "Step 8 (72); Episode 2/100; Loss: 0.5933822989463806\n",
      "Step 9 (73); Episode 2/100; Loss: 0.5876176357269287\n",
      "Step 0 (74); Episode 3/100; Loss: 0.5955238342285156\n",
      "Step 1 (75); Episode 3/100; Loss: 0.5774903297424316\n",
      "Step 2 (76); Episode 3/100; Loss: 0.5736711621284485\n",
      "Step 3 (77); Episode 3/100; Loss: 0.5736026167869568\n",
      "Step 4 (78); Episode 3/100; Loss: 0.5675044655799866\n",
      "Step 5 (79); Episode 3/100; Loss: 0.5758098363876343\n",
      "Step 6 (80); Episode 3/100; Loss: 0.5566968321800232\n",
      "Step 7 (81); Episode 3/100; Loss: 0.5663806796073914\n",
      "Step 8 (82); Episode 3/100; Loss: 0.5470165014266968\n",
      "Step 9 (83); Episode 3/100; Loss: 0.5432831645011902\n",
      "Step 0 (84); Episode 4/100; Loss: 0.5505250096321106\n",
      "Step 1 (85); Episode 4/100; Loss: 0.5427200794219971\n",
      "Step 2 (86); Episode 4/100; Loss: 0.5348780751228333\n",
      "Step 3 (87); Episode 4/100; Loss: 0.5359296798706055\n",
      "Step 4 (88); Episode 4/100; Loss: 0.526172935962677\n",
      "Step 5 (89); Episode 4/100; Loss: 0.5310914516448975\n",
      "Step 6 (90); Episode 4/100; Loss: 0.5233935117721558\n",
      "Step 7 (91); Episode 4/100; Loss: 0.5254814624786377\n",
      "Step 8 (92); Episode 4/100; Loss: 0.5233454704284668\n",
      "Step 9 (93); Episode 4/100; Loss: 0.5136463046073914\n",
      "Step 10 (94); Episode 4/100; Loss: 0.5039628744125366\n",
      "Step 11 (95); Episode 4/100; Loss: 0.5023165345191956\n",
      "Step 12 (96); Episode 4/100; Loss: 0.494428426027298\n",
      "Step 0 (97); Episode 5/100; Loss: 0.48771902918815613\n",
      "Step 1 (98); Episode 5/100; Loss: 0.49372756481170654\n",
      "Step 2 (99); Episode 5/100; Loss: 0.48678094148635864\n",
      "Step 3 (100); Episode 5/100; Loss: 0.47797369956970215\n",
      "Step 4 (101); Episode 5/100; Loss: 0.48044753074645996\n",
      "Step 5 (102); Episode 5/100; Loss: 0.48307132720947266\n",
      "Step 6 (103); Episode 5/100; Loss: 0.48137447237968445\n",
      "Step 7 (104); Episode 5/100; Loss: 0.47901371121406555\n",
      "Step 8 (105); Episode 5/100; Loss: 0.4627157747745514\n",
      "Step 9 (106); Episode 5/100; Loss: 0.4805110991001129\n",
      "Step 10 (107); Episode 5/100; Loss: 0.45602449774742126\n",
      "Step 11 (108); Episode 5/100; Loss: 0.46816787123680115\n",
      "Step 12 (109); Episode 5/100; Loss: 0.4625515341758728\n",
      "Step 13 (110); Episode 5/100; Loss: 0.45228293538093567\n",
      "Step 0 (111); Episode 6/100; Loss: 0.441685289144516\n",
      "Step 1 (112); Episode 6/100; Loss: 0.45942944288253784\n",
      "Step 2 (113); Episode 6/100; Loss: 0.4400743246078491\n",
      "Step 3 (114); Episode 6/100; Loss: 0.446834534406662\n",
      "Step 4 (115); Episode 6/100; Loss: 0.4378158748149872\n",
      "Step 5 (116); Episode 6/100; Loss: 0.4385056495666504\n",
      "Step 6 (117); Episode 6/100; Loss: 0.44299086928367615\n",
      "Step 7 (118); Episode 6/100; Loss: 0.4208833575248718\n",
      "Step 8 (119); Episode 6/100; Loss: 0.41332462430000305\n",
      "Step 9 (120); Episode 6/100; Loss: 0.41922804713249207\n",
      "Step 10 (121); Episode 6/100; Loss: 0.42907676100730896\n",
      "Step 11 (122); Episode 6/100; Loss: 0.43308576941490173\n",
      "Step 0 (123); Episode 7/100; Loss: 0.42328566312789917\n",
      "Step 1 (124); Episode 7/100; Loss: 0.4209752082824707\n",
      "Step 2 (125); Episode 7/100; Loss: 0.4060124158859253\n",
      "Step 3 (126); Episode 7/100; Loss: 0.41571423411369324\n",
      "Step 4 (127); Episode 7/100; Loss: 0.4535762667655945\n",
      "Step 5 (128); Episode 7/100; Loss: 0.41190192103385925\n",
      "Step 6 (129); Episode 7/100; Loss: 0.415976881980896\n",
      "Step 7 (130); Episode 7/100; Loss: 0.37629905343055725\n",
      "Step 8 (131); Episode 7/100; Loss: 0.3943711519241333\n",
      "Step 9 (132); Episode 7/100; Loss: 0.42561620473861694\n",
      "Step 10 (133); Episode 7/100; Loss: 0.3768056035041809\n",
      "Step 11 (134); Episode 7/100; Loss: 0.361918568611145\n",
      "Step 12 (135); Episode 7/100; Loss: 0.4109724164009094\n",
      "Step 13 (136); Episode 7/100; Loss: 0.38472965359687805\n",
      "Step 14 (137); Episode 7/100; Loss: 0.42922472953796387\n",
      "Step 15 (138); Episode 7/100; Loss: 0.4251626133918762\n",
      "Step 16 (139); Episode 7/100; Loss: 0.394618421792984\n",
      "Step 17 (140); Episode 7/100; Loss: 0.430768221616745\n",
      "Step 18 (141); Episode 7/100; Loss: 0.3976861834526062\n",
      "Step 19 (142); Episode 7/100; Loss: 0.33838480710983276\n",
      "Step 20 (143); Episode 7/100; Loss: 0.38000407814979553\n",
      "Step 0 (144); Episode 8/100; Loss: 0.36582082509994507\n",
      "Step 1 (145); Episode 8/100; Loss: 0.38848328590393066\n",
      "Step 2 (146); Episode 8/100; Loss: 0.3365519344806671\n",
      "Step 3 (147); Episode 8/100; Loss: 0.37886595726013184\n",
      "Step 4 (148); Episode 8/100; Loss: 0.36942896246910095\n",
      "Step 5 (149); Episode 8/100; Loss: 0.41751769185066223\n",
      "Step 6 (150); Episode 8/100; Loss: 0.36256733536720276\n",
      "Step 7 (151); Episode 8/100; Loss: 0.323743999004364\n",
      "Step 8 (152); Episode 8/100; Loss: 0.3620424270629883\n",
      "Step 9 (153); Episode 8/100; Loss: 0.404636949300766\n",
      "Step 10 (154); Episode 8/100; Loss: 0.40886190533638\n",
      "Step 11 (155); Episode 8/100; Loss: 0.3485777974128723\n",
      "Step 12 (156); Episode 8/100; Loss: 0.45973917841911316\n",
      "Step 13 (157); Episode 8/100; Loss: 0.31704097986221313\n",
      "Step 14 (158); Episode 8/100; Loss: 0.3364117443561554\n",
      "Step 15 (159); Episode 8/100; Loss: 0.34877973794937134\n",
      "Step 16 (160); Episode 8/100; Loss: 0.29151344299316406\n",
      "Step 17 (161); Episode 8/100; Loss: 0.44989603757858276\n",
      "Step 18 (162); Episode 8/100; Loss: 0.35066837072372437\n",
      "Step 0 (163); Episode 9/100; Loss: 0.47600656747817993\n",
      "Step 1 (164); Episode 9/100; Loss: 0.4380004405975342\n",
      "Step 2 (165); Episode 9/100; Loss: 0.4512789249420166\n",
      "Step 3 (166); Episode 9/100; Loss: 0.4413219392299652\n",
      "Step 4 (167); Episode 9/100; Loss: 0.3949899673461914\n",
      "Step 5 (168); Episode 9/100; Loss: 0.4375138282775879\n",
      "Step 6 (169); Episode 9/100; Loss: 0.3397679924964905\n",
      "Step 7 (170); Episode 9/100; Loss: 0.26644378900527954\n",
      "Step 8 (171); Episode 9/100; Loss: 0.29819369316101074\n",
      "Step 9 (172); Episode 9/100; Loss: 0.3632853329181671\n",
      "Step 10 (173); Episode 9/100; Loss: 0.37372440099716187\n",
      "Step 11 (174); Episode 9/100; Loss: 0.5177911520004272\n",
      "Step 0 (175); Episode 10/100; Loss: 0.37157779932022095\n",
      "Step 1 (176); Episode 10/100; Loss: 0.3602796494960785\n",
      "Step 2 (177); Episode 10/100; Loss: 0.5188438892364502\n",
      "Step 3 (178); Episode 10/100; Loss: 0.47325843572616577\n",
      "Step 4 (179); Episode 10/100; Loss: 0.45077893137931824\n",
      "Step 5 (180); Episode 10/100; Loss: 0.3926670253276825\n",
      "Step 6 (181); Episode 10/100; Loss: 0.4438764452934265\n",
      "Step 7 (182); Episode 10/100; Loss: 0.2148607075214386\n",
      "Step 8 (183); Episode 10/100; Loss: 0.44467267394065857\n",
      "Step 9 (184); Episode 10/100; Loss: 0.44061079621315\n",
      "Step 10 (185); Episode 10/100; Loss: 0.47614583373069763\n",
      "Step 11 (186); Episode 10/100; Loss: 0.4538755416870117\n",
      "Step 12 (187); Episode 10/100; Loss: 0.5102134346961975\n",
      "Step 13 (188); Episode 10/100; Loss: 0.3851318955421448\n",
      "Step 14 (189); Episode 10/100; Loss: 0.4271881878376007\n",
      "Step 15 (190); Episode 10/100; Loss: 0.34793737530708313\n",
      "Step 0 (191); Episode 11/100; Loss: 0.2581930160522461\n",
      "Step 1 (192); Episode 11/100; Loss: 0.35462719202041626\n",
      "Step 2 (193); Episode 11/100; Loss: 0.46658819913864136\n",
      "Step 3 (194); Episode 11/100; Loss: 0.3916093409061432\n",
      "Step 4 (195); Episode 11/100; Loss: 0.4730431139469147\n",
      "Step 5 (196); Episode 11/100; Loss: 0.2887127995491028\n",
      "Step 6 (197); Episode 11/100; Loss: 0.17686523497104645\n",
      "Step 7 (198); Episode 11/100; Loss: 0.48218846321105957\n",
      "Step 0 (199); Episode 12/100; Loss: 0.59159255027771\n",
      "Step 1 (200); Episode 12/100; Loss: 0.5499783754348755\n",
      "Step 2 (201); Episode 12/100; Loss: 0.4023493230342865\n",
      "Step 3 (202); Episode 12/100; Loss: 0.5938735008239746\n",
      "Step 4 (203); Episode 12/100; Loss: 0.47561153769493103\n",
      "Step 5 (204); Episode 12/100; Loss: 0.3853300213813782\n",
      "Step 6 (205); Episode 12/100; Loss: 0.3041905462741852\n",
      "Step 7 (206); Episode 12/100; Loss: 0.24039556086063385\n",
      "Step 8 (207); Episode 12/100; Loss: 0.5046824216842651\n",
      "Step 9 (208); Episode 12/100; Loss: 0.4317389130592346\n",
      "Step 10 (209); Episode 12/100; Loss: 0.5477255582809448\n",
      "Step 11 (210); Episode 12/100; Loss: 0.34074369072914124\n",
      "Step 12 (211); Episode 12/100; Loss: 0.40418827533721924\n",
      "Step 13 (212); Episode 12/100; Loss: 0.4773171842098236\n",
      "Step 14 (213); Episode 12/100; Loss: 0.3177463114261627\n",
      "Step 15 (214); Episode 12/100; Loss: 0.5098124146461487\n",
      "Step 16 (215); Episode 12/100; Loss: 0.5700239539146423\n",
      "Step 17 (216); Episode 12/100; Loss: 0.7106050848960876\n",
      "Step 18 (217); Episode 12/100; Loss: 0.7826060652732849\n",
      "Step 19 (218); Episode 12/100; Loss: 0.16672994196414948\n",
      "Step 0 (219); Episode 13/100; Loss: 0.3533014953136444\n",
      "Step 1 (220); Episode 13/100; Loss: 0.5765035152435303\n",
      "Step 2 (221); Episode 13/100; Loss: 0.1987948715686798\n",
      "Step 3 (222); Episode 13/100; Loss: 0.5749481916427612\n",
      "Step 4 (223); Episode 13/100; Loss: 0.2873900830745697\n",
      "Step 5 (224); Episode 13/100; Loss: 0.5636764168739319\n",
      "Step 6 (225); Episode 13/100; Loss: 0.47851601243019104\n",
      "Step 7 (226); Episode 13/100; Loss: 0.48359766602516174\n",
      "Step 8 (227); Episode 13/100; Loss: 0.8488197922706604\n",
      "Step 9 (228); Episode 13/100; Loss: 0.366100549697876\n",
      "Step 10 (229); Episode 13/100; Loss: 0.37770113348960876\n",
      "Step 11 (230); Episode 13/100; Loss: 0.5945092439651489\n",
      "Step 12 (231); Episode 13/100; Loss: 0.6483260989189148\n",
      "Step 13 (232); Episode 13/100; Loss: 0.5033193826675415\n",
      "Step 14 (233); Episode 13/100; Loss: 0.8849420547485352\n",
      "Step 15 (234); Episode 13/100; Loss: 0.7230873703956604\n",
      "Step 0 (235); Episode 14/100; Loss: 0.3630151152610779\n",
      "Step 1 (236); Episode 14/100; Loss: 0.3759066164493561\n",
      "Step 2 (237); Episode 14/100; Loss: 0.7387318015098572\n",
      "Step 3 (238); Episode 14/100; Loss: 0.21260134875774384\n",
      "Step 4 (239); Episode 14/100; Loss: 0.9035102725028992\n",
      "Step 5 (240); Episode 14/100; Loss: 0.270457923412323\n",
      "Step 6 (241); Episode 14/100; Loss: 0.33060455322265625\n",
      "Step 7 (242); Episode 14/100; Loss: 0.4409085512161255\n",
      "Step 8 (243); Episode 14/100; Loss: 0.6675266623497009\n",
      "Step 9 (244); Episode 14/100; Loss: 0.9409930109977722\n",
      "Step 0 (245); Episode 15/100; Loss: 0.533411979675293\n",
      "Step 1 (246); Episode 15/100; Loss: 1.0545729398727417\n",
      "Step 2 (247); Episode 15/100; Loss: 0.5961791276931763\n",
      "Step 3 (248); Episode 15/100; Loss: 1.1026973724365234\n",
      "Step 4 (249); Episode 15/100; Loss: 0.2679706811904907\n",
      "Step 5 (250); Episode 15/100; Loss: 0.31431567668914795\n",
      "Step 6 (251); Episode 15/100; Loss: 0.7625244855880737\n",
      "Step 7 (252); Episode 15/100; Loss: 0.643585741519928\n",
      "Step 8 (253); Episode 15/100; Loss: 0.7476466298103333\n",
      "Step 9 (254); Episode 15/100; Loss: 0.5707061886787415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 (255); Episode 15/100; Loss: 0.4598398506641388\n",
      "Step 0 (256); Episode 16/100; Loss: 0.786130964756012\n",
      "Step 1 (257); Episode 16/100; Loss: 0.9614671468734741\n",
      "Step 2 (258); Episode 16/100; Loss: 0.7259426116943359\n",
      "Step 3 (259); Episode 16/100; Loss: 0.6379468441009521\n",
      "Step 4 (260); Episode 16/100; Loss: 0.7861831784248352\n",
      "Step 5 (261); Episode 16/100; Loss: 0.5646349787712097\n",
      "Step 6 (262); Episode 16/100; Loss: 0.2791557013988495\n",
      "Step 7 (263); Episode 16/100; Loss: 0.8421782851219177\n",
      "Step 8 (264); Episode 16/100; Loss: 0.9706583023071289\n",
      "Step 9 (265); Episode 16/100; Loss: 0.48753610253334045\n",
      "Step 10 (266); Episode 16/100; Loss: 0.39739200472831726\n",
      "Step 0 (267); Episode 17/100; Loss: 0.7164843082427979\n",
      "Step 1 (268); Episode 17/100; Loss: 0.6025497317314148\n",
      "Step 2 (269); Episode 17/100; Loss: 1.0404343605041504\n",
      "Step 3 (270); Episode 17/100; Loss: 0.5167807340621948\n",
      "Step 4 (271); Episode 17/100; Loss: 0.4399937391281128\n",
      "Step 5 (272); Episode 17/100; Loss: 0.9024041891098022\n",
      "Step 6 (273); Episode 17/100; Loss: 0.8590593338012695\n",
      "Step 7 (274); Episode 17/100; Loss: 1.099839210510254\n",
      "Step 8 (275); Episode 17/100; Loss: 0.6570477485656738\n",
      "Step 9 (276); Episode 17/100; Loss: 0.7204868197441101\n",
      "Step 0 (277); Episode 18/100; Loss: 0.3903310000896454\n",
      "Step 1 (278); Episode 18/100; Loss: 0.538947343826294\n",
      "Step 2 (279); Episode 18/100; Loss: 0.21979182958602905\n",
      "Step 3 (280); Episode 18/100; Loss: 0.799656867980957\n",
      "Step 4 (281); Episode 18/100; Loss: 0.5182470679283142\n",
      "Step 5 (282); Episode 18/100; Loss: 0.607265830039978\n",
      "Step 6 (283); Episode 18/100; Loss: 1.1718127727508545\n",
      "Step 7 (284); Episode 18/100; Loss: 0.9892672300338745\n",
      "Step 8 (285); Episode 18/100; Loss: 0.7466978430747986\n",
      "Step 9 (286); Episode 18/100; Loss: 1.0159538984298706\n",
      "Step 10 (287); Episode 18/100; Loss: 0.5292792916297913\n",
      "Step 11 (288); Episode 18/100; Loss: 0.5280090570449829\n",
      "Step 12 (289); Episode 18/100; Loss: 0.5390779972076416\n",
      "Step 13 (290); Episode 18/100; Loss: 0.6494793891906738\n",
      "Step 14 (291); Episode 18/100; Loss: 0.7789137959480286\n",
      "Step 15 (292); Episode 18/100; Loss: 0.7498971223831177\n",
      "Step 0 (293); Episode 19/100; Loss: 0.30219578742980957\n",
      "Step 1 (294); Episode 19/100; Loss: 0.4987564980983734\n",
      "Step 2 (295); Episode 19/100; Loss: 0.3752482533454895\n",
      "Step 3 (296); Episode 19/100; Loss: 0.9871330857276917\n",
      "Step 4 (297); Episode 19/100; Loss: 0.9938427209854126\n",
      "Step 5 (298); Episode 19/100; Loss: 0.39950308203697205\n",
      "Step 6 (299); Episode 19/100; Loss: 1.0268968343734741\n",
      "Step 7 (300); Episode 19/100; Loss: 0.40869128704071045\n",
      "Step 8 (301); Episode 19/100; Loss: 0.8433961868286133\n",
      "Step 9 (302); Episode 19/100; Loss: 0.3634059727191925\n",
      "Step 10 (303); Episode 19/100; Loss: 0.319783478975296\n",
      "Step 11 (304); Episode 19/100; Loss: 0.5771409273147583\n",
      "Step 12 (305); Episode 19/100; Loss: 0.33614203333854675\n",
      "Step 0 (306); Episode 20/100; Loss: 0.44969484210014343\n",
      "Step 1 (307); Episode 20/100; Loss: 0.4043899476528168\n",
      "Step 2 (308); Episode 20/100; Loss: 0.7968317270278931\n",
      "Step 3 (309); Episode 20/100; Loss: 0.8210334777832031\n",
      "Step 4 (310); Episode 20/100; Loss: 0.8898587226867676\n",
      "Step 5 (311); Episode 20/100; Loss: 0.8727931380271912\n",
      "Step 6 (312); Episode 20/100; Loss: 0.7027091383934021\n",
      "Step 7 (313); Episode 20/100; Loss: 0.621085524559021\n",
      "Step 8 (314); Episode 20/100; Loss: 0.7241888642311096\n",
      "Step 0 (315); Episode 21/100; Loss: 0.6757832765579224\n",
      "Step 1 (316); Episode 21/100; Loss: 0.7996290326118469\n",
      "Step 2 (317); Episode 21/100; Loss: 0.41834330558776855\n",
      "Step 3 (318); Episode 21/100; Loss: 0.775873601436615\n",
      "Step 4 (319); Episode 21/100; Loss: 0.5902232527732849\n",
      "Step 5 (320); Episode 21/100; Loss: 0.9059330224990845\n",
      "Step 6 (321); Episode 21/100; Loss: 0.5934625864028931\n",
      "Step 7 (322); Episode 21/100; Loss: 0.4538083076477051\n",
      "Step 8 (323); Episode 21/100; Loss: 0.5153288245201111\n",
      "Step 9 (324); Episode 21/100; Loss: 0.7998554706573486\n",
      "Step 10 (325); Episode 21/100; Loss: 0.5203582644462585\n",
      "Step 11 (326); Episode 21/100; Loss: 0.8035427331924438\n",
      "Step 12 (327); Episode 21/100; Loss: 0.5626425743103027\n",
      "Step 13 (328); Episode 21/100; Loss: 0.46808379888534546\n",
      "Step 14 (329); Episode 21/100; Loss: 0.7614709734916687\n",
      "Step 15 (330); Episode 21/100; Loss: 0.49701374769210815\n",
      "Step 16 (331); Episode 21/100; Loss: 0.15979072451591492\n",
      "Step 0 (332); Episode 22/100; Loss: 1.1351652145385742\n",
      "Step 1 (333); Episode 22/100; Loss: 0.7677457928657532\n",
      "Step 2 (334); Episode 22/100; Loss: 0.7606512308120728\n",
      "Step 3 (335); Episode 22/100; Loss: 0.7015533447265625\n",
      "Step 4 (336); Episode 22/100; Loss: 0.9428647756576538\n",
      "Step 5 (337); Episode 22/100; Loss: 0.39518746733665466\n",
      "Step 6 (338); Episode 22/100; Loss: 0.7457283139228821\n",
      "Step 7 (339); Episode 22/100; Loss: 0.6201816201210022\n",
      "Step 8 (340); Episode 22/100; Loss: 0.8117257356643677\n",
      "Step 9 (341); Episode 22/100; Loss: 0.7460346221923828\n",
      "Step 10 (342); Episode 22/100; Loss: 0.6798809766769409\n",
      "Step 11 (343); Episode 22/100; Loss: 0.26401227712631226\n",
      "Step 12 (344); Episode 22/100; Loss: 0.5789252519607544\n",
      "Step 13 (345); Episode 22/100; Loss: 0.5032466053962708\n",
      "Step 14 (346); Episode 22/100; Loss: 0.7002604007720947\n",
      "Step 15 (347); Episode 22/100; Loss: 0.27137815952301025\n",
      "Step 0 (348); Episode 23/100; Loss: 0.8292965292930603\n",
      "Step 1 (349); Episode 23/100; Loss: 0.1656312644481659\n",
      "Step 2 (350); Episode 23/100; Loss: 0.5532000064849854\n",
      "Step 3 (351); Episode 23/100; Loss: 0.5582210421562195\n",
      "Step 4 (352); Episode 23/100; Loss: 0.8071165680885315\n",
      "Step 5 (353); Episode 23/100; Loss: 1.2454921007156372\n",
      "Step 6 (354); Episode 23/100; Loss: 0.310332715511322\n",
      "Step 7 (355); Episode 23/100; Loss: 0.581872284412384\n",
      "Step 8 (356); Episode 23/100; Loss: 1.0577468872070312\n",
      "Step 9 (357); Episode 23/100; Loss: 0.3109654188156128\n",
      "Step 0 (358); Episode 24/100; Loss: 0.25787660479545593\n",
      "Step 1 (359); Episode 24/100; Loss: 0.3603888750076294\n",
      "Step 2 (360); Episode 24/100; Loss: 0.2660520076751709\n",
      "Step 3 (361); Episode 24/100; Loss: 0.4979988932609558\n",
      "Step 4 (362); Episode 24/100; Loss: 0.375657856464386\n",
      "Step 5 (363); Episode 24/100; Loss: 0.7705019116401672\n",
      "Step 6 (364); Episode 24/100; Loss: 0.6057568788528442\n",
      "Step 7 (365); Episode 24/100; Loss: 0.4508775472640991\n",
      "Step 8 (366); Episode 24/100; Loss: 0.6484302878379822\n",
      "Step 9 (367); Episode 24/100; Loss: 0.6317451596260071\n",
      "Step 10 (368); Episode 24/100; Loss: 0.42406439781188965\n",
      "Step 0 (369); Episode 25/100; Loss: 0.38493672013282776\n",
      "Step 1 (370); Episode 25/100; Loss: 0.678371787071228\n",
      "Step 2 (371); Episode 25/100; Loss: 0.5332115888595581\n",
      "Step 3 (372); Episode 25/100; Loss: 0.3248305022716522\n",
      "Step 4 (373); Episode 25/100; Loss: 0.5844985842704773\n",
      "Step 5 (374); Episode 25/100; Loss: 0.5302536487579346\n",
      "Step 6 (375); Episode 25/100; Loss: 0.6645187139511108\n",
      "Step 7 (376); Episode 25/100; Loss: 0.25711557269096375\n",
      "Step 8 (377); Episode 25/100; Loss: 0.1354818046092987\n",
      "Step 9 (378); Episode 25/100; Loss: 0.37995585799217224\n",
      "Step 10 (379); Episode 25/100; Loss: 0.5853208899497986\n",
      "Step 11 (380); Episode 25/100; Loss: 0.5809051394462585\n",
      "Step 12 (381); Episode 25/100; Loss: 0.22486869990825653\n",
      "Step 13 (382); Episode 25/100; Loss: 0.6083043217658997\n",
      "Step 14 (383); Episode 25/100; Loss: 0.48535704612731934\n",
      "Step 15 (384); Episode 25/100; Loss: 0.48710039258003235\n",
      "Step 16 (385); Episode 25/100; Loss: 0.3150002360343933\n",
      "Step 0 (386); Episode 26/100; Loss: 0.4173806309700012\n",
      "Step 1 (387); Episode 26/100; Loss: 0.35889020562171936\n",
      "Step 2 (388); Episode 26/100; Loss: 0.20705361664295197\n",
      "Step 3 (389); Episode 26/100; Loss: 0.1879526674747467\n",
      "Step 4 (390); Episode 26/100; Loss: 0.2534361183643341\n",
      "Step 5 (391); Episode 26/100; Loss: 0.2596057057380676\n",
      "Step 6 (392); Episode 26/100; Loss: 0.3241511285305023\n",
      "Step 7 (393); Episode 26/100; Loss: 0.4469238519668579\n",
      "Step 8 (394); Episode 26/100; Loss: 0.29193025827407837\n",
      "Step 9 (395); Episode 26/100; Loss: 0.4291931390762329\n",
      "Step 10 (396); Episode 26/100; Loss: 0.6036083102226257\n",
      "Step 11 (397); Episode 26/100; Loss: 0.4578292667865753\n",
      "Step 12 (398); Episode 26/100; Loss: 0.26301106810569763\n",
      "Step 13 (399); Episode 26/100; Loss: 0.35334715247154236\n",
      "Step 14 (400); Episode 26/100; Loss: 0.24127747118473053\n",
      "Step 15 (401); Episode 26/100; Loss: 0.35661450028419495\n",
      "Step 0 (402); Episode 27/100; Loss: 0.31600654125213623\n",
      "Step 1 (403); Episode 27/100; Loss: 0.3216676414012909\n",
      "Step 2 (404); Episode 27/100; Loss: 0.32353997230529785\n",
      "Step 3 (405); Episode 27/100; Loss: 0.48963046073913574\n",
      "Step 4 (406); Episode 27/100; Loss: 0.2385178953409195\n",
      "Step 5 (407); Episode 27/100; Loss: 0.36447468400001526\n",
      "Step 6 (408); Episode 27/100; Loss: 0.5251642465591431\n",
      "Step 7 (409); Episode 27/100; Loss: 0.0797533467411995\n",
      "Step 8 (410); Episode 27/100; Loss: 0.21822227537631989\n",
      "Step 0 (411); Episode 28/100; Loss: 0.22110921144485474\n",
      "Step 1 (412); Episode 28/100; Loss: 0.22290590405464172\n",
      "Step 2 (413); Episode 28/100; Loss: 0.4351262152194977\n",
      "Step 3 (414); Episode 28/100; Loss: 0.14879517257213593\n",
      "Step 4 (415); Episode 28/100; Loss: 0.2799808979034424\n",
      "Step 5 (416); Episode 28/100; Loss: 0.4724862277507782\n",
      "Step 6 (417); Episode 28/100; Loss: 0.2666272819042206\n",
      "Step 7 (418); Episode 28/100; Loss: 0.06785623729228973\n",
      "Step 8 (419); Episode 28/100; Loss: 0.26916518807411194\n",
      "Step 9 (420); Episode 28/100; Loss: 0.3896813988685608\n",
      "Step 10 (421); Episode 28/100; Loss: 0.2629498839378357\n",
      "Step 0 (422); Episode 29/100; Loss: 0.5707001686096191\n",
      "Step 1 (423); Episode 29/100; Loss: 0.1898135095834732\n",
      "Step 2 (424); Episode 29/100; Loss: 0.5026111602783203\n",
      "Step 3 (425); Episode 29/100; Loss: 0.06103978306055069\n",
      "Step 4 (426); Episode 29/100; Loss: 0.19049112498760223\n",
      "Step 5 (427); Episode 29/100; Loss: 0.3520035147666931\n",
      "Step 6 (428); Episode 29/100; Loss: 0.12452630698680878\n",
      "Step 7 (429); Episode 29/100; Loss: 0.23789995908737183\n",
      "Step 8 (430); Episode 29/100; Loss: 0.24490192532539368\n",
      "Step 9 (431); Episode 29/100; Loss: 0.21103191375732422\n",
      "Step 10 (432); Episode 29/100; Loss: 0.17099688947200775\n",
      "Step 11 (433); Episode 29/100; Loss: 0.16532160341739655\n",
      "Step 12 (434); Episode 29/100; Loss: 0.16859270632266998\n",
      "Step 13 (435); Episode 29/100; Loss: 0.16929298639297485\n",
      "Step 14 (436); Episode 29/100; Loss: 0.37658244371414185\n",
      "Step 0 (437); Episode 30/100; Loss: 0.42432937026023865\n",
      "Step 1 (438); Episode 30/100; Loss: 0.2193535566329956\n",
      "Step 2 (439); Episode 30/100; Loss: 0.20700091123580933\n",
      "Step 3 (440); Episode 30/100; Loss: 0.317851185798645\n",
      "Step 4 (441); Episode 30/100; Loss: 0.2607487142086029\n",
      "Step 5 (442); Episode 30/100; Loss: 0.11111323535442352\n",
      "Step 6 (443); Episode 30/100; Loss: 0.15043294429779053\n",
      "Step 7 (444); Episode 30/100; Loss: 0.20482982695102692\n",
      "Step 8 (445); Episode 30/100; Loss: 0.196567565202713\n",
      "Step 9 (446); Episode 30/100; Loss: 0.19076047837734222\n",
      "Step 10 (447); Episode 30/100; Loss: 0.11053065210580826\n",
      "Step 11 (448); Episode 30/100; Loss: 0.05961562320590019\n",
      "Step 12 (449); Episode 30/100; Loss: 0.23572340607643127\n",
      "Step 13 (450); Episode 30/100; Loss: 0.18624553084373474\n",
      "Step 14 (451); Episode 30/100; Loss: 0.25619208812713623\n",
      "Step 15 (452); Episode 30/100; Loss: 0.1974153369665146\n",
      "Step 0 (453); Episode 31/100; Loss: 0.34002065658569336\n",
      "Step 1 (454); Episode 31/100; Loss: 0.15027207136154175\n",
      "Step 2 (455); Episode 31/100; Loss: 0.2803507447242737\n",
      "Step 3 (456); Episode 31/100; Loss: 0.19584627449512482\n",
      "Step 4 (457); Episode 31/100; Loss: 0.1519353836774826\n",
      "Step 5 (458); Episode 31/100; Loss: 0.3618084788322449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 (459); Episode 31/100; Loss: 0.191765695810318\n",
      "Step 7 (460); Episode 31/100; Loss: 0.10705292224884033\n",
      "Step 8 (461); Episode 31/100; Loss: 0.10459644347429276\n",
      "Step 9 (462); Episode 31/100; Loss: 0.27249273657798767\n",
      "Step 10 (463); Episode 31/100; Loss: 0.14506936073303223\n",
      "Step 11 (464); Episode 31/100; Loss: 0.05934857204556465\n",
      "Step 12 (465); Episode 31/100; Loss: 0.19391363859176636\n",
      "Step 13 (466); Episode 31/100; Loss: 0.10158553719520569\n",
      "Step 14 (467); Episode 31/100; Loss: 0.23668164014816284\n",
      "Step 15 (468); Episode 31/100; Loss: 0.25671714544296265\n",
      "Step 16 (469); Episode 31/100; Loss: 0.19325025379657745\n",
      "Step 17 (470); Episode 31/100; Loss: 0.1482297033071518\n",
      "Step 18 (471); Episode 31/100; Loss: 0.09835708886384964\n",
      "Step 19 (472); Episode 31/100; Loss: 0.24608588218688965\n",
      "Step 20 (473); Episode 31/100; Loss: 0.2473219484090805\n",
      "Step 21 (474); Episode 31/100; Loss: 0.10014627128839493\n",
      "Step 22 (475); Episode 31/100; Loss: 0.1864972859621048\n",
      "Step 23 (476); Episode 31/100; Loss: 0.22499841451644897\n",
      "Step 24 (477); Episode 31/100; Loss: 0.36348021030426025\n",
      "Step 25 (478); Episode 31/100; Loss: 0.14675061404705048\n",
      "Step 0 (479); Episode 32/100; Loss: 0.10647398978471756\n",
      "Step 1 (480); Episode 32/100; Loss: 0.22670207917690277\n",
      "Step 2 (481); Episode 32/100; Loss: 0.2501084804534912\n",
      "Step 3 (482); Episode 32/100; Loss: 0.148049995303154\n",
      "Step 4 (483); Episode 32/100; Loss: 0.09409605711698532\n",
      "Step 5 (484); Episode 32/100; Loss: 0.09974807500839233\n",
      "Step 6 (485); Episode 32/100; Loss: 0.22085373103618622\n",
      "Step 7 (486); Episode 32/100; Loss: 0.15153315663337708\n",
      "Step 8 (487); Episode 32/100; Loss: 0.17571166157722473\n",
      "Step 9 (488); Episode 32/100; Loss: 0.1116911768913269\n",
      "Step 10 (489); Episode 32/100; Loss: 0.14316412806510925\n",
      "Step 11 (490); Episode 32/100; Loss: 0.13797737658023834\n",
      "Step 12 (491); Episode 32/100; Loss: 0.3243277072906494\n",
      "Step 13 (492); Episode 32/100; Loss: 0.17056405544281006\n",
      "Step 14 (493); Episode 32/100; Loss: 0.1345403492450714\n",
      "Step 15 (494); Episode 32/100; Loss: 0.23232825100421906\n",
      "Step 16 (495); Episode 32/100; Loss: 0.19252878427505493\n",
      "Step 0 (496); Episode 33/100; Loss: 0.1338539719581604\n",
      "Step 1 (497); Episode 33/100; Loss: 0.14950257539749146\n",
      "Step 2 (498); Episode 33/100; Loss: 0.22213055193424225\n",
      "Step 3 (499); Episode 33/100; Loss: 0.25105005502700806\n",
      "Step 4 (500); Episode 33/100; Loss: 0.21686598658561707\n",
      "Step 5 (501); Episode 33/100; Loss: 0.09805524349212646\n",
      "Step 6 (502); Episode 33/100; Loss: 0.18283306062221527\n",
      "Step 7 (503); Episode 33/100; Loss: 0.19186602532863617\n",
      "Step 8 (504); Episode 33/100; Loss: 0.1983145773410797\n",
      "Step 9 (505); Episode 33/100; Loss: 0.2492336630821228\n",
      "Step 10 (506); Episode 33/100; Loss: 0.19772732257843018\n",
      "Step 11 (507); Episode 33/100; Loss: 0.15524646639823914\n",
      "Step 12 (508); Episode 33/100; Loss: 0.17471931874752045\n",
      "Step 13 (509); Episode 33/100; Loss: 0.18971525132656097\n",
      "Step 14 (510); Episode 33/100; Loss: 0.10821229219436646\n",
      "Step 15 (511); Episode 33/100; Loss: 0.2845107316970825\n",
      "Step 16 (512); Episode 33/100; Loss: 0.13903237879276276\n",
      "Step 17 (513); Episode 33/100; Loss: 0.06227297708392143\n",
      "Step 18 (514); Episode 33/100; Loss: 0.20016536116600037\n",
      "Step 19 (515); Episode 33/100; Loss: 0.2850237786769867\n",
      "Step 20 (516); Episode 33/100; Loss: 0.1884758323431015\n",
      "Step 21 (517); Episode 33/100; Loss: 0.18921920657157898\n",
      "Step 22 (518); Episode 33/100; Loss: 0.24066099524497986\n",
      "Step 23 (519); Episode 33/100; Loss: 0.09810815751552582\n",
      "Step 24 (520); Episode 33/100; Loss: 0.323866605758667\n",
      "Step 25 (521); Episode 33/100; Loss: 0.22751754522323608\n",
      "Step 26 (522); Episode 33/100; Loss: 0.24109844863414764\n",
      "Step 27 (523); Episode 33/100; Loss: 0.09905938804149628\n",
      "Step 28 (524); Episode 33/100; Loss: 0.1519288867712021\n",
      "Step 29 (525); Episode 33/100; Loss: 0.2954515516757965\n",
      "Step 30 (526); Episode 33/100; Loss: 0.25489360094070435\n",
      "Step 0 (527); Episode 34/100; Loss: 0.19465957581996918\n",
      "Step 1 (528); Episode 34/100; Loss: 0.06369495391845703\n",
      "Step 2 (529); Episode 34/100; Loss: 0.1051076278090477\n",
      "Step 3 (530); Episode 34/100; Loss: 0.1920647770166397\n",
      "Step 4 (531); Episode 34/100; Loss: 0.15184837579727173\n",
      "Step 5 (532); Episode 34/100; Loss: 0.2392176240682602\n",
      "Step 6 (533); Episode 34/100; Loss: 0.36488524079322815\n",
      "Step 7 (534); Episode 34/100; Loss: 0.13462013006210327\n",
      "Step 8 (535); Episode 34/100; Loss: 0.3217141628265381\n",
      "Step 9 (536); Episode 34/100; Loss: 0.143620565533638\n",
      "Step 10 (537); Episode 34/100; Loss: 0.24495060741901398\n",
      "Step 11 (538); Episode 34/100; Loss: 0.2619008719921112\n",
      "Step 12 (539); Episode 34/100; Loss: 0.09554163366556168\n",
      "Step 13 (540); Episode 34/100; Loss: 0.2181113362312317\n",
      "Step 14 (541); Episode 34/100; Loss: 0.13342130184173584\n",
      "Step 15 (542); Episode 34/100; Loss: 0.1768091470003128\n",
      "Step 16 (543); Episode 34/100; Loss: 0.04779454320669174\n",
      "Step 17 (544); Episode 34/100; Loss: 0.11001866310834885\n",
      "Step 18 (545); Episode 34/100; Loss: 0.2323060780763626\n",
      "Step 19 (546); Episode 34/100; Loss: 0.1743238866329193\n",
      "Step 20 (547); Episode 34/100; Loss: 0.12557052075862885\n",
      "Step 21 (548); Episode 34/100; Loss: 0.24418720602989197\n",
      "Step 22 (549); Episode 34/100; Loss: 0.08960244804620743\n",
      "Step 23 (550); Episode 34/100; Loss: 0.19581322371959686\n",
      "Step 24 (551); Episode 34/100; Loss: 0.12201490253210068\n",
      "Step 25 (552); Episode 34/100; Loss: 0.0750475525856018\n",
      "Step 26 (553); Episode 34/100; Loss: 0.42538169026374817\n",
      "Step 27 (554); Episode 34/100; Loss: 0.07379033416509628\n",
      "Step 28 (555); Episode 34/100; Loss: 0.09136169403791428\n",
      "Step 29 (556); Episode 34/100; Loss: 0.18335163593292236\n",
      "Step 30 (557); Episode 34/100; Loss: 0.10973604023456573\n",
      "Step 31 (558); Episode 34/100; Loss: 0.0912623256444931\n",
      "Step 0 (559); Episode 35/100; Loss: 0.18097060918807983\n",
      "Step 1 (560); Episode 35/100; Loss: 0.18685701489448547\n",
      "Step 2 (561); Episode 35/100; Loss: 0.2704169750213623\n",
      "Step 3 (562); Episode 35/100; Loss: 0.23513780534267426\n",
      "Step 4 (563); Episode 35/100; Loss: 0.3819179832935333\n",
      "Step 5 (564); Episode 35/100; Loss: 0.23431116342544556\n",
      "Step 6 (565); Episode 35/100; Loss: 0.1001630574464798\n",
      "Step 7 (566); Episode 35/100; Loss: 0.14623905718326569\n",
      "Step 8 (567); Episode 35/100; Loss: 0.13318303227424622\n",
      "Step 9 (568); Episode 35/100; Loss: 0.17179526388645172\n",
      "Step 10 (569); Episode 35/100; Loss: 0.14336037635803223\n",
      "Step 11 (570); Episode 35/100; Loss: 0.17823520302772522\n",
      "Step 12 (571); Episode 35/100; Loss: 0.22410309314727783\n",
      "Step 13 (572); Episode 35/100; Loss: 0.10205959528684616\n",
      "Step 14 (573); Episode 35/100; Loss: 0.05555061995983124\n",
      "Step 15 (574); Episode 35/100; Loss: 0.1773001104593277\n",
      "Step 16 (575); Episode 35/100; Loss: 0.27299612760543823\n",
      "Step 17 (576); Episode 35/100; Loss: 0.17680063843727112\n",
      "Step 18 (577); Episode 35/100; Loss: 0.18813982605934143\n",
      "Step 19 (578); Episode 35/100; Loss: 0.18019574880599976\n",
      "Step 20 (579); Episode 35/100; Loss: 0.2520669400691986\n",
      "Step 21 (580); Episode 35/100; Loss: 0.12455414235591888\n",
      "Step 22 (581); Episode 35/100; Loss: 0.1698608100414276\n",
      "Step 23 (582); Episode 35/100; Loss: 0.08920293301343918\n",
      "Step 24 (583); Episode 35/100; Loss: 0.13310876488685608\n",
      "Step 25 (584); Episode 35/100; Loss: 0.23017749190330505\n",
      "Step 26 (585); Episode 35/100; Loss: 0.139211505651474\n",
      "Step 27 (586); Episode 35/100; Loss: 0.0505659356713295\n",
      "Step 28 (587); Episode 35/100; Loss: 0.24164588749408722\n",
      "Step 29 (588); Episode 35/100; Loss: 0.21245743334293365\n",
      "Step 30 (589); Episode 35/100; Loss: 0.29162734746932983\n",
      "Step 31 (590); Episode 35/100; Loss: 0.13017556071281433\n",
      "Step 32 (591); Episode 35/100; Loss: 0.18200162053108215\n",
      "Step 33 (592); Episode 35/100; Loss: 0.13806791603565216\n",
      "Step 34 (593); Episode 35/100; Loss: 0.2560614049434662\n",
      "Step 35 (594); Episode 35/100; Loss: 0.20468240976333618\n",
      "Step 36 (595); Episode 35/100; Loss: 0.1843435913324356\n",
      "Step 37 (596); Episode 35/100; Loss: 0.20444859564304352\n",
      "Step 38 (597); Episode 35/100; Loss: 0.12364844232797623\n",
      "Step 39 (598); Episode 35/100; Loss: 0.19662562012672424\n",
      "Step 40 (599); Episode 35/100; Loss: 0.17334027588367462\n",
      "Step 41 (600); Episode 35/100; Loss: 0.10543221235275269\n",
      "Step 42 (601); Episode 35/100; Loss: 0.16233953833580017\n",
      "Step 43 (602); Episode 35/100; Loss: 0.2999122142791748\n",
      "Step 44 (603); Episode 35/100; Loss: 0.015624329447746277\n",
      "Step 45 (604); Episode 35/100; Loss: 0.11798045784235\n",
      "Step 46 (605); Episode 35/100; Loss: 0.06350813806056976\n",
      "Step 47 (606); Episode 35/100; Loss: 0.11652428656816483\n",
      "Step 48 (607); Episode 35/100; Loss: 0.015050170943140984\n",
      "Step 49 (608); Episode 35/100; Loss: 0.08758530765771866\n",
      "Step 50 (609); Episode 35/100; Loss: 0.0876578539609909\n",
      "Step 51 (610); Episode 35/100; Loss: 0.12797944247722626\n",
      "Step 52 (611); Episode 35/100; Loss: 0.14616276323795319\n",
      "Step 53 (612); Episode 35/100; Loss: 0.0106969578191638\n",
      "Step 54 (613); Episode 35/100; Loss: 0.08264376223087311\n",
      "Step 55 (614); Episode 35/100; Loss: 0.23771576583385468\n",
      "Step 56 (615); Episode 35/100; Loss: 0.1706746220588684\n",
      "Step 57 (616); Episode 35/100; Loss: 0.19106824696063995\n",
      "Step 58 (617); Episode 35/100; Loss: 0.16193485260009766\n",
      "Step 59 (618); Episode 35/100; Loss: 0.23270876705646515\n",
      "Step 60 (619); Episode 35/100; Loss: 0.13729940354824066\n",
      "Step 61 (620); Episode 35/100; Loss: 0.10257112234830856\n",
      "Step 62 (621); Episode 35/100; Loss: 0.047199636697769165\n",
      "Step 63 (622); Episode 35/100; Loss: 0.05901717767119408\n",
      "Step 64 (623); Episode 35/100; Loss: 0.10036905854940414\n",
      "Step 65 (624); Episode 35/100; Loss: 0.15828773379325867\n",
      "Step 66 (625); Episode 35/100; Loss: 0.06481606513261795\n",
      "Step 67 (626); Episode 35/100; Loss: 0.23383644223213196\n",
      "Step 68 (627); Episode 35/100; Loss: 0.1671016365289688\n",
      "Step 69 (628); Episode 35/100; Loss: 0.14162686467170715\n",
      "Step 70 (629); Episode 35/100; Loss: 0.1918853223323822\n",
      "Step 71 (630); Episode 35/100; Loss: 0.24376101791858673\n",
      "Step 72 (631); Episode 35/100; Loss: 0.24992570281028748\n",
      "Step 73 (632); Episode 35/100; Loss: 0.1832684427499771\n",
      "Step 74 (633); Episode 35/100; Loss: 0.18630163371562958\n",
      "Step 75 (634); Episode 35/100; Loss: 0.1848137229681015\n",
      "Step 76 (635); Episode 35/100; Loss: 0.1082567647099495\n",
      "Step 77 (636); Episode 35/100; Loss: 0.05543237552046776\n",
      "Step 78 (637); Episode 35/100; Loss: 0.25224876403808594\n",
      "Step 79 (638); Episode 35/100; Loss: 0.22669635713100433\n",
      "Step 80 (639); Episode 35/100; Loss: 0.1008966863155365\n",
      "Step 81 (640); Episode 35/100; Loss: 0.16014957427978516\n",
      "Step 82 (641); Episode 35/100; Loss: 0.10568422079086304\n",
      "Step 83 (642); Episode 35/100; Loss: 0.10130009800195694\n",
      "Step 84 (643); Episode 35/100; Loss: 0.1009291410446167\n",
      "Step 85 (644); Episode 35/100; Loss: 0.09716031700372696\n",
      "Step 86 (645); Episode 35/100; Loss: 0.37355148792266846\n",
      "Step 87 (646); Episode 35/100; Loss: 0.051734596490859985\n",
      "Step 88 (647); Episode 35/100; Loss: 0.18860100209712982\n",
      "Step 89 (648); Episode 35/100; Loss: 0.18728779256343842\n",
      "Step 90 (649); Episode 35/100; Loss: 0.1259198635816574\n",
      "Step 91 (650); Episode 35/100; Loss: 0.09861496090888977\n",
      "Step 92 (651); Episode 35/100; Loss: 0.10392508655786514\n",
      "Step 93 (652); Episode 35/100; Loss: 0.1264401078224182\n",
      "Step 94 (653); Episode 35/100; Loss: 0.09282847493886948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 95 (654); Episode 35/100; Loss: 0.17829541862010956\n",
      "Step 96 (655); Episode 35/100; Loss: 0.19106420874595642\n",
      "Step 97 (656); Episode 35/100; Loss: 0.170136496424675\n",
      "Step 98 (657); Episode 35/100; Loss: 0.09898021072149277\n",
      "Step 99 (658); Episode 35/100; Loss: 0.14148907363414764\n",
      "Step 100 (659); Episode 35/100; Loss: 0.39434105157852173\n",
      "Step 101 (660); Episode 35/100; Loss: 0.18015821278095245\n",
      "Step 102 (661); Episode 35/100; Loss: 0.17878513038158417\n",
      "Step 103 (662); Episode 35/100; Loss: 0.09911856800317764\n",
      "Step 104 (663); Episode 35/100; Loss: 0.21769852936267853\n",
      "Step 105 (664); Episode 35/100; Loss: 0.09930728375911713\n",
      "Step 106 (665); Episode 35/100; Loss: 0.008832769468426704\n",
      "Step 107 (666); Episode 35/100; Loss: 0.1345086693763733\n",
      "Step 108 (667); Episode 35/100; Loss: 0.13099096715450287\n",
      "Step 109 (668); Episode 35/100; Loss: 0.01405431516468525\n",
      "Step 110 (669); Episode 35/100; Loss: 0.16936668753623962\n",
      "Step 111 (670); Episode 35/100; Loss: 0.21639208495616913\n",
      "Step 112 (671); Episode 35/100; Loss: 0.08133944869041443\n",
      "Step 113 (672); Episode 35/100; Loss: 0.1804378777742386\n",
      "Step 114 (673); Episode 35/100; Loss: 0.18400295078754425\n",
      "Step 115 (674); Episode 35/100; Loss: 0.1357612907886505\n",
      "Step 116 (675); Episode 35/100; Loss: 0.15366485714912415\n",
      "Step 117 (676); Episode 35/100; Loss: 0.09477133303880692\n",
      "Step 118 (677); Episode 35/100; Loss: 0.05343155562877655\n",
      "Step 119 (678); Episode 35/100; Loss: 0.10479968786239624\n",
      "Step 120 (679); Episode 35/100; Loss: 0.22631223499774933\n",
      "Step 121 (680); Episode 35/100; Loss: 0.14420369267463684\n",
      "Step 122 (681); Episode 35/100; Loss: 0.0976526141166687\n",
      "Step 123 (682); Episode 35/100; Loss: 0.07426688820123672\n",
      "Step 124 (683); Episode 35/100; Loss: 0.06366955488920212\n",
      "Step 0 (684); Episode 36/100; Loss: 0.14788818359375\n",
      "Step 1 (685); Episode 36/100; Loss: 0.09242639690637589\n",
      "Step 2 (686); Episode 36/100; Loss: 0.09885508567094803\n",
      "Step 3 (687); Episode 36/100; Loss: 0.1401333063840866\n",
      "Step 4 (688); Episode 36/100; Loss: 0.17786605656147003\n",
      "Step 5 (689); Episode 36/100; Loss: 0.14159457385540009\n",
      "Step 6 (690); Episode 36/100; Loss: 0.14774870872497559\n",
      "Step 7 (691); Episode 36/100; Loss: 0.23446440696716309\n",
      "Step 8 (692); Episode 36/100; Loss: 0.18667474389076233\n",
      "Step 9 (693); Episode 36/100; Loss: 0.13982224464416504\n",
      "Step 10 (694); Episode 36/100; Loss: 0.1360516995191574\n",
      "Step 11 (695); Episode 36/100; Loss: 0.19537466764450073\n",
      "Step 12 (696); Episode 36/100; Loss: 0.10572975873947144\n",
      "Step 13 (697); Episode 36/100; Loss: 0.14971923828125\n",
      "Step 14 (698); Episode 36/100; Loss: 0.181756392121315\n",
      "Step 15 (699); Episode 36/100; Loss: 0.0492246150970459\n",
      "Step 16 (700); Episode 36/100; Loss: 0.09225927293300629\n",
      "Step 17 (701); Episode 36/100; Loss: 0.2288210391998291\n",
      "Step 18 (702); Episode 36/100; Loss: 0.34828564524650574\n",
      "Step 19 (703); Episode 36/100; Loss: 0.22257573902606964\n",
      "Step 20 (704); Episode 36/100; Loss: 0.16785413026809692\n",
      "Step 21 (705); Episode 36/100; Loss: 0.1386246383190155\n",
      "Step 22 (706); Episode 36/100; Loss: 0.05394449457526207\n",
      "Step 23 (707); Episode 36/100; Loss: 0.14789621531963348\n",
      "Step 24 (708); Episode 36/100; Loss: 0.18831968307495117\n",
      "Step 25 (709); Episode 36/100; Loss: 0.08890669792890549\n",
      "Step 26 (710); Episode 36/100; Loss: 0.12680231034755707\n",
      "Step 27 (711); Episode 36/100; Loss: 0.043766748160123825\n",
      "Step 28 (712); Episode 36/100; Loss: 0.18757756054401398\n",
      "Step 29 (713); Episode 36/100; Loss: 0.13860079646110535\n",
      "Step 30 (714); Episode 36/100; Loss: 0.2286105751991272\n",
      "Step 31 (715); Episode 36/100; Loss: 0.05264158174395561\n",
      "Step 32 (716); Episode 36/100; Loss: 0.07750115543603897\n",
      "Step 33 (717); Episode 36/100; Loss: 0.05411398783326149\n",
      "Step 34 (718); Episode 36/100; Loss: 0.06371001154184341\n",
      "Step 35 (719); Episode 36/100; Loss: 0.08331616967916489\n",
      "Step 36 (720); Episode 36/100; Loss: 0.14662541449069977\n",
      "Step 37 (721); Episode 36/100; Loss: 0.13795094192028046\n",
      "Step 38 (722); Episode 36/100; Loss: 0.09781882911920547\n",
      "Step 39 (723); Episode 36/100; Loss: 0.08793667703866959\n",
      "Step 40 (724); Episode 36/100; Loss: 0.254366010427475\n",
      "Step 41 (725); Episode 36/100; Loss: 0.1804918795824051\n",
      "Step 42 (726); Episode 36/100; Loss: 0.09504439681768417\n",
      "Step 43 (727); Episode 36/100; Loss: 0.16182008385658264\n",
      "Step 44 (728); Episode 36/100; Loss: 0.11920440196990967\n",
      "Step 45 (729); Episode 36/100; Loss: 0.19001799821853638\n",
      "Step 46 (730); Episode 36/100; Loss: 0.22761157155036926\n",
      "Step 47 (731); Episode 36/100; Loss: 0.05105644464492798\n",
      "Step 48 (732); Episode 36/100; Loss: 0.1048469990491867\n",
      "Step 49 (733); Episode 36/100; Loss: 0.1405058354139328\n",
      "Step 50 (734); Episode 36/100; Loss: 0.0888153687119484\n",
      "Step 51 (735); Episode 36/100; Loss: 0.16557444632053375\n",
      "Step 52 (736); Episode 36/100; Loss: 0.24677014350891113\n",
      "Step 53 (737); Episode 36/100; Loss: 0.18219907581806183\n",
      "Step 54 (738); Episode 36/100; Loss: 0.16315346956253052\n",
      "Step 55 (739); Episode 36/100; Loss: 0.24963156878948212\n",
      "Step 56 (740); Episode 36/100; Loss: 0.18909749388694763\n",
      "Step 57 (741); Episode 36/100; Loss: 0.15757080912590027\n",
      "Step 58 (742); Episode 36/100; Loss: 0.17001914978027344\n",
      "Step 59 (743); Episode 36/100; Loss: 0.09376990050077438\n",
      "Step 60 (744); Episode 36/100; Loss: 0.17616534233093262\n",
      "Step 61 (745); Episode 36/100; Loss: 0.13927391171455383\n",
      "Step 62 (746); Episode 36/100; Loss: 0.08362291753292084\n",
      "Step 63 (747); Episode 36/100; Loss: 0.13300231099128723\n",
      "Step 0 (748); Episode 37/100; Loss: 0.12581506371498108\n",
      "Step 1 (749); Episode 37/100; Loss: 0.0845269188284874\n",
      "Step 2 (750); Episode 37/100; Loss: 0.14448221027851105\n",
      "Step 3 (751); Episode 37/100; Loss: 0.2644459903240204\n",
      "Step 4 (752); Episode 37/100; Loss: 0.09004057943820953\n",
      "Step 5 (753); Episode 37/100; Loss: 0.2930595278739929\n",
      "Step 6 (754); Episode 37/100; Loss: 0.130561962723732\n",
      "Step 7 (755); Episode 37/100; Loss: 0.16436603665351868\n",
      "Step 8 (756); Episode 37/100; Loss: 0.050348687916994095\n",
      "Step 9 (757); Episode 37/100; Loss: 0.04295754060149193\n",
      "Step 10 (758); Episode 37/100; Loss: 0.22415846586227417\n",
      "Step 11 (759); Episode 37/100; Loss: 0.12107596546411514\n",
      "Step 12 (760); Episode 37/100; Loss: 0.08459295332431793\n",
      "Step 13 (761); Episode 37/100; Loss: 0.1261596977710724\n",
      "Step 14 (762); Episode 37/100; Loss: 0.10203785449266434\n",
      "Step 15 (763); Episode 37/100; Loss: 0.1562466323375702\n",
      "Step 16 (764); Episode 37/100; Loss: 0.11478649079799652\n",
      "Step 17 (765); Episode 37/100; Loss: 0.1278786063194275\n",
      "Step 18 (766); Episode 37/100; Loss: 0.23032158613204956\n",
      "Step 19 (767); Episode 37/100; Loss: 0.2278820425271988\n",
      "Step 20 (768); Episode 37/100; Loss: 0.012827754020690918\n",
      "Step 21 (769); Episode 37/100; Loss: 0.061861973255872726\n",
      "Step 22 (770); Episode 37/100; Loss: 0.17271196842193604\n",
      "Step 23 (771); Episode 37/100; Loss: 0.09687791764736176\n",
      "Step 24 (772); Episode 37/100; Loss: 0.010954508557915688\n",
      "Step 25 (773); Episode 37/100; Loss: 0.09260980039834976\n",
      "Step 26 (774); Episode 37/100; Loss: 0.16665343940258026\n",
      "Step 27 (775); Episode 37/100; Loss: 0.12741467356681824\n",
      "Step 28 (776); Episode 37/100; Loss: 0.0819966048002243\n",
      "Step 29 (777); Episode 37/100; Loss: 0.20200808346271515\n",
      "Step 30 (778); Episode 37/100; Loss: 0.25677254796028137\n",
      "Step 31 (779); Episode 37/100; Loss: 0.19429410994052887\n",
      "Step 32 (780); Episode 37/100; Loss: 0.13968239724636078\n",
      "Step 33 (781); Episode 37/100; Loss: 0.13009107112884521\n",
      "Step 34 (782); Episode 37/100; Loss: 0.13437122106552124\n",
      "Step 35 (783); Episode 37/100; Loss: 0.0894261971116066\n",
      "Step 36 (784); Episode 37/100; Loss: 0.09047846496105194\n",
      "Step 37 (785); Episode 37/100; Loss: 0.00877651758491993\n",
      "Step 38 (786); Episode 37/100; Loss: 0.0515001155436039\n",
      "Step 39 (787); Episode 37/100; Loss: 0.006365678273141384\n",
      "Step 40 (788); Episode 37/100; Loss: 0.049639809876680374\n",
      "Step 41 (789); Episode 37/100; Loss: 0.0979170948266983\n",
      "Step 42 (790); Episode 37/100; Loss: 0.19390447437763214\n",
      "Step 0 (791); Episode 38/100; Loss: 0.13524307310581207\n",
      "Step 1 (792); Episode 38/100; Loss: 0.09254303574562073\n",
      "Step 2 (793); Episode 38/100; Loss: 0.09062446653842926\n",
      "Step 3 (794); Episode 38/100; Loss: 0.04411279782652855\n",
      "Step 4 (795); Episode 38/100; Loss: 0.20640908181667328\n",
      "Step 5 (796); Episode 38/100; Loss: 0.08479262888431549\n",
      "Step 6 (797); Episode 38/100; Loss: 0.13638103008270264\n",
      "Step 7 (798); Episode 38/100; Loss: 0.14263255894184113\n",
      "Step 8 (799); Episode 38/100; Loss: 0.08616278320550919\n",
      "Step 9 (800); Episode 38/100; Loss: 0.1302676647901535\n",
      "Step 10 (801); Episode 38/100; Loss: 0.08043618500232697\n",
      "Step 11 (802); Episode 38/100; Loss: 0.24409803748130798\n",
      "Step 12 (803); Episode 38/100; Loss: 0.08703994750976562\n",
      "Step 13 (804); Episode 38/100; Loss: 0.19117799401283264\n",
      "Step 14 (805); Episode 38/100; Loss: 0.09505472332239151\n",
      "Step 15 (806); Episode 38/100; Loss: 0.07420150190591812\n",
      "Step 16 (807); Episode 38/100; Loss: 0.22442705929279327\n",
      "Step 17 (808); Episode 38/100; Loss: 0.13841474056243896\n",
      "Step 18 (809); Episode 38/100; Loss: 0.1373002678155899\n",
      "Step 19 (810); Episode 38/100; Loss: 0.05416054278612137\n",
      "Step 20 (811); Episode 38/100; Loss: 0.21717368066310883\n",
      "Step 21 (812); Episode 38/100; Loss: 0.12268586456775665\n",
      "Step 22 (813); Episode 38/100; Loss: 0.08923093974590302\n",
      "Step 23 (814); Episode 38/100; Loss: 0.1279146820306778\n",
      "Step 24 (815); Episode 38/100; Loss: 0.12913201749324799\n",
      "Step 25 (816); Episode 38/100; Loss: 0.23327001929283142\n",
      "Step 26 (817); Episode 38/100; Loss: 0.09714245051145554\n",
      "Step 27 (818); Episode 38/100; Loss: 0.1617966890335083\n",
      "Step 28 (819); Episode 38/100; Loss: 0.0537407360970974\n",
      "Step 29 (820); Episode 38/100; Loss: 0.09580503404140472\n",
      "Step 30 (821); Episode 38/100; Loss: 0.14111670851707458\n",
      "Step 31 (822); Episode 38/100; Loss: 0.1819450557231903\n",
      "Step 32 (823); Episode 38/100; Loss: 0.12735863029956818\n",
      "Step 33 (824); Episode 38/100; Loss: 0.18151432275772095\n",
      "Step 34 (825); Episode 38/100; Loss: 0.16088275611400604\n",
      "Step 35 (826); Episode 38/100; Loss: 0.274638295173645\n",
      "Step 36 (827); Episode 38/100; Loss: 0.07965873181819916\n",
      "Step 37 (828); Episode 38/100; Loss: 0.3100277781486511\n",
      "Step 38 (829); Episode 38/100; Loss: 0.05730585381388664\n",
      "Step 39 (830); Episode 38/100; Loss: 0.05082468315958977\n",
      "Step 40 (831); Episode 38/100; Loss: 0.051709335297346115\n",
      "Step 41 (832); Episode 38/100; Loss: 0.05132868140935898\n",
      "Step 42 (833); Episode 38/100; Loss: 0.0934908464550972\n",
      "Step 43 (834); Episode 38/100; Loss: 0.16406473517417908\n",
      "Step 44 (835); Episode 38/100; Loss: 0.08423538506031036\n",
      "Step 45 (836); Episode 38/100; Loss: 0.04941780865192413\n",
      "Step 46 (837); Episode 38/100; Loss: 0.08681195974349976\n",
      "Step 47 (838); Episode 38/100; Loss: 0.17186951637268066\n",
      "Step 48 (839); Episode 38/100; Loss: 0.23054355382919312\n",
      "Step 49 (840); Episode 38/100; Loss: 0.10295381397008896\n",
      "Step 50 (841); Episode 38/100; Loss: 0.10808750241994858\n",
      "Step 51 (842); Episode 38/100; Loss: 0.05372015759348869\n",
      "Step 52 (843); Episode 38/100; Loss: 0.2050192654132843\n",
      "Step 53 (844); Episode 38/100; Loss: 0.14533500373363495\n",
      "Step 54 (845); Episode 38/100; Loss: 0.06733175367116928\n",
      "Step 55 (846); Episode 38/100; Loss: 0.14096762239933014\n",
      "Step 56 (847); Episode 38/100; Loss: 0.16344532370567322\n",
      "Step 57 (848); Episode 38/100; Loss: 0.13755764067173004\n",
      "Step 58 (849); Episode 38/100; Loss: 0.09186317771673203\n",
      "Step 59 (850); Episode 38/100; Loss: 0.1443748027086258\n",
      "Step 60 (851); Episode 38/100; Loss: 0.08159617334604263\n",
      "Step 61 (852); Episode 38/100; Loss: 0.0431901179254055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62 (853); Episode 38/100; Loss: 0.09516187012195587\n",
      "Step 63 (854); Episode 38/100; Loss: 0.084538534283638\n",
      "Step 64 (855); Episode 38/100; Loss: 0.05344371125102043\n",
      "Step 65 (856); Episode 38/100; Loss: 0.14254701137542725\n",
      "Step 66 (857); Episode 38/100; Loss: 0.050706081092357635\n",
      "Step 67 (858); Episode 38/100; Loss: 0.14282238483428955\n",
      "Step 0 (859); Episode 39/100; Loss: 0.09913188219070435\n",
      "Step 1 (860); Episode 39/100; Loss: 0.13388046622276306\n",
      "Step 2 (861); Episode 39/100; Loss: 0.1255934238433838\n",
      "Step 3 (862); Episode 39/100; Loss: 0.16985741257667542\n",
      "Step 4 (863); Episode 39/100; Loss: 0.08692503720521927\n",
      "Step 5 (864); Episode 39/100; Loss: 0.07988477498292923\n",
      "Step 6 (865); Episode 39/100; Loss: 0.14108574390411377\n",
      "Step 7 (866); Episode 39/100; Loss: 0.05477878078818321\n",
      "Step 8 (867); Episode 39/100; Loss: 0.044446103274822235\n",
      "Step 9 (868); Episode 39/100; Loss: 0.0901806652545929\n",
      "Step 10 (869); Episode 39/100; Loss: 0.1017920970916748\n",
      "Step 11 (870); Episode 39/100; Loss: 0.13900087773799896\n",
      "Step 12 (871); Episode 39/100; Loss: 0.0641099289059639\n",
      "Step 13 (872); Episode 39/100; Loss: 0.10456592589616776\n",
      "Step 14 (873); Episode 39/100; Loss: 0.04610320180654526\n",
      "Step 15 (874); Episode 39/100; Loss: 0.1455206423997879\n",
      "Step 16 (875); Episode 39/100; Loss: 0.006948797032237053\n",
      "Step 17 (876); Episode 39/100; Loss: 0.0959082692861557\n",
      "Step 18 (877); Episode 39/100; Loss: 0.005610266700387001\n",
      "Step 19 (878); Episode 39/100; Loss: 0.2206428050994873\n",
      "Step 20 (879); Episode 39/100; Loss: 0.04536117613315582\n",
      "Step 21 (880); Episode 39/100; Loss: 0.14844731986522675\n",
      "Step 22 (881); Episode 39/100; Loss: 0.13406714797019958\n",
      "Step 23 (882); Episode 39/100; Loss: 0.24103963375091553\n",
      "Step 24 (883); Episode 39/100; Loss: 0.1869116723537445\n",
      "Step 25 (884); Episode 39/100; Loss: 0.05329758673906326\n",
      "Step 26 (885); Episode 39/100; Loss: 0.17882464826107025\n",
      "Step 27 (886); Episode 39/100; Loss: 0.2343723177909851\n",
      "Step 28 (887); Episode 39/100; Loss: 0.0034093731082975864\n",
      "Step 29 (888); Episode 39/100; Loss: 0.09501183778047562\n",
      "Step 30 (889); Episode 39/100; Loss: 0.088405080139637\n",
      "Step 31 (890); Episode 39/100; Loss: 0.17886124551296234\n",
      "Step 32 (891); Episode 39/100; Loss: 0.1180310994386673\n",
      "Step 33 (892); Episode 39/100; Loss: 0.159621000289917\n",
      "Step 34 (893); Episode 39/100; Loss: 0.1336120367050171\n",
      "Step 35 (894); Episode 39/100; Loss: 0.04268226400017738\n",
      "Step 36 (895); Episode 39/100; Loss: 0.042425114661455154\n",
      "Step 37 (896); Episode 39/100; Loss: 0.005407096352428198\n",
      "Step 38 (897); Episode 39/100; Loss: 0.05163904279470444\n",
      "Step 39 (898); Episode 39/100; Loss: 0.041578974574804306\n",
      "Step 40 (899); Episode 39/100; Loss: 0.043807853013277054\n",
      "Step 41 (900); Episode 39/100; Loss: 0.13061310350894928\n",
      "Step 42 (901); Episode 39/100; Loss: 0.04464874789118767\n",
      "Step 43 (902); Episode 39/100; Loss: 0.045623160898685455\n",
      "Step 44 (903); Episode 39/100; Loss: 0.13793586194515228\n",
      "Step 45 (904); Episode 39/100; Loss: 0.03786104544997215\n",
      "Step 46 (905); Episode 39/100; Loss: 0.17254213988780975\n",
      "Step 47 (906); Episode 39/100; Loss: 0.008188859559595585\n",
      "Step 48 (907); Episode 39/100; Loss: 0.0978105291724205\n",
      "Step 49 (908); Episode 39/100; Loss: 0.05411689355969429\n",
      "Step 50 (909); Episode 39/100; Loss: 0.07320471107959747\n",
      "Step 51 (910); Episode 39/100; Loss: 0.1382928192615509\n",
      "Step 52 (911); Episode 39/100; Loss: 0.14091695845127106\n",
      "Step 53 (912); Episode 39/100; Loss: 0.20846952497959137\n",
      "Step 54 (913); Episode 39/100; Loss: 0.09674844145774841\n",
      "Step 55 (914); Episode 39/100; Loss: 0.09777868539094925\n",
      "Step 56 (915); Episode 39/100; Loss: 0.04509606957435608\n",
      "Step 57 (916); Episode 39/100; Loss: 0.24732761085033417\n",
      "Step 58 (917); Episode 39/100; Loss: 0.006866620387881994\n",
      "Step 59 (918); Episode 39/100; Loss: 0.058061257004737854\n",
      "Step 60 (919); Episode 39/100; Loss: 0.1348039209842682\n",
      "Step 61 (920); Episode 39/100; Loss: 0.2943504750728607\n",
      "Step 62 (921); Episode 39/100; Loss: 0.24399994313716888\n",
      "Step 63 (922); Episode 39/100; Loss: 0.07967106997966766\n",
      "Step 64 (923); Episode 39/100; Loss: 0.09269628673791885\n",
      "Step 65 (924); Episode 39/100; Loss: 0.044539254158735275\n",
      "Step 0 (925); Episode 40/100; Loss: 0.1300438642501831\n",
      "Step 1 (926); Episode 40/100; Loss: 0.12496165186166763\n",
      "Step 2 (927); Episode 40/100; Loss: 0.08918876945972443\n",
      "Step 3 (928); Episode 40/100; Loss: 0.0054593877866864204\n",
      "Step 4 (929); Episode 40/100; Loss: 0.10190341621637344\n",
      "Step 5 (930); Episode 40/100; Loss: 0.08027666807174683\n",
      "Step 6 (931); Episode 40/100; Loss: 0.16496753692626953\n",
      "Step 7 (932); Episode 40/100; Loss: 0.13382701575756073\n",
      "Step 8 (933); Episode 40/100; Loss: 0.13126641511917114\n",
      "Step 9 (934); Episode 40/100; Loss: 0.2113010436296463\n",
      "Step 10 (935); Episode 40/100; Loss: 0.15632806718349457\n",
      "Step 11 (936); Episode 40/100; Loss: 0.12633566558361053\n",
      "Step 12 (937); Episode 40/100; Loss: 0.12459655851125717\n",
      "Step 13 (938); Episode 40/100; Loss: 0.08910056948661804\n",
      "Step 14 (939); Episode 40/100; Loss: 0.2550930082798004\n",
      "Step 15 (940); Episode 40/100; Loss: 0.042133480310440063\n",
      "Step 16 (941); Episode 40/100; Loss: 0.12222710996866226\n",
      "Step 17 (942); Episode 40/100; Loss: 0.12327920645475388\n",
      "Step 18 (943); Episode 40/100; Loss: 0.07481388002634048\n",
      "Step 19 (944); Episode 40/100; Loss: 0.052967775613069534\n",
      "Step 20 (945); Episode 40/100; Loss: 0.12281018495559692\n",
      "Step 21 (946); Episode 40/100; Loss: 0.12672603130340576\n",
      "Step 22 (947); Episode 40/100; Loss: 0.0580834299325943\n",
      "Step 23 (948); Episode 40/100; Loss: 0.05827311798930168\n",
      "Step 24 (949); Episode 40/100; Loss: 0.08002585917711258\n",
      "Step 25 (950); Episode 40/100; Loss: 0.12726500630378723\n",
      "Step 26 (951); Episode 40/100; Loss: 0.07580971717834473\n",
      "Step 27 (952); Episode 40/100; Loss: 0.11935848742723465\n",
      "Step 28 (953); Episode 40/100; Loss: 0.09469135105609894\n",
      "Step 29 (954); Episode 40/100; Loss: 0.2112380713224411\n",
      "Step 30 (955); Episode 40/100; Loss: 0.15194791555404663\n",
      "Step 31 (956); Episode 40/100; Loss: 0.18996009230613708\n",
      "Step 32 (957); Episode 40/100; Loss: 0.07978193461894989\n",
      "Step 33 (958); Episode 40/100; Loss: 0.0061475494876503944\n",
      "Step 34 (959); Episode 40/100; Loss: 0.006451077293604612\n",
      "Step 35 (960); Episode 40/100; Loss: 0.0968208834528923\n",
      "Step 36 (961); Episode 40/100; Loss: 0.10048231482505798\n",
      "Step 37 (962); Episode 40/100; Loss: 0.08648259937763214\n",
      "Step 38 (963); Episode 40/100; Loss: 0.08139538019895554\n",
      "Step 39 (964); Episode 40/100; Loss: 0.08364497870206833\n",
      "Step 40 (965); Episode 40/100; Loss: 0.1621626615524292\n",
      "Step 41 (966); Episode 40/100; Loss: 0.21043606102466583\n",
      "Step 42 (967); Episode 40/100; Loss: 0.19213947653770447\n",
      "Step 43 (968); Episode 40/100; Loss: 0.2281040996313095\n",
      "Step 44 (969); Episode 40/100; Loss: 0.08817510306835175\n",
      "Step 45 (970); Episode 40/100; Loss: 0.2667994499206543\n",
      "Step 46 (971); Episode 40/100; Loss: 0.2151830494403839\n",
      "Step 47 (972); Episode 40/100; Loss: 0.09088923037052155\n",
      "Step 48 (973); Episode 40/100; Loss: 0.08916226774454117\n",
      "Step 49 (974); Episode 40/100; Loss: 0.12512169778347015\n",
      "Step 50 (975); Episode 40/100; Loss: 0.1405675858259201\n",
      "Step 51 (976); Episode 40/100; Loss: 0.17400574684143066\n",
      "Step 52 (977); Episode 40/100; Loss: 0.07943829149007797\n",
      "Step 53 (978); Episode 40/100; Loss: 0.07700762897729874\n",
      "Step 54 (979); Episode 40/100; Loss: 0.12940815091133118\n",
      "Step 55 (980); Episode 40/100; Loss: 0.005316324532032013\n",
      "Step 56 (981); Episode 40/100; Loss: 0.04752553254365921\n",
      "Step 57 (982); Episode 40/100; Loss: 0.2227051556110382\n",
      "Step 58 (983); Episode 40/100; Loss: 0.16186131536960602\n",
      "Step 59 (984); Episode 40/100; Loss: 0.1569153219461441\n",
      "Step 60 (985); Episode 40/100; Loss: 0.15975822508335114\n",
      "Step 61 (986); Episode 40/100; Loss: 0.04562067613005638\n",
      "Step 62 (987); Episode 40/100; Loss: 0.05483589321374893\n",
      "Step 63 (988); Episode 40/100; Loss: 0.005611773114651442\n",
      "Step 64 (989); Episode 40/100; Loss: 0.1197991818189621\n",
      "Step 65 (990); Episode 40/100; Loss: 0.10016565769910812\n",
      "Step 66 (991); Episode 40/100; Loss: 0.17831046879291534\n",
      "Step 67 (992); Episode 40/100; Loss: 0.04166284203529358\n",
      "Step 68 (993); Episode 40/100; Loss: 0.0862211063504219\n",
      "Step 69 (994); Episode 40/100; Loss: 0.21653348207473755\n",
      "Step 70 (995); Episode 40/100; Loss: 0.15052440762519836\n",
      "Step 71 (996); Episode 40/100; Loss: 0.034126121550798416\n",
      "Step 72 (997); Episode 40/100; Loss: 0.12445373833179474\n",
      "Step 73 (998); Episode 40/100; Loss: 0.19465503096580505\n",
      "Step 0 (999); Episode 41/100; Loss: 0.07108994573354721\n",
      "Step 1 (1000); Episode 41/100; Loss: 0.10326677560806274\n",
      "Step 2 (1001); Episode 41/100; Loss: 0.11146415770053864\n",
      "Step 3 (1002); Episode 41/100; Loss: 0.07958587259054184\n",
      "Step 4 (1003); Episode 41/100; Loss: 0.10722915828227997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 (1004); Episode 41/100; Loss: 0.13359540700912476\n",
      "Step 6 (1005); Episode 41/100; Loss: 0.1072969138622284\n",
      "Step 7 (1006); Episode 41/100; Loss: 0.07361982017755508\n",
      "Step 8 (1007); Episode 41/100; Loss: 0.07462933659553528\n",
      "Step 9 (1008); Episode 41/100; Loss: 0.1357107162475586\n",
      "Step 10 (1009); Episode 41/100; Loss: 0.20803065598011017\n",
      "Step 11 (1010); Episode 41/100; Loss: 0.08971957117319107\n",
      "Step 12 (1011); Episode 41/100; Loss: 0.007269961293786764\n",
      "Step 13 (1012); Episode 41/100; Loss: 0.08799625933170319\n",
      "Step 14 (1013); Episode 41/100; Loss: 0.1619785577058792\n",
      "Step 15 (1014); Episode 41/100; Loss: 0.2411065250635147\n",
      "Step 16 (1015); Episode 41/100; Loss: 0.004624460823833942\n",
      "Step 17 (1016); Episode 41/100; Loss: 0.04041529819369316\n",
      "Step 18 (1017); Episode 41/100; Loss: 0.04455070570111275\n",
      "Step 19 (1018); Episode 41/100; Loss: 0.1447620987892151\n",
      "Step 20 (1019); Episode 41/100; Loss: 0.11969763040542603\n",
      "Step 21 (1020); Episode 41/100; Loss: 0.04695787653326988\n",
      "Step 22 (1021); Episode 41/100; Loss: 0.11239802092313766\n",
      "Step 23 (1022); Episode 41/100; Loss: 0.18433117866516113\n",
      "Step 24 (1023); Episode 41/100; Loss: 0.16671600937843323\n",
      "Step 25 (1024); Episode 41/100; Loss: 0.007885593920946121\n",
      "Step 26 (1025); Episode 41/100; Loss: 0.14540131390094757\n",
      "Step 27 (1026); Episode 41/100; Loss: 0.10218839347362518\n",
      "Step 28 (1027); Episode 41/100; Loss: 0.15590961277484894\n",
      "Step 29 (1028); Episode 41/100; Loss: 0.23657037317752838\n",
      "Step 30 (1029); Episode 41/100; Loss: 0.03925231844186783\n",
      "Step 31 (1030); Episode 41/100; Loss: 0.07635027915239334\n",
      "Step 32 (1031); Episode 41/100; Loss: 0.12092739343643188\n",
      "Step 33 (1032); Episode 41/100; Loss: 0.08177632093429565\n",
      "Step 34 (1033); Episode 41/100; Loss: 0.07830316573381424\n",
      "Step 35 (1034); Episode 41/100; Loss: 0.053462013602256775\n",
      "Step 36 (1035); Episode 41/100; Loss: 0.07568049430847168\n",
      "Step 37 (1036); Episode 41/100; Loss: 0.2383279800415039\n",
      "Step 38 (1037); Episode 41/100; Loss: 0.053468815982341766\n",
      "Step 39 (1038); Episode 41/100; Loss: 0.2027190923690796\n",
      "Step 40 (1039); Episode 41/100; Loss: 0.1295565664768219\n",
      "Step 41 (1040); Episode 41/100; Loss: 0.057096172124147415\n",
      "Step 42 (1041); Episode 41/100; Loss: 0.1363595575094223\n",
      "Step 43 (1042); Episode 41/100; Loss: 0.05275148153305054\n",
      "Step 44 (1043); Episode 41/100; Loss: 0.07178325951099396\n",
      "Step 45 (1044); Episode 41/100; Loss: 0.08943413943052292\n",
      "Step 46 (1045); Episode 41/100; Loss: 0.11816548556089401\n",
      "Step 47 (1046); Episode 41/100; Loss: 0.18532301485538483\n",
      "Step 48 (1047); Episode 41/100; Loss: 0.14681346714496613\n",
      "Step 49 (1048); Episode 41/100; Loss: 0.07446099072694778\n",
      "Step 50 (1049); Episode 41/100; Loss: 0.0995992049574852\n",
      "Step 51 (1050); Episode 41/100; Loss: 0.08662690222263336\n",
      "Step 52 (1051); Episode 41/100; Loss: 0.0467538982629776\n",
      "Step 53 (1052); Episode 41/100; Loss: 0.13914310932159424\n",
      "Step 0 (1053); Episode 42/100; Loss: 0.12232008576393127\n",
      "Step 1 (1054); Episode 42/100; Loss: 0.08317960798740387\n",
      "Step 2 (1055); Episode 42/100; Loss: 0.04547729343175888\n",
      "Step 3 (1056); Episode 42/100; Loss: 0.14127637445926666\n",
      "Step 4 (1057); Episode 42/100; Loss: 0.03800547122955322\n",
      "Step 5 (1058); Episode 42/100; Loss: 0.15809862315654755\n",
      "Step 6 (1059); Episode 42/100; Loss: 0.2406446933746338\n",
      "Step 7 (1060); Episode 42/100; Loss: 0.1917494237422943\n",
      "Step 8 (1061); Episode 42/100; Loss: 0.2641925811767578\n",
      "Step 9 (1062); Episode 42/100; Loss: 0.08851300179958344\n",
      "Step 10 (1063); Episode 42/100; Loss: 0.22229121625423431\n",
      "Step 11 (1064); Episode 42/100; Loss: 0.17937012016773224\n",
      "Step 12 (1065); Episode 42/100; Loss: 0.21215270459651947\n",
      "Step 13 (1066); Episode 42/100; Loss: 0.07814428210258484\n",
      "Step 14 (1067); Episode 42/100; Loss: 0.08536285907030106\n",
      "Step 15 (1068); Episode 42/100; Loss: 0.10797233879566193\n",
      "Step 16 (1069); Episode 42/100; Loss: 0.12383649498224258\n",
      "Step 17 (1070); Episode 42/100; Loss: 0.04258549213409424\n",
      "Step 18 (1071); Episode 42/100; Loss: 0.11464331299066544\n",
      "Step 19 (1072); Episode 42/100; Loss: 0.21889734268188477\n",
      "Step 20 (1073); Episode 42/100; Loss: 0.045189268887043\n",
      "Step 21 (1074); Episode 42/100; Loss: 0.04159801825881004\n",
      "Step 22 (1075); Episode 42/100; Loss: 0.006830073893070221\n",
      "Step 23 (1076); Episode 42/100; Loss: 0.1438087522983551\n",
      "Step 24 (1077); Episode 42/100; Loss: 0.007457069121301174\n",
      "Step 25 (1078); Episode 42/100; Loss: 0.05145856738090515\n",
      "Step 26 (1079); Episode 42/100; Loss: 0.07198982685804367\n",
      "Step 27 (1080); Episode 42/100; Loss: 0.1937684863805771\n",
      "Step 28 (1081); Episode 42/100; Loss: 0.13403458893299103\n",
      "Step 29 (1082); Episode 42/100; Loss: 0.21215492486953735\n",
      "Step 30 (1083); Episode 42/100; Loss: 0.23813720047473907\n",
      "Step 31 (1084); Episode 42/100; Loss: 0.11996021121740341\n",
      "Step 32 (1085); Episode 42/100; Loss: 0.22870062291622162\n",
      "Step 33 (1086); Episode 42/100; Loss: 0.04373771697282791\n",
      "Step 34 (1087); Episode 42/100; Loss: 0.18096065521240234\n",
      "Step 35 (1088); Episode 42/100; Loss: 0.06831382215023041\n",
      "Step 36 (1089); Episode 42/100; Loss: 0.10215748101472855\n",
      "Step 37 (1090); Episode 42/100; Loss: 0.13171599805355072\n",
      "Step 38 (1091); Episode 42/100; Loss: 0.17521341145038605\n",
      "Step 39 (1092); Episode 42/100; Loss: 0.092796191573143\n",
      "Step 40 (1093); Episode 42/100; Loss: 0.230011448264122\n",
      "Step 41 (1094); Episode 42/100; Loss: 0.058042049407958984\n",
      "Step 42 (1095); Episode 42/100; Loss: 0.09131287038326263\n",
      "Step 43 (1096); Episode 42/100; Loss: 0.12098582088947296\n",
      "Step 44 (1097); Episode 42/100; Loss: 0.14274653792381287\n",
      "Step 45 (1098); Episode 42/100; Loss: 0.0975838229060173\n",
      "Step 46 (1099); Episode 42/100; Loss: 0.12064319849014282\n",
      "Step 47 (1100); Episode 42/100; Loss: 0.12932075560092926\n",
      "Step 48 (1101); Episode 42/100; Loss: 0.009525416418910027\n",
      "Step 49 (1102); Episode 42/100; Loss: 0.12091274559497833\n",
      "Step 50 (1103); Episode 42/100; Loss: 0.12225967645645142\n",
      "Step 51 (1104); Episode 42/100; Loss: 0.052376750856637955\n",
      "Step 52 (1105); Episode 42/100; Loss: 0.05974060669541359\n",
      "Step 0 (1106); Episode 43/100; Loss: 0.20828303694725037\n",
      "Step 1 (1107); Episode 43/100; Loss: 0.08907584846019745\n",
      "Step 2 (1108); Episode 43/100; Loss: 0.07910281419754028\n",
      "Step 3 (1109); Episode 43/100; Loss: 0.009941240772604942\n",
      "Step 4 (1110); Episode 43/100; Loss: 0.11382285505533218\n",
      "Step 5 (1111); Episode 43/100; Loss: 0.21475830674171448\n",
      "Step 6 (1112); Episode 43/100; Loss: 0.11422590166330338\n",
      "Step 7 (1113); Episode 43/100; Loss: 0.00752096064388752\n",
      "Step 8 (1114); Episode 43/100; Loss: 0.17359866201877594\n",
      "Step 9 (1115); Episode 43/100; Loss: 0.12566795945167542\n",
      "Step 10 (1116); Episode 43/100; Loss: 0.0514962337911129\n",
      "Step 11 (1117); Episode 43/100; Loss: 0.1345309466123581\n",
      "Step 12 (1118); Episode 43/100; Loss: 0.0354076623916626\n",
      "Step 13 (1119); Episode 43/100; Loss: 0.039966192096471786\n",
      "Step 14 (1120); Episode 43/100; Loss: 0.05525151640176773\n",
      "Step 15 (1121); Episode 43/100; Loss: 0.2571837604045868\n",
      "Step 16 (1122); Episode 43/100; Loss: 0.11105030030012131\n",
      "Step 17 (1123); Episode 43/100; Loss: 0.05294020473957062\n",
      "Step 18 (1124); Episode 43/100; Loss: 0.13411211967468262\n",
      "Step 19 (1125); Episode 43/100; Loss: 0.14234289526939392\n",
      "Step 20 (1126); Episode 43/100; Loss: 0.0065545979887247086\n",
      "Step 21 (1127); Episode 43/100; Loss: 0.04009201005101204\n",
      "Step 22 (1128); Episode 43/100; Loss: 0.12153688073158264\n",
      "Step 23 (1129); Episode 43/100; Loss: 0.12081846594810486\n",
      "Step 24 (1130); Episode 43/100; Loss: 0.040448106825351715\n",
      "Step 25 (1131); Episode 43/100; Loss: 0.08751844614744186\n",
      "Step 26 (1132); Episode 43/100; Loss: 0.04536203667521477\n",
      "Step 27 (1133); Episode 43/100; Loss: 0.07907865941524506\n",
      "Step 28 (1134); Episode 43/100; Loss: 0.17519545555114746\n",
      "Step 29 (1135); Episode 43/100; Loss: 0.06318017840385437\n",
      "Step 30 (1136); Episode 43/100; Loss: 0.05985723435878754\n",
      "Step 31 (1137); Episode 43/100; Loss: 0.053112439811229706\n",
      "Step 32 (1138); Episode 43/100; Loss: 0.006532677914947271\n",
      "Step 33 (1139); Episode 43/100; Loss: 0.09337109327316284\n",
      "Step 34 (1140); Episode 43/100; Loss: 0.14242734014987946\n",
      "Step 35 (1141); Episode 43/100; Loss: 0.1812945455312729\n",
      "Step 36 (1142); Episode 43/100; Loss: 0.07102368772029877\n",
      "Step 37 (1143); Episode 43/100; Loss: 0.05782546103000641\n",
      "Step 38 (1144); Episode 43/100; Loss: 0.09896635264158249\n",
      "Step 39 (1145); Episode 43/100; Loss: 0.13764303922653198\n",
      "Step 40 (1146); Episode 43/100; Loss: 0.2425345480442047\n",
      "Step 41 (1147); Episode 43/100; Loss: 0.07802212238311768\n",
      "Step 42 (1148); Episode 43/100; Loss: 0.1792713850736618\n",
      "Step 43 (1149); Episode 43/100; Loss: 0.13204367458820343\n",
      "Step 44 (1150); Episode 43/100; Loss: 0.09144926816225052\n",
      "Step 45 (1151); Episode 43/100; Loss: 0.16979925334453583\n",
      "Step 46 (1152); Episode 43/100; Loss: 0.0824725404381752\n",
      "Step 47 (1153); Episode 43/100; Loss: 0.07460803538560867\n",
      "Step 48 (1154); Episode 43/100; Loss: 0.12692955136299133\n",
      "Step 49 (1155); Episode 43/100; Loss: 0.09542370587587357\n",
      "Step 50 (1156); Episode 43/100; Loss: 0.16318084299564362\n",
      "Step 51 (1157); Episode 43/100; Loss: 0.11528997868299484\n",
      "Step 52 (1158); Episode 43/100; Loss: 0.1330672651529312\n",
      "Step 53 (1159); Episode 43/100; Loss: 0.16205719113349915\n",
      "Step 54 (1160); Episode 43/100; Loss: 0.08499204367399216\n",
      "Step 55 (1161); Episode 43/100; Loss: 0.11573317646980286\n",
      "Step 56 (1162); Episode 43/100; Loss: 0.0538802295923233\n",
      "Step 57 (1163); Episode 43/100; Loss: 0.13734975457191467\n",
      "Step 58 (1164); Episode 43/100; Loss: 0.07055508345365524\n",
      "Step 59 (1165); Episode 43/100; Loss: 0.1408909410238266\n",
      "Step 60 (1166); Episode 43/100; Loss: 0.08804971724748611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (1167); Episode 44/100; Loss: 0.004141584504395723\n",
      "Step 1 (1168); Episode 44/100; Loss: 0.12252340465784073\n",
      "Step 2 (1169); Episode 44/100; Loss: 0.0400957390666008\n",
      "Step 3 (1170); Episode 44/100; Loss: 0.11743838340044022\n",
      "Step 4 (1171); Episode 44/100; Loss: 0.10844437032938004\n",
      "Step 5 (1172); Episode 44/100; Loss: 0.08759623765945435\n",
      "Step 6 (1173); Episode 44/100; Loss: 0.10025351494550705\n",
      "Step 7 (1174); Episode 44/100; Loss: 0.056987214833498\n",
      "Step 8 (1175); Episode 44/100; Loss: 0.13325439393520355\n",
      "Step 9 (1176); Episode 44/100; Loss: 0.0074659897945821285\n",
      "Step 10 (1177); Episode 44/100; Loss: 0.11720254272222519\n",
      "Step 11 (1178); Episode 44/100; Loss: 0.10898982733488083\n",
      "Step 12 (1179); Episode 44/100; Loss: 0.038253817707300186\n",
      "Step 13 (1180); Episode 44/100; Loss: 0.11715536564588547\n",
      "Step 14 (1181); Episode 44/100; Loss: 0.048711199313402176\n",
      "Step 15 (1182); Episode 44/100; Loss: 0.15775160491466522\n",
      "Step 16 (1183); Episode 44/100; Loss: 0.1055958941578865\n",
      "Step 17 (1184); Episode 44/100; Loss: 0.0048178499564528465\n",
      "Step 18 (1185); Episode 44/100; Loss: 0.1006166860461235\n",
      "Step 19 (1186); Episode 44/100; Loss: 0.13674324750900269\n",
      "Step 20 (1187); Episode 44/100; Loss: 0.10460814088582993\n",
      "Step 21 (1188); Episode 44/100; Loss: 0.09057189524173737\n",
      "Step 22 (1189); Episode 44/100; Loss: 0.21618647873401642\n",
      "Step 23 (1190); Episode 44/100; Loss: 0.04109286144375801\n",
      "Step 24 (1191); Episode 44/100; Loss: 0.2191169112920761\n",
      "Step 25 (1192); Episode 44/100; Loss: 0.034671906381845474\n",
      "Step 26 (1193); Episode 44/100; Loss: 0.06661864370107651\n",
      "Step 27 (1194); Episode 44/100; Loss: 0.11251594871282578\n",
      "Step 28 (1195); Episode 44/100; Loss: 0.0041102394461631775\n",
      "Step 29 (1196); Episode 44/100; Loss: 0.09910472482442856\n",
      "Step 30 (1197); Episode 44/100; Loss: 0.1539352685213089\n",
      "Step 31 (1198); Episode 44/100; Loss: 0.006286198273301125\n",
      "Step 32 (1199); Episode 44/100; Loss: 0.06889542937278748\n",
      "Step 33 (1200); Episode 44/100; Loss: 0.006064411718398333\n",
      "Step 34 (1201); Episode 44/100; Loss: 0.17986367642879486\n",
      "Step 35 (1202); Episode 44/100; Loss: 0.08880099654197693\n",
      "Step 36 (1203); Episode 44/100; Loss: 0.006274705287069082\n",
      "Step 37 (1204); Episode 44/100; Loss: 0.1939510554075241\n",
      "Step 38 (1205); Episode 44/100; Loss: 0.05329703167080879\n",
      "Step 39 (1206); Episode 44/100; Loss: 0.004198530223220587\n",
      "Step 40 (1207); Episode 44/100; Loss: 0.036085959523916245\n",
      "Step 41 (1208); Episode 44/100; Loss: 0.1378488689661026\n",
      "Step 42 (1209); Episode 44/100; Loss: 0.053949594497680664\n",
      "Step 43 (1210); Episode 44/100; Loss: 0.046292319893836975\n",
      "Step 44 (1211); Episode 44/100; Loss: 0.05433015152812004\n",
      "Step 45 (1212); Episode 44/100; Loss: 0.08101995289325714\n",
      "Step 46 (1213); Episode 44/100; Loss: 0.03406090661883354\n",
      "Step 47 (1214); Episode 44/100; Loss: 0.03372562676668167\n",
      "Step 48 (1215); Episode 44/100; Loss: 0.05490409582853317\n",
      "Step 49 (1216); Episode 44/100; Loss: 0.11929375678300858\n",
      "Step 50 (1217); Episode 44/100; Loss: 0.0941372960805893\n",
      "Step 51 (1218); Episode 44/100; Loss: 0.030540931969881058\n",
      "Step 52 (1219); Episode 44/100; Loss: 0.0673249363899231\n",
      "Step 53 (1220); Episode 44/100; Loss: 0.06480520963668823\n",
      "Step 54 (1221); Episode 44/100; Loss: 0.12665259838104248\n",
      "Step 55 (1222); Episode 44/100; Loss: 0.12271911650896072\n",
      "Step 56 (1223); Episode 44/100; Loss: 0.033924344927072525\n",
      "Step 57 (1224); Episode 44/100; Loss: 0.1310761719942093\n",
      "Step 58 (1225); Episode 44/100; Loss: 0.14105942845344543\n",
      "Step 59 (1226); Episode 44/100; Loss: 0.0412227064371109\n",
      "Step 60 (1227); Episode 44/100; Loss: 0.1245492696762085\n",
      "Step 61 (1228); Episode 44/100; Loss: 0.04339682683348656\n",
      "Step 62 (1229); Episode 44/100; Loss: 0.004729833919554949\n",
      "Step 63 (1230); Episode 44/100; Loss: 0.005271608009934425\n",
      "Step 64 (1231); Episode 44/100; Loss: 0.13471505045890808\n",
      "Step 65 (1232); Episode 44/100; Loss: 0.0034421721938997507\n",
      "Step 66 (1233); Episode 44/100; Loss: 0.09039593487977982\n",
      "Step 67 (1234); Episode 44/100; Loss: 0.11588415503501892\n",
      "Step 68 (1235); Episode 44/100; Loss: 0.10633870214223862\n",
      "Step 69 (1236); Episode 44/100; Loss: 0.13978762924671173\n",
      "Step 70 (1237); Episode 44/100; Loss: 0.17165103554725647\n",
      "Step 71 (1238); Episode 44/100; Loss: 0.05628494173288345\n",
      "Step 72 (1239); Episode 44/100; Loss: 0.07251578569412231\n",
      "Step 73 (1240); Episode 44/100; Loss: 0.0713895857334137\n",
      "Step 74 (1241); Episode 44/100; Loss: 0.0034307376481592655\n",
      "Step 75 (1242); Episode 44/100; Loss: 0.05020204558968544\n",
      "Step 76 (1243); Episode 44/100; Loss: 0.037950705736875534\n",
      "Step 77 (1244); Episode 44/100; Loss: 0.16759809851646423\n",
      "Step 78 (1245); Episode 44/100; Loss: 0.11707084625959396\n",
      "Step 79 (1246); Episode 44/100; Loss: 0.17180949449539185\n",
      "Step 80 (1247); Episode 44/100; Loss: 0.0966516062617302\n",
      "Step 81 (1248); Episode 44/100; Loss: 0.07804760336875916\n",
      "Step 82 (1249); Episode 44/100; Loss: 0.05456024035811424\n",
      "Step 83 (1250); Episode 44/100; Loss: 0.15448857843875885\n",
      "Step 84 (1251); Episode 44/100; Loss: 0.004071052651852369\n",
      "Step 85 (1252); Episode 44/100; Loss: 0.051293764263391495\n",
      "Step 86 (1253); Episode 44/100; Loss: 0.11038009822368622\n",
      "Step 87 (1254); Episode 44/100; Loss: 0.07892322540283203\n",
      "Step 88 (1255); Episode 44/100; Loss: 0.1087452843785286\n",
      "Step 89 (1256); Episode 44/100; Loss: 0.004351064562797546\n",
      "Step 90 (1257); Episode 44/100; Loss: 0.09923207014799118\n",
      "Step 91 (1258); Episode 44/100; Loss: 0.2401924729347229\n",
      "Step 92 (1259); Episode 44/100; Loss: 0.14546798169612885\n",
      "Step 93 (1260); Episode 44/100; Loss: 0.08471516519784927\n",
      "Step 94 (1261); Episode 44/100; Loss: 0.032410457730293274\n",
      "Step 95 (1262); Episode 44/100; Loss: 0.2223082184791565\n",
      "Step 96 (1263); Episode 44/100; Loss: 0.1295628845691681\n",
      "Step 97 (1264); Episode 44/100; Loss: 0.07195182889699936\n",
      "Step 98 (1265); Episode 44/100; Loss: 0.09476350992918015\n",
      "Step 99 (1266); Episode 44/100; Loss: 0.11758634448051453\n",
      "Step 100 (1267); Episode 44/100; Loss: 0.1153033971786499\n",
      "Step 101 (1268); Episode 44/100; Loss: 0.0357050895690918\n",
      "Step 102 (1269); Episode 44/100; Loss: 0.08730459958314896\n",
      "Step 103 (1270); Episode 44/100; Loss: 0.11463743448257446\n",
      "Step 104 (1271); Episode 44/100; Loss: 0.1628984808921814\n",
      "Step 105 (1272); Episode 44/100; Loss: 0.10908735543489456\n",
      "Step 106 (1273); Episode 44/100; Loss: 0.056760549545288086\n",
      "Step 107 (1274); Episode 44/100; Loss: 0.12457148730754852\n",
      "Step 108 (1275); Episode 44/100; Loss: 0.08243609219789505\n",
      "Step 109 (1276); Episode 44/100; Loss: 0.05531826242804527\n",
      "Step 0 (1277); Episode 45/100; Loss: 0.05780010297894478\n",
      "Step 1 (1278); Episode 45/100; Loss: 0.1117996871471405\n",
      "Step 2 (1279); Episode 45/100; Loss: 0.20118559896945953\n",
      "Step 3 (1280); Episode 45/100; Loss: 0.03682636842131615\n",
      "Step 4 (1281); Episode 45/100; Loss: 0.09151960909366608\n",
      "Step 5 (1282); Episode 45/100; Loss: 0.06922195106744766\n",
      "Step 6 (1283); Episode 45/100; Loss: 0.03501519933342934\n",
      "Step 7 (1284); Episode 45/100; Loss: 0.04460185021162033\n",
      "Step 8 (1285); Episode 45/100; Loss: 0.004981952253729105\n",
      "Step 9 (1286); Episode 45/100; Loss: 0.09272052347660065\n",
      "Step 10 (1287); Episode 45/100; Loss: 0.030810920521616936\n",
      "Step 11 (1288); Episode 45/100; Loss: 0.03547574207186699\n",
      "Step 12 (1289); Episode 45/100; Loss: 0.051926981657743454\n",
      "Step 13 (1290); Episode 45/100; Loss: 0.09985819458961487\n",
      "Step 14 (1291); Episode 45/100; Loss: 0.09199710935354233\n",
      "Step 15 (1292); Episode 45/100; Loss: 0.20115472376346588\n",
      "Step 16 (1293); Episode 45/100; Loss: 0.12229878455400467\n",
      "Step 17 (1294); Episode 45/100; Loss: 0.10074987262487411\n",
      "Step 18 (1295); Episode 45/100; Loss: 0.20148694515228271\n",
      "Step 19 (1296); Episode 45/100; Loss: 0.16381217539310455\n",
      "Step 20 (1297); Episode 45/100; Loss: 0.05563873425126076\n",
      "Step 21 (1298); Episode 45/100; Loss: 0.10277510434389114\n",
      "Step 22 (1299); Episode 45/100; Loss: 0.08417300134897232\n",
      "Step 23 (1300); Episode 45/100; Loss: 0.2429303675889969\n",
      "Step 24 (1301); Episode 45/100; Loss: 0.39942702651023865\n",
      "Step 25 (1302); Episode 45/100; Loss: 0.1285325586795807\n",
      "Step 26 (1303); Episode 45/100; Loss: 0.13299062848091125\n",
      "Step 27 (1304); Episode 45/100; Loss: 0.11775728315114975\n",
      "Step 28 (1305); Episode 45/100; Loss: 0.07632460445165634\n",
      "Step 29 (1306); Episode 45/100; Loss: 0.03663264587521553\n",
      "Step 30 (1307); Episode 45/100; Loss: 0.13393475115299225\n",
      "Step 31 (1308); Episode 45/100; Loss: 0.13778619468212128\n",
      "Step 32 (1309); Episode 45/100; Loss: 0.13786502182483673\n",
      "Step 33 (1310); Episode 45/100; Loss: 0.0692678689956665\n",
      "Step 34 (1311); Episode 45/100; Loss: 0.09824484586715698\n",
      "Step 35 (1312); Episode 45/100; Loss: 0.037444911897182465\n",
      "Step 36 (1313); Episode 45/100; Loss: 0.04643820598721504\n",
      "Step 37 (1314); Episode 45/100; Loss: 0.004232976585626602\n",
      "Step 38 (1315); Episode 45/100; Loss: 0.13507990539073944\n",
      "Step 39 (1316); Episode 45/100; Loss: 0.09750975668430328\n",
      "Step 40 (1317); Episode 45/100; Loss: 0.00714230677112937\n",
      "Step 41 (1318); Episode 45/100; Loss: 0.1273433119058609\n",
      "Step 42 (1319); Episode 45/100; Loss: 0.14590495824813843\n",
      "Step 43 (1320); Episode 45/100; Loss: 0.07792207598686218\n",
      "Step 44 (1321); Episode 45/100; Loss: 0.05074114352464676\n",
      "Step 45 (1322); Episode 45/100; Loss: 0.03990050405263901\n",
      "Step 46 (1323); Episode 45/100; Loss: 0.08431426435709\n",
      "Step 47 (1324); Episode 45/100; Loss: 0.16013692319393158\n",
      "Step 48 (1325); Episode 45/100; Loss: 0.19508875906467438\n",
      "Step 49 (1326); Episode 45/100; Loss: 0.05237821489572525\n",
      "Step 50 (1327); Episode 45/100; Loss: 0.034667424857616425\n",
      "Step 51 (1328); Episode 45/100; Loss: 0.16403433680534363\n",
      "Step 52 (1329); Episode 45/100; Loss: 0.12304536998271942\n",
      "Step 53 (1330); Episode 45/100; Loss: 0.05008958280086517\n",
      "Step 54 (1331); Episode 45/100; Loss: 0.05705484375357628\n",
      "Step 55 (1332); Episode 45/100; Loss: 0.04158321022987366\n",
      "Step 56 (1333); Episode 45/100; Loss: 0.07065706700086594\n",
      "Step 57 (1334); Episode 45/100; Loss: 0.13190211355686188\n",
      "Step 58 (1335); Episode 45/100; Loss: 0.13883081078529358\n",
      "Step 59 (1336); Episode 45/100; Loss: 0.049458276480436325\n",
      "Step 60 (1337); Episode 45/100; Loss: 0.005572953727096319\n",
      "Step 61 (1338); Episode 45/100; Loss: 0.05258290469646454\n",
      "Step 62 (1339); Episode 45/100; Loss: 0.11396947503089905\n",
      "Step 63 (1340); Episode 45/100; Loss: 0.029406700283288956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 64 (1341); Episode 45/100; Loss: 0.1661788821220398\n",
      "Step 65 (1342); Episode 45/100; Loss: 0.06932628899812698\n",
      "Step 66 (1343); Episode 45/100; Loss: 0.06711579859256744\n",
      "Step 67 (1344); Episode 45/100; Loss: 0.0840875506401062\n",
      "Step 68 (1345); Episode 45/100; Loss: 0.20043863356113434\n",
      "Step 69 (1346); Episode 45/100; Loss: 0.20740492641925812\n",
      "Step 70 (1347); Episode 45/100; Loss: 0.16929952800273895\n",
      "Step 71 (1348); Episode 45/100; Loss: 0.044977862387895584\n",
      "Step 72 (1349); Episode 45/100; Loss: 0.07434828579425812\n",
      "Step 73 (1350); Episode 45/100; Loss: 0.08928845077753067\n",
      "Step 74 (1351); Episode 45/100; Loss: 0.08005764335393906\n",
      "Step 75 (1352); Episode 45/100; Loss: 0.10087340325117111\n",
      "Step 76 (1353); Episode 45/100; Loss: 0.04393664002418518\n",
      "Step 77 (1354); Episode 45/100; Loss: 0.0037874109111726284\n",
      "Step 78 (1355); Episode 45/100; Loss: 0.0954957902431488\n",
      "Step 79 (1356); Episode 45/100; Loss: 0.054032981395721436\n",
      "Step 80 (1357); Episode 45/100; Loss: 0.033992502838373184\n",
      "Step 81 (1358); Episode 45/100; Loss: 0.13948336243629456\n",
      "Step 82 (1359); Episode 45/100; Loss: 0.06980880349874496\n",
      "Step 83 (1360); Episode 45/100; Loss: 0.0973835289478302\n",
      "Step 84 (1361); Episode 45/100; Loss: 0.03572380915284157\n",
      "Step 85 (1362); Episode 45/100; Loss: 0.035047292709350586\n",
      "Step 86 (1363); Episode 45/100; Loss: 0.07275407761335373\n",
      "Step 87 (1364); Episode 45/100; Loss: 0.005191592033952475\n",
      "Step 88 (1365); Episode 45/100; Loss: 0.05514703318476677\n",
      "Step 89 (1366); Episode 45/100; Loss: 0.007620129268616438\n",
      "Step 90 (1367); Episode 45/100; Loss: 0.10487071424722672\n",
      "Step 91 (1368); Episode 45/100; Loss: 0.03250943869352341\n",
      "Step 92 (1369); Episode 45/100; Loss: 0.06475799530744553\n",
      "Step 93 (1370); Episode 45/100; Loss: 0.03706290200352669\n",
      "Step 94 (1371); Episode 45/100; Loss: 0.14308546483516693\n",
      "Step 95 (1372); Episode 45/100; Loss: 0.11019141227006912\n",
      "Step 96 (1373); Episode 45/100; Loss: 0.03665969520807266\n",
      "Step 97 (1374); Episode 45/100; Loss: 0.10763919353485107\n",
      "Step 98 (1375); Episode 45/100; Loss: 0.005106853321194649\n",
      "Step 99 (1376); Episode 45/100; Loss: 0.15912769734859467\n",
      "Step 100 (1377); Episode 45/100; Loss: 0.16889670491218567\n",
      "Step 101 (1378); Episode 45/100; Loss: 0.1680039018392563\n",
      "Step 102 (1379); Episode 45/100; Loss: 0.10202284902334213\n",
      "Step 103 (1380); Episode 45/100; Loss: 0.03582777455449104\n",
      "Step 104 (1381); Episode 45/100; Loss: 0.009276758879423141\n",
      "Step 105 (1382); Episode 45/100; Loss: 0.07621390372514725\n",
      "Step 106 (1383); Episode 45/100; Loss: 0.005300112999975681\n",
      "Step 107 (1384); Episode 45/100; Loss: 0.05465669184923172\n",
      "Step 108 (1385); Episode 45/100; Loss: 0.044784363359212875\n",
      "Step 109 (1386); Episode 45/100; Loss: 0.05559292435646057\n",
      "Step 110 (1387); Episode 45/100; Loss: 0.07896531373262405\n",
      "Step 111 (1388); Episode 45/100; Loss: 0.09630615264177322\n",
      "Step 0 (1389); Episode 46/100; Loss: 0.08592002093791962\n",
      "Step 1 (1390); Episode 46/100; Loss: 0.15716643631458282\n",
      "Step 2 (1391); Episode 46/100; Loss: 0.03330176696181297\n",
      "Step 3 (1392); Episode 46/100; Loss: 0.05257618799805641\n",
      "Step 4 (1393); Episode 46/100; Loss: 0.05713287740945816\n",
      "Step 5 (1394); Episode 46/100; Loss: 0.05450966954231262\n",
      "Step 6 (1395); Episode 46/100; Loss: 0.08953651040792465\n",
      "Step 7 (1396); Episode 46/100; Loss: 0.04328359663486481\n",
      "Step 8 (1397); Episode 46/100; Loss: 0.0032071848399937153\n",
      "Step 9 (1398); Episode 46/100; Loss: 0.07790954411029816\n",
      "Step 10 (1399); Episode 46/100; Loss: 0.003076516790315509\n",
      "Step 11 (1400); Episode 46/100; Loss: 0.10429711639881134\n",
      "Step 12 (1401); Episode 46/100; Loss: 0.035582348704338074\n",
      "Step 13 (1402); Episode 46/100; Loss: 0.08315989375114441\n",
      "Step 14 (1403); Episode 46/100; Loss: 0.07807495445013046\n",
      "Step 15 (1404); Episode 46/100; Loss: 0.08491382002830505\n",
      "Step 16 (1405); Episode 46/100; Loss: 0.11557379364967346\n",
      "Step 17 (1406); Episode 46/100; Loss: 0.1201774924993515\n",
      "Step 18 (1407); Episode 46/100; Loss: 0.03949183225631714\n",
      "Step 19 (1408); Episode 46/100; Loss: 0.08332131803035736\n",
      "Step 20 (1409); Episode 46/100; Loss: 0.05753597244620323\n",
      "Step 21 (1410); Episode 46/100; Loss: 0.002747015096247196\n",
      "Step 22 (1411); Episode 46/100; Loss: 0.03327129781246185\n",
      "Step 23 (1412); Episode 46/100; Loss: 0.0424080528318882\n",
      "Step 24 (1413); Episode 46/100; Loss: 0.13652922213077545\n",
      "Step 25 (1414); Episode 46/100; Loss: 0.03779749199748039\n",
      "Step 26 (1415); Episode 46/100; Loss: 0.1716373711824417\n",
      "Step 27 (1416); Episode 46/100; Loss: 0.1639052927494049\n",
      "Step 28 (1417); Episode 46/100; Loss: 0.0035498421639204025\n",
      "Step 29 (1418); Episode 46/100; Loss: 0.11919180303812027\n",
      "Step 30 (1419); Episode 46/100; Loss: 0.10438814759254456\n",
      "Step 31 (1420); Episode 46/100; Loss: 0.0024365403223782778\n",
      "Step 32 (1421); Episode 46/100; Loss: 0.15256023406982422\n",
      "Step 33 (1422); Episode 46/100; Loss: 0.025343237444758415\n",
      "Step 34 (1423); Episode 46/100; Loss: 0.004249469377100468\n",
      "Step 35 (1424); Episode 46/100; Loss: 0.1735079437494278\n",
      "Step 36 (1425); Episode 46/100; Loss: 0.005686274264007807\n",
      "Step 37 (1426); Episode 46/100; Loss: 0.03639848157763481\n",
      "Step 38 (1427); Episode 46/100; Loss: 0.14947715401649475\n",
      "Step 39 (1428); Episode 46/100; Loss: 0.05848661810159683\n",
      "Step 40 (1429); Episode 46/100; Loss: 0.13924631476402283\n",
      "Step 41 (1430); Episode 46/100; Loss: 0.06872549653053284\n",
      "Step 42 (1431); Episode 46/100; Loss: 0.008781070820987225\n",
      "Step 43 (1432); Episode 46/100; Loss: 0.11050058901309967\n",
      "Step 44 (1433); Episode 46/100; Loss: 0.15000063180923462\n",
      "Step 45 (1434); Episode 46/100; Loss: 0.16697640717029572\n",
      "Step 46 (1435); Episode 46/100; Loss: 0.1904841959476471\n",
      "Step 47 (1436); Episode 46/100; Loss: 0.03359444811940193\n",
      "Step 48 (1437); Episode 46/100; Loss: 0.004613823257386684\n",
      "Step 49 (1438); Episode 46/100; Loss: 0.0738910362124443\n",
      "Step 50 (1439); Episode 46/100; Loss: 0.23922036588191986\n",
      "Step 51 (1440); Episode 46/100; Loss: 0.10826612263917923\n",
      "Step 52 (1441); Episode 46/100; Loss: 0.10914146155118942\n",
      "Step 53 (1442); Episode 46/100; Loss: 0.11391958594322205\n",
      "Step 54 (1443); Episode 46/100; Loss: 0.03990083932876587\n",
      "Step 55 (1444); Episode 46/100; Loss: 0.08582792431116104\n",
      "Step 56 (1445); Episode 46/100; Loss: 0.03171608969569206\n",
      "Step 57 (1446); Episode 46/100; Loss: 0.10233461856842041\n",
      "Step 58 (1447); Episode 46/100; Loss: 0.08500251173973083\n",
      "Step 59 (1448); Episode 46/100; Loss: 0.04765189066529274\n",
      "Step 60 (1449); Episode 46/100; Loss: 0.08401811122894287\n",
      "Step 61 (1450); Episode 46/100; Loss: 0.043927013874053955\n",
      "Step 62 (1451); Episode 46/100; Loss: 0.1326628476381302\n",
      "Step 63 (1452); Episode 46/100; Loss: 0.08071669936180115\n",
      "Step 64 (1453); Episode 46/100; Loss: 0.046633969992399216\n",
      "Step 65 (1454); Episode 46/100; Loss: 0.16539233922958374\n",
      "Step 66 (1455); Episode 46/100; Loss: 0.05571368336677551\n",
      "Step 67 (1456); Episode 46/100; Loss: 0.17056137323379517\n",
      "Step 68 (1457); Episode 46/100; Loss: 0.005739522632211447\n",
      "Step 69 (1458); Episode 46/100; Loss: 0.055323634296655655\n",
      "Step 70 (1459); Episode 46/100; Loss: 0.0511728934943676\n",
      "Step 71 (1460); Episode 46/100; Loss: 0.05324807018041611\n",
      "Step 72 (1461); Episode 46/100; Loss: 0.18647392094135284\n",
      "Step 73 (1462); Episode 46/100; Loss: 0.10823529958724976\n",
      "Step 74 (1463); Episode 46/100; Loss: 0.08837635070085526\n",
      "Step 75 (1464); Episode 46/100; Loss: 0.07563775032758713\n",
      "Step 76 (1465); Episode 46/100; Loss: 0.05129634588956833\n",
      "Step 77 (1466); Episode 46/100; Loss: 0.003373930463567376\n",
      "Step 78 (1467); Episode 46/100; Loss: 0.05244322493672371\n",
      "Step 79 (1468); Episode 46/100; Loss: 0.03917792811989784\n",
      "Step 80 (1469); Episode 46/100; Loss: 0.0026573091745376587\n",
      "Step 81 (1470); Episode 46/100; Loss: 0.03189630061388016\n",
      "Step 82 (1471); Episode 46/100; Loss: 0.054678041487932205\n",
      "Step 83 (1472); Episode 46/100; Loss: 0.13163113594055176\n",
      "Step 84 (1473); Episode 46/100; Loss: 0.07576902955770493\n",
      "Step 85 (1474); Episode 46/100; Loss: 0.03640420362353325\n",
      "Step 86 (1475); Episode 46/100; Loss: 0.05202803760766983\n",
      "Step 87 (1476); Episode 46/100; Loss: 0.14467385411262512\n",
      "Step 88 (1477); Episode 46/100; Loss: 0.09411130845546722\n",
      "Step 89 (1478); Episode 46/100; Loss: 0.20439133048057556\n",
      "Step 90 (1479); Episode 46/100; Loss: 0.11692584306001663\n",
      "Step 91 (1480); Episode 46/100; Loss: 0.08141037821769714\n",
      "Step 92 (1481); Episode 46/100; Loss: 0.1841706782579422\n",
      "Step 93 (1482); Episode 46/100; Loss: 0.14611096680164337\n",
      "Step 94 (1483); Episode 46/100; Loss: 0.10504373162984848\n",
      "Step 95 (1484); Episode 46/100; Loss: 0.0907491073012352\n",
      "Step 96 (1485); Episode 46/100; Loss: 0.22152338922023773\n",
      "Step 97 (1486); Episode 46/100; Loss: 0.0030731302686035633\n",
      "Step 98 (1487); Episode 46/100; Loss: 0.08007203787565231\n",
      "Step 99 (1488); Episode 46/100; Loss: 0.13509899377822876\n",
      "Step 100 (1489); Episode 46/100; Loss: 0.025900227949023247\n",
      "Step 101 (1490); Episode 46/100; Loss: 0.13239915668964386\n",
      "Step 102 (1491); Episode 46/100; Loss: 0.053672872483730316\n",
      "Step 103 (1492); Episode 46/100; Loss: 0.07547499239444733\n",
      "Step 104 (1493); Episode 46/100; Loss: 0.07540111243724823\n",
      "Step 105 (1494); Episode 46/100; Loss: 0.0737437829375267\n",
      "Step 106 (1495); Episode 46/100; Loss: 0.00804655347019434\n",
      "Step 107 (1496); Episode 46/100; Loss: 0.004684274550527334\n",
      "Step 108 (1497); Episode 46/100; Loss: 0.12478920817375183\n",
      "Step 109 (1498); Episode 46/100; Loss: 0.1991700977087021\n",
      "Step 110 (1499); Episode 46/100; Loss: 0.06985674798488617\n",
      "Step 111 (1500); Episode 46/100; Loss: 0.029824240133166313\n",
      "Step 112 (1501); Episode 46/100; Loss: 0.05464855208992958\n",
      "Step 113 (1502); Episode 46/100; Loss: 0.04315783828496933\n",
      "Step 114 (1503); Episode 46/100; Loss: 0.11810757964849472\n",
      "Step 115 (1504); Episode 46/100; Loss: 0.0731511190533638\n",
      "Step 116 (1505); Episode 46/100; Loss: 0.12517109513282776\n",
      "Step 117 (1506); Episode 46/100; Loss: 0.0315186083316803\n",
      "Step 118 (1507); Episode 46/100; Loss: 0.0034902775660157204\n",
      "Step 119 (1508); Episode 46/100; Loss: 0.039964910596609116\n",
      "Step 120 (1509); Episode 46/100; Loss: 0.004393208306282759\n",
      "Step 121 (1510); Episode 46/100; Loss: 0.0060490393079817295\n",
      "Step 122 (1511); Episode 46/100; Loss: 0.056093111634254456\n",
      "Step 123 (1512); Episode 46/100; Loss: 0.0864742249250412\n",
      "Step 124 (1513); Episode 46/100; Loss: 0.025849701836705208\n",
      "Step 125 (1514); Episode 46/100; Loss: 0.06460169702768326\n",
      "Step 126 (1515); Episode 46/100; Loss: 0.06777508556842804\n",
      "Step 127 (1516); Episode 46/100; Loss: 0.004540153779089451\n",
      "Step 128 (1517); Episode 46/100; Loss: 0.002615471836179495\n",
      "Step 129 (1518); Episode 46/100; Loss: 0.04925953596830368\n",
      "Step 130 (1519); Episode 46/100; Loss: 0.07147246599197388\n",
      "Step 131 (1520); Episode 46/100; Loss: 0.09743527323007584\n",
      "Step 132 (1521); Episode 46/100; Loss: 0.03345869109034538\n",
      "Step 133 (1522); Episode 46/100; Loss: 0.0025380991864949465\n",
      "Step 134 (1523); Episode 46/100; Loss: 0.004015900194644928\n",
      "Step 135 (1524); Episode 46/100; Loss: 0.0386502742767334\n",
      "Step 136 (1525); Episode 46/100; Loss: 0.006357294507324696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 137 (1526); Episode 46/100; Loss: 0.14884527027606964\n",
      "Step 138 (1527); Episode 46/100; Loss: 0.16083236038684845\n",
      "Step 139 (1528); Episode 46/100; Loss: 0.054001715034246445\n",
      "Step 140 (1529); Episode 46/100; Loss: 0.09336616843938828\n",
      "Step 141 (1530); Episode 46/100; Loss: 0.11433223634958267\n",
      "Step 142 (1531); Episode 46/100; Loss: 0.039128389209508896\n",
      "Step 143 (1532); Episode 46/100; Loss: 0.059676267206668854\n",
      "Step 144 (1533); Episode 46/100; Loss: 0.11594097316265106\n",
      "Step 145 (1534); Episode 46/100; Loss: 0.03522362560033798\n",
      "Step 146 (1535); Episode 46/100; Loss: 0.10524958372116089\n",
      "Step 147 (1536); Episode 46/100; Loss: 0.09740292280912399\n",
      "Step 148 (1537); Episode 46/100; Loss: 0.025029204785823822\n",
      "Step 149 (1538); Episode 46/100; Loss: 0.004589793737977743\n",
      "Step 150 (1539); Episode 46/100; Loss: 0.0038459422066807747\n",
      "Step 151 (1540); Episode 46/100; Loss: 0.16480232775211334\n",
      "Step 152 (1541); Episode 46/100; Loss: 0.136204332113266\n",
      "Step 153 (1542); Episode 46/100; Loss: 0.052587516605854034\n",
      "Step 154 (1543); Episode 46/100; Loss: 0.12204711139202118\n",
      "Step 155 (1544); Episode 46/100; Loss: 0.04436584934592247\n",
      "Step 156 (1545); Episode 46/100; Loss: 0.0789756253361702\n",
      "Step 157 (1546); Episode 46/100; Loss: 0.09956707060337067\n",
      "Step 158 (1547); Episode 46/100; Loss: 0.09427987784147263\n",
      "Step 159 (1548); Episode 46/100; Loss: 0.04892084002494812\n",
      "Step 160 (1549); Episode 46/100; Loss: 0.020600685849785805\n",
      "Step 0 (1550); Episode 47/100; Loss: 0.07558529078960419\n",
      "Step 1 (1551); Episode 47/100; Loss: 0.10202863067388535\n",
      "Step 2 (1552); Episode 47/100; Loss: 0.0573681965470314\n",
      "Step 3 (1553); Episode 47/100; Loss: 0.13671335577964783\n",
      "Step 4 (1554); Episode 47/100; Loss: 0.01922050677239895\n",
      "Step 5 (1555); Episode 47/100; Loss: 0.08349376916885376\n",
      "Step 6 (1556); Episode 47/100; Loss: 0.04789913073182106\n",
      "Step 7 (1557); Episode 47/100; Loss: 0.004643975757062435\n",
      "Step 8 (1558); Episode 47/100; Loss: 0.049945298582315445\n",
      "Step 9 (1559); Episode 47/100; Loss: 0.04469761624932289\n",
      "Step 10 (1560); Episode 47/100; Loss: 0.0872005894780159\n",
      "Step 11 (1561); Episode 47/100; Loss: 0.03933694213628769\n",
      "Step 12 (1562); Episode 47/100; Loss: 0.08663450926542282\n",
      "Step 13 (1563); Episode 47/100; Loss: 0.006500998046249151\n",
      "Step 14 (1564); Episode 47/100; Loss: 0.037419047206640244\n",
      "Step 15 (1565); Episode 47/100; Loss: 0.10336228460073471\n",
      "Step 16 (1566); Episode 47/100; Loss: 0.08346841484308243\n",
      "Step 17 (1567); Episode 47/100; Loss: 0.05019916221499443\n",
      "Step 18 (1568); Episode 47/100; Loss: 0.072995126247406\n",
      "Step 19 (1569); Episode 47/100; Loss: 0.08849231898784637\n",
      "Step 20 (1570); Episode 47/100; Loss: 0.2325829565525055\n",
      "Step 21 (1571); Episode 47/100; Loss: 0.053960978984832764\n",
      "Step 22 (1572); Episode 47/100; Loss: 0.07978800684213638\n",
      "Step 23 (1573); Episode 47/100; Loss: 0.14397189021110535\n",
      "Step 24 (1574); Episode 47/100; Loss: 0.09545747190713882\n",
      "Step 25 (1575); Episode 47/100; Loss: 0.014423458836972713\n",
      "Step 26 (1576); Episode 47/100; Loss: 0.054267507046461105\n",
      "Step 27 (1577); Episode 47/100; Loss: 0.13273736834526062\n",
      "Step 28 (1578); Episode 47/100; Loss: 0.09841934591531754\n",
      "Step 29 (1579); Episode 47/100; Loss: 0.1383470892906189\n",
      "Step 30 (1580); Episode 47/100; Loss: 0.042155615985393524\n",
      "Step 31 (1581); Episode 47/100; Loss: 0.004233171697705984\n",
      "Step 32 (1582); Episode 47/100; Loss: 0.05630664899945259\n",
      "Step 33 (1583); Episode 47/100; Loss: 0.10176201164722443\n",
      "Step 34 (1584); Episode 47/100; Loss: 0.10424602776765823\n",
      "Step 35 (1585); Episode 47/100; Loss: 0.08077909797430038\n",
      "Step 36 (1586); Episode 47/100; Loss: 0.004307488910853863\n",
      "Step 37 (1587); Episode 47/100; Loss: 0.07930241525173187\n",
      "Step 38 (1588); Episode 47/100; Loss: 0.10012216866016388\n",
      "Step 39 (1589); Episode 47/100; Loss: 0.043428320437669754\n",
      "Step 40 (1590); Episode 47/100; Loss: 0.05117371678352356\n",
      "Step 41 (1591); Episode 47/100; Loss: 0.03062289021909237\n",
      "Step 42 (1592); Episode 47/100; Loss: 0.09452696144580841\n",
      "Step 43 (1593); Episode 47/100; Loss: 0.07937396317720413\n",
      "Step 44 (1594); Episode 47/100; Loss: 0.004341179970651865\n",
      "Step 45 (1595); Episode 47/100; Loss: 0.04236510396003723\n",
      "Step 46 (1596); Episode 47/100; Loss: 0.002350004855543375\n",
      "Step 47 (1597); Episode 47/100; Loss: 0.1714606136083603\n",
      "Step 48 (1598); Episode 47/100; Loss: 0.036147262901067734\n",
      "Step 49 (1599); Episode 47/100; Loss: 0.03620588034391403\n",
      "Step 50 (1600); Episode 47/100; Loss: 0.05108564719557762\n",
      "Step 51 (1601); Episode 47/100; Loss: 0.07017681747674942\n",
      "Step 52 (1602); Episode 47/100; Loss: 0.08072708547115326\n",
      "Step 53 (1603); Episode 47/100; Loss: 0.0802234560251236\n",
      "Step 54 (1604); Episode 47/100; Loss: 0.04125816002488136\n",
      "Step 55 (1605); Episode 47/100; Loss: 0.023796765133738518\n",
      "Step 56 (1606); Episode 47/100; Loss: 0.03134077042341232\n",
      "Step 57 (1607); Episode 47/100; Loss: 0.027688756585121155\n",
      "Step 58 (1608); Episode 47/100; Loss: 0.08930522948503494\n",
      "Step 59 (1609); Episode 47/100; Loss: 0.11761577427387238\n",
      "Step 60 (1610); Episode 47/100; Loss: 0.11469637602567673\n",
      "Step 61 (1611); Episode 47/100; Loss: 0.00309914187528193\n",
      "Step 62 (1612); Episode 47/100; Loss: 0.07938644289970398\n",
      "Step 63 (1613); Episode 47/100; Loss: 0.02337549440562725\n",
      "Step 64 (1614); Episode 47/100; Loss: 0.07144025713205338\n",
      "Step 65 (1615); Episode 47/100; Loss: 0.06429876387119293\n",
      "Step 66 (1616); Episode 47/100; Loss: 0.04948153719305992\n",
      "Step 67 (1617); Episode 47/100; Loss: 0.0023712257388979197\n",
      "Step 68 (1618); Episode 47/100; Loss: 0.1334899216890335\n",
      "Step 69 (1619); Episode 47/100; Loss: 0.09086976945400238\n",
      "Step 70 (1620); Episode 47/100; Loss: 0.04265140742063522\n",
      "Step 71 (1621); Episode 47/100; Loss: 0.09100187569856644\n",
      "Step 72 (1622); Episode 47/100; Loss: 0.054636985063552856\n",
      "Step 73 (1623); Episode 47/100; Loss: 0.10151520371437073\n",
      "Step 74 (1624); Episode 47/100; Loss: 0.08705685287714005\n",
      "Step 75 (1625); Episode 47/100; Loss: 0.037360791116952896\n",
      "Step 76 (1626); Episode 47/100; Loss: 0.052099525928497314\n",
      "Step 77 (1627); Episode 47/100; Loss: 0.0031083112116903067\n",
      "Step 78 (1628); Episode 47/100; Loss: 0.15997114777565002\n",
      "Step 79 (1629); Episode 47/100; Loss: 0.08640459924936295\n",
      "Step 80 (1630); Episode 47/100; Loss: 0.05375238135457039\n",
      "Step 81 (1631); Episode 47/100; Loss: 0.14489124715328217\n",
      "Step 82 (1632); Episode 47/100; Loss: 0.0019822828471660614\n",
      "Step 83 (1633); Episode 47/100; Loss: 0.03390280902385712\n",
      "Step 84 (1634); Episode 47/100; Loss: 0.003949744626879692\n",
      "Step 85 (1635); Episode 47/100; Loss: 0.03943289443850517\n",
      "Step 86 (1636); Episode 47/100; Loss: 0.004593683406710625\n",
      "Step 87 (1637); Episode 47/100; Loss: 0.11366693675518036\n",
      "Step 88 (1638); Episode 47/100; Loss: 0.052279748022556305\n",
      "Step 89 (1639); Episode 47/100; Loss: 0.1324128955602646\n",
      "Step 90 (1640); Episode 47/100; Loss: 0.12116681784391403\n",
      "Step 91 (1641); Episode 47/100; Loss: 0.07265676558017731\n",
      "Step 92 (1642); Episode 47/100; Loss: 0.1158345565199852\n",
      "Step 93 (1643); Episode 47/100; Loss: 0.1509227752685547\n",
      "Step 94 (1644); Episode 47/100; Loss: 0.09805313497781754\n",
      "Step 95 (1645); Episode 47/100; Loss: 0.029851770028471947\n",
      "Step 96 (1646); Episode 47/100; Loss: 0.1351717859506607\n",
      "Step 97 (1647); Episode 47/100; Loss: 0.13850606977939606\n",
      "Step 98 (1648); Episode 47/100; Loss: 0.06874016672372818\n",
      "Step 99 (1649); Episode 47/100; Loss: 0.0026791165582835674\n",
      "Step 100 (1650); Episode 47/100; Loss: 0.054163917899131775\n",
      "Step 101 (1651); Episode 47/100; Loss: 0.17820176482200623\n",
      "Step 102 (1652); Episode 47/100; Loss: 0.002556984080001712\n",
      "Step 103 (1653); Episode 47/100; Loss: 0.1640164852142334\n",
      "Step 104 (1654); Episode 47/100; Loss: 0.08348188549280167\n",
      "Step 105 (1655); Episode 47/100; Loss: 0.1132013276219368\n",
      "Step 106 (1656); Episode 47/100; Loss: 0.1269337385892868\n",
      "Step 107 (1657); Episode 47/100; Loss: 0.0034095277078449726\n",
      "Step 108 (1658); Episode 47/100; Loss: 0.15909487009048462\n",
      "Step 109 (1659); Episode 47/100; Loss: 0.001883930410258472\n",
      "Step 110 (1660); Episode 47/100; Loss: 0.005684876814484596\n",
      "Step 111 (1661); Episode 47/100; Loss: 0.05437976121902466\n",
      "Step 112 (1662); Episode 47/100; Loss: 0.0279199481010437\n",
      "Step 113 (1663); Episode 47/100; Loss: 0.14498403668403625\n",
      "Step 114 (1664); Episode 47/100; Loss: 0.1072894036769867\n",
      "Step 115 (1665); Episode 47/100; Loss: 0.030117463320493698\n",
      "Step 116 (1666); Episode 47/100; Loss: 0.11889827996492386\n",
      "Step 117 (1667); Episode 47/100; Loss: 0.10129030048847198\n",
      "Step 118 (1668); Episode 47/100; Loss: 0.004344099201261997\n",
      "Step 119 (1669); Episode 47/100; Loss: 0.05792609974741936\n",
      "Step 120 (1670); Episode 47/100; Loss: 0.08131219446659088\n",
      "Step 121 (1671); Episode 47/100; Loss: 0.0710304006934166\n",
      "Step 122 (1672); Episode 47/100; Loss: 0.08535471558570862\n",
      "Step 123 (1673); Episode 47/100; Loss: 0.005611417815089226\n",
      "Step 124 (1674); Episode 47/100; Loss: 0.13009758293628693\n",
      "Step 125 (1675); Episode 47/100; Loss: 0.1357433944940567\n",
      "Step 126 (1676); Episode 47/100; Loss: 0.13356582820415497\n",
      "Step 127 (1677); Episode 47/100; Loss: 0.14896611869335175\n",
      "Step 128 (1678); Episode 47/100; Loss: 0.08195783197879791\n",
      "Step 129 (1679); Episode 47/100; Loss: 0.08924470841884613\n",
      "Step 130 (1680); Episode 47/100; Loss: 0.0037121528293937445\n",
      "Step 131 (1681); Episode 47/100; Loss: 0.05710231140255928\n",
      "Step 132 (1682); Episode 47/100; Loss: 0.08083099871873856\n",
      "Step 133 (1683); Episode 47/100; Loss: 0.0677952989935875\n",
      "Step 134 (1684); Episode 47/100; Loss: 0.0538564994931221\n",
      "Step 135 (1685); Episode 47/100; Loss: 0.10785707086324692\n",
      "Step 136 (1686); Episode 47/100; Loss: 0.02905801497399807\n",
      "Step 137 (1687); Episode 47/100; Loss: 0.11482154577970505\n",
      "Step 138 (1688); Episode 47/100; Loss: 0.04662182182073593\n",
      "Step 139 (1689); Episode 47/100; Loss: 0.1850215345621109\n",
      "Step 140 (1690); Episode 47/100; Loss: 0.14211757481098175\n",
      "Step 141 (1691); Episode 47/100; Loss: 0.0030280095525085926\n",
      "Step 142 (1692); Episode 47/100; Loss: 0.07553048431873322\n",
      "Step 143 (1693); Episode 47/100; Loss: 0.0786714255809784\n",
      "Step 144 (1694); Episode 47/100; Loss: 0.02714592032134533\n",
      "Step 145 (1695); Episode 47/100; Loss: 0.0693160742521286\n",
      "Step 146 (1696); Episode 47/100; Loss: 0.14130207896232605\n",
      "Step 147 (1697); Episode 47/100; Loss: 0.011011791415512562\n",
      "Step 148 (1698); Episode 47/100; Loss: 0.003206197638064623\n",
      "Step 149 (1699); Episode 47/100; Loss: 0.03496478125452995\n",
      "Step 150 (1700); Episode 47/100; Loss: 0.07441689074039459\n",
      "Step 151 (1701); Episode 47/100; Loss: 0.11858607828617096\n",
      "Step 152 (1702); Episode 47/100; Loss: 0.03889006748795509\n",
      "Step 153 (1703); Episode 47/100; Loss: 0.040071502327919006\n",
      "Step 154 (1704); Episode 47/100; Loss: 0.06824379414319992\n",
      "Step 155 (1705); Episode 47/100; Loss: 0.07372711598873138\n",
      "Step 156 (1706); Episode 47/100; Loss: 0.05697730556130409\n",
      "Step 157 (1707); Episode 47/100; Loss: 0.07975729554891586\n",
      "Step 158 (1708); Episode 47/100; Loss: 0.09561668336391449\n",
      "Step 159 (1709); Episode 47/100; Loss: 0.10089339315891266\n",
      "Step 160 (1710); Episode 47/100; Loss: 0.07588408142328262\n",
      "Step 161 (1711); Episode 47/100; Loss: 0.012590423226356506\n",
      "Step 162 (1712); Episode 47/100; Loss: 0.02641560323536396\n",
      "Step 163 (1713); Episode 47/100; Loss: 0.12470946460962296\n",
      "Step 164 (1714); Episode 47/100; Loss: 0.10242616385221481\n",
      "Step 165 (1715); Episode 47/100; Loss: 0.06293755769729614\n",
      "Step 166 (1716); Episode 47/100; Loss: 0.006025787442922592\n",
      "Step 167 (1717); Episode 47/100; Loss: 0.025895502418279648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 168 (1718); Episode 47/100; Loss: 0.007044011261314154\n",
      "Step 169 (1719); Episode 47/100; Loss: 0.048545531928539276\n",
      "Step 170 (1720); Episode 47/100; Loss: 0.019383328035473824\n",
      "Step 171 (1721); Episode 47/100; Loss: 0.13267455995082855\n",
      "Step 172 (1722); Episode 47/100; Loss: 0.027916492894291878\n",
      "Step 173 (1723); Episode 47/100; Loss: 0.07261722534894943\n",
      "Step 174 (1724); Episode 47/100; Loss: 0.08367647975683212\n",
      "Step 175 (1725); Episode 47/100; Loss: 0.05442381277680397\n",
      "Step 176 (1726); Episode 47/100; Loss: 0.02853483147919178\n",
      "Step 0 (1727); Episode 48/100; Loss: 0.053255483508110046\n",
      "Step 1 (1728); Episode 48/100; Loss: 0.004744095727801323\n",
      "Step 2 (1729); Episode 48/100; Loss: 0.08866756409406662\n",
      "Step 3 (1730); Episode 48/100; Loss: 0.032348450273275375\n",
      "Step 4 (1731); Episode 48/100; Loss: 0.11855810880661011\n",
      "Step 5 (1732); Episode 48/100; Loss: 0.003256899770349264\n",
      "Step 6 (1733); Episode 48/100; Loss: 0.08100111037492752\n",
      "Step 7 (1734); Episode 48/100; Loss: 0.003848061664029956\n",
      "Step 8 (1735); Episode 48/100; Loss: 0.08457702398300171\n",
      "Step 9 (1736); Episode 48/100; Loss: 0.09363441914319992\n",
      "Step 10 (1737); Episode 48/100; Loss: 0.11752916127443314\n",
      "Step 11 (1738); Episode 48/100; Loss: 0.13150526583194733\n",
      "Step 12 (1739); Episode 48/100; Loss: 0.06926030665636063\n",
      "Step 13 (1740); Episode 48/100; Loss: 0.029028555378317833\n",
      "Step 14 (1741); Episode 48/100; Loss: 0.004199164919555187\n",
      "Step 15 (1742); Episode 48/100; Loss: 0.03747919201850891\n",
      "Step 16 (1743); Episode 48/100; Loss: 0.05499548092484474\n",
      "Step 17 (1744); Episode 48/100; Loss: 0.05486924573779106\n",
      "Step 18 (1745); Episode 48/100; Loss: 0.18756736814975739\n",
      "Step 19 (1746); Episode 48/100; Loss: 0.1143360584974289\n",
      "Step 20 (1747); Episode 48/100; Loss: 0.04795029014348984\n",
      "Step 21 (1748); Episode 48/100; Loss: 0.09817633032798767\n",
      "Step 22 (1749); Episode 48/100; Loss: 0.12657184898853302\n",
      "Step 23 (1750); Episode 48/100; Loss: 0.08364417403936386\n",
      "Step 24 (1751); Episode 48/100; Loss: 0.06104397773742676\n",
      "Step 25 (1752); Episode 48/100; Loss: 0.049642689526081085\n",
      "Step 26 (1753); Episode 48/100; Loss: 0.16692116856575012\n",
      "Step 27 (1754); Episode 48/100; Loss: 0.05305441841483116\n",
      "Step 28 (1755); Episode 48/100; Loss: 0.006604734808206558\n",
      "Step 29 (1756); Episode 48/100; Loss: 0.003958011977374554\n",
      "Step 30 (1757); Episode 48/100; Loss: 0.08759111911058426\n",
      "Step 31 (1758); Episode 48/100; Loss: 0.12303078174591064\n",
      "Step 32 (1759); Episode 48/100; Loss: 0.08409725874662399\n",
      "Step 33 (1760); Episode 48/100; Loss: 0.05175383388996124\n",
      "Step 34 (1761); Episode 48/100; Loss: 0.16628463566303253\n",
      "Step 35 (1762); Episode 48/100; Loss: 0.06412573158740997\n",
      "Step 36 (1763); Episode 48/100; Loss: 0.1152861937880516\n",
      "Step 37 (1764); Episode 48/100; Loss: 0.10704992711544037\n",
      "Step 38 (1765); Episode 48/100; Loss: 0.12185133248567581\n",
      "Step 39 (1766); Episode 48/100; Loss: 0.13708752393722534\n",
      "Step 40 (1767); Episode 48/100; Loss: 0.034052494913339615\n",
      "Step 41 (1768); Episode 48/100; Loss: 0.04546651244163513\n",
      "Step 42 (1769); Episode 48/100; Loss: 0.04762149229645729\n",
      "Step 43 (1770); Episode 48/100; Loss: 0.08001551032066345\n",
      "Step 44 (1771); Episode 48/100; Loss: 0.029191380366683006\n",
      "Step 45 (1772); Episode 48/100; Loss: 0.007126971613615751\n",
      "Step 46 (1773); Episode 48/100; Loss: 0.052580978721380234\n",
      "Step 47 (1774); Episode 48/100; Loss: 0.055080197751522064\n",
      "Step 48 (1775); Episode 48/100; Loss: 0.05316784605383873\n",
      "Step 49 (1776); Episode 48/100; Loss: 0.005329230334609747\n",
      "Step 50 (1777); Episode 48/100; Loss: 0.08589774370193481\n",
      "Step 51 (1778); Episode 48/100; Loss: 0.04502478614449501\n",
      "Step 52 (1779); Episode 48/100; Loss: 0.14378777146339417\n",
      "Step 53 (1780); Episode 48/100; Loss: 0.0823482945561409\n",
      "Step 54 (1781); Episode 48/100; Loss: 0.14115267992019653\n",
      "Step 55 (1782); Episode 48/100; Loss: 0.08840785920619965\n",
      "Step 56 (1783); Episode 48/100; Loss: 0.004340977407991886\n",
      "Step 57 (1784); Episode 48/100; Loss: 0.036092907190322876\n",
      "Step 58 (1785); Episode 48/100; Loss: 0.05367821455001831\n",
      "Step 59 (1786); Episode 48/100; Loss: 0.13537269830703735\n",
      "Step 60 (1787); Episode 48/100; Loss: 0.0487634651362896\n",
      "Step 61 (1788); Episode 48/100; Loss: 0.05797075480222702\n",
      "Step 62 (1789); Episode 48/100; Loss: 0.11773601174354553\n",
      "Step 63 (1790); Episode 48/100; Loss: 0.0892513319849968\n",
      "Step 64 (1791); Episode 48/100; Loss: 0.09900938719511032\n",
      "Step 65 (1792); Episode 48/100; Loss: 0.08651681989431381\n",
      "Step 66 (1793); Episode 48/100; Loss: 0.08688484132289886\n",
      "Step 67 (1794); Episode 48/100; Loss: 0.13426901400089264\n",
      "Step 68 (1795); Episode 48/100; Loss: 0.1323171705007553\n",
      "Step 69 (1796); Episode 48/100; Loss: 0.03146519139409065\n",
      "Step 70 (1797); Episode 48/100; Loss: 0.033034514635801315\n",
      "Step 71 (1798); Episode 48/100; Loss: 0.08402006328105927\n",
      "Step 72 (1799); Episode 48/100; Loss: 0.005657372064888477\n",
      "Step 73 (1800); Episode 48/100; Loss: 0.12490585446357727\n",
      "Step 74 (1801); Episode 48/100; Loss: 0.00407845014706254\n",
      "Step 75 (1802); Episode 48/100; Loss: 0.07177311927080154\n",
      "Step 76 (1803); Episode 48/100; Loss: 0.035975124686956406\n",
      "Step 77 (1804); Episode 48/100; Loss: 0.06741657108068466\n",
      "Step 78 (1805); Episode 48/100; Loss: 0.0030520851723849773\n",
      "Step 79 (1806); Episode 48/100; Loss: 0.040504250675439835\n",
      "Step 80 (1807); Episode 48/100; Loss: 0.08994925767183304\n",
      "Step 81 (1808); Episode 48/100; Loss: 0.07954557985067368\n",
      "Step 82 (1809); Episode 48/100; Loss: 0.006106715649366379\n",
      "Step 83 (1810); Episode 48/100; Loss: 0.005683225113898516\n",
      "Step 84 (1811); Episode 48/100; Loss: 0.0514766201376915\n",
      "Step 85 (1812); Episode 48/100; Loss: 0.08824912458658218\n",
      "Step 86 (1813); Episode 48/100; Loss: 0.08659999072551727\n",
      "Step 87 (1814); Episode 48/100; Loss: 0.029680432751774788\n",
      "Step 88 (1815); Episode 48/100; Loss: 0.0035447783302515745\n",
      "Step 89 (1816); Episode 48/100; Loss: 0.07298672199249268\n",
      "Step 90 (1817); Episode 48/100; Loss: 0.06830580532550812\n",
      "Step 91 (1818); Episode 48/100; Loss: 0.06988608837127686\n",
      "Step 92 (1819); Episode 48/100; Loss: 0.12032733112573624\n",
      "Step 93 (1820); Episode 48/100; Loss: 0.0030422876589000225\n",
      "Step 94 (1821); Episode 48/100; Loss: 0.16579291224479675\n",
      "Step 95 (1822); Episode 48/100; Loss: 0.07642966508865356\n",
      "Step 96 (1823); Episode 48/100; Loss: 0.0025798771530389786\n",
      "Step 97 (1824); Episode 48/100; Loss: 0.06732694059610367\n",
      "Step 98 (1825); Episode 48/100; Loss: 0.04619615897536278\n",
      "Step 99 (1826); Episode 48/100; Loss: 0.16578105092048645\n",
      "Step 100 (1827); Episode 48/100; Loss: 0.23323020339012146\n",
      "Step 101 (1828); Episode 48/100; Loss: 0.02409248612821102\n",
      "Step 102 (1829); Episode 48/100; Loss: 0.1929149627685547\n",
      "Step 103 (1830); Episode 48/100; Loss: 0.04494085535407066\n",
      "Step 104 (1831); Episode 48/100; Loss: 0.09123481810092926\n",
      "Step 105 (1832); Episode 48/100; Loss: 0.1210571750998497\n",
      "Step 106 (1833); Episode 48/100; Loss: 0.03495784476399422\n",
      "Step 107 (1834); Episode 48/100; Loss: 0.0034810935612767935\n",
      "Step 108 (1835); Episode 48/100; Loss: 0.11355793476104736\n",
      "Step 109 (1836); Episode 48/100; Loss: 0.15114662051200867\n",
      "Step 110 (1837); Episode 48/100; Loss: 0.06081404164433479\n",
      "Step 111 (1838); Episode 48/100; Loss: 0.15043342113494873\n",
      "Step 112 (1839); Episode 48/100; Loss: 0.035725247114896774\n",
      "Step 113 (1840); Episode 48/100; Loss: 0.10863479971885681\n",
      "Step 114 (1841); Episode 48/100; Loss: 0.029494743794202805\n",
      "Step 115 (1842); Episode 48/100; Loss: 0.058711353689432144\n",
      "Step 116 (1843); Episode 48/100; Loss: 0.027787696570158005\n",
      "Step 117 (1844); Episode 48/100; Loss: 0.1506834626197815\n",
      "Step 118 (1845); Episode 48/100; Loss: 0.0987454503774643\n",
      "Step 119 (1846); Episode 48/100; Loss: 0.004952427465468645\n",
      "Step 120 (1847); Episode 48/100; Loss: 0.0701703280210495\n",
      "Step 121 (1848); Episode 48/100; Loss: 0.004766948986798525\n",
      "Step 122 (1849); Episode 48/100; Loss: 0.030565908178687096\n",
      "Step 123 (1850); Episode 48/100; Loss: 0.08767680823802948\n",
      "Step 124 (1851); Episode 48/100; Loss: 0.050557855516672134\n",
      "Step 125 (1852); Episode 48/100; Loss: 0.0787629783153534\n",
      "Step 126 (1853); Episode 48/100; Loss: 0.004510269034653902\n",
      "Step 127 (1854); Episode 48/100; Loss: 0.07067005336284637\n",
      "Step 128 (1855); Episode 48/100; Loss: 0.15428169071674347\n",
      "Step 129 (1856); Episode 48/100; Loss: 0.05779150128364563\n",
      "Step 130 (1857); Episode 48/100; Loss: 0.0021335878409445286\n",
      "Step 131 (1858); Episode 48/100; Loss: 0.024724237620830536\n",
      "Step 132 (1859); Episode 48/100; Loss: 0.043615665286779404\n",
      "Step 133 (1860); Episode 48/100; Loss: 0.11873970180749893\n",
      "Step 134 (1861); Episode 48/100; Loss: 0.17689912021160126\n",
      "Step 135 (1862); Episode 48/100; Loss: 0.041438568383455276\n",
      "Step 136 (1863); Episode 48/100; Loss: 0.17101342976093292\n",
      "Step 137 (1864); Episode 48/100; Loss: 0.1323954164981842\n",
      "Step 138 (1865); Episode 48/100; Loss: 0.1255197525024414\n",
      "Step 139 (1866); Episode 48/100; Loss: 0.09767196327447891\n",
      "Step 140 (1867); Episode 48/100; Loss: 0.13159888982772827\n",
      "Step 141 (1868); Episode 48/100; Loss: 0.19043156504631042\n",
      "Step 142 (1869); Episode 48/100; Loss: 0.007667710073292255\n",
      "Step 143 (1870); Episode 48/100; Loss: 0.03931708261370659\n",
      "Step 144 (1871); Episode 48/100; Loss: 0.0021912630181759596\n",
      "Step 145 (1872); Episode 48/100; Loss: 0.1386444866657257\n",
      "Step 146 (1873); Episode 48/100; Loss: 0.1269657164812088\n",
      "Step 147 (1874); Episode 48/100; Loss: 0.045756593346595764\n",
      "Step 148 (1875); Episode 48/100; Loss: 0.040686093270778656\n",
      "Step 149 (1876); Episode 48/100; Loss: 0.09063263982534409\n",
      "Step 150 (1877); Episode 48/100; Loss: 0.06772346049547195\n",
      "Step 151 (1878); Episode 48/100; Loss: 0.053895700722932816\n",
      "Step 152 (1879); Episode 48/100; Loss: 0.03690202534198761\n",
      "Step 0 (1880); Episode 49/100; Loss: 0.051566749811172485\n",
      "Step 1 (1881); Episode 49/100; Loss: 0.05211639404296875\n",
      "Step 2 (1882); Episode 49/100; Loss: 0.11468364298343658\n",
      "Step 3 (1883); Episode 49/100; Loss: 0.028387317433953285\n",
      "Step 4 (1884); Episode 49/100; Loss: 0.03670921549201012\n",
      "Step 5 (1885); Episode 49/100; Loss: 0.12074227631092072\n",
      "Step 6 (1886); Episode 49/100; Loss: 0.004080574493855238\n",
      "Step 7 (1887); Episode 49/100; Loss: 0.049085840582847595\n",
      "Step 8 (1888); Episode 49/100; Loss: 0.08017556369304657\n",
      "Step 9 (1889); Episode 49/100; Loss: 0.06410978734493256\n",
      "Step 10 (1890); Episode 49/100; Loss: 0.00269988551735878\n",
      "Step 11 (1891); Episode 49/100; Loss: 0.05225006863474846\n",
      "Step 12 (1892); Episode 49/100; Loss: 0.010076171718537807\n",
      "Step 13 (1893); Episode 49/100; Loss: 0.02070392481982708\n",
      "Step 14 (1894); Episode 49/100; Loss: 0.05912517383694649\n",
      "Step 15 (1895); Episode 49/100; Loss: 0.06515412032604218\n",
      "Step 16 (1896); Episode 49/100; Loss: 0.05588172376155853\n",
      "Step 17 (1897); Episode 49/100; Loss: 0.05401357635855675\n",
      "Step 18 (1898); Episode 49/100; Loss: 0.09534724056720734\n",
      "Step 19 (1899); Episode 49/100; Loss: 0.05054612085223198\n",
      "Step 20 (1900); Episode 49/100; Loss: 0.054389838129282\n",
      "Step 21 (1901); Episode 49/100; Loss: 0.1416538655757904\n",
      "Step 22 (1902); Episode 49/100; Loss: 0.18972806632518768\n",
      "Step 23 (1903); Episode 49/100; Loss: 0.005709471181035042\n",
      "Step 24 (1904); Episode 49/100; Loss: 0.13658134639263153\n",
      "Step 25 (1905); Episode 49/100; Loss: 0.06624391674995422\n",
      "Step 26 (1906); Episode 49/100; Loss: 0.1738271415233612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27 (1907); Episode 49/100; Loss: 0.193536639213562\n",
      "Step 28 (1908); Episode 49/100; Loss: 0.027884453535079956\n",
      "Step 29 (1909); Episode 49/100; Loss: 0.07458421587944031\n",
      "Step 30 (1910); Episode 49/100; Loss: 0.1463252604007721\n",
      "Step 31 (1911); Episode 49/100; Loss: 0.06399668753147125\n",
      "Step 32 (1912); Episode 49/100; Loss: 0.043492432683706284\n",
      "Step 33 (1913); Episode 49/100; Loss: 0.07090524584054947\n",
      "Step 34 (1914); Episode 49/100; Loss: 0.09791509062051773\n",
      "Step 35 (1915); Episode 49/100; Loss: 0.004935913253575563\n",
      "Step 36 (1916); Episode 49/100; Loss: 0.09153813123703003\n",
      "Step 37 (1917); Episode 49/100; Loss: 0.12907974421977997\n",
      "Step 38 (1918); Episode 49/100; Loss: 0.04601111635565758\n",
      "Step 39 (1919); Episode 49/100; Loss: 0.0673963651061058\n",
      "Step 40 (1920); Episode 49/100; Loss: 0.10070112347602844\n",
      "Step 41 (1921); Episode 49/100; Loss: 0.0435238815844059\n",
      "Step 42 (1922); Episode 49/100; Loss: 0.09509938955307007\n",
      "Step 43 (1923); Episode 49/100; Loss: 0.06492399424314499\n",
      "Step 44 (1924); Episode 49/100; Loss: 0.058007217943668365\n",
      "Step 45 (1925); Episode 49/100; Loss: 0.03175666183233261\n",
      "Step 46 (1926); Episode 49/100; Loss: 0.04288667067885399\n",
      "Step 47 (1927); Episode 49/100; Loss: 0.041139885783195496\n",
      "Step 48 (1928); Episode 49/100; Loss: 0.07408647984266281\n",
      "Step 49 (1929); Episode 49/100; Loss: 0.06401742994785309\n",
      "Step 50 (1930); Episode 49/100; Loss: 0.10985536873340607\n",
      "Step 51 (1931); Episode 49/100; Loss: 0.004373960662633181\n",
      "Step 52 (1932); Episode 49/100; Loss: 0.11531634628772736\n",
      "Step 53 (1933); Episode 49/100; Loss: 0.053432662039995193\n",
      "Step 54 (1934); Episode 49/100; Loss: 0.0611526183784008\n",
      "Step 55 (1935); Episode 49/100; Loss: 0.02367963083088398\n",
      "Step 56 (1936); Episode 49/100; Loss: 0.10627704858779907\n",
      "Step 57 (1937); Episode 49/100; Loss: 0.1224566400051117\n",
      "Step 58 (1938); Episode 49/100; Loss: 0.14105021953582764\n",
      "Step 59 (1939); Episode 49/100; Loss: 0.06627915054559708\n",
      "Step 60 (1940); Episode 49/100; Loss: 0.004923077300190926\n",
      "Step 61 (1941); Episode 49/100; Loss: 0.03781330585479736\n",
      "Step 62 (1942); Episode 49/100; Loss: 0.010570531710982323\n",
      "Step 63 (1943); Episode 49/100; Loss: 0.008830307051539421\n",
      "Step 64 (1944); Episode 49/100; Loss: 0.042270611971616745\n",
      "Step 65 (1945); Episode 49/100; Loss: 0.05437006056308746\n",
      "Step 66 (1946); Episode 49/100; Loss: 0.05456504598259926\n",
      "Step 67 (1947); Episode 49/100; Loss: 0.05256851762533188\n",
      "Step 68 (1948); Episode 49/100; Loss: 0.00353792030364275\n",
      "Step 69 (1949); Episode 49/100; Loss: 0.05197880417108536\n",
      "Step 70 (1950); Episode 49/100; Loss: 0.029982419684529305\n",
      "Step 71 (1951); Episode 49/100; Loss: 0.027444634586572647\n",
      "Step 72 (1952); Episode 49/100; Loss: 0.12108257412910461\n",
      "Step 73 (1953); Episode 49/100; Loss: 0.006324667017906904\n",
      "Step 74 (1954); Episode 49/100; Loss: 0.04179460555315018\n",
      "Step 75 (1955); Episode 49/100; Loss: 0.18013672530651093\n",
      "Step 76 (1956); Episode 49/100; Loss: 0.0054383291862905025\n",
      "Step 77 (1957); Episode 49/100; Loss: 0.02806551195681095\n",
      "Step 78 (1958); Episode 49/100; Loss: 0.13506142795085907\n",
      "Step 79 (1959); Episode 49/100; Loss: 0.006995333358645439\n",
      "Step 80 (1960); Episode 49/100; Loss: 0.11071851849555969\n",
      "Step 81 (1961); Episode 49/100; Loss: 0.05549701675772667\n",
      "Step 82 (1962); Episode 49/100; Loss: 0.030602220445871353\n",
      "Step 83 (1963); Episode 49/100; Loss: 0.03821811452507973\n",
      "Step 84 (1964); Episode 49/100; Loss: 0.004043808672577143\n",
      "Step 85 (1965); Episode 49/100; Loss: 0.07889583706855774\n",
      "Step 86 (1966); Episode 49/100; Loss: 0.040567584335803986\n",
      "Step 87 (1967); Episode 49/100; Loss: 0.0050536589697003365\n",
      "Step 88 (1968); Episode 49/100; Loss: 0.0040423087775707245\n",
      "Step 89 (1969); Episode 49/100; Loss: 0.02220454066991806\n",
      "Step 90 (1970); Episode 49/100; Loss: 0.031669918447732925\n",
      "Step 91 (1971); Episode 49/100; Loss: 0.05443252623081207\n",
      "Step 92 (1972); Episode 49/100; Loss: 0.028481589630246162\n",
      "Step 93 (1973); Episode 49/100; Loss: 0.0017927601002156734\n",
      "Step 94 (1974); Episode 49/100; Loss: 0.1491716504096985\n",
      "Step 95 (1975); Episode 49/100; Loss: 0.12399665266275406\n",
      "Step 96 (1976); Episode 49/100; Loss: 0.039531491696834564\n",
      "Step 97 (1977); Episode 49/100; Loss: 0.04507837072014809\n",
      "Step 98 (1978); Episode 49/100; Loss: 0.032793689519166946\n",
      "Step 99 (1979); Episode 49/100; Loss: 0.03500603139400482\n",
      "Step 100 (1980); Episode 49/100; Loss: 0.17253823578357697\n",
      "Step 101 (1981); Episode 49/100; Loss: 0.005552411545068026\n",
      "Step 102 (1982); Episode 49/100; Loss: 0.003415363375097513\n",
      "Step 103 (1983); Episode 49/100; Loss: 0.12053834646940231\n",
      "Step 104 (1984); Episode 49/100; Loss: 0.05341702327132225\n",
      "Step 105 (1985); Episode 49/100; Loss: 0.10745418071746826\n",
      "Step 106 (1986); Episode 49/100; Loss: 0.2177572399377823\n",
      "Step 107 (1987); Episode 49/100; Loss: 0.005908701103180647\n",
      "Step 108 (1988); Episode 49/100; Loss: 0.07990282028913498\n",
      "Step 109 (1989); Episode 49/100; Loss: 0.0977637991309166\n",
      "Step 110 (1990); Episode 49/100; Loss: 0.1377355009317398\n",
      "Step 111 (1991); Episode 49/100; Loss: 0.049759987741708755\n",
      "Step 112 (1992); Episode 49/100; Loss: 0.08556924760341644\n",
      "Step 113 (1993); Episode 49/100; Loss: 0.03442772105336189\n",
      "Step 114 (1994); Episode 49/100; Loss: 0.04232744127511978\n",
      "Step 115 (1995); Episode 49/100; Loss: 0.002693549497053027\n",
      "Step 116 (1996); Episode 49/100; Loss: 0.054367322474718094\n",
      "Step 117 (1997); Episode 49/100; Loss: 0.002787044970318675\n",
      "Step 118 (1998); Episode 49/100; Loss: 0.0763380154967308\n",
      "Step 119 (1999); Episode 49/100; Loss: 0.03359318524599075\n",
      "Step 120 (2000); Episode 49/100; Loss: 0.0801636204123497\n",
      "Step 121 (2001); Episode 49/100; Loss: 0.04968396574258804\n",
      "Step 122 (2002); Episode 49/100; Loss: 0.004398118704557419\n",
      "Step 123 (2003); Episode 49/100; Loss: 0.170480877161026\n",
      "Step 124 (2004); Episode 49/100; Loss: 0.05410226806998253\n",
      "Step 125 (2005); Episode 49/100; Loss: 0.0615447536110878\n",
      "Step 126 (2006); Episode 49/100; Loss: 0.005498554557561874\n",
      "Step 127 (2007); Episode 49/100; Loss: 0.10526665300130844\n",
      "Step 128 (2008); Episode 49/100; Loss: 0.01842147298157215\n",
      "Step 129 (2009); Episode 49/100; Loss: 0.1030205637216568\n",
      "Step 130 (2010); Episode 49/100; Loss: 0.10995233803987503\n",
      "Step 131 (2011); Episode 49/100; Loss: 0.04110690951347351\n",
      "Step 132 (2012); Episode 49/100; Loss: 0.17065280675888062\n",
      "Step 133 (2013); Episode 49/100; Loss: 0.0528508685529232\n",
      "Step 134 (2014); Episode 49/100; Loss: 0.08683136105537415\n",
      "Step 135 (2015); Episode 49/100; Loss: 0.09796777367591858\n",
      "Step 136 (2016); Episode 49/100; Loss: 0.061010394245386124\n",
      "Step 137 (2017); Episode 49/100; Loss: 0.17152610421180725\n",
      "Step 138 (2018); Episode 49/100; Loss: 0.06937626749277115\n",
      "Step 139 (2019); Episode 49/100; Loss: 0.006281015928834677\n",
      "Step 140 (2020); Episode 49/100; Loss: 0.054571155458688736\n",
      "Step 141 (2021); Episode 49/100; Loss: 0.023042429238557816\n",
      "Step 142 (2022); Episode 49/100; Loss: 0.007173452991992235\n",
      "Step 143 (2023); Episode 49/100; Loss: 0.14277851581573486\n",
      "Step 144 (2024); Episode 49/100; Loss: 0.07324906438589096\n",
      "Step 145 (2025); Episode 49/100; Loss: 0.14132769405841827\n",
      "Step 146 (2026); Episode 49/100; Loss: 0.04168631136417389\n",
      "Step 147 (2027); Episode 49/100; Loss: 0.1094130352139473\n",
      "Step 148 (2028); Episode 49/100; Loss: 0.1552521288394928\n",
      "Step 149 (2029); Episode 49/100; Loss: 0.076816625893116\n",
      "Step 150 (2030); Episode 49/100; Loss: 0.0028617477510124445\n",
      "Step 151 (2031); Episode 49/100; Loss: 0.004154664929956198\n",
      "Step 152 (2032); Episode 49/100; Loss: 0.03725774213671684\n",
      "Step 153 (2033); Episode 49/100; Loss: 0.004510470665991306\n",
      "Step 154 (2034); Episode 49/100; Loss: 0.0651773139834404\n",
      "Step 155 (2035); Episode 49/100; Loss: 0.03848548233509064\n",
      "Step 156 (2036); Episode 49/100; Loss: 0.08709073066711426\n",
      "Step 157 (2037); Episode 49/100; Loss: 0.005184239242225885\n",
      "Step 158 (2038); Episode 49/100; Loss: 0.07042902708053589\n",
      "Step 159 (2039); Episode 49/100; Loss: 0.02332502044737339\n",
      "Step 160 (2040); Episode 49/100; Loss: 0.08579592406749725\n",
      "Step 161 (2041); Episode 49/100; Loss: 0.12404614686965942\n",
      "Step 162 (2042); Episode 49/100; Loss: 0.005464865826070309\n",
      "Step 163 (2043); Episode 49/100; Loss: 0.041581202298402786\n",
      "Step 164 (2044); Episode 49/100; Loss: 0.006825943011790514\n",
      "Step 165 (2045); Episode 49/100; Loss: 0.07675471156835556\n",
      "Step 166 (2046); Episode 49/100; Loss: 0.005229645408689976\n",
      "Step 167 (2047); Episode 49/100; Loss: 0.004280854947865009\n",
      "Step 168 (2048); Episode 49/100; Loss: 0.022281434386968613\n",
      "Step 169 (2049); Episode 49/100; Loss: 0.003987420350313187\n",
      "Step 170 (2050); Episode 49/100; Loss: 0.08869016170501709\n",
      "Step 171 (2051); Episode 49/100; Loss: 0.06549115478992462\n",
      "Step 172 (2052); Episode 49/100; Loss: 0.0026195368263870478\n",
      "Step 173 (2053); Episode 49/100; Loss: 0.019199149683117867\n",
      "Step 174 (2054); Episode 49/100; Loss: 0.20462319254875183\n",
      "Step 175 (2055); Episode 49/100; Loss: 0.085446298122406\n",
      "Step 176 (2056); Episode 49/100; Loss: 0.03071032091975212\n",
      "Step 177 (2057); Episode 49/100; Loss: 0.11269578337669373\n",
      "Step 178 (2058); Episode 49/100; Loss: 0.004403094761073589\n",
      "Step 179 (2059); Episode 49/100; Loss: 0.002757958136498928\n",
      "Step 180 (2060); Episode 49/100; Loss: 0.032845646142959595\n",
      "Step 181 (2061); Episode 49/100; Loss: 0.08262763172388077\n",
      "Step 182 (2062); Episode 49/100; Loss: 0.03446296975016594\n",
      "Step 183 (2063); Episode 49/100; Loss: 0.031823430210351944\n",
      "Step 184 (2064); Episode 49/100; Loss: 0.05964989960193634\n",
      "Step 185 (2065); Episode 49/100; Loss: 0.0945977047085762\n",
      "Step 186 (2066); Episode 49/100; Loss: 0.052557989954948425\n",
      "Step 187 (2067); Episode 49/100; Loss: 0.030349070206284523\n",
      "Step 188 (2068); Episode 49/100; Loss: 0.08335533738136292\n",
      "Step 189 (2069); Episode 49/100; Loss: 0.009537842124700546\n",
      "Step 190 (2070); Episode 49/100; Loss: 0.004558180924504995\n",
      "Step 191 (2071); Episode 49/100; Loss: 0.045891787856817245\n",
      "Step 192 (2072); Episode 49/100; Loss: 0.03124622441828251\n",
      "Step 193 (2073); Episode 49/100; Loss: 0.005462829023599625\n",
      "Step 194 (2074); Episode 49/100; Loss: 0.00457355473190546\n",
      "Step 195 (2075); Episode 49/100; Loss: 0.07732849568128586\n",
      "Step 196 (2076); Episode 49/100; Loss: 0.0023834300227463245\n",
      "Step 197 (2077); Episode 49/100; Loss: 0.010056908242404461\n",
      "Step 198 (2078); Episode 49/100; Loss: 0.004251494538038969\n",
      "Step 199 (2079); Episode 49/100; Loss: 0.0719389021396637\n",
      "Step 0 (2080); Episode 50/100; Loss: 0.04714089632034302\n",
      "Step 1 (2081); Episode 50/100; Loss: 0.009238891303539276\n",
      "Step 2 (2082); Episode 50/100; Loss: 0.03141231834888458\n",
      "Step 3 (2083); Episode 50/100; Loss: 0.0031403640750795603\n",
      "Step 4 (2084); Episode 50/100; Loss: 0.031945228576660156\n",
      "Step 5 (2085); Episode 50/100; Loss: 0.05457055941224098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 (2086); Episode 50/100; Loss: 0.14125697314739227\n",
      "Step 7 (2087); Episode 50/100; Loss: 0.04781680926680565\n",
      "Step 8 (2088); Episode 50/100; Loss: 0.07905017584562302\n",
      "Step 9 (2089); Episode 50/100; Loss: 0.06271355599164963\n",
      "Step 10 (2090); Episode 50/100; Loss: 0.1099969819188118\n",
      "Step 11 (2091); Episode 50/100; Loss: 0.11724671721458435\n",
      "Step 12 (2092); Episode 50/100; Loss: 0.03113371506333351\n",
      "Step 13 (2093); Episode 50/100; Loss: 0.04788554832339287\n",
      "Step 14 (2094); Episode 50/100; Loss: 0.034105390310287476\n",
      "Step 15 (2095); Episode 50/100; Loss: 0.05400800704956055\n",
      "Step 16 (2096); Episode 50/100; Loss: 0.04732024297118187\n",
      "Step 17 (2097); Episode 50/100; Loss: 0.024902276694774628\n",
      "Step 18 (2098); Episode 50/100; Loss: 0.050460975617170334\n",
      "Step 19 (2099); Episode 50/100; Loss: 0.05560257285833359\n",
      "Step 20 (2100); Episode 50/100; Loss: 0.06316166371107101\n",
      "Step 21 (2101); Episode 50/100; Loss: 0.00568693270906806\n",
      "Step 22 (2102); Episode 50/100; Loss: 0.05194750055670738\n",
      "Step 23 (2103); Episode 50/100; Loss: 0.034422438591718674\n",
      "Step 24 (2104); Episode 50/100; Loss: 0.002641673432663083\n",
      "Step 25 (2105); Episode 50/100; Loss: 0.03639444336295128\n",
      "Step 26 (2106); Episode 50/100; Loss: 0.08162325620651245\n",
      "Step 27 (2107); Episode 50/100; Loss: 0.07784108072519302\n",
      "Step 28 (2108); Episode 50/100; Loss: 0.08670540899038315\n",
      "Step 29 (2109); Episode 50/100; Loss: 0.00569085543975234\n",
      "Step 30 (2110); Episode 50/100; Loss: 0.0035066825803369284\n",
      "Step 31 (2111); Episode 50/100; Loss: 0.003772357478737831\n",
      "Step 32 (2112); Episode 50/100; Loss: 0.003133331658318639\n",
      "Step 33 (2113); Episode 50/100; Loss: 0.0473373718559742\n",
      "Step 34 (2114); Episode 50/100; Loss: 0.10879278182983398\n",
      "Step 35 (2115); Episode 50/100; Loss: 0.18901574611663818\n",
      "Step 36 (2116); Episode 50/100; Loss: 0.004461590200662613\n",
      "Step 37 (2117); Episode 50/100; Loss: 0.07749293744564056\n",
      "Step 38 (2118); Episode 50/100; Loss: 0.09638819843530655\n",
      "Step 39 (2119); Episode 50/100; Loss: 0.02922792173922062\n",
      "Step 40 (2120); Episode 50/100; Loss: 0.001012440538033843\n",
      "Step 41 (2121); Episode 50/100; Loss: 0.05065339431166649\n",
      "Step 42 (2122); Episode 50/100; Loss: 0.03363919258117676\n",
      "Step 43 (2123); Episode 50/100; Loss: 0.1517808586359024\n",
      "Step 44 (2124); Episode 50/100; Loss: 0.05063151195645332\n",
      "Step 45 (2125); Episode 50/100; Loss: 0.11706537753343582\n",
      "Step 46 (2126); Episode 50/100; Loss: 0.09238280355930328\n",
      "Step 47 (2127); Episode 50/100; Loss: 0.09836098551750183\n",
      "Step 48 (2128); Episode 50/100; Loss: 0.06244075670838356\n",
      "Step 49 (2129); Episode 50/100; Loss: 0.07480333745479584\n",
      "Step 50 (2130); Episode 50/100; Loss: 0.0020393673330545425\n",
      "Step 51 (2131); Episode 50/100; Loss: 0.0400577187538147\n",
      "Step 52 (2132); Episode 50/100; Loss: 0.004783050622791052\n",
      "Step 53 (2133); Episode 50/100; Loss: 0.1209292933344841\n",
      "Step 54 (2134); Episode 50/100; Loss: 0.0028282799758017063\n",
      "Step 55 (2135); Episode 50/100; Loss: 0.10209310054779053\n",
      "Step 56 (2136); Episode 50/100; Loss: 0.002422627294436097\n",
      "Step 57 (2137); Episode 50/100; Loss: 0.06046680361032486\n",
      "Step 58 (2138); Episode 50/100; Loss: 0.1059180423617363\n",
      "Step 59 (2139); Episode 50/100; Loss: 0.0024786361027508974\n",
      "Step 60 (2140); Episode 50/100; Loss: 0.04800372198224068\n",
      "Step 61 (2141); Episode 50/100; Loss: 0.08465002477169037\n",
      "Step 62 (2142); Episode 50/100; Loss: 0.003172491677105427\n",
      "Step 63 (2143); Episode 50/100; Loss: 0.03186259791254997\n",
      "Step 64 (2144); Episode 50/100; Loss: 0.03611444681882858\n",
      "Step 65 (2145); Episode 50/100; Loss: 0.05144606903195381\n",
      "Step 66 (2146); Episode 50/100; Loss: 0.06756918877363205\n",
      "Step 67 (2147); Episode 50/100; Loss: 0.05014168471097946\n",
      "Step 68 (2148); Episode 50/100; Loss: 0.005043198820203543\n",
      "Step 69 (2149); Episode 50/100; Loss: 0.003382171504199505\n",
      "Step 70 (2150); Episode 50/100; Loss: 0.006116748787462711\n",
      "Step 71 (2151); Episode 50/100; Loss: 0.025018244981765747\n",
      "Step 72 (2152); Episode 50/100; Loss: 0.139505073428154\n",
      "Step 73 (2153); Episode 50/100; Loss: 0.03228062018752098\n",
      "Step 74 (2154); Episode 50/100; Loss: 0.16927799582481384\n",
      "Step 75 (2155); Episode 50/100; Loss: 0.14742420613765717\n",
      "Step 76 (2156); Episode 50/100; Loss: 0.048045892268419266\n",
      "Step 77 (2157); Episode 50/100; Loss: 0.005741759203374386\n",
      "Step 78 (2158); Episode 50/100; Loss: 0.0834239199757576\n",
      "Step 79 (2159); Episode 50/100; Loss: 0.052132755517959595\n",
      "Step 80 (2160); Episode 50/100; Loss: 0.08845183998346329\n",
      "Step 81 (2161); Episode 50/100; Loss: 0.042734041810035706\n",
      "Step 82 (2162); Episode 50/100; Loss: 0.0013715167297050357\n",
      "Step 83 (2163); Episode 50/100; Loss: 0.05033077672123909\n",
      "Step 84 (2164); Episode 50/100; Loss: 0.002327370923012495\n",
      "Step 85 (2165); Episode 50/100; Loss: 0.0026966447476297617\n",
      "Step 86 (2166); Episode 50/100; Loss: 0.003933152183890343\n",
      "Step 87 (2167); Episode 50/100; Loss: 0.003799541387706995\n",
      "Step 88 (2168); Episode 50/100; Loss: 0.09903696179389954\n",
      "Step 89 (2169); Episode 50/100; Loss: 0.0014066948788240552\n",
      "Step 90 (2170); Episode 50/100; Loss: 0.24531200528144836\n",
      "Step 91 (2171); Episode 50/100; Loss: 0.003210630966350436\n",
      "Step 92 (2172); Episode 50/100; Loss: 0.04007377475500107\n",
      "Step 93 (2173); Episode 50/100; Loss: 0.028436806052923203\n",
      "Step 94 (2174); Episode 50/100; Loss: 0.10174611955881119\n",
      "Step 95 (2175); Episode 50/100; Loss: 0.004021850880235434\n",
      "Step 96 (2176); Episode 50/100; Loss: 0.005764370784163475\n",
      "Step 97 (2177); Episode 50/100; Loss: 0.06745433807373047\n",
      "Step 98 (2178); Episode 50/100; Loss: 0.07536175847053528\n",
      "Step 99 (2179); Episode 50/100; Loss: 0.03080648183822632\n",
      "Step 100 (2180); Episode 50/100; Loss: 0.07709842920303345\n",
      "Step 101 (2181); Episode 50/100; Loss: 0.053391266614198685\n",
      "Step 102 (2182); Episode 50/100; Loss: 0.05123152211308479\n",
      "Step 103 (2183); Episode 50/100; Loss: 0.06572267413139343\n",
      "Step 104 (2184); Episode 50/100; Loss: 0.06312596797943115\n",
      "Step 105 (2185); Episode 50/100; Loss: 0.0038153657224029303\n",
      "Step 106 (2186); Episode 50/100; Loss: 0.049065444618463516\n",
      "Step 107 (2187); Episode 50/100; Loss: 0.032537225633859634\n",
      "Step 108 (2188); Episode 50/100; Loss: 0.05427877977490425\n",
      "Step 109 (2189); Episode 50/100; Loss: 0.0033199377357959747\n",
      "Step 110 (2190); Episode 50/100; Loss: 0.03437170758843422\n",
      "Step 111 (2191); Episode 50/100; Loss: 0.028536079451441765\n",
      "Step 112 (2192); Episode 50/100; Loss: 0.04875994473695755\n",
      "Step 113 (2193); Episode 50/100; Loss: 0.0034968513064086437\n",
      "Step 114 (2194); Episode 50/100; Loss: 0.03641674295067787\n",
      "Step 115 (2195); Episode 50/100; Loss: 0.004459059797227383\n",
      "Step 116 (2196); Episode 50/100; Loss: 0.001286758342757821\n",
      "Step 117 (2197); Episode 50/100; Loss: 0.06603499501943588\n",
      "Step 118 (2198); Episode 50/100; Loss: 0.04837553948163986\n",
      "Step 119 (2199); Episode 50/100; Loss: 0.03743577003479004\n",
      "Step 120 (2200); Episode 50/100; Loss: 0.13877134025096893\n",
      "Step 121 (2201); Episode 50/100; Loss: 0.13256590068340302\n",
      "Step 122 (2202); Episode 50/100; Loss: 0.11375181376934052\n",
      "Step 123 (2203); Episode 50/100; Loss: 0.08190882951021194\n",
      "Step 124 (2204); Episode 50/100; Loss: 0.02601267583668232\n",
      "Step 125 (2205); Episode 50/100; Loss: 0.03509277105331421\n",
      "Step 126 (2206); Episode 50/100; Loss: 0.02761143259704113\n",
      "Step 127 (2207); Episode 50/100; Loss: 0.007071035914123058\n",
      "Step 128 (2208); Episode 50/100; Loss: 0.02525438368320465\n",
      "Step 129 (2209); Episode 50/100; Loss: 0.008085429668426514\n",
      "Step 0 (2210); Episode 51/100; Loss: 0.08366484940052032\n",
      "Step 1 (2211); Episode 51/100; Loss: 0.049511365592479706\n",
      "Step 2 (2212); Episode 51/100; Loss: 0.06898738443851471\n",
      "Step 3 (2213); Episode 51/100; Loss: 0.08076664805412292\n",
      "Step 4 (2214); Episode 51/100; Loss: 0.03467786684632301\n",
      "Step 5 (2215); Episode 51/100; Loss: 0.008243727497756481\n",
      "Step 6 (2216); Episode 51/100; Loss: 0.06112980470061302\n",
      "Step 7 (2217); Episode 51/100; Loss: 0.18694068491458893\n",
      "Step 8 (2218); Episode 51/100; Loss: 0.07553035765886307\n",
      "Step 9 (2219); Episode 51/100; Loss: 0.04282306134700775\n",
      "Step 10 (2220); Episode 51/100; Loss: 0.10933388024568558\n",
      "Step 11 (2221); Episode 51/100; Loss: 0.04771912097930908\n",
      "Step 12 (2222); Episode 51/100; Loss: 0.0425017885863781\n",
      "Step 13 (2223); Episode 51/100; Loss: 0.08990427851676941\n",
      "Step 14 (2224); Episode 51/100; Loss: 0.019632771611213684\n",
      "Step 15 (2225); Episode 51/100; Loss: 0.039537061005830765\n",
      "Step 16 (2226); Episode 51/100; Loss: 0.053083084523677826\n",
      "Step 17 (2227); Episode 51/100; Loss: 0.023309865966439247\n",
      "Step 18 (2228); Episode 51/100; Loss: 0.04146486520767212\n",
      "Step 19 (2229); Episode 51/100; Loss: 0.08041654527187347\n",
      "Step 20 (2230); Episode 51/100; Loss: 0.04761989414691925\n",
      "Step 21 (2231); Episode 51/100; Loss: 0.003400361631065607\n",
      "Step 22 (2232); Episode 51/100; Loss: 0.03890170529484749\n",
      "Step 23 (2233); Episode 51/100; Loss: 0.003188525792211294\n",
      "Step 24 (2234); Episode 51/100; Loss: 0.04026181623339653\n",
      "Step 25 (2235); Episode 51/100; Loss: 0.04574323073029518\n",
      "Step 26 (2236); Episode 51/100; Loss: 0.062413159757852554\n",
      "Step 27 (2237); Episode 51/100; Loss: 0.014714915305376053\n",
      "Step 28 (2238); Episode 51/100; Loss: 0.0446651317179203\n",
      "Step 29 (2239); Episode 51/100; Loss: 0.05289071798324585\n",
      "Step 30 (2240); Episode 51/100; Loss: 0.06217389926314354\n",
      "Step 31 (2241); Episode 51/100; Loss: 0.060496460646390915\n",
      "Step 32 (2242); Episode 51/100; Loss: 0.11971938610076904\n",
      "Step 33 (2243); Episode 51/100; Loss: 0.1407427042722702\n",
      "Step 34 (2244); Episode 51/100; Loss: 0.0017109628533944488\n",
      "Step 35 (2245); Episode 51/100; Loss: 0.12434445321559906\n",
      "Step 36 (2246); Episode 51/100; Loss: 0.08883395791053772\n",
      "Step 37 (2247); Episode 51/100; Loss: 0.004270627163350582\n",
      "Step 38 (2248); Episode 51/100; Loss: 0.1412459909915924\n",
      "Step 39 (2249); Episode 51/100; Loss: 0.04821814224123955\n",
      "Step 40 (2250); Episode 51/100; Loss: 0.0020458537619560957\n",
      "Step 41 (2251); Episode 51/100; Loss: 0.044873494654893875\n",
      "Step 42 (2252); Episode 51/100; Loss: 0.056719180196523666\n",
      "Step 43 (2253); Episode 51/100; Loss: 0.052764929831027985\n",
      "Step 44 (2254); Episode 51/100; Loss: 0.08466414362192154\n",
      "Step 45 (2255); Episode 51/100; Loss: 0.07239358127117157\n",
      "Step 46 (2256); Episode 51/100; Loss: 0.003186845453456044\n",
      "Step 47 (2257); Episode 51/100; Loss: 0.0023284158669412136\n",
      "Step 48 (2258); Episode 51/100; Loss: 0.09800569713115692\n",
      "Step 49 (2259); Episode 51/100; Loss: 0.001963954418897629\n",
      "Step 50 (2260); Episode 51/100; Loss: 0.032315511256456375\n",
      "Step 51 (2261); Episode 51/100; Loss: 0.0029626437462866306\n",
      "Step 52 (2262); Episode 51/100; Loss: 0.002302489709109068\n",
      "Step 53 (2263); Episode 51/100; Loss: 0.06451261788606644\n",
      "Step 54 (2264); Episode 51/100; Loss: 0.1267712414264679\n",
      "Step 55 (2265); Episode 51/100; Loss: 0.08755060285329819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56 (2266); Episode 51/100; Loss: 0.0034863664768636227\n",
      "Step 57 (2267); Episode 51/100; Loss: 0.04623878747224808\n",
      "Step 58 (2268); Episode 51/100; Loss: 0.04111144691705704\n",
      "Step 59 (2269); Episode 51/100; Loss: 0.11795167624950409\n",
      "Step 60 (2270); Episode 51/100; Loss: 0.07739996165037155\n",
      "Step 61 (2271); Episode 51/100; Loss: 0.08808251470327377\n",
      "Step 62 (2272); Episode 51/100; Loss: 0.07709842175245285\n",
      "Step 63 (2273); Episode 51/100; Loss: 0.04883188754320145\n",
      "Step 64 (2274); Episode 51/100; Loss: 0.07304243743419647\n",
      "Step 65 (2275); Episode 51/100; Loss: 0.09596864879131317\n",
      "Step 66 (2276); Episode 51/100; Loss: 0.07789519429206848\n",
      "Step 67 (2277); Episode 51/100; Loss: 0.054569970816373825\n",
      "Step 68 (2278); Episode 51/100; Loss: 0.08577388525009155\n",
      "Step 69 (2279); Episode 51/100; Loss: 0.042946357280015945\n",
      "Step 70 (2280); Episode 51/100; Loss: 0.0023553052451461554\n",
      "Step 71 (2281); Episode 51/100; Loss: 0.06830891221761703\n",
      "Step 72 (2282); Episode 51/100; Loss: 0.07953924685716629\n",
      "Step 73 (2283); Episode 51/100; Loss: 0.02179638482630253\n",
      "Step 74 (2284); Episode 51/100; Loss: 0.00678055128082633\n",
      "Step 75 (2285); Episode 51/100; Loss: 0.0024056429974734783\n",
      "Step 76 (2286); Episode 51/100; Loss: 0.11701980233192444\n",
      "Step 77 (2287); Episode 51/100; Loss: 0.0024224778171628714\n",
      "Step 78 (2288); Episode 51/100; Loss: 0.03698258474469185\n",
      "Step 79 (2289); Episode 51/100; Loss: 0.03749280050396919\n",
      "Step 80 (2290); Episode 51/100; Loss: 0.06070366129279137\n",
      "Step 81 (2291); Episode 51/100; Loss: 0.0398191474378109\n",
      "Step 82 (2292); Episode 51/100; Loss: 0.08775223791599274\n",
      "Step 83 (2293); Episode 51/100; Loss: 0.023118097335100174\n",
      "Step 84 (2294); Episode 51/100; Loss: 0.07081054896116257\n",
      "Step 85 (2295); Episode 51/100; Loss: 0.004995562601834536\n",
      "Step 86 (2296); Episode 51/100; Loss: 0.019807450473308563\n",
      "Step 87 (2297); Episode 51/100; Loss: 0.07165834307670593\n",
      "Step 88 (2298); Episode 51/100; Loss: 0.005162147805094719\n",
      "Step 89 (2299); Episode 51/100; Loss: 0.07078469544649124\n",
      "Step 90 (2300); Episode 51/100; Loss: 0.05845041945576668\n",
      "Step 91 (2301); Episode 51/100; Loss: 0.07175497710704803\n",
      "Step 92 (2302); Episode 51/100; Loss: 0.003259699558839202\n",
      "Step 93 (2303); Episode 51/100; Loss: 0.06670314073562622\n",
      "Step 94 (2304); Episode 51/100; Loss: 0.006012813188135624\n",
      "Step 95 (2305); Episode 51/100; Loss: 0.059442128986120224\n",
      "Step 96 (2306); Episode 51/100; Loss: 0.07902704924345016\n",
      "Step 97 (2307); Episode 51/100; Loss: 0.04860440641641617\n",
      "Step 98 (2308); Episode 51/100; Loss: 0.007477435749024153\n",
      "Step 99 (2309); Episode 51/100; Loss: 0.05069741979241371\n",
      "Step 100 (2310); Episode 51/100; Loss: 0.16394324600696564\n",
      "Step 101 (2311); Episode 51/100; Loss: 0.08168527483940125\n",
      "Step 102 (2312); Episode 51/100; Loss: 0.11279069632291794\n",
      "Step 103 (2313); Episode 51/100; Loss: 0.029807409271597862\n",
      "Step 104 (2314); Episode 51/100; Loss: 0.0705358162522316\n",
      "Step 105 (2315); Episode 51/100; Loss: 0.11315750330686569\n",
      "Step 106 (2316); Episode 51/100; Loss: 0.008141305297613144\n",
      "Step 107 (2317); Episode 51/100; Loss: 0.07029519230127335\n",
      "Step 108 (2318); Episode 51/100; Loss: 0.0036862045526504517\n",
      "Step 109 (2319); Episode 51/100; Loss: 0.006580670829862356\n",
      "Step 110 (2320); Episode 51/100; Loss: 0.13554047048091888\n",
      "Step 111 (2321); Episode 51/100; Loss: 0.026441315189003944\n",
      "Step 112 (2322); Episode 51/100; Loss: 0.16295795142650604\n",
      "Step 113 (2323); Episode 51/100; Loss: 0.0105294743552804\n",
      "Step 114 (2324); Episode 51/100; Loss: 0.09352385997772217\n",
      "Step 115 (2325); Episode 51/100; Loss: 0.08207440376281738\n",
      "Step 116 (2326); Episode 51/100; Loss: 0.02543461322784424\n",
      "Step 117 (2327); Episode 51/100; Loss: 0.0031985370442271233\n",
      "Step 118 (2328); Episode 51/100; Loss: 0.0334649384021759\n",
      "Step 119 (2329); Episode 51/100; Loss: 0.030391355976462364\n",
      "Step 120 (2330); Episode 51/100; Loss: 0.034378498792648315\n",
      "Step 121 (2331); Episode 51/100; Loss: 0.031031137332320213\n",
      "Step 122 (2332); Episode 51/100; Loss: 0.10640979558229446\n",
      "Step 123 (2333); Episode 51/100; Loss: 0.027095956727862358\n",
      "Step 124 (2334); Episode 51/100; Loss: 0.003834818722680211\n",
      "Step 125 (2335); Episode 51/100; Loss: 0.022660627961158752\n",
      "Step 126 (2336); Episode 51/100; Loss: 0.0984119400382042\n",
      "Step 127 (2337); Episode 51/100; Loss: 0.006161114200949669\n",
      "Step 128 (2338); Episode 51/100; Loss: 0.0786021277308464\n",
      "Step 129 (2339); Episode 51/100; Loss: 0.00232821237295866\n",
      "Step 130 (2340); Episode 51/100; Loss: 0.04222451522946358\n",
      "Step 131 (2341); Episode 51/100; Loss: 0.10848823189735413\n",
      "Step 132 (2342); Episode 51/100; Loss: 0.02978469245135784\n",
      "Step 133 (2343); Episode 51/100; Loss: 0.08250021934509277\n",
      "Step 134 (2344); Episode 51/100; Loss: 0.07514951378107071\n",
      "Step 135 (2345); Episode 51/100; Loss: 0.06678253412246704\n",
      "Step 136 (2346); Episode 51/100; Loss: 0.0025489414110779762\n",
      "Step 137 (2347); Episode 51/100; Loss: 0.0939636081457138\n",
      "Step 138 (2348); Episode 51/100; Loss: 0.026932014152407646\n",
      "Step 139 (2349); Episode 51/100; Loss: 0.0034053046256303787\n",
      "Step 140 (2350); Episode 51/100; Loss: 0.025827089324593544\n",
      "Step 141 (2351); Episode 51/100; Loss: 0.05280894413590431\n",
      "Step 142 (2352); Episode 51/100; Loss: 0.002459887880831957\n",
      "Step 143 (2353); Episode 51/100; Loss: 0.08086927235126495\n",
      "Step 144 (2354); Episode 51/100; Loss: 0.046139925718307495\n",
      "Step 145 (2355); Episode 51/100; Loss: 0.0024551949463784695\n",
      "Step 146 (2356); Episode 51/100; Loss: 0.003103884169831872\n",
      "Step 147 (2357); Episode 51/100; Loss: 0.03590463101863861\n",
      "Step 148 (2358); Episode 51/100; Loss: 0.0031772227957844734\n",
      "Step 149 (2359); Episode 51/100; Loss: 0.10705190896987915\n",
      "Step 150 (2360); Episode 51/100; Loss: 0.0015599431935697794\n",
      "Step 0 (2361); Episode 52/100; Loss: 0.05286994203925133\n",
      "Step 1 (2362); Episode 52/100; Loss: 0.07527992129325867\n",
      "Step 2 (2363); Episode 52/100; Loss: 0.09582991898059845\n",
      "Step 3 (2364); Episode 52/100; Loss: 0.0031400551088154316\n",
      "Step 4 (2365); Episode 52/100; Loss: 0.0015310777816921473\n",
      "Step 5 (2366); Episode 52/100; Loss: 0.05195359140634537\n",
      "Step 6 (2367); Episode 52/100; Loss: 0.05740303546190262\n",
      "Step 7 (2368); Episode 52/100; Loss: 0.024993211030960083\n",
      "Step 8 (2369); Episode 52/100; Loss: 0.03814530000090599\n",
      "Step 9 (2370); Episode 52/100; Loss: 0.014322257600724697\n",
      "Step 10 (2371); Episode 52/100; Loss: 0.050393614917993546\n",
      "Step 11 (2372); Episode 52/100; Loss: 0.09415742754936218\n",
      "Step 12 (2373); Episode 52/100; Loss: 0.04010873660445213\n",
      "Step 13 (2374); Episode 52/100; Loss: 0.04841265454888344\n",
      "Step 14 (2375); Episode 52/100; Loss: 0.004094068892300129\n",
      "Step 15 (2376); Episode 52/100; Loss: 0.022131433710455894\n",
      "Step 16 (2377); Episode 52/100; Loss: 0.0517076775431633\n",
      "Step 17 (2378); Episode 52/100; Loss: 0.005021545570343733\n",
      "Step 18 (2379); Episode 52/100; Loss: 0.007342757657170296\n",
      "Step 19 (2380); Episode 52/100; Loss: 0.08850473910570145\n",
      "Step 20 (2381); Episode 52/100; Loss: 0.027138439938426018\n",
      "Step 21 (2382); Episode 52/100; Loss: 0.05165527015924454\n",
      "Step 22 (2383); Episode 52/100; Loss: 0.002223901217803359\n",
      "Step 23 (2384); Episode 52/100; Loss: 0.0035014215391129255\n",
      "Step 24 (2385); Episode 52/100; Loss: 0.039449553936719894\n",
      "Step 25 (2386); Episode 52/100; Loss: 0.03997785970568657\n",
      "Step 26 (2387); Episode 52/100; Loss: 0.08444131910800934\n",
      "Step 27 (2388); Episode 52/100; Loss: 0.003740274580195546\n",
      "Step 28 (2389); Episode 52/100; Loss: 0.002451410284265876\n",
      "Step 29 (2390); Episode 52/100; Loss: 0.0013340195873752236\n",
      "Step 30 (2391); Episode 52/100; Loss: 0.11979848146438599\n",
      "Step 31 (2392); Episode 52/100; Loss: 0.004618555307388306\n",
      "Step 32 (2393); Episode 52/100; Loss: 0.037199921905994415\n",
      "Step 33 (2394); Episode 52/100; Loss: 0.031511805951595306\n",
      "Step 34 (2395); Episode 52/100; Loss: 0.0042220610193908215\n",
      "Step 35 (2396); Episode 52/100; Loss: 0.06689012050628662\n",
      "Step 36 (2397); Episode 52/100; Loss: 0.08378245681524277\n",
      "Step 37 (2398); Episode 52/100; Loss: 0.0039286380633711815\n",
      "Step 38 (2399); Episode 52/100; Loss: 0.037023480981588364\n",
      "Step 39 (2400); Episode 52/100; Loss: 0.0029397597536444664\n",
      "Step 40 (2401); Episode 52/100; Loss: 0.022701669484376907\n",
      "Step 41 (2402); Episode 52/100; Loss: 0.08324725180864334\n",
      "Step 42 (2403); Episode 52/100; Loss: 0.0015450411010533571\n",
      "Step 43 (2404); Episode 52/100; Loss: 0.06412456184625626\n",
      "Step 44 (2405); Episode 52/100; Loss: 0.048336416482925415\n",
      "Step 45 (2406); Episode 52/100; Loss: 0.1325378268957138\n",
      "Step 46 (2407); Episode 52/100; Loss: 0.05090462788939476\n",
      "Step 47 (2408); Episode 52/100; Loss: 0.054365456104278564\n",
      "Step 48 (2409); Episode 52/100; Loss: 0.0026328053791075945\n",
      "Step 49 (2410); Episode 52/100; Loss: 0.0029214920941740274\n",
      "Step 50 (2411); Episode 52/100; Loss: 0.07724470645189285\n",
      "Step 51 (2412); Episode 52/100; Loss: 0.07933426648378372\n",
      "Step 52 (2413); Episode 52/100; Loss: 0.024807095527648926\n",
      "Step 53 (2414); Episode 52/100; Loss: 0.1294569969177246\n",
      "Step 54 (2415); Episode 52/100; Loss: 0.0763077512383461\n",
      "Step 55 (2416); Episode 52/100; Loss: 0.05972839146852493\n",
      "Step 56 (2417); Episode 52/100; Loss: 0.04182898625731468\n",
      "Step 57 (2418); Episode 52/100; Loss: 0.004597899038344622\n",
      "Step 58 (2419); Episode 52/100; Loss: 0.03234858438372612\n",
      "Step 59 (2420); Episode 52/100; Loss: 0.004134983289986849\n",
      "Step 60 (2421); Episode 52/100; Loss: 0.023008447140455246\n",
      "Step 61 (2422); Episode 52/100; Loss: 0.007268004585057497\n",
      "Step 62 (2423); Episode 52/100; Loss: 0.06881561130285263\n",
      "Step 63 (2424); Episode 52/100; Loss: 0.06272765249013901\n",
      "Step 64 (2425); Episode 52/100; Loss: 0.03415008261799812\n",
      "Step 65 (2426); Episode 52/100; Loss: 0.051359619945287704\n",
      "Step 66 (2427); Episode 52/100; Loss: 0.042680200189352036\n",
      "Step 67 (2428); Episode 52/100; Loss: 0.04827255383133888\n",
      "Step 68 (2429); Episode 52/100; Loss: 0.009980426169931889\n",
      "Step 69 (2430); Episode 52/100; Loss: 0.07488682121038437\n",
      "Step 70 (2431); Episode 52/100; Loss: 0.0035113163758069277\n",
      "Step 71 (2432); Episode 52/100; Loss: 0.05392639711499214\n",
      "Step 72 (2433); Episode 52/100; Loss: 0.14441660046577454\n",
      "Step 73 (2434); Episode 52/100; Loss: 0.034219205379486084\n",
      "Step 74 (2435); Episode 52/100; Loss: 0.03142916038632393\n",
      "Step 75 (2436); Episode 52/100; Loss: 0.061992451548576355\n",
      "Step 76 (2437); Episode 52/100; Loss: 0.02738327533006668\n",
      "Step 77 (2438); Episode 52/100; Loss: 0.006594088859856129\n",
      "Step 78 (2439); Episode 52/100; Loss: 0.04843754321336746\n",
      "Step 79 (2440); Episode 52/100; Loss: 0.08977311104536057\n",
      "Step 80 (2441); Episode 52/100; Loss: 0.0034997581969946623\n",
      "Step 81 (2442); Episode 52/100; Loss: 0.003590633161365986\n",
      "Step 82 (2443); Episode 52/100; Loss: 0.004492787644267082\n",
      "Step 83 (2444); Episode 52/100; Loss: 0.04748229682445526\n",
      "Step 84 (2445); Episode 52/100; Loss: 0.09445960819721222\n",
      "Step 85 (2446); Episode 52/100; Loss: 0.004968869034200907\n",
      "Step 86 (2447); Episode 52/100; Loss: 0.05023936927318573\n",
      "Step 87 (2448); Episode 52/100; Loss: 0.0038880298379808664\n",
      "Step 88 (2449); Episode 52/100; Loss: 0.11998575925827026\n",
      "Step 89 (2450); Episode 52/100; Loss: 0.0022217952646315098\n",
      "Step 90 (2451); Episode 52/100; Loss: 0.003003031248226762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 91 (2452); Episode 52/100; Loss: 0.039793580770492554\n",
      "Step 92 (2453); Episode 52/100; Loss: 0.004000494256615639\n",
      "Step 93 (2454); Episode 52/100; Loss: 0.10923914611339569\n",
      "Step 94 (2455); Episode 52/100; Loss: 0.06840799003839493\n",
      "Step 95 (2456); Episode 52/100; Loss: 0.09933933615684509\n",
      "Step 96 (2457); Episode 52/100; Loss: 0.04750113934278488\n",
      "Step 97 (2458); Episode 52/100; Loss: 0.04677398130297661\n",
      "Step 98 (2459); Episode 52/100; Loss: 0.032259147614240646\n",
      "Step 99 (2460); Episode 52/100; Loss: 0.07955504953861237\n",
      "Step 100 (2461); Episode 52/100; Loss: 0.05541829764842987\n",
      "Step 101 (2462); Episode 52/100; Loss: 0.049134042114019394\n",
      "Step 102 (2463); Episode 52/100; Loss: 0.0014913523336872458\n",
      "Step 103 (2464); Episode 52/100; Loss: 0.07115406543016434\n",
      "Step 104 (2465); Episode 52/100; Loss: 0.019588567316532135\n",
      "Step 105 (2466); Episode 52/100; Loss: 0.0018280914518982172\n",
      "Step 106 (2467); Episode 52/100; Loss: 0.12229067832231522\n",
      "Step 107 (2468); Episode 52/100; Loss: 0.04963285103440285\n",
      "Step 108 (2469); Episode 52/100; Loss: 0.04389690235257149\n",
      "Step 109 (2470); Episode 52/100; Loss: 0.10964205861091614\n",
      "Step 110 (2471); Episode 52/100; Loss: 0.05359867960214615\n",
      "Step 111 (2472); Episode 52/100; Loss: 0.0502813383936882\n",
      "Step 112 (2473); Episode 52/100; Loss: 0.0034850738011300564\n",
      "Step 113 (2474); Episode 52/100; Loss: 0.06710740178823471\n",
      "Step 114 (2475); Episode 52/100; Loss: 0.06700003892183304\n",
      "Step 115 (2476); Episode 52/100; Loss: 0.0032006113324314356\n",
      "Step 116 (2477); Episode 52/100; Loss: 0.0011295918375253677\n",
      "Step 117 (2478); Episode 52/100; Loss: 0.0025563340168446302\n",
      "Step 118 (2479); Episode 52/100; Loss: 0.09418749809265137\n",
      "Step 119 (2480); Episode 52/100; Loss: 0.049776457250118256\n",
      "Step 120 (2481); Episode 52/100; Loss: 0.0022759963758289814\n",
      "Step 121 (2482); Episode 52/100; Loss: 0.0035318632144480944\n",
      "Step 122 (2483); Episode 52/100; Loss: 0.004104469902813435\n",
      "Step 123 (2484); Episode 52/100; Loss: 0.052742816507816315\n",
      "Step 124 (2485); Episode 52/100; Loss: 0.04496777430176735\n",
      "Step 125 (2486); Episode 52/100; Loss: 0.0025867631193250418\n",
      "Step 126 (2487); Episode 52/100; Loss: 0.08569221943616867\n",
      "Step 127 (2488); Episode 52/100; Loss: 0.0029774231370538473\n",
      "Step 128 (2489); Episode 52/100; Loss: 0.043536435812711716\n",
      "Step 129 (2490); Episode 52/100; Loss: 0.12265610694885254\n",
      "Step 130 (2491); Episode 52/100; Loss: 0.048018839210271835\n",
      "Step 131 (2492); Episode 52/100; Loss: 0.001815872616134584\n",
      "Step 132 (2493); Episode 52/100; Loss: 0.033232323825359344\n",
      "Step 0 (2494); Episode 53/100; Loss: 0.09564698487520218\n",
      "Step 1 (2495); Episode 53/100; Loss: 0.0344398133456707\n",
      "Step 2 (2496); Episode 53/100; Loss: 0.05883815512061119\n",
      "Step 3 (2497); Episode 53/100; Loss: 0.030206071212887764\n",
      "Step 4 (2498); Episode 53/100; Loss: 0.014198907651007175\n",
      "Step 5 (2499); Episode 53/100; Loss: 0.03443455696105957\n",
      "Step 6 (2500); Episode 53/100; Loss: 0.03995050489902496\n",
      "Step 7 (2501); Episode 53/100; Loss: 0.034221671521663666\n",
      "Step 8 (2502); Episode 53/100; Loss: 0.03081411123275757\n",
      "Step 9 (2503); Episode 53/100; Loss: 0.09690546989440918\n",
      "Step 10 (2504); Episode 53/100; Loss: 0.07062782347202301\n",
      "Step 11 (2505); Episode 53/100; Loss: 0.08363738656044006\n",
      "Step 12 (2506); Episode 53/100; Loss: 0.004738559015095234\n",
      "Step 13 (2507); Episode 53/100; Loss: 0.0701560452580452\n",
      "Step 14 (2508); Episode 53/100; Loss: 0.04348442703485489\n",
      "Step 15 (2509); Episode 53/100; Loss: 0.0045984406024217606\n",
      "Step 16 (2510); Episode 53/100; Loss: 0.10029220581054688\n",
      "Step 17 (2511); Episode 53/100; Loss: 0.0024932126980274916\n",
      "Step 18 (2512); Episode 53/100; Loss: 0.05928342416882515\n",
      "Step 19 (2513); Episode 53/100; Loss: 0.0032459937501698732\n",
      "Step 20 (2514); Episode 53/100; Loss: 0.0026994338259100914\n",
      "Step 21 (2515); Episode 53/100; Loss: 0.04318566992878914\n",
      "Step 22 (2516); Episode 53/100; Loss: 0.00174605508800596\n",
      "Step 23 (2517); Episode 53/100; Loss: 0.09431766718626022\n",
      "Step 24 (2518); Episode 53/100; Loss: 0.0022472767159342766\n",
      "Step 25 (2519); Episode 53/100; Loss: 0.11750606447458267\n",
      "Step 26 (2520); Episode 53/100; Loss: 0.08343549072742462\n",
      "Step 27 (2521); Episode 53/100; Loss: 0.00257257092744112\n",
      "Step 28 (2522); Episode 53/100; Loss: 0.0032538557425141335\n",
      "Step 29 (2523); Episode 53/100; Loss: 0.003113193204626441\n",
      "Step 30 (2524); Episode 53/100; Loss: 0.037668097764253616\n",
      "Step 31 (2525); Episode 53/100; Loss: 0.1511344313621521\n",
      "Step 32 (2526); Episode 53/100; Loss: 0.0509679913520813\n",
      "Step 33 (2527); Episode 53/100; Loss: 0.17849288880825043\n",
      "Step 34 (2528); Episode 53/100; Loss: 0.0036252979189157486\n",
      "Step 35 (2529); Episode 53/100; Loss: 0.0022210103925317526\n",
      "Step 36 (2530); Episode 53/100; Loss: 0.0019880176987499\n",
      "Step 37 (2531); Episode 53/100; Loss: 0.05603935942053795\n",
      "Step 38 (2532); Episode 53/100; Loss: 0.003076444147154689\n",
      "Step 39 (2533); Episode 53/100; Loss: 0.034303903579711914\n",
      "Step 40 (2534); Episode 53/100; Loss: 0.0812448114156723\n",
      "Step 41 (2535); Episode 53/100; Loss: 0.051455773413181305\n",
      "Step 42 (2536); Episode 53/100; Loss: 0.048049796372652054\n",
      "Step 43 (2537); Episode 53/100; Loss: 0.0410858690738678\n",
      "Step 44 (2538); Episode 53/100; Loss: 0.11035564541816711\n",
      "Step 45 (2539); Episode 53/100; Loss: 0.05236063152551651\n",
      "Step 46 (2540); Episode 53/100; Loss: 0.051238641142845154\n",
      "Step 47 (2541); Episode 53/100; Loss: 0.037884436547756195\n",
      "Step 48 (2542); Episode 53/100; Loss: 0.15110164880752563\n",
      "Step 49 (2543); Episode 53/100; Loss: 0.011383681558072567\n",
      "Step 50 (2544); Episode 53/100; Loss: 0.04370389133691788\n",
      "Step 51 (2545); Episode 53/100; Loss: 0.05920422449707985\n",
      "Step 52 (2546); Episode 53/100; Loss: 0.0446305125951767\n",
      "Step 53 (2547); Episode 53/100; Loss: 0.07768647372722626\n",
      "Step 54 (2548); Episode 53/100; Loss: 0.027683144435286522\n",
      "Step 55 (2549); Episode 53/100; Loss: 0.057295896112918854\n",
      "Step 56 (2550); Episode 53/100; Loss: 0.03631598502397537\n",
      "Step 57 (2551); Episode 53/100; Loss: 0.0745430663228035\n",
      "Step 58 (2552); Episode 53/100; Loss: 0.05002546310424805\n",
      "Step 59 (2553); Episode 53/100; Loss: 0.0382964164018631\n",
      "Step 60 (2554); Episode 53/100; Loss: 0.024231741204857826\n",
      "Step 61 (2555); Episode 53/100; Loss: 0.029157573357224464\n",
      "Step 62 (2556); Episode 53/100; Loss: 0.0015895921969786286\n",
      "Step 63 (2557); Episode 53/100; Loss: 0.0444217249751091\n",
      "Step 64 (2558); Episode 53/100; Loss: 0.004998852964490652\n",
      "Step 65 (2559); Episode 53/100; Loss: 0.003268518717959523\n",
      "Step 66 (2560); Episode 53/100; Loss: 0.002034590346738696\n",
      "Step 67 (2561); Episode 53/100; Loss: 0.038387514650821686\n",
      "Step 68 (2562); Episode 53/100; Loss: 0.0030583401676267385\n",
      "Step 69 (2563); Episode 53/100; Loss: 0.0660361498594284\n",
      "Step 70 (2564); Episode 53/100; Loss: 0.04419495537877083\n",
      "Step 71 (2565); Episode 53/100; Loss: 0.04873208329081535\n",
      "Step 72 (2566); Episode 53/100; Loss: 0.04886360093951225\n",
      "Step 73 (2567); Episode 53/100; Loss: 0.07712157815694809\n",
      "Step 74 (2568); Episode 53/100; Loss: 0.028656495735049248\n",
      "Step 75 (2569); Episode 53/100; Loss: 0.015377171337604523\n",
      "Step 76 (2570); Episode 53/100; Loss: 0.034731265157461166\n",
      "Step 77 (2571); Episode 53/100; Loss: 0.0678180381655693\n",
      "Step 78 (2572); Episode 53/100; Loss: 0.02785012498497963\n",
      "Step 79 (2573); Episode 53/100; Loss: 0.04267480596899986\n",
      "Step 80 (2574); Episode 53/100; Loss: 0.049789492040872574\n",
      "Step 81 (2575); Episode 53/100; Loss: 0.043892908841371536\n",
      "Step 82 (2576); Episode 53/100; Loss: 0.04893812537193298\n",
      "Step 83 (2577); Episode 53/100; Loss: 0.06743179261684418\n",
      "Step 84 (2578); Episode 53/100; Loss: 0.0037372042424976826\n",
      "Step 85 (2579); Episode 53/100; Loss: 0.002120155841112137\n",
      "Step 86 (2580); Episode 53/100; Loss: 0.06060479208827019\n",
      "Step 87 (2581); Episode 53/100; Loss: 0.15833918750286102\n",
      "Step 88 (2582); Episode 53/100; Loss: 0.0017225738847628236\n",
      "Step 89 (2583); Episode 53/100; Loss: 0.03249674662947655\n",
      "Step 90 (2584); Episode 53/100; Loss: 0.05877247080206871\n",
      "Step 91 (2585); Episode 53/100; Loss: 0.03241812065243721\n",
      "Step 92 (2586); Episode 53/100; Loss: 0.0019229573663324118\n",
      "Step 93 (2587); Episode 53/100; Loss: 0.04303138330578804\n",
      "Step 94 (2588); Episode 53/100; Loss: 0.07188566029071808\n",
      "Step 95 (2589); Episode 53/100; Loss: 0.002397834090515971\n",
      "Step 96 (2590); Episode 53/100; Loss: 0.16865448653697968\n",
      "Step 97 (2591); Episode 53/100; Loss: 0.03639927878975868\n",
      "Step 98 (2592); Episode 53/100; Loss: 0.030798640102148056\n",
      "Step 99 (2593); Episode 53/100; Loss: 0.043686456978321075\n",
      "Step 100 (2594); Episode 53/100; Loss: 0.00432090787217021\n",
      "Step 101 (2595); Episode 53/100; Loss: 0.043859321624040604\n",
      "Step 102 (2596); Episode 53/100; Loss: 0.1240801066160202\n",
      "Step 103 (2597); Episode 53/100; Loss: 0.07124405354261398\n",
      "Step 104 (2598); Episode 53/100; Loss: 0.028687579557299614\n",
      "Step 105 (2599); Episode 53/100; Loss: 0.0994468554854393\n",
      "Step 106 (2600); Episode 53/100; Loss: 0.08384646475315094\n",
      "Step 107 (2601); Episode 53/100; Loss: 0.05811924487352371\n",
      "Step 108 (2602); Episode 53/100; Loss: 0.14059141278266907\n",
      "Step 109 (2603); Episode 53/100; Loss: 0.003508332883939147\n",
      "Step 110 (2604); Episode 53/100; Loss: 0.039244394749403\n",
      "Step 111 (2605); Episode 53/100; Loss: 0.0030475265812128782\n",
      "Step 112 (2606); Episode 53/100; Loss: 0.06665079295635223\n",
      "Step 113 (2607); Episode 53/100; Loss: 0.05096478760242462\n",
      "Step 114 (2608); Episode 53/100; Loss: 0.12260981649160385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 115 (2609); Episode 53/100; Loss: 0.0015941457822918892\n",
      "Step 116 (2610); Episode 53/100; Loss: 0.002760707400739193\n",
      "Step 117 (2611); Episode 53/100; Loss: 0.0024158672895282507\n",
      "Step 118 (2612); Episode 53/100; Loss: 0.0014032713370397687\n",
      "Step 119 (2613); Episode 53/100; Loss: 0.029728040099143982\n",
      "Step 120 (2614); Episode 53/100; Loss: 0.04445905238389969\n",
      "Step 121 (2615); Episode 53/100; Loss: 0.026504971086978912\n",
      "Step 122 (2616); Episode 53/100; Loss: 0.0045823752880096436\n",
      "Step 123 (2617); Episode 53/100; Loss: 0.04586762562394142\n",
      "Step 124 (2618); Episode 53/100; Loss: 0.14267563819885254\n",
      "Step 125 (2619); Episode 53/100; Loss: 0.0035265381447970867\n",
      "Step 126 (2620); Episode 53/100; Loss: 0.06264249980449677\n",
      "Step 127 (2621); Episode 53/100; Loss: 0.03155582770705223\n",
      "Step 128 (2622); Episode 53/100; Loss: 0.043407343327999115\n",
      "Step 129 (2623); Episode 53/100; Loss: 0.004633223172277212\n",
      "Step 130 (2624); Episode 53/100; Loss: 0.004368475638329983\n",
      "Step 131 (2625); Episode 53/100; Loss: 0.0025454461574554443\n",
      "Step 132 (2626); Episode 53/100; Loss: 0.12585337460041046\n",
      "Step 133 (2627); Episode 53/100; Loss: 0.0032778868917375803\n",
      "Step 134 (2628); Episode 53/100; Loss: 0.002574751852080226\n",
      "Step 135 (2629); Episode 53/100; Loss: 0.027290739119052887\n",
      "Step 136 (2630); Episode 53/100; Loss: 0.0020447976421564817\n",
      "Step 137 (2631); Episode 53/100; Loss: 0.11223368346691132\n",
      "Step 138 (2632); Episode 53/100; Loss: 0.06647082418203354\n",
      "Step 139 (2633); Episode 53/100; Loss: 0.12005434930324554\n",
      "Step 140 (2634); Episode 53/100; Loss: 0.002118521835654974\n",
      "Step 141 (2635); Episode 53/100; Loss: 0.006065933499485254\n",
      "Step 142 (2636); Episode 53/100; Loss: 0.08584147691726685\n",
      "Step 143 (2637); Episode 53/100; Loss: 0.05019719898700714\n",
      "Step 144 (2638); Episode 53/100; Loss: 0.02183295227587223\n",
      "Step 145 (2639); Episode 53/100; Loss: 0.17964085936546326\n",
      "Step 146 (2640); Episode 53/100; Loss: 0.10504496842622757\n",
      "Step 147 (2641); Episode 53/100; Loss: 0.007476055528968573\n",
      "Step 148 (2642); Episode 53/100; Loss: 0.002997440518811345\n",
      "Step 149 (2643); Episode 53/100; Loss: 0.06713765114545822\n",
      "Step 150 (2644); Episode 53/100; Loss: 0.0026487610302865505\n",
      "Step 151 (2645); Episode 53/100; Loss: 0.044421423226594925\n",
      "Step 152 (2646); Episode 53/100; Loss: 0.002649901667609811\n",
      "Step 153 (2647); Episode 53/100; Loss: 0.0332440622150898\n",
      "Step 154 (2648); Episode 53/100; Loss: 0.00481451814994216\n",
      "Step 155 (2649); Episode 53/100; Loss: 0.002772508654743433\n",
      "Step 156 (2650); Episode 53/100; Loss: 0.04970317706465721\n",
      "Step 157 (2651); Episode 53/100; Loss: 0.05445180833339691\n",
      "Step 158 (2652); Episode 53/100; Loss: 0.0031959381885826588\n",
      "Step 159 (2653); Episode 53/100; Loss: 0.0021262113004922867\n",
      "Step 160 (2654); Episode 53/100; Loss: 0.002578222658485174\n",
      "Step 161 (2655); Episode 53/100; Loss: 0.034730009734630585\n",
      "Step 162 (2656); Episode 53/100; Loss: 0.0819280743598938\n",
      "Step 163 (2657); Episode 53/100; Loss: 0.08981284499168396\n",
      "Step 164 (2658); Episode 53/100; Loss: 0.028607115149497986\n",
      "Step 165 (2659); Episode 53/100; Loss: 0.09479714930057526\n",
      "Step 166 (2660); Episode 53/100; Loss: 0.028730574995279312\n",
      "Step 167 (2661); Episode 53/100; Loss: 0.03880118578672409\n",
      "Step 168 (2662); Episode 53/100; Loss: 0.07062298059463501\n",
      "Step 169 (2663); Episode 53/100; Loss: 0.030179832130670547\n",
      "Step 170 (2664); Episode 53/100; Loss: 0.019893281161785126\n",
      "Step 171 (2665); Episode 53/100; Loss: 0.08721327036619186\n",
      "Step 172 (2666); Episode 53/100; Loss: 0.026656849309802055\n",
      "Step 173 (2667); Episode 53/100; Loss: 0.023871537297964096\n",
      "Step 174 (2668); Episode 53/100; Loss: 0.0024757578503340483\n",
      "Step 175 (2669); Episode 53/100; Loss: 0.003666465636342764\n",
      "Step 176 (2670); Episode 53/100; Loss: 0.09590838104486465\n",
      "Step 177 (2671); Episode 53/100; Loss: 0.05958031117916107\n",
      "Step 178 (2672); Episode 53/100; Loss: 0.048546940088272095\n",
      "Step 179 (2673); Episode 53/100; Loss: 0.07891327142715454\n",
      "Step 180 (2674); Episode 53/100; Loss: 0.004018562380224466\n",
      "Step 181 (2675); Episode 53/100; Loss: 0.030556919053196907\n",
      "Step 182 (2676); Episode 53/100; Loss: 0.10133446007966995\n",
      "Step 183 (2677); Episode 53/100; Loss: 0.1159149631857872\n",
      "Step 184 (2678); Episode 53/100; Loss: 0.024376098066568375\n",
      "Step 185 (2679); Episode 53/100; Loss: 0.0037656042259186506\n",
      "Step 186 (2680); Episode 53/100; Loss: 0.016439370810985565\n",
      "Step 187 (2681); Episode 53/100; Loss: 0.07028011977672577\n",
      "Step 188 (2682); Episode 53/100; Loss: 0.005494550336152315\n",
      "Step 189 (2683); Episode 53/100; Loss: 0.004490295425057411\n",
      "Step 190 (2684); Episode 53/100; Loss: 0.10816527903079987\n",
      "Step 191 (2685); Episode 53/100; Loss: 0.1676322966814041\n",
      "Step 192 (2686); Episode 53/100; Loss: 0.04400412365794182\n",
      "Step 193 (2687); Episode 53/100; Loss: 0.03482555225491524\n",
      "Step 194 (2688); Episode 53/100; Loss: 0.0018471674993634224\n",
      "Step 195 (2689); Episode 53/100; Loss: 0.08739269524812698\n",
      "Step 196 (2690); Episode 53/100; Loss: 0.0018587976228445768\n",
      "Step 197 (2691); Episode 53/100; Loss: 0.031807925552129745\n",
      "Step 198 (2692); Episode 53/100; Loss: 0.07561468333005905\n",
      "Step 199 (2693); Episode 53/100; Loss: 0.05105215683579445\n",
      "Step 0 (2694); Episode 54/100; Loss: 0.033676646649837494\n",
      "Step 1 (2695); Episode 54/100; Loss: 0.0015506255440413952\n",
      "Step 2 (2696); Episode 54/100; Loss: 0.00321704288944602\n",
      "Step 3 (2697); Episode 54/100; Loss: 0.063991978764534\n",
      "Step 4 (2698); Episode 54/100; Loss: 0.05739562585949898\n",
      "Step 5 (2699); Episode 54/100; Loss: 0.04259219020605087\n",
      "Step 6 (2700); Episode 54/100; Loss: 0.03510332107543945\n",
      "Step 7 (2701); Episode 54/100; Loss: 0.13688617944717407\n",
      "Step 8 (2702); Episode 54/100; Loss: 0.09919189661741257\n",
      "Step 9 (2703); Episode 54/100; Loss: 0.10930823534727097\n",
      "Step 10 (2704); Episode 54/100; Loss: 0.09682676941156387\n",
      "Step 11 (2705); Episode 54/100; Loss: 0.10230361670255661\n",
      "Step 12 (2706); Episode 54/100; Loss: 0.0028011531103402376\n",
      "Step 13 (2707); Episode 54/100; Loss: 0.03422776237130165\n",
      "Step 14 (2708); Episode 54/100; Loss: 0.0672762393951416\n",
      "Step 15 (2709); Episode 54/100; Loss: 0.02776804007589817\n",
      "Step 16 (2710); Episode 54/100; Loss: 0.02973027154803276\n",
      "Step 17 (2711); Episode 54/100; Loss: 0.002153279958292842\n",
      "Step 18 (2712); Episode 54/100; Loss: 0.061213601380586624\n",
      "Step 19 (2713); Episode 54/100; Loss: 0.05149818956851959\n",
      "Step 20 (2714); Episode 54/100; Loss: 0.002832221332937479\n",
      "Step 21 (2715); Episode 54/100; Loss: 0.0727565810084343\n",
      "Step 22 (2716); Episode 54/100; Loss: 0.075653076171875\n",
      "Step 23 (2717); Episode 54/100; Loss: 0.0785512924194336\n",
      "Step 24 (2718); Episode 54/100; Loss: 0.001625214470550418\n",
      "Step 25 (2719); Episode 54/100; Loss: 0.06751140207052231\n",
      "Step 26 (2720); Episode 54/100; Loss: 0.0059224823489785194\n",
      "Step 27 (2721); Episode 54/100; Loss: 0.02723599597811699\n",
      "Step 28 (2722); Episode 54/100; Loss: 0.010365289635956287\n",
      "Step 29 (2723); Episode 54/100; Loss: 0.11393071711063385\n",
      "Step 30 (2724); Episode 54/100; Loss: 0.0752500593662262\n",
      "Step 31 (2725); Episode 54/100; Loss: 0.029293254017829895\n",
      "Step 32 (2726); Episode 54/100; Loss: 0.07412095367908478\n",
      "Step 33 (2727); Episode 54/100; Loss: 0.06743153184652328\n",
      "Step 34 (2728); Episode 54/100; Loss: 0.09708350151777267\n",
      "Step 35 (2729); Episode 54/100; Loss: 0.04585989564657211\n",
      "Step 36 (2730); Episode 54/100; Loss: 0.23451083898544312\n",
      "Step 37 (2731); Episode 54/100; Loss: 0.026833517476916313\n",
      "Step 38 (2732); Episode 54/100; Loss: 0.07036980241537094\n",
      "Step 39 (2733); Episode 54/100; Loss: 0.022638406604528427\n",
      "Step 40 (2734); Episode 54/100; Loss: 0.03134572133421898\n",
      "Step 41 (2735); Episode 54/100; Loss: 0.003994083032011986\n",
      "Step 42 (2736); Episode 54/100; Loss: 0.05296306684613228\n",
      "Step 43 (2737); Episode 54/100; Loss: 0.08231570571660995\n",
      "Step 44 (2738); Episode 54/100; Loss: 0.06949573755264282\n",
      "Step 45 (2739); Episode 54/100; Loss: 0.0032886036206036806\n",
      "Step 46 (2740); Episode 54/100; Loss: 0.0057609728537499905\n",
      "Step 47 (2741); Episode 54/100; Loss: 0.07830795645713806\n",
      "Step 48 (2742); Episode 54/100; Loss: 0.04824085161089897\n",
      "Step 49 (2743); Episode 54/100; Loss: 0.05803506448864937\n",
      "Step 50 (2744); Episode 54/100; Loss: 0.03016180545091629\n",
      "Step 51 (2745); Episode 54/100; Loss: 0.0030597250442951918\n",
      "Step 52 (2746); Episode 54/100; Loss: 0.023316478356719017\n",
      "Step 53 (2747); Episode 54/100; Loss: 0.1320822536945343\n",
      "Step 54 (2748); Episode 54/100; Loss: 0.023099742829799652\n",
      "Step 55 (2749); Episode 54/100; Loss: 0.0032668691128492355\n",
      "Step 56 (2750); Episode 54/100; Loss: 0.029924526810646057\n",
      "Step 57 (2751); Episode 54/100; Loss: 0.0016230118926614523\n",
      "Step 58 (2752); Episode 54/100; Loss: 0.0033403937704861164\n",
      "Step 59 (2753); Episode 54/100; Loss: 0.1020335778594017\n",
      "Step 60 (2754); Episode 54/100; Loss: 0.026585808023810387\n",
      "Step 61 (2755); Episode 54/100; Loss: 0.04455713927745819\n",
      "Step 62 (2756); Episode 54/100; Loss: 0.0544869638979435\n",
      "Step 63 (2757); Episode 54/100; Loss: 0.0020349835976958275\n",
      "Step 64 (2758); Episode 54/100; Loss: 0.007735732942819595\n",
      "Step 65 (2759); Episode 54/100; Loss: 0.0022176564671099186\n",
      "Step 66 (2760); Episode 54/100; Loss: 0.02581426315009594\n",
      "Step 67 (2761); Episode 54/100; Loss: 0.00437196483835578\n",
      "Step 68 (2762); Episode 54/100; Loss: 0.10775566101074219\n",
      "Step 69 (2763); Episode 54/100; Loss: 0.021378152072429657\n",
      "Step 70 (2764); Episode 54/100; Loss: 0.0384477935731411\n",
      "Step 71 (2765); Episode 54/100; Loss: 0.0346761979162693\n",
      "Step 72 (2766); Episode 54/100; Loss: 0.005982459522783756\n",
      "Step 73 (2767); Episode 54/100; Loss: 0.09143634885549545\n",
      "Step 74 (2768); Episode 54/100; Loss: 0.041345976293087006\n",
      "Step 75 (2769); Episode 54/100; Loss: 0.11809022724628448\n",
      "Step 76 (2770); Episode 54/100; Loss: 0.06022202968597412\n",
      "Step 77 (2771); Episode 54/100; Loss: 0.00310241780243814\n",
      "Step 78 (2772); Episode 54/100; Loss: 0.002596745267510414\n",
      "Step 79 (2773); Episode 54/100; Loss: 0.036424312740564346\n",
      "Step 80 (2774); Episode 54/100; Loss: 0.07750094681978226\n",
      "Step 81 (2775); Episode 54/100; Loss: 0.025823421776294708\n",
      "Step 82 (2776); Episode 54/100; Loss: 0.010573023930191994\n",
      "Step 83 (2777); Episode 54/100; Loss: 0.004011949524283409\n",
      "Step 84 (2778); Episode 54/100; Loss: 0.001039224909618497\n",
      "Step 85 (2779); Episode 54/100; Loss: 0.00387142994441092\n",
      "Step 86 (2780); Episode 54/100; Loss: 0.10495893657207489\n",
      "Step 87 (2781); Episode 54/100; Loss: 0.0019337241537868977\n",
      "Step 88 (2782); Episode 54/100; Loss: 0.07600294798612595\n",
      "Step 89 (2783); Episode 54/100; Loss: 0.0027023835573345423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90 (2784); Episode 54/100; Loss: 0.09837517142295837\n",
      "Step 91 (2785); Episode 54/100; Loss: 0.029510747641324997\n",
      "Step 92 (2786); Episode 54/100; Loss: 0.0504511296749115\n",
      "Step 93 (2787); Episode 54/100; Loss: 0.05607207864522934\n",
      "Step 94 (2788); Episode 54/100; Loss: 0.08634240925312042\n",
      "Step 95 (2789); Episode 54/100; Loss: 0.030989238992333412\n",
      "Step 96 (2790); Episode 54/100; Loss: 0.2012120485305786\n",
      "Step 97 (2791); Episode 54/100; Loss: 0.0036979601718485355\n",
      "Step 98 (2792); Episode 54/100; Loss: 0.06765132397413254\n",
      "Step 99 (2793); Episode 54/100; Loss: 0.004009220749139786\n",
      "Step 0 (2794); Episode 55/100; Loss: 0.05246244743466377\n",
      "Step 1 (2795); Episode 55/100; Loss: 0.04254303127527237\n",
      "Step 2 (2796); Episode 55/100; Loss: 0.07304054498672485\n",
      "Step 3 (2797); Episode 55/100; Loss: 0.07511593401432037\n",
      "Step 4 (2798); Episode 55/100; Loss: 0.08635053783655167\n",
      "Step 5 (2799); Episode 55/100; Loss: 0.05104970559477806\n",
      "Step 6 (2800); Episode 55/100; Loss: 0.06969548761844635\n",
      "Step 7 (2801); Episode 55/100; Loss: 0.048378486186265945\n",
      "Step 8 (2802); Episode 55/100; Loss: 0.037358514964580536\n",
      "Step 9 (2803); Episode 55/100; Loss: 0.12780852615833282\n",
      "Step 10 (2804); Episode 55/100; Loss: 0.0036415739450603724\n",
      "Step 11 (2805); Episode 55/100; Loss: 0.03341008722782135\n",
      "Step 12 (2806); Episode 55/100; Loss: 0.08765749633312225\n",
      "Step 13 (2807); Episode 55/100; Loss: 0.09276913851499557\n",
      "Step 14 (2808); Episode 55/100; Loss: 0.026422107592225075\n",
      "Step 15 (2809); Episode 55/100; Loss: 0.05103215202689171\n",
      "Step 16 (2810); Episode 55/100; Loss: 0.0022248371969908476\n",
      "Step 17 (2811); Episode 55/100; Loss: 0.0292068924754858\n",
      "Step 18 (2812); Episode 55/100; Loss: 0.1497499942779541\n",
      "Step 19 (2813); Episode 55/100; Loss: 0.029471851885318756\n",
      "Step 20 (2814); Episode 55/100; Loss: 0.05849289894104004\n",
      "Step 21 (2815); Episode 55/100; Loss: 0.0028948637191206217\n",
      "Step 22 (2816); Episode 55/100; Loss: 0.0031536014284938574\n",
      "Step 23 (2817); Episode 55/100; Loss: 0.003625566838309169\n",
      "Step 24 (2818); Episode 55/100; Loss: 0.0040574511513113976\n",
      "Step 25 (2819); Episode 55/100; Loss: 0.022715570405125618\n",
      "Step 26 (2820); Episode 55/100; Loss: 0.004515181295573711\n",
      "Step 27 (2821); Episode 55/100; Loss: 0.04980991780757904\n",
      "Step 28 (2822); Episode 55/100; Loss: 0.08598540723323822\n",
      "Step 29 (2823); Episode 55/100; Loss: 0.08201085031032562\n",
      "Step 30 (2824); Episode 55/100; Loss: 0.13573788106441498\n",
      "Step 31 (2825); Episode 55/100; Loss: 0.12517185509204865\n",
      "Step 32 (2826); Episode 55/100; Loss: 0.10451573133468628\n",
      "Step 33 (2827); Episode 55/100; Loss: 0.004435592330992222\n",
      "Step 34 (2828); Episode 55/100; Loss: 0.0033827703446149826\n",
      "Step 35 (2829); Episode 55/100; Loss: 0.1269620656967163\n",
      "Step 36 (2830); Episode 55/100; Loss: 0.10452160984277725\n",
      "Step 37 (2831); Episode 55/100; Loss: 0.11687611788511276\n",
      "Step 38 (2832); Episode 55/100; Loss: 0.04330620542168617\n",
      "Step 39 (2833); Episode 55/100; Loss: 0.10855147242546082\n",
      "Step 40 (2834); Episode 55/100; Loss: 0.035600002855062485\n",
      "Step 41 (2835); Episode 55/100; Loss: 0.001369696343317628\n",
      "Step 42 (2836); Episode 55/100; Loss: 0.04745461419224739\n",
      "Step 43 (2837); Episode 55/100; Loss: 0.05273887887597084\n",
      "Step 44 (2838); Episode 55/100; Loss: 0.0022595375776290894\n",
      "Step 45 (2839); Episode 55/100; Loss: 0.005498239770531654\n",
      "Step 46 (2840); Episode 55/100; Loss: 0.04874693602323532\n",
      "Step 47 (2841); Episode 55/100; Loss: 0.04916209727525711\n",
      "Step 48 (2842); Episode 55/100; Loss: 0.08936258405447006\n",
      "Step 49 (2843); Episode 55/100; Loss: 0.03242748975753784\n",
      "Step 50 (2844); Episode 55/100; Loss: 0.001710324315354228\n",
      "Step 51 (2845); Episode 55/100; Loss: 0.00990340206772089\n",
      "Step 52 (2846); Episode 55/100; Loss: 0.043350622057914734\n",
      "Step 53 (2847); Episode 55/100; Loss: 0.04544242471456528\n",
      "Step 54 (2848); Episode 55/100; Loss: 0.018810253590345383\n",
      "Step 55 (2849); Episode 55/100; Loss: 0.001366480253636837\n",
      "Step 56 (2850); Episode 55/100; Loss: 0.04826439172029495\n",
      "Step 57 (2851); Episode 55/100; Loss: 0.058525070548057556\n",
      "Step 58 (2852); Episode 55/100; Loss: 0.0037503198254853487\n",
      "Step 59 (2853); Episode 55/100; Loss: 0.0012220037169754505\n",
      "Step 60 (2854); Episode 55/100; Loss: 0.013644521124660969\n",
      "Step 61 (2855); Episode 55/100; Loss: 0.04851853847503662\n",
      "Step 62 (2856); Episode 55/100; Loss: 0.13093318045139313\n",
      "Step 63 (2857); Episode 55/100; Loss: 0.032168660312891006\n",
      "Step 64 (2858); Episode 55/100; Loss: 0.029831407591700554\n",
      "Step 65 (2859); Episode 55/100; Loss: 0.07418788969516754\n",
      "Step 66 (2860); Episode 55/100; Loss: 0.004293953068554401\n",
      "Step 67 (2861); Episode 55/100; Loss: 0.029017122462391853\n",
      "Step 68 (2862); Episode 55/100; Loss: 0.08427531272172928\n",
      "Step 69 (2863); Episode 55/100; Loss: 0.00250442698597908\n",
      "Step 70 (2864); Episode 55/100; Loss: 0.06453429907560349\n",
      "Step 71 (2865); Episode 55/100; Loss: 0.0021901153959333897\n",
      "Step 72 (2866); Episode 55/100; Loss: 0.0630941167473793\n",
      "Step 73 (2867); Episode 55/100; Loss: 0.005224035121500492\n",
      "Step 74 (2868); Episode 55/100; Loss: 0.0022636023350059986\n",
      "Step 75 (2869); Episode 55/100; Loss: 0.031153425574302673\n",
      "Step 76 (2870); Episode 55/100; Loss: 0.003698526183143258\n",
      "Step 77 (2871); Episode 55/100; Loss: 0.0450381301343441\n",
      "Step 78 (2872); Episode 55/100; Loss: 0.09294614940881729\n",
      "Step 79 (2873); Episode 55/100; Loss: 0.050926487892866135\n",
      "Step 80 (2874); Episode 55/100; Loss: 0.13655002415180206\n",
      "Step 81 (2875); Episode 55/100; Loss: 0.002183155855163932\n",
      "Step 82 (2876); Episode 55/100; Loss: 0.05607598274946213\n",
      "Step 83 (2877); Episode 55/100; Loss: 0.021763231605291367\n",
      "Step 84 (2878); Episode 55/100; Loss: 0.04444225877523422\n",
      "Step 85 (2879); Episode 55/100; Loss: 0.0032540741376578808\n",
      "Step 86 (2880); Episode 55/100; Loss: 0.14268428087234497\n",
      "Step 87 (2881); Episode 55/100; Loss: 0.09255208820104599\n",
      "Step 88 (2882); Episode 55/100; Loss: 0.052519381046295166\n",
      "Step 89 (2883); Episode 55/100; Loss: 0.002018736209720373\n",
      "Step 90 (2884); Episode 55/100; Loss: 0.02564929611980915\n",
      "Step 91 (2885); Episode 55/100; Loss: 0.00366202718578279\n",
      "Step 92 (2886); Episode 55/100; Loss: 0.003960782196372747\n",
      "Step 93 (2887); Episode 55/100; Loss: 0.03753780573606491\n",
      "Step 94 (2888); Episode 55/100; Loss: 0.07498276978731155\n",
      "Step 95 (2889); Episode 55/100; Loss: 0.0018072136444970965\n",
      "Step 96 (2890); Episode 55/100; Loss: 0.05195114389061928\n",
      "Step 97 (2891); Episode 55/100; Loss: 0.07608042657375336\n",
      "Step 98 (2892); Episode 55/100; Loss: 0.09028483182191849\n",
      "Step 99 (2893); Episode 55/100; Loss: 0.0035265733022242785\n",
      "Step 100 (2894); Episode 55/100; Loss: 0.051810700446367264\n",
      "Step 101 (2895); Episode 55/100; Loss: 0.0022969895508140326\n",
      "Step 102 (2896); Episode 55/100; Loss: 0.0018182287458330393\n",
      "Step 103 (2897); Episode 55/100; Loss: 0.07956325262784958\n",
      "Step 104 (2898); Episode 55/100; Loss: 0.047864533960819244\n",
      "Step 105 (2899); Episode 55/100; Loss: 0.09146331250667572\n",
      "Step 106 (2900); Episode 55/100; Loss: 0.08032944053411484\n",
      "Step 107 (2901); Episode 55/100; Loss: 0.07627568393945694\n",
      "Step 108 (2902); Episode 55/100; Loss: 0.09481668472290039\n",
      "Step 109 (2903); Episode 55/100; Loss: 0.04000465199351311\n",
      "Step 110 (2904); Episode 55/100; Loss: 0.026007378473877907\n",
      "Step 111 (2905); Episode 55/100; Loss: 0.014799179509282112\n",
      "Step 112 (2906); Episode 55/100; Loss: 0.003019358729943633\n",
      "Step 113 (2907); Episode 55/100; Loss: 0.12298231571912766\n",
      "Step 114 (2908); Episode 55/100; Loss: 0.055780068039894104\n",
      "Step 115 (2909); Episode 55/100; Loss: 0.028677908703684807\n",
      "Step 116 (2910); Episode 55/100; Loss: 0.051759958267211914\n",
      "Step 117 (2911); Episode 55/100; Loss: 0.09173460304737091\n",
      "Step 118 (2912); Episode 55/100; Loss: 0.058643192052841187\n",
      "Step 119 (2913); Episode 55/100; Loss: 0.09367550164461136\n",
      "Step 120 (2914); Episode 55/100; Loss: 0.06734032928943634\n",
      "Step 121 (2915); Episode 55/100; Loss: 0.004847315140068531\n",
      "Step 122 (2916); Episode 55/100; Loss: 0.09207908064126968\n",
      "Step 123 (2917); Episode 55/100; Loss: 0.03446768596768379\n",
      "Step 124 (2918); Episode 55/100; Loss: 0.003347716759890318\n",
      "Step 125 (2919); Episode 55/100; Loss: 0.10243195295333862\n",
      "Step 126 (2920); Episode 55/100; Loss: 0.08958997577428818\n",
      "Step 127 (2921); Episode 55/100; Loss: 0.02516249567270279\n",
      "Step 128 (2922); Episode 55/100; Loss: 0.02205715700984001\n",
      "Step 129 (2923); Episode 55/100; Loss: 0.028980020433664322\n",
      "Step 130 (2924); Episode 55/100; Loss: 0.00478545855730772\n",
      "Step 131 (2925); Episode 55/100; Loss: 0.1178504228591919\n",
      "Step 0 (2926); Episode 56/100; Loss: 0.0034788844641298056\n",
      "Step 1 (2927); Episode 56/100; Loss: 0.058535974472761154\n",
      "Step 2 (2928); Episode 56/100; Loss: 0.07856451719999313\n",
      "Step 3 (2929); Episode 56/100; Loss: 0.03686615824699402\n",
      "Step 4 (2930); Episode 56/100; Loss: 0.02129567600786686\n",
      "Step 5 (2931); Episode 56/100; Loss: 0.0029698912985622883\n",
      "Step 6 (2932); Episode 56/100; Loss: 0.004505922086536884\n",
      "Step 7 (2933); Episode 56/100; Loss: 0.04703224450349808\n",
      "Step 8 (2934); Episode 56/100; Loss: 0.030785655602812767\n",
      "Step 9 (2935); Episode 56/100; Loss: 0.0016443826025351882\n",
      "Step 10 (2936); Episode 56/100; Loss: 0.04810328036546707\n",
      "Step 11 (2937); Episode 56/100; Loss: 0.028439808636903763\n",
      "Step 12 (2938); Episode 56/100; Loss: 0.11447181552648544\n",
      "Step 13 (2939); Episode 56/100; Loss: 0.05124277621507645\n",
      "Step 14 (2940); Episode 56/100; Loss: 0.024815568700432777\n",
      "Step 15 (2941); Episode 56/100; Loss: 0.12684765458106995\n",
      "Step 16 (2942); Episode 56/100; Loss: 0.0027285797987133265\n",
      "Step 17 (2943); Episode 56/100; Loss: 0.003970359452068806\n",
      "Step 18 (2944); Episode 56/100; Loss: 0.07016999274492264\n",
      "Step 19 (2945); Episode 56/100; Loss: 0.038597315549850464\n",
      "Step 20 (2946); Episode 56/100; Loss: 0.10946042090654373\n",
      "Step 21 (2947); Episode 56/100; Loss: 0.003330504521727562\n",
      "Step 22 (2948); Episode 56/100; Loss: 0.06084061414003372\n",
      "Step 23 (2949); Episode 56/100; Loss: 0.059514764696359634\n",
      "Step 24 (2950); Episode 56/100; Loss: 0.01549165602773428\n",
      "Step 25 (2951); Episode 56/100; Loss: 0.0017242423491552472\n",
      "Step 26 (2952); Episode 56/100; Loss: 0.09773340821266174\n",
      "Step 27 (2953); Episode 56/100; Loss: 0.04866879805922508\n",
      "Step 28 (2954); Episode 56/100; Loss: 0.05448563024401665\n",
      "Step 29 (2955); Episode 56/100; Loss: 0.02917647548019886\n",
      "Step 30 (2956); Episode 56/100; Loss: 0.008707452565431595\n",
      "Step 31 (2957); Episode 56/100; Loss: 0.0031509175896644592\n",
      "Step 32 (2958); Episode 56/100; Loss: 0.00638408400118351\n",
      "Step 33 (2959); Episode 56/100; Loss: 0.05323851481080055\n",
      "Step 34 (2960); Episode 56/100; Loss: 0.16102507710456848\n",
      "Step 35 (2961); Episode 56/100; Loss: 0.0980084240436554\n",
      "Step 36 (2962); Episode 56/100; Loss: 0.07117071002721786\n",
      "Step 37 (2963); Episode 56/100; Loss: 0.10399919003248215\n",
      "Step 38 (2964); Episode 56/100; Loss: 0.0016138543142005801\n",
      "Step 39 (2965); Episode 56/100; Loss: 0.05734071880578995\n",
      "Step 40 (2966); Episode 56/100; Loss: 0.04299752041697502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41 (2967); Episode 56/100; Loss: 0.02585463412106037\n",
      "Step 42 (2968); Episode 56/100; Loss: 0.002683523576706648\n",
      "Step 43 (2969); Episode 56/100; Loss: 0.10924030840396881\n",
      "Step 44 (2970); Episode 56/100; Loss: 0.004556475207209587\n",
      "Step 45 (2971); Episode 56/100; Loss: 0.08837321400642395\n",
      "Step 46 (2972); Episode 56/100; Loss: 0.0037530516274273396\n",
      "Step 47 (2973); Episode 56/100; Loss: 0.10002443939447403\n",
      "Step 48 (2974); Episode 56/100; Loss: 0.04687469080090523\n",
      "Step 49 (2975); Episode 56/100; Loss: 0.006763364654034376\n",
      "Step 50 (2976); Episode 56/100; Loss: 0.053539957851171494\n",
      "Step 51 (2977); Episode 56/100; Loss: 0.002706187078729272\n",
      "Step 52 (2978); Episode 56/100; Loss: 0.01674696058034897\n",
      "Step 53 (2979); Episode 56/100; Loss: 0.06165410205721855\n",
      "Step 54 (2980); Episode 56/100; Loss: 0.004910042975097895\n",
      "Step 55 (2981); Episode 56/100; Loss: 0.0216438639909029\n",
      "Step 56 (2982); Episode 56/100; Loss: 0.005970596335828304\n",
      "Step 57 (2983); Episode 56/100; Loss: 0.036020077764987946\n",
      "Step 58 (2984); Episode 56/100; Loss: 0.054034847766160965\n",
      "Step 59 (2985); Episode 56/100; Loss: 0.09481312334537506\n",
      "Step 60 (2986); Episode 56/100; Loss: 0.0491032749414444\n",
      "Step 61 (2987); Episode 56/100; Loss: 0.040228135883808136\n",
      "Step 62 (2988); Episode 56/100; Loss: 0.001497940975241363\n",
      "Step 63 (2989); Episode 56/100; Loss: 0.003522407030686736\n",
      "Step 64 (2990); Episode 56/100; Loss: 0.08773523569107056\n",
      "Step 65 (2991); Episode 56/100; Loss: 0.01616174541413784\n",
      "Step 66 (2992); Episode 56/100; Loss: 0.0036544224712997675\n",
      "Step 67 (2993); Episode 56/100; Loss: 0.03726542741060257\n",
      "Step 68 (2994); Episode 56/100; Loss: 0.1495533585548401\n",
      "Step 69 (2995); Episode 56/100; Loss: 0.15281790494918823\n",
      "Step 70 (2996); Episode 56/100; Loss: 0.0014079641550779343\n",
      "Step 71 (2997); Episode 56/100; Loss: 0.07686125487089157\n",
      "Step 72 (2998); Episode 56/100; Loss: 0.005052334163337946\n",
      "Step 73 (2999); Episode 56/100; Loss: 0.004330754280090332\n",
      "Step 74 (3000); Episode 56/100; Loss: 0.08245950192213058\n",
      "Step 75 (3001); Episode 56/100; Loss: 0.09167922288179398\n",
      "Step 76 (3002); Episode 56/100; Loss: 0.0025055059231817722\n",
      "Step 77 (3003); Episode 56/100; Loss: 0.10907691717147827\n",
      "Step 78 (3004); Episode 56/100; Loss: 0.04590720683336258\n",
      "Step 79 (3005); Episode 56/100; Loss: 0.007520901970565319\n",
      "Step 80 (3006); Episode 56/100; Loss: 0.09152539819478989\n",
      "Step 81 (3007); Episode 56/100; Loss: 0.0701216459274292\n",
      "Step 82 (3008); Episode 56/100; Loss: 0.04713958501815796\n",
      "Step 83 (3009); Episode 56/100; Loss: 0.0017750116530805826\n",
      "Step 84 (3010); Episode 56/100; Loss: 0.09623013436794281\n",
      "Step 85 (3011); Episode 56/100; Loss: 0.048226743936538696\n",
      "Step 86 (3012); Episode 56/100; Loss: 0.003979044500738382\n",
      "Step 87 (3013); Episode 56/100; Loss: 0.0019107930129393935\n",
      "Step 88 (3014); Episode 56/100; Loss: 0.15905532240867615\n",
      "Step 89 (3015); Episode 56/100; Loss: 0.0013704142766073346\n",
      "Step 90 (3016); Episode 56/100; Loss: 0.058419909328222275\n",
      "Step 91 (3017); Episode 56/100; Loss: 0.0023545296862721443\n",
      "Step 92 (3018); Episode 56/100; Loss: 0.04413117468357086\n",
      "Step 93 (3019); Episode 56/100; Loss: 0.0028179329819977283\n",
      "Step 94 (3020); Episode 56/100; Loss: 0.004366412293165922\n",
      "Step 95 (3021); Episode 56/100; Loss: 0.044862162321805954\n",
      "Step 96 (3022); Episode 56/100; Loss: 0.03277014195919037\n",
      "Step 97 (3023); Episode 56/100; Loss: 0.06176373362541199\n",
      "Step 98 (3024); Episode 56/100; Loss: 0.05692754313349724\n",
      "Step 99 (3025); Episode 56/100; Loss: 0.002144112018868327\n",
      "Step 100 (3026); Episode 56/100; Loss: 0.04960808902978897\n",
      "Step 101 (3027); Episode 56/100; Loss: 0.11830316483974457\n",
      "Step 102 (3028); Episode 56/100; Loss: 0.028023049235343933\n",
      "Step 103 (3029); Episode 56/100; Loss: 0.09774469584226608\n",
      "Step 104 (3030); Episode 56/100; Loss: 0.14164042472839355\n",
      "Step 105 (3031); Episode 56/100; Loss: 0.002019279869273305\n",
      "Step 106 (3032); Episode 56/100; Loss: 0.05977004021406174\n",
      "Step 107 (3033); Episode 56/100; Loss: 0.06840336322784424\n",
      "Step 108 (3034); Episode 56/100; Loss: 0.09027248620986938\n",
      "Step 109 (3035); Episode 56/100; Loss: 0.12470318377017975\n",
      "Step 110 (3036); Episode 56/100; Loss: 0.006476732436567545\n",
      "Step 111 (3037); Episode 56/100; Loss: 0.03111572004854679\n",
      "Step 112 (3038); Episode 56/100; Loss: 0.004893878474831581\n",
      "Step 113 (3039); Episode 56/100; Loss: 0.0037585380487143993\n",
      "Step 114 (3040); Episode 56/100; Loss: 0.04667261987924576\n",
      "Step 115 (3041); Episode 56/100; Loss: 0.055891383439302444\n",
      "Step 116 (3042); Episode 56/100; Loss: 0.05309179052710533\n",
      "Step 117 (3043); Episode 56/100; Loss: 0.04563644528388977\n",
      "Step 118 (3044); Episode 56/100; Loss: 0.018616875633597374\n",
      "Step 119 (3045); Episode 56/100; Loss: 0.046959951519966125\n",
      "Step 120 (3046); Episode 56/100; Loss: 0.04803689569234848\n",
      "Step 121 (3047); Episode 56/100; Loss: 0.0015171699924394488\n",
      "Step 122 (3048); Episode 56/100; Loss: 0.04738984629511833\n",
      "Step 123 (3049); Episode 56/100; Loss: 0.002246574964374304\n",
      "Step 124 (3050); Episode 56/100; Loss: 0.051187340170145035\n",
      "Step 125 (3051); Episode 56/100; Loss: 0.1753290891647339\n",
      "Step 126 (3052); Episode 56/100; Loss: 0.07710468024015427\n",
      "Step 127 (3053); Episode 56/100; Loss: 0.035542890429496765\n",
      "Step 128 (3054); Episode 56/100; Loss: 0.11718572676181793\n",
      "Step 129 (3055); Episode 56/100; Loss: 0.052857108414173126\n",
      "Step 130 (3056); Episode 56/100; Loss: 0.09660141915082932\n",
      "Step 131 (3057); Episode 56/100; Loss: 0.004230826627463102\n",
      "Step 132 (3058); Episode 56/100; Loss: 0.002071799011901021\n",
      "Step 133 (3059); Episode 56/100; Loss: 0.003847849555313587\n",
      "Step 134 (3060); Episode 56/100; Loss: 0.12993603944778442\n",
      "Step 135 (3061); Episode 56/100; Loss: 0.0023912149481475353\n",
      "Step 136 (3062); Episode 56/100; Loss: 0.0019006786169484258\n",
      "Step 137 (3063); Episode 56/100; Loss: 0.002500186674296856\n",
      "Step 138 (3064); Episode 56/100; Loss: 0.0030877452809363604\n",
      "Step 139 (3065); Episode 56/100; Loss: 0.04370326176285744\n",
      "Step 140 (3066); Episode 56/100; Loss: 0.06847940385341644\n",
      "Step 141 (3067); Episode 56/100; Loss: 0.031564727425575256\n",
      "Step 0 (3068); Episode 57/100; Loss: 0.10876625031232834\n",
      "Step 1 (3069); Episode 57/100; Loss: 0.0025836003478616476\n",
      "Step 2 (3070); Episode 57/100; Loss: 0.09680584818124771\n",
      "Step 3 (3071); Episode 57/100; Loss: 0.003046611323952675\n",
      "Step 4 (3072); Episode 57/100; Loss: 0.044368550181388855\n",
      "Step 5 (3073); Episode 57/100; Loss: 0.10636604577302933\n",
      "Step 6 (3074); Episode 57/100; Loss: 0.12485294044017792\n",
      "Step 7 (3075); Episode 57/100; Loss: 0.020174747332930565\n",
      "Step 8 (3076); Episode 57/100; Loss: 0.051358748227357864\n",
      "Step 9 (3077); Episode 57/100; Loss: 0.0975547581911087\n",
      "Step 10 (3078); Episode 57/100; Loss: 0.10361945629119873\n",
      "Step 11 (3079); Episode 57/100; Loss: 0.01505983155220747\n",
      "Step 12 (3080); Episode 57/100; Loss: 0.0744311735033989\n",
      "Step 13 (3081); Episode 57/100; Loss: 0.04784201458096504\n",
      "Step 14 (3082); Episode 57/100; Loss: 0.0023222954478114843\n",
      "Step 15 (3083); Episode 57/100; Loss: 0.13751007616519928\n",
      "Step 16 (3084); Episode 57/100; Loss: 0.12970146536827087\n",
      "Step 17 (3085); Episode 57/100; Loss: 0.003509471658617258\n",
      "Step 18 (3086); Episode 57/100; Loss: 0.05132981017231941\n",
      "Step 19 (3087); Episode 57/100; Loss: 0.04716702178120613\n",
      "Step 20 (3088); Episode 57/100; Loss: 0.03990994393825531\n",
      "Step 21 (3089); Episode 57/100; Loss: 0.002331142546609044\n",
      "Step 22 (3090); Episode 57/100; Loss: 0.04832635447382927\n",
      "Step 23 (3091); Episode 57/100; Loss: 0.007474603597074747\n",
      "Step 24 (3092); Episode 57/100; Loss: 0.02593950927257538\n",
      "Step 25 (3093); Episode 57/100; Loss: 0.06814160197973251\n",
      "Step 26 (3094); Episode 57/100; Loss: 0.0037522201891988516\n",
      "Step 27 (3095); Episode 57/100; Loss: 0.07270223647356033\n",
      "Step 28 (3096); Episode 57/100; Loss: 0.0054927715100348\n",
      "Step 29 (3097); Episode 57/100; Loss: 0.060076095163822174\n",
      "Step 30 (3098); Episode 57/100; Loss: 0.04207064211368561\n",
      "Step 31 (3099); Episode 57/100; Loss: 0.022339383140206337\n",
      "Step 32 (3100); Episode 57/100; Loss: 0.038900770246982574\n",
      "Step 33 (3101); Episode 57/100; Loss: 0.040282685309648514\n",
      "Step 34 (3102); Episode 57/100; Loss: 0.003149875672534108\n",
      "Step 35 (3103); Episode 57/100; Loss: 0.058223024010658264\n",
      "Step 36 (3104); Episode 57/100; Loss: 0.06074691191315651\n",
      "Step 37 (3105); Episode 57/100; Loss: 0.08308744430541992\n",
      "Step 38 (3106); Episode 57/100; Loss: 0.00261584110558033\n",
      "Step 39 (3107); Episode 57/100; Loss: 0.08854812383651733\n",
      "Step 40 (3108); Episode 57/100; Loss: 0.0643511638045311\n",
      "Step 41 (3109); Episode 57/100; Loss: 0.001467929221689701\n",
      "Step 42 (3110); Episode 57/100; Loss: 0.0010575149208307266\n",
      "Step 43 (3111); Episode 57/100; Loss: 0.04405320808291435\n",
      "Step 44 (3112); Episode 57/100; Loss: 0.034711189568042755\n",
      "Step 45 (3113); Episode 57/100; Loss: 0.042683735489845276\n",
      "Step 46 (3114); Episode 57/100; Loss: 0.0018130691023543477\n",
      "Step 47 (3115); Episode 57/100; Loss: 0.057637203484773636\n",
      "Step 48 (3116); Episode 57/100; Loss: 0.04349610581994057\n",
      "Step 49 (3117); Episode 57/100; Loss: 0.0026747381780296564\n",
      "Step 50 (3118); Episode 57/100; Loss: 0.005671665538102388\n",
      "Step 51 (3119); Episode 57/100; Loss: 0.0390288382768631\n",
      "Step 52 (3120); Episode 57/100; Loss: 0.0499425083398819\n",
      "Step 53 (3121); Episode 57/100; Loss: 0.08737386763095856\n",
      "Step 54 (3122); Episode 57/100; Loss: 0.05541227385401726\n",
      "Step 55 (3123); Episode 57/100; Loss: 0.00341579201631248\n",
      "Step 56 (3124); Episode 57/100; Loss: 0.09198947250843048\n",
      "Step 57 (3125); Episode 57/100; Loss: 0.0035162351559847593\n",
      "Step 58 (3126); Episode 57/100; Loss: 0.033858511596918106\n",
      "Step 59 (3127); Episode 57/100; Loss: 0.04729747027158737\n",
      "Step 60 (3128); Episode 57/100; Loss: 0.06733095645904541\n",
      "Step 61 (3129); Episode 57/100; Loss: 0.001335595385171473\n",
      "Step 62 (3130); Episode 57/100; Loss: 0.002341985935345292\n",
      "Step 63 (3131); Episode 57/100; Loss: 0.09316693991422653\n",
      "Step 64 (3132); Episode 57/100; Loss: 0.0024521928280591965\n",
      "Step 65 (3133); Episode 57/100; Loss: 0.0040486338548362255\n",
      "Step 66 (3134); Episode 57/100; Loss: 0.031373221427202225\n",
      "Step 67 (3135); Episode 57/100; Loss: 0.09801721572875977\n",
      "Step 68 (3136); Episode 57/100; Loss: 0.001907153520733118\n",
      "Step 69 (3137); Episode 57/100; Loss: 0.0650247260928154\n",
      "Step 70 (3138); Episode 57/100; Loss: 0.072394460439682\n",
      "Step 71 (3139); Episode 57/100; Loss: 0.005103910341858864\n",
      "Step 72 (3140); Episode 57/100; Loss: 0.003914062399417162\n",
      "Step 73 (3141); Episode 57/100; Loss: 0.004647710360586643\n",
      "Step 74 (3142); Episode 57/100; Loss: 0.10905253887176514\n",
      "Step 75 (3143); Episode 57/100; Loss: 0.07530229538679123\n",
      "Step 76 (3144); Episode 57/100; Loss: 0.05485935136675835\n",
      "Step 77 (3145); Episode 57/100; Loss: 0.025887588039040565\n",
      "Step 78 (3146); Episode 57/100; Loss: 0.03147003427147865\n",
      "Step 79 (3147); Episode 57/100; Loss: 0.11749035120010376\n",
      "Step 80 (3148); Episode 57/100; Loss: 0.03597386181354523\n",
      "Step 81 (3149); Episode 57/100; Loss: 0.03631286695599556\n",
      "Step 82 (3150); Episode 57/100; Loss: 0.10024014115333557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 83 (3151); Episode 57/100; Loss: 0.08249319344758987\n",
      "Step 84 (3152); Episode 57/100; Loss: 0.0011525844456627965\n",
      "Step 85 (3153); Episode 57/100; Loss: 0.0032111688051372766\n",
      "Step 86 (3154); Episode 57/100; Loss: 0.0014754008734598756\n",
      "Step 87 (3155); Episode 57/100; Loss: 0.09711551666259766\n",
      "Step 88 (3156); Episode 57/100; Loss: 0.1506994515657425\n",
      "Step 89 (3157); Episode 57/100; Loss: 0.0018343229312449694\n",
      "Step 90 (3158); Episode 57/100; Loss: 0.002272150944918394\n",
      "Step 91 (3159); Episode 57/100; Loss: 0.039011936634778976\n",
      "Step 92 (3160); Episode 57/100; Loss: 0.11714858561754227\n",
      "Step 93 (3161); Episode 57/100; Loss: 0.005155968479812145\n",
      "Step 94 (3162); Episode 57/100; Loss: 0.031619731336832047\n",
      "Step 95 (3163); Episode 57/100; Loss: 0.015377451665699482\n",
      "Step 96 (3164); Episode 57/100; Loss: 0.023272790014743805\n",
      "Step 97 (3165); Episode 57/100; Loss: 0.0023163610603660345\n",
      "Step 98 (3166); Episode 57/100; Loss: 0.001707257004454732\n",
      "Step 99 (3167); Episode 57/100; Loss: 0.0016207894077524543\n",
      "Step 100 (3168); Episode 57/100; Loss: 0.00216915225610137\n",
      "Step 101 (3169); Episode 57/100; Loss: 0.0016573913162574172\n",
      "Step 102 (3170); Episode 57/100; Loss: 0.002319588093087077\n",
      "Step 103 (3171); Episode 57/100; Loss: 0.09547694772481918\n",
      "Step 104 (3172); Episode 57/100; Loss: 0.02629491128027439\n",
      "Step 105 (3173); Episode 57/100; Loss: 0.09350522607564926\n",
      "Step 106 (3174); Episode 57/100; Loss: 0.06354667246341705\n",
      "Step 107 (3175); Episode 57/100; Loss: 0.034402430057525635\n",
      "Step 108 (3176); Episode 57/100; Loss: 0.088048055768013\n",
      "Step 109 (3177); Episode 57/100; Loss: 0.03640446439385414\n",
      "Step 110 (3178); Episode 57/100; Loss: 0.07559353858232498\n",
      "Step 111 (3179); Episode 57/100; Loss: 0.0024783744011074305\n",
      "Step 112 (3180); Episode 57/100; Loss: 0.05753638967871666\n",
      "Step 113 (3181); Episode 57/100; Loss: 0.0045728618279099464\n",
      "Step 114 (3182); Episode 57/100; Loss: 0.09910068660974503\n",
      "Step 115 (3183); Episode 57/100; Loss: 0.017624251544475555\n",
      "Step 116 (3184); Episode 57/100; Loss: 0.05257964879274368\n",
      "Step 117 (3185); Episode 57/100; Loss: 0.06868960708379745\n",
      "Step 118 (3186); Episode 57/100; Loss: 0.042536962777376175\n",
      "Step 119 (3187); Episode 57/100; Loss: 0.029181234538555145\n",
      "Step 120 (3188); Episode 57/100; Loss: 0.10846376419067383\n",
      "Step 121 (3189); Episode 57/100; Loss: 0.07439844310283661\n",
      "Step 122 (3190); Episode 57/100; Loss: 0.06557653099298477\n",
      "Step 123 (3191); Episode 57/100; Loss: 0.0545574314892292\n",
      "Step 124 (3192); Episode 57/100; Loss: 0.06182001903653145\n",
      "Step 125 (3193); Episode 57/100; Loss: 0.034525733441114426\n",
      "Step 126 (3194); Episode 57/100; Loss: 0.018031373620033264\n",
      "Step 127 (3195); Episode 57/100; Loss: 0.1016879603266716\n",
      "Step 128 (3196); Episode 57/100; Loss: 0.0045947362668812275\n",
      "Step 129 (3197); Episode 57/100; Loss: 0.003783287014812231\n",
      "Step 130 (3198); Episode 57/100; Loss: 0.05112938955426216\n",
      "Step 131 (3199); Episode 57/100; Loss: 0.001907886820845306\n",
      "Step 132 (3200); Episode 57/100; Loss: 0.001280766911804676\n",
      "Step 133 (3201); Episode 57/100; Loss: 0.0031419487204402685\n",
      "Step 134 (3202); Episode 57/100; Loss: 0.03413810953497887\n",
      "Step 135 (3203); Episode 57/100; Loss: 0.06389385461807251\n",
      "Step 136 (3204); Episode 57/100; Loss: 0.03238549456000328\n",
      "Step 137 (3205); Episode 57/100; Loss: 0.14193998277187347\n",
      "Step 138 (3206); Episode 57/100; Loss: 0.005044595338404179\n",
      "Step 139 (3207); Episode 57/100; Loss: 0.07411056756973267\n",
      "Step 140 (3208); Episode 57/100; Loss: 0.0022582297679036856\n",
      "Step 141 (3209); Episode 57/100; Loss: 0.1166054829955101\n",
      "Step 142 (3210); Episode 57/100; Loss: 0.09306517243385315\n",
      "Step 143 (3211); Episode 57/100; Loss: 0.00502704456448555\n",
      "Step 144 (3212); Episode 57/100; Loss: 0.0014102199347689748\n",
      "Step 145 (3213); Episode 57/100; Loss: 0.08754277974367142\n",
      "Step 146 (3214); Episode 57/100; Loss: 0.038758717477321625\n",
      "Step 147 (3215); Episode 57/100; Loss: 0.02741614356637001\n",
      "Step 148 (3216); Episode 57/100; Loss: 0.0018238885095342994\n",
      "Step 149 (3217); Episode 57/100; Loss: 0.001452418859116733\n",
      "Step 150 (3218); Episode 57/100; Loss: 0.03334195166826248\n",
      "Step 151 (3219); Episode 57/100; Loss: 0.03475295752286911\n",
      "Step 152 (3220); Episode 57/100; Loss: 0.10741011053323746\n",
      "Step 153 (3221); Episode 57/100; Loss: 0.04429864510893822\n",
      "Step 154 (3222); Episode 57/100; Loss: 0.0018994115525856614\n",
      "Step 155 (3223); Episode 57/100; Loss: 0.0698041170835495\n",
      "Step 156 (3224); Episode 57/100; Loss: 0.004423226695507765\n",
      "Step 157 (3225); Episode 57/100; Loss: 0.013599342666566372\n",
      "Step 158 (3226); Episode 57/100; Loss: 0.14057603478431702\n",
      "Step 159 (3227); Episode 57/100; Loss: 0.04269103333353996\n",
      "Step 160 (3228); Episode 57/100; Loss: 0.002724400954321027\n",
      "Step 161 (3229); Episode 57/100; Loss: 0.03300781548023224\n",
      "Step 162 (3230); Episode 57/100; Loss: 0.004956893622875214\n",
      "Step 163 (3231); Episode 57/100; Loss: 0.023394422605633736\n",
      "Step 164 (3232); Episode 57/100; Loss: 0.04891367256641388\n",
      "Step 165 (3233); Episode 57/100; Loss: 0.0021723145619034767\n",
      "Step 166 (3234); Episode 57/100; Loss: 0.06996185332536697\n",
      "Step 167 (3235); Episode 57/100; Loss: 0.07174588739871979\n",
      "Step 168 (3236); Episode 57/100; Loss: 0.0031298408284783363\n",
      "Step 169 (3237); Episode 57/100; Loss: 0.025752132758498192\n",
      "Step 170 (3238); Episode 57/100; Loss: 0.0032823930960148573\n",
      "Step 171 (3239); Episode 57/100; Loss: 0.013084590435028076\n",
      "Step 172 (3240); Episode 57/100; Loss: 0.029860345646739006\n",
      "Step 173 (3241); Episode 57/100; Loss: 0.040288060903549194\n",
      "Step 174 (3242); Episode 57/100; Loss: 0.0035718006547540426\n",
      "Step 175 (3243); Episode 57/100; Loss: 0.09209490567445755\n",
      "Step 176 (3244); Episode 57/100; Loss: 0.04939482361078262\n",
      "Step 0 (3245); Episode 58/100; Loss: 0.0021801230031996965\n",
      "Step 1 (3246); Episode 58/100; Loss: 0.04812891036272049\n",
      "Step 2 (3247); Episode 58/100; Loss: 0.05657994747161865\n",
      "Step 3 (3248); Episode 58/100; Loss: 0.04307190701365471\n",
      "Step 4 (3249); Episode 58/100; Loss: 0.0435190349817276\n",
      "Step 5 (3250); Episode 58/100; Loss: 0.03679434582591057\n",
      "Step 6 (3251); Episode 58/100; Loss: 0.052237316966056824\n",
      "Step 7 (3252); Episode 58/100; Loss: 0.022548483684659004\n",
      "Step 8 (3253); Episode 58/100; Loss: 0.002058138372376561\n",
      "Step 9 (3254); Episode 58/100; Loss: 0.00571688124909997\n",
      "Step 10 (3255); Episode 58/100; Loss: 0.05359460040926933\n",
      "Step 11 (3256); Episode 58/100; Loss: 0.05003243684768677\n",
      "Step 12 (3257); Episode 58/100; Loss: 0.07172879576683044\n",
      "Step 13 (3258); Episode 58/100; Loss: 0.04900826886296272\n",
      "Step 14 (3259); Episode 58/100; Loss: 0.02011173777282238\n",
      "Step 15 (3260); Episode 58/100; Loss: 0.0038125424180179834\n",
      "Step 16 (3261); Episode 58/100; Loss: 0.08412256836891174\n",
      "Step 17 (3262); Episode 58/100; Loss: 0.12308815866708755\n",
      "Step 18 (3263); Episode 58/100; Loss: 0.05091732740402222\n",
      "Step 19 (3264); Episode 58/100; Loss: 0.02611609734594822\n",
      "Step 20 (3265); Episode 58/100; Loss: 0.08921229839324951\n",
      "Step 21 (3266); Episode 58/100; Loss: 0.003797805868089199\n",
      "Step 22 (3267); Episode 58/100; Loss: 0.05091818794608116\n",
      "Step 23 (3268); Episode 58/100; Loss: 0.01169506087899208\n",
      "Step 24 (3269); Episode 58/100; Loss: 0.0015142073389142752\n",
      "Step 25 (3270); Episode 58/100; Loss: 0.001812634989619255\n",
      "Step 26 (3271); Episode 58/100; Loss: 0.014735525473952293\n",
      "Step 27 (3272); Episode 58/100; Loss: 0.04283163323998451\n",
      "Step 28 (3273); Episode 58/100; Loss: 0.0015660260105505586\n",
      "Step 29 (3274); Episode 58/100; Loss: 0.05303289368748665\n",
      "Step 30 (3275); Episode 58/100; Loss: 0.1450021117925644\n",
      "Step 31 (3276); Episode 58/100; Loss: 0.17586733400821686\n",
      "Step 32 (3277); Episode 58/100; Loss: 0.032641440629959106\n",
      "Step 33 (3278); Episode 58/100; Loss: 0.06677569448947906\n",
      "Step 34 (3279); Episode 58/100; Loss: 0.05942369997501373\n",
      "Step 35 (3280); Episode 58/100; Loss: 0.04197319597005844\n",
      "Step 36 (3281); Episode 58/100; Loss: 0.009375954046845436\n",
      "Step 37 (3282); Episode 58/100; Loss: 0.007299436256289482\n",
      "Step 38 (3283); Episode 58/100; Loss: 0.04117058217525482\n",
      "Step 39 (3284); Episode 58/100; Loss: 0.005896089598536491\n",
      "Step 40 (3285); Episode 58/100; Loss: 0.04851924628019333\n",
      "Step 41 (3286); Episode 58/100; Loss: 0.07616110891103745\n",
      "Step 42 (3287); Episode 58/100; Loss: 0.04340413212776184\n",
      "Step 43 (3288); Episode 58/100; Loss: 0.0038427349645644426\n",
      "Step 44 (3289); Episode 58/100; Loss: 0.033444907516241074\n",
      "Step 45 (3290); Episode 58/100; Loss: 0.001033678068779409\n",
      "Step 46 (3291); Episode 58/100; Loss: 0.018162433058023453\n",
      "Step 47 (3292); Episode 58/100; Loss: 0.001747427391819656\n",
      "Step 48 (3293); Episode 58/100; Loss: 0.04151130095124245\n",
      "Step 49 (3294); Episode 58/100; Loss: 0.0048967585898935795\n",
      "Step 50 (3295); Episode 58/100; Loss: 0.002194845350459218\n",
      "Step 51 (3296); Episode 58/100; Loss: 0.005563072860240936\n",
      "Step 52 (3297); Episode 58/100; Loss: 0.02778194285929203\n",
      "Step 53 (3298); Episode 58/100; Loss: 0.03986653685569763\n",
      "Step 54 (3299); Episode 58/100; Loss: 0.04711677134037018\n",
      "Step 55 (3300); Episode 58/100; Loss: 0.0051321471109986305\n",
      "Step 56 (3301); Episode 58/100; Loss: 0.027457866817712784\n",
      "Step 57 (3302); Episode 58/100; Loss: 0.0036925950553268194\n",
      "Step 58 (3303); Episode 58/100; Loss: 0.023465590551495552\n",
      "Step 59 (3304); Episode 58/100; Loss: 0.006519341375678778\n",
      "Step 60 (3305); Episode 58/100; Loss: 0.0022252043709158897\n",
      "Step 61 (3306); Episode 58/100; Loss: 0.16078175604343414\n",
      "Step 62 (3307); Episode 58/100; Loss: 0.044263359159231186\n",
      "Step 63 (3308); Episode 58/100; Loss: 0.0016289554769173265\n",
      "Step 64 (3309); Episode 58/100; Loss: 0.03364880010485649\n",
      "Step 65 (3310); Episode 58/100; Loss: 0.001096266321837902\n",
      "Step 66 (3311); Episode 58/100; Loss: 0.00158217316493392\n",
      "Step 67 (3312); Episode 58/100; Loss: 0.11079400777816772\n",
      "Step 68 (3313); Episode 58/100; Loss: 0.050209637731313705\n",
      "Step 69 (3314); Episode 58/100; Loss: 0.04830962419509888\n",
      "Step 70 (3315); Episode 58/100; Loss: 0.08513228595256805\n",
      "Step 71 (3316); Episode 58/100; Loss: 0.041066475212574005\n",
      "Step 72 (3317); Episode 58/100; Loss: 0.002256779931485653\n",
      "Step 73 (3318); Episode 58/100; Loss: 0.026234325021505356\n",
      "Step 74 (3319); Episode 58/100; Loss: 0.0015869169728830457\n",
      "Step 75 (3320); Episode 58/100; Loss: 0.04546481370925903\n",
      "Step 76 (3321); Episode 58/100; Loss: 0.0024040876887738705\n",
      "Step 77 (3322); Episode 58/100; Loss: 0.12276758253574371\n",
      "Step 78 (3323); Episode 58/100; Loss: 0.031031526625156403\n",
      "Step 79 (3324); Episode 58/100; Loss: 0.04077296704053879\n",
      "Step 80 (3325); Episode 58/100; Loss: 0.0034885331988334656\n",
      "Step 81 (3326); Episode 58/100; Loss: 0.004453044384717941\n",
      "Step 82 (3327); Episode 58/100; Loss: 0.004872068762779236\n",
      "Step 83 (3328); Episode 58/100; Loss: 0.04569263756275177\n",
      "Step 84 (3329); Episode 58/100; Loss: 0.04524524137377739\n",
      "Step 85 (3330); Episode 58/100; Loss: 0.0019379127770662308\n",
      "Step 86 (3331); Episode 58/100; Loss: 0.007761561777442694\n",
      "Step 87 (3332); Episode 58/100; Loss: 0.10365176200866699\n",
      "Step 88 (3333); Episode 58/100; Loss: 0.0453563891351223\n",
      "Step 89 (3334); Episode 58/100; Loss: 0.12345657497644424\n",
      "Step 90 (3335); Episode 58/100; Loss: 0.0827765241265297\n",
      "Step 91 (3336); Episode 58/100; Loss: 0.05418882146477699\n",
      "Step 92 (3337); Episode 58/100; Loss: 0.0324249230325222\n",
      "Step 93 (3338); Episode 58/100; Loss: 0.09651017189025879\n",
      "Step 94 (3339); Episode 58/100; Loss: 0.05310797691345215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 95 (3340); Episode 58/100; Loss: 0.0021392102353274822\n",
      "Step 96 (3341); Episode 58/100; Loss: 0.00226671458221972\n",
      "Step 97 (3342); Episode 58/100; Loss: 0.055516090244054794\n",
      "Step 98 (3343); Episode 58/100; Loss: 0.05129029229283333\n",
      "Step 99 (3344); Episode 58/100; Loss: 0.04820932075381279\n",
      "Step 100 (3345); Episode 58/100; Loss: 0.001230674795806408\n",
      "Step 101 (3346); Episode 58/100; Loss: 0.049473073333501816\n",
      "Step 102 (3347); Episode 58/100; Loss: 0.0955161526799202\n",
      "Step 103 (3348); Episode 58/100; Loss: 0.05894326791167259\n",
      "Step 104 (3349); Episode 58/100; Loss: 0.0023267227225005627\n",
      "Step 105 (3350); Episode 58/100; Loss: 0.03215387091040611\n",
      "Step 106 (3351); Episode 58/100; Loss: 0.025615187361836433\n",
      "Step 107 (3352); Episode 58/100; Loss: 0.09187263250350952\n",
      "Step 108 (3353); Episode 58/100; Loss: 0.01339096948504448\n",
      "Step 109 (3354); Episode 58/100; Loss: 0.04664197936654091\n",
      "Step 110 (3355); Episode 58/100; Loss: 0.020419500768184662\n",
      "Step 111 (3356); Episode 58/100; Loss: 0.03353163227438927\n",
      "Step 112 (3357); Episode 58/100; Loss: 0.08500288426876068\n",
      "Step 113 (3358); Episode 58/100; Loss: 0.0029390701092779636\n",
      "Step 114 (3359); Episode 58/100; Loss: 0.08216705173254013\n",
      "Step 115 (3360); Episode 58/100; Loss: 0.002640932099893689\n",
      "Step 116 (3361); Episode 58/100; Loss: 0.0778069719672203\n",
      "Step 117 (3362); Episode 58/100; Loss: 0.07472050935029984\n",
      "Step 118 (3363); Episode 58/100; Loss: 0.0024408812168985605\n",
      "Step 119 (3364); Episode 58/100; Loss: 0.03339985013008118\n",
      "Step 120 (3365); Episode 58/100; Loss: 0.0042419941164553165\n",
      "Step 121 (3366); Episode 58/100; Loss: 0.14287570118904114\n",
      "Step 122 (3367); Episode 58/100; Loss: 0.0015599854523316026\n",
      "Step 123 (3368); Episode 58/100; Loss: 0.08452285826206207\n",
      "Step 124 (3369); Episode 58/100; Loss: 0.0033091185614466667\n",
      "Step 125 (3370); Episode 58/100; Loss: 0.13935573399066925\n",
      "Step 126 (3371); Episode 58/100; Loss: 0.0387759730219841\n",
      "Step 127 (3372); Episode 58/100; Loss: 0.03167065232992172\n",
      "Step 128 (3373); Episode 58/100; Loss: 0.01662864349782467\n",
      "Step 129 (3374); Episode 58/100; Loss: 0.09196385741233826\n",
      "Step 130 (3375); Episode 58/100; Loss: 0.049030520021915436\n",
      "Step 131 (3376); Episode 58/100; Loss: 0.026527998968958855\n",
      "Step 132 (3377); Episode 58/100; Loss: 0.027712224051356316\n",
      "Step 133 (3378); Episode 58/100; Loss: 0.003655876498669386\n",
      "Step 134 (3379); Episode 58/100; Loss: 0.08803237974643707\n",
      "Step 135 (3380); Episode 58/100; Loss: 0.04423041269183159\n",
      "Step 136 (3381); Episode 58/100; Loss: 0.03762681037187576\n",
      "Step 137 (3382); Episode 58/100; Loss: 0.0023677146527916193\n",
      "Step 138 (3383); Episode 58/100; Loss: 0.040977463126182556\n",
      "Step 139 (3384); Episode 58/100; Loss: 0.15192417800426483\n",
      "Step 140 (3385); Episode 58/100; Loss: 0.14615793526172638\n",
      "Step 141 (3386); Episode 58/100; Loss: 0.002009465591982007\n",
      "Step 142 (3387); Episode 58/100; Loss: 0.026931462809443474\n",
      "Step 143 (3388); Episode 58/100; Loss: 0.005207261070609093\n",
      "Step 144 (3389); Episode 58/100; Loss: 0.0025241326075047255\n",
      "Step 145 (3390); Episode 58/100; Loss: 0.05083649232983589\n",
      "Step 146 (3391); Episode 58/100; Loss: 0.0019113383023068309\n",
      "Step 147 (3392); Episode 58/100; Loss: 0.03130162134766579\n",
      "Step 148 (3393); Episode 58/100; Loss: 0.08270645886659622\n",
      "Step 149 (3394); Episode 58/100; Loss: 0.07700765132904053\n",
      "Step 150 (3395); Episode 58/100; Loss: 0.0041830954141914845\n",
      "Step 151 (3396); Episode 58/100; Loss: 0.0278359092772007\n",
      "Step 152 (3397); Episode 58/100; Loss: 0.055699579417705536\n",
      "Step 153 (3398); Episode 58/100; Loss: 0.04834630712866783\n",
      "Step 154 (3399); Episode 58/100; Loss: 0.006282394286245108\n",
      "Step 155 (3400); Episode 58/100; Loss: 0.09254506230354309\n",
      "Step 156 (3401); Episode 58/100; Loss: 0.09322302043437958\n",
      "Step 157 (3402); Episode 58/100; Loss: 0.04983442276716232\n",
      "Step 158 (3403); Episode 58/100; Loss: 0.02212376520037651\n",
      "Step 159 (3404); Episode 58/100; Loss: 0.0819997787475586\n",
      "Step 160 (3405); Episode 58/100; Loss: 0.04837612807750702\n",
      "Step 161 (3406); Episode 58/100; Loss: 0.0016122497618198395\n",
      "Step 162 (3407); Episode 58/100; Loss: 0.06936779618263245\n",
      "Step 163 (3408); Episode 58/100; Loss: 0.004343765787780285\n",
      "Step 164 (3409); Episode 58/100; Loss: 0.01471039466559887\n",
      "Step 165 (3410); Episode 58/100; Loss: 0.07339239120483398\n",
      "Step 166 (3411); Episode 58/100; Loss: 0.0268733911216259\n",
      "Step 167 (3412); Episode 58/100; Loss: 0.10188896209001541\n",
      "Step 168 (3413); Episode 58/100; Loss: 0.03728592023253441\n",
      "Step 169 (3414); Episode 58/100; Loss: 0.043565016239881516\n",
      "Step 170 (3415); Episode 58/100; Loss: 0.058823708444833755\n",
      "Step 171 (3416); Episode 58/100; Loss: 0.0023247762583196163\n",
      "Step 172 (3417); Episode 58/100; Loss: 0.06498190760612488\n",
      "Step 173 (3418); Episode 58/100; Loss: 0.02895341068506241\n",
      "Step 174 (3419); Episode 58/100; Loss: 0.008227693848311901\n",
      "Step 175 (3420); Episode 58/100; Loss: 0.020014578476548195\n",
      "Step 176 (3421); Episode 58/100; Loss: 0.021185604855418205\n",
      "Step 177 (3422); Episode 58/100; Loss: 0.004201135598123074\n",
      "Step 178 (3423); Episode 58/100; Loss: 0.005490152630954981\n",
      "Step 179 (3424); Episode 58/100; Loss: 0.09636427462100983\n",
      "Step 180 (3425); Episode 58/100; Loss: 0.002559676766395569\n",
      "Step 181 (3426); Episode 58/100; Loss: 0.08405870944261551\n",
      "Step 182 (3427); Episode 58/100; Loss: 0.041806939989328384\n",
      "Step 183 (3428); Episode 58/100; Loss: 0.0437813363969326\n",
      "Step 184 (3429); Episode 58/100; Loss: 0.03300449624657631\n",
      "Step 185 (3430); Episode 58/100; Loss: 0.08841503411531448\n",
      "Step 186 (3431); Episode 58/100; Loss: 0.05263673514127731\n",
      "Step 187 (3432); Episode 58/100; Loss: 0.06251489371061325\n",
      "Step 188 (3433); Episode 58/100; Loss: 0.06754032522439957\n",
      "Step 189 (3434); Episode 58/100; Loss: 0.03408241271972656\n",
      "Step 190 (3435); Episode 58/100; Loss: 0.03693370893597603\n",
      "Step 191 (3436); Episode 58/100; Loss: 0.006562945898622274\n",
      "Step 192 (3437); Episode 58/100; Loss: 0.007640292402356863\n",
      "Step 193 (3438); Episode 58/100; Loss: 0.08139695972204208\n",
      "Step 194 (3439); Episode 58/100; Loss: 0.004591390956193209\n",
      "Step 195 (3440); Episode 58/100; Loss: 0.01432926207780838\n",
      "Step 196 (3441); Episode 58/100; Loss: 0.0261654332280159\n",
      "Step 197 (3442); Episode 58/100; Loss: 0.019451620057225227\n",
      "Step 198 (3443); Episode 58/100; Loss: 0.12355548143386841\n",
      "Step 199 (3444); Episode 58/100; Loss: 0.004124440718442202\n",
      "Step 0 (3445); Episode 59/100; Loss: 0.009578469209372997\n",
      "Step 1 (3446); Episode 59/100; Loss: 0.002904545748606324\n",
      "Step 2 (3447); Episode 59/100; Loss: 0.04203185439109802\n",
      "Step 3 (3448); Episode 59/100; Loss: 0.04492897912859917\n",
      "Step 4 (3449); Episode 59/100; Loss: 0.06970056146383286\n",
      "Step 5 (3450); Episode 59/100; Loss: 0.07454777508974075\n",
      "Step 6 (3451); Episode 59/100; Loss: 0.005661707371473312\n",
      "Step 7 (3452); Episode 59/100; Loss: 0.047937192022800446\n",
      "Step 8 (3453); Episode 59/100; Loss: 0.0014370714779943228\n",
      "Step 9 (3454); Episode 59/100; Loss: 0.08483342826366425\n",
      "Step 10 (3455); Episode 59/100; Loss: 0.04943075776100159\n",
      "Step 11 (3456); Episode 59/100; Loss: 0.0014323652721941471\n",
      "Step 12 (3457); Episode 59/100; Loss: 0.07359255850315094\n",
      "Step 13 (3458); Episode 59/100; Loss: 0.12566374242305756\n",
      "Step 14 (3459); Episode 59/100; Loss: 0.054364822804927826\n",
      "Step 15 (3460); Episode 59/100; Loss: 0.05582663044333458\n",
      "Step 16 (3461); Episode 59/100; Loss: 0.0028798768762499094\n",
      "Step 17 (3462); Episode 59/100; Loss: 0.11849610507488251\n",
      "Step 18 (3463); Episode 59/100; Loss: 0.003212203271687031\n",
      "Step 19 (3464); Episode 59/100; Loss: 0.030606361106038094\n",
      "Step 20 (3465); Episode 59/100; Loss: 0.001748264185152948\n",
      "Step 21 (3466); Episode 59/100; Loss: 0.015727544203400612\n",
      "Step 22 (3467); Episode 59/100; Loss: 0.001232241396792233\n",
      "Step 23 (3468); Episode 59/100; Loss: 0.004722175188362598\n",
      "Step 24 (3469); Episode 59/100; Loss: 0.05855166167020798\n",
      "Step 25 (3470); Episode 59/100; Loss: 0.07034285366535187\n",
      "Step 26 (3471); Episode 59/100; Loss: 0.0037582151126116514\n",
      "Step 27 (3472); Episode 59/100; Loss: 0.0018654271261766553\n",
      "Step 28 (3473); Episode 59/100; Loss: 0.04714890196919441\n",
      "Step 29 (3474); Episode 59/100; Loss: 0.07192270457744598\n",
      "Step 30 (3475); Episode 59/100; Loss: 0.03558063879609108\n",
      "Step 31 (3476); Episode 59/100; Loss: 0.005317415576428175\n",
      "Step 32 (3477); Episode 59/100; Loss: 0.05015340447425842\n",
      "Step 33 (3478); Episode 59/100; Loss: 0.054080065339803696\n",
      "Step 34 (3479); Episode 59/100; Loss: 0.001616659457795322\n",
      "Step 35 (3480); Episode 59/100; Loss: 0.001998048974201083\n",
      "Step 36 (3481); Episode 59/100; Loss: 0.025176987051963806\n",
      "Step 37 (3482); Episode 59/100; Loss: 0.0027860370464622974\n",
      "Step 38 (3483); Episode 59/100; Loss: 0.05751108378171921\n",
      "Step 39 (3484); Episode 59/100; Loss: 0.031964968889951706\n",
      "Step 40 (3485); Episode 59/100; Loss: 0.003622832242399454\n",
      "Step 41 (3486); Episode 59/100; Loss: 0.010988082736730576\n",
      "Step 42 (3487); Episode 59/100; Loss: 0.003130046185106039\n",
      "Step 43 (3488); Episode 59/100; Loss: 0.038159433752298355\n",
      "Step 44 (3489); Episode 59/100; Loss: 0.04464771971106529\n",
      "Step 45 (3490); Episode 59/100; Loss: 0.0035045354161411524\n",
      "Step 46 (3491); Episode 59/100; Loss: 0.042457737028598785\n",
      "Step 47 (3492); Episode 59/100; Loss: 0.008033465594053268\n",
      "Step 48 (3493); Episode 59/100; Loss: 0.08596286922693253\n",
      "Step 49 (3494); Episode 59/100; Loss: 0.004191846586763859\n",
      "Step 50 (3495); Episode 59/100; Loss: 0.002314882818609476\n",
      "Step 51 (3496); Episode 59/100; Loss: 0.0021051743533462286\n",
      "Step 52 (3497); Episode 59/100; Loss: 0.02000509202480316\n",
      "Step 53 (3498); Episode 59/100; Loss: 0.008859054185450077\n",
      "Step 54 (3499); Episode 59/100; Loss: 0.009595831856131554\n",
      "Step 55 (3500); Episode 59/100; Loss: 0.038874778896570206\n",
      "Step 56 (3501); Episode 59/100; Loss: 0.07141458243131638\n",
      "Step 57 (3502); Episode 59/100; Loss: 0.0027816256042569876\n",
      "Step 58 (3503); Episode 59/100; Loss: 0.06562148779630661\n",
      "Step 59 (3504); Episode 59/100; Loss: 0.09924949705600739\n",
      "Step 60 (3505); Episode 59/100; Loss: 0.0026086056604981422\n",
      "Step 61 (3506); Episode 59/100; Loss: 0.006222759839147329\n",
      "Step 62 (3507); Episode 59/100; Loss: 0.004614200908690691\n",
      "Step 63 (3508); Episode 59/100; Loss: 0.02890690788626671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 64 (3509); Episode 59/100; Loss: 0.0033228995744138956\n",
      "Step 65 (3510); Episode 59/100; Loss: 0.026005437597632408\n",
      "Step 66 (3511); Episode 59/100; Loss: 0.04463900253176689\n",
      "Step 67 (3512); Episode 59/100; Loss: 0.04935944452881813\n",
      "Step 68 (3513); Episode 59/100; Loss: 0.04791748896241188\n",
      "Step 69 (3514); Episode 59/100; Loss: 0.09957734495401382\n",
      "Step 70 (3515); Episode 59/100; Loss: 0.04669453948736191\n",
      "Step 71 (3516); Episode 59/100; Loss: 0.0786048024892807\n",
      "Step 72 (3517); Episode 59/100; Loss: 0.041226789355278015\n",
      "Step 73 (3518); Episode 59/100; Loss: 0.08135376125574112\n",
      "Step 74 (3519); Episode 59/100; Loss: 0.0027352154720574617\n",
      "Step 75 (3520); Episode 59/100; Loss: 0.0024322897661477327\n",
      "Step 76 (3521); Episode 59/100; Loss: 0.043384794145822525\n",
      "Step 77 (3522); Episode 59/100; Loss: 0.03244376555085182\n",
      "Step 78 (3523); Episode 59/100; Loss: 0.09604654461145401\n",
      "Step 79 (3524); Episode 59/100; Loss: 0.025995032861828804\n",
      "Step 80 (3525); Episode 59/100; Loss: 0.04658979922533035\n",
      "Step 81 (3526); Episode 59/100; Loss: 0.04827270284295082\n",
      "Step 82 (3527); Episode 59/100; Loss: 0.004361508414149284\n",
      "Step 83 (3528); Episode 59/100; Loss: 0.020055994391441345\n",
      "Step 84 (3529); Episode 59/100; Loss: 0.003147974843159318\n",
      "Step 85 (3530); Episode 59/100; Loss: 0.0590609647333622\n",
      "Step 86 (3531); Episode 59/100; Loss: 0.0053198994137346745\n",
      "Step 87 (3532); Episode 59/100; Loss: 0.020901421085000038\n",
      "Step 88 (3533); Episode 59/100; Loss: 0.0022310097701847553\n",
      "Step 89 (3534); Episode 59/100; Loss: 0.0013672603527083993\n",
      "Step 90 (3535); Episode 59/100; Loss: 0.001999640604481101\n",
      "Step 91 (3536); Episode 59/100; Loss: 0.10177560895681381\n",
      "Step 92 (3537); Episode 59/100; Loss: 0.040494754910469055\n",
      "Step 93 (3538); Episode 59/100; Loss: 0.05227936431765556\n",
      "Step 94 (3539); Episode 59/100; Loss: 0.0059039597399532795\n",
      "Step 95 (3540); Episode 59/100; Loss: 0.07571595907211304\n",
      "Step 96 (3541); Episode 59/100; Loss: 0.06738878041505814\n",
      "Step 97 (3542); Episode 59/100; Loss: 0.042147137224674225\n",
      "Step 98 (3543); Episode 59/100; Loss: 0.002163487486541271\n",
      "Step 99 (3544); Episode 59/100; Loss: 0.032761018723249435\n",
      "Step 100 (3545); Episode 59/100; Loss: 0.05229059234261513\n",
      "Step 101 (3546); Episode 59/100; Loss: 0.004061875399202108\n",
      "Step 102 (3547); Episode 59/100; Loss: 0.04892595112323761\n",
      "Step 103 (3548); Episode 59/100; Loss: 0.17510774731636047\n",
      "Step 104 (3549); Episode 59/100; Loss: 0.03196345642209053\n",
      "Step 105 (3550); Episode 59/100; Loss: 0.0981304869055748\n",
      "Step 106 (3551); Episode 59/100; Loss: 0.002660500118508935\n",
      "Step 107 (3552); Episode 59/100; Loss: 0.0291218850761652\n",
      "Step 108 (3553); Episode 59/100; Loss: 0.004017277155071497\n",
      "Step 109 (3554); Episode 59/100; Loss: 0.03400702401995659\n",
      "Step 110 (3555); Episode 59/100; Loss: 0.04655500873923302\n",
      "Step 111 (3556); Episode 59/100; Loss: 0.10417550802230835\n",
      "Step 112 (3557); Episode 59/100; Loss: 0.03224264457821846\n",
      "Step 113 (3558); Episode 59/100; Loss: 0.003210925031453371\n",
      "Step 114 (3559); Episode 59/100; Loss: 0.025523439049720764\n",
      "Step 115 (3560); Episode 59/100; Loss: 0.04177376255393028\n",
      "Step 116 (3561); Episode 59/100; Loss: 0.001924874377436936\n",
      "Step 117 (3562); Episode 59/100; Loss: 0.08620205521583557\n",
      "Step 118 (3563); Episode 59/100; Loss: 0.007738932501524687\n",
      "Step 119 (3564); Episode 59/100; Loss: 0.06336931139230728\n",
      "Step 120 (3565); Episode 59/100; Loss: 0.07769912481307983\n",
      "Step 121 (3566); Episode 59/100; Loss: 0.04546920955181122\n",
      "Step 122 (3567); Episode 59/100; Loss: 0.04730359837412834\n",
      "Step 123 (3568); Episode 59/100; Loss: 0.009099208749830723\n",
      "Step 124 (3569); Episode 59/100; Loss: 0.005446655210107565\n",
      "Step 125 (3570); Episode 59/100; Loss: 0.1910572499036789\n",
      "Step 126 (3571); Episode 59/100; Loss: 0.002463843207806349\n",
      "Step 127 (3572); Episode 59/100; Loss: 0.0018153100972995162\n",
      "Step 128 (3573); Episode 59/100; Loss: 0.03209562227129936\n",
      "Step 129 (3574); Episode 59/100; Loss: 0.04787101969122887\n",
      "Step 130 (3575); Episode 59/100; Loss: 0.005032765679061413\n",
      "Step 131 (3576); Episode 59/100; Loss: 0.0025224960409104824\n",
      "Step 132 (3577); Episode 59/100; Loss: 0.07826126366853714\n",
      "Step 133 (3578); Episode 59/100; Loss: 0.04178765043616295\n",
      "Step 134 (3579); Episode 59/100; Loss: 0.08689556270837784\n",
      "Step 135 (3580); Episode 59/100; Loss: 0.08012795448303223\n",
      "Step 136 (3581); Episode 59/100; Loss: 0.0019229677272960544\n",
      "Step 137 (3582); Episode 59/100; Loss: 0.05019138380885124\n",
      "Step 138 (3583); Episode 59/100; Loss: 0.06771065294742584\n",
      "Step 139 (3584); Episode 59/100; Loss: 0.06200984865427017\n",
      "Step 140 (3585); Episode 59/100; Loss: 0.09923215955495834\n",
      "Step 141 (3586); Episode 59/100; Loss: 0.0017794477753341198\n",
      "Step 142 (3587); Episode 59/100; Loss: 0.036308541893959045\n",
      "Step 143 (3588); Episode 59/100; Loss: 0.010435925796627998\n",
      "Step 144 (3589); Episode 59/100; Loss: 0.0019476365996524692\n",
      "Step 145 (3590); Episode 59/100; Loss: 0.02148200199007988\n",
      "Step 0 (3591); Episode 60/100; Loss: 0.0379609689116478\n",
      "Step 1 (3592); Episode 60/100; Loss: 0.098077192902565\n",
      "Step 2 (3593); Episode 60/100; Loss: 0.045850709080696106\n",
      "Step 3 (3594); Episode 60/100; Loss: 0.016553614288568497\n",
      "Step 4 (3595); Episode 60/100; Loss: 0.0024863246362656355\n",
      "Step 5 (3596); Episode 60/100; Loss: 0.0012304269475862384\n",
      "Step 6 (3597); Episode 60/100; Loss: 0.041148774325847626\n",
      "Step 7 (3598); Episode 60/100; Loss: 0.049776796251535416\n",
      "Step 8 (3599); Episode 60/100; Loss: 0.0481465645134449\n",
      "Step 9 (3600); Episode 60/100; Loss: 0.05042152851819992\n",
      "Step 10 (3601); Episode 60/100; Loss: 0.046703919768333435\n",
      "Step 11 (3602); Episode 60/100; Loss: 0.04999365657567978\n",
      "Step 12 (3603); Episode 60/100; Loss: 0.06053456664085388\n",
      "Step 13 (3604); Episode 60/100; Loss: 0.04813206195831299\n",
      "Step 14 (3605); Episode 60/100; Loss: 0.07012273371219635\n",
      "Step 15 (3606); Episode 60/100; Loss: 0.1545218527317047\n",
      "Step 16 (3607); Episode 60/100; Loss: 0.0754767581820488\n",
      "Step 17 (3608); Episode 60/100; Loss: 0.0028306711465120316\n",
      "Step 18 (3609); Episode 60/100; Loss: 0.0022957578767091036\n",
      "Step 19 (3610); Episode 60/100; Loss: 0.04983129724860191\n",
      "Step 20 (3611); Episode 60/100; Loss: 0.14103513956069946\n",
      "Step 21 (3612); Episode 60/100; Loss: 0.0035199474077671766\n",
      "Step 22 (3613); Episode 60/100; Loss: 0.0022947771940380335\n",
      "Step 23 (3614); Episode 60/100; Loss: 0.003231736831367016\n",
      "Step 24 (3615); Episode 60/100; Loss: 0.002280245069414377\n",
      "Step 25 (3616); Episode 60/100; Loss: 0.0032730717211961746\n",
      "Step 26 (3617); Episode 60/100; Loss: 0.08451694250106812\n",
      "Step 27 (3618); Episode 60/100; Loss: 0.05210881307721138\n",
      "Step 28 (3619); Episode 60/100; Loss: 0.0027363677509129047\n",
      "Step 29 (3620); Episode 60/100; Loss: 0.001012271037325263\n",
      "Step 30 (3621); Episode 60/100; Loss: 0.02041936106979847\n",
      "Step 31 (3622); Episode 60/100; Loss: 0.0029979010578244925\n",
      "Step 32 (3623); Episode 60/100; Loss: 0.0015380020486190915\n",
      "Step 33 (3624); Episode 60/100; Loss: 0.0055057816207408905\n",
      "Step 34 (3625); Episode 60/100; Loss: 0.08279267698526382\n",
      "Step 35 (3626); Episode 60/100; Loss: 0.008866514079272747\n",
      "Step 36 (3627); Episode 60/100; Loss: 0.03565378859639168\n",
      "Step 37 (3628); Episode 60/100; Loss: 0.001688516465947032\n",
      "Step 38 (3629); Episode 60/100; Loss: 0.0040312944911420345\n",
      "Step 39 (3630); Episode 60/100; Loss: 0.0023423349484801292\n",
      "Step 40 (3631); Episode 60/100; Loss: 0.06738299876451492\n",
      "Step 41 (3632); Episode 60/100; Loss: 0.045137204229831696\n",
      "Step 42 (3633); Episode 60/100; Loss: 0.0328909307718277\n",
      "Step 43 (3634); Episode 60/100; Loss: 0.03801015019416809\n",
      "Step 44 (3635); Episode 60/100; Loss: 0.05843273177742958\n",
      "Step 45 (3636); Episode 60/100; Loss: 0.0028913274873048067\n",
      "Step 46 (3637); Episode 60/100; Loss: 0.04900646582245827\n",
      "Step 47 (3638); Episode 60/100; Loss: 0.0032581163104623556\n",
      "Step 48 (3639); Episode 60/100; Loss: 0.003042037831619382\n",
      "Step 49 (3640); Episode 60/100; Loss: 0.0017207973869517446\n",
      "Step 50 (3641); Episode 60/100; Loss: 0.0009435242391191423\n",
      "Step 51 (3642); Episode 60/100; Loss: 0.08601497113704681\n",
      "Step 52 (3643); Episode 60/100; Loss: 0.001643260708078742\n",
      "Step 53 (3644); Episode 60/100; Loss: 0.040882695466279984\n",
      "Step 54 (3645); Episode 60/100; Loss: 0.08931176364421844\n",
      "Step 55 (3646); Episode 60/100; Loss: 0.025964265689253807\n",
      "Step 56 (3647); Episode 60/100; Loss: 0.03399483487010002\n",
      "Step 57 (3648); Episode 60/100; Loss: 0.02102825604379177\n",
      "Step 58 (3649); Episode 60/100; Loss: 0.08260349929332733\n",
      "Step 59 (3650); Episode 60/100; Loss: 0.004588531795889139\n",
      "Step 60 (3651); Episode 60/100; Loss: 0.038782112300395966\n",
      "Step 61 (3652); Episode 60/100; Loss: 0.001980886096134782\n",
      "Step 62 (3653); Episode 60/100; Loss: 0.0015800577821210027\n",
      "Step 63 (3654); Episode 60/100; Loss: 0.05521298944950104\n",
      "Step 64 (3655); Episode 60/100; Loss: 0.0014975772937759757\n",
      "Step 65 (3656); Episode 60/100; Loss: 0.09328493475914001\n",
      "Step 66 (3657); Episode 60/100; Loss: 0.07949378341436386\n",
      "Step 67 (3658); Episode 60/100; Loss: 0.16977626085281372\n",
      "Step 68 (3659); Episode 60/100; Loss: 0.10194487869739532\n",
      "Step 69 (3660); Episode 60/100; Loss: 0.003556265030056238\n",
      "Step 70 (3661); Episode 60/100; Loss: 0.038420695811510086\n",
      "Step 71 (3662); Episode 60/100; Loss: 0.06509394198656082\n",
      "Step 72 (3663); Episode 60/100; Loss: 0.03796361759305\n",
      "Step 73 (3664); Episode 60/100; Loss: 0.05820980668067932\n",
      "Step 74 (3665); Episode 60/100; Loss: 0.0948176309466362\n",
      "Step 75 (3666); Episode 60/100; Loss: 0.16186338663101196\n",
      "Step 76 (3667); Episode 60/100; Loss: 0.03130459040403366\n",
      "Step 77 (3668); Episode 60/100; Loss: 0.007005668710917234\n",
      "Step 78 (3669); Episode 60/100; Loss: 0.030035337433218956\n",
      "Step 79 (3670); Episode 60/100; Loss: 0.01721133477985859\n",
      "Step 80 (3671); Episode 60/100; Loss: 0.000991750624962151\n",
      "Step 81 (3672); Episode 60/100; Loss: 0.08035124093294144\n",
      "Step 82 (3673); Episode 60/100; Loss: 0.0498722605407238\n",
      "Step 83 (3674); Episode 60/100; Loss: 0.06658030301332474\n",
      "Step 84 (3675); Episode 60/100; Loss: 0.007696999702602625\n",
      "Step 85 (3676); Episode 60/100; Loss: 0.005648533347994089\n",
      "Step 86 (3677); Episode 60/100; Loss: 0.0014990748604759574\n",
      "Step 87 (3678); Episode 60/100; Loss: 0.007796735968440771\n",
      "Step 88 (3679); Episode 60/100; Loss: 0.007620272692292929\n",
      "Step 89 (3680); Episode 60/100; Loss: 0.02487960271537304\n",
      "Step 90 (3681); Episode 60/100; Loss: 0.0034511489793658257\n",
      "Step 91 (3682); Episode 60/100; Loss: 0.017323294654488564\n",
      "Step 92 (3683); Episode 60/100; Loss: 0.04545540362596512\n",
      "Step 93 (3684); Episode 60/100; Loss: 0.08825135976076126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 94 (3685); Episode 60/100; Loss: 0.02186685986816883\n",
      "Step 95 (3686); Episode 60/100; Loss: 0.019786639139056206\n",
      "Step 96 (3687); Episode 60/100; Loss: 0.02135048247873783\n",
      "Step 97 (3688); Episode 60/100; Loss: 0.08874791860580444\n",
      "Step 98 (3689); Episode 60/100; Loss: 0.09633971750736237\n",
      "Step 99 (3690); Episode 60/100; Loss: 0.0013976197224110365\n",
      "Step 100 (3691); Episode 60/100; Loss: 0.001973845297470689\n",
      "Step 101 (3692); Episode 60/100; Loss: 0.0013077446492388844\n",
      "Step 102 (3693); Episode 60/100; Loss: 0.02078385278582573\n",
      "Step 103 (3694); Episode 60/100; Loss: 0.02044488862156868\n",
      "Step 104 (3695); Episode 60/100; Loss: 0.0014958427054807544\n",
      "Step 105 (3696); Episode 60/100; Loss: 0.017774434760212898\n",
      "Step 106 (3697); Episode 60/100; Loss: 0.0014039080124348402\n",
      "Step 107 (3698); Episode 60/100; Loss: 0.048986803740262985\n",
      "Step 108 (3699); Episode 60/100; Loss: 0.0451691597700119\n",
      "Step 109 (3700); Episode 60/100; Loss: 0.05787178501486778\n",
      "Step 110 (3701); Episode 60/100; Loss: 0.007886379025876522\n",
      "Step 111 (3702); Episode 60/100; Loss: 0.008494063280522823\n",
      "Step 112 (3703); Episode 60/100; Loss: 0.03733173757791519\n",
      "Step 113 (3704); Episode 60/100; Loss: 0.11457659304141998\n",
      "Step 114 (3705); Episode 60/100; Loss: 0.0012737272772938013\n",
      "Step 115 (3706); Episode 60/100; Loss: 0.015615036711096764\n",
      "Step 116 (3707); Episode 60/100; Loss: 0.06086202338337898\n",
      "Step 117 (3708); Episode 60/100; Loss: 0.002908090129494667\n",
      "Step 118 (3709); Episode 60/100; Loss: 0.022330744192004204\n",
      "Step 119 (3710); Episode 60/100; Loss: 0.003106229705736041\n",
      "Step 120 (3711); Episode 60/100; Loss: 0.08724991232156754\n",
      "Step 121 (3712); Episode 60/100; Loss: 0.0030132774263620377\n",
      "Step 122 (3713); Episode 60/100; Loss: 0.003862956538796425\n",
      "Step 123 (3714); Episode 60/100; Loss: 0.0033802178222686052\n",
      "Step 124 (3715); Episode 60/100; Loss: 0.1032811775803566\n",
      "Step 125 (3716); Episode 60/100; Loss: 0.138389453291893\n",
      "Step 126 (3717); Episode 60/100; Loss: 0.11337587237358093\n",
      "Step 127 (3718); Episode 60/100; Loss: 0.043131984770298004\n",
      "Step 128 (3719); Episode 60/100; Loss: 0.002351816976442933\n",
      "Step 129 (3720); Episode 60/100; Loss: 0.033485252410173416\n",
      "Step 130 (3721); Episode 60/100; Loss: 0.04167291522026062\n",
      "Step 131 (3722); Episode 60/100; Loss: 0.019419239833950996\n",
      "Step 132 (3723); Episode 60/100; Loss: 0.045110009610652924\n",
      "Step 133 (3724); Episode 60/100; Loss: 0.05562160909175873\n",
      "Step 134 (3725); Episode 60/100; Loss: 0.002897297265008092\n",
      "Step 0 (3726); Episode 61/100; Loss: 0.0050209080800414085\n",
      "Step 1 (3727); Episode 61/100; Loss: 0.051363833248615265\n",
      "Step 2 (3728); Episode 61/100; Loss: 0.11671579629182816\n",
      "Step 3 (3729); Episode 61/100; Loss: 0.006743629928678274\n",
      "Step 4 (3730); Episode 61/100; Loss: 0.019670996814966202\n",
      "Step 5 (3731); Episode 61/100; Loss: 0.0021110668312758207\n",
      "Step 6 (3732); Episode 61/100; Loss: 0.04715960472822189\n",
      "Step 7 (3733); Episode 61/100; Loss: 0.027986567467451096\n",
      "Step 8 (3734); Episode 61/100; Loss: 0.06849786639213562\n",
      "Step 9 (3735); Episode 61/100; Loss: 0.0020916976500302553\n",
      "Step 10 (3736); Episode 61/100; Loss: 0.002895024139434099\n",
      "Step 11 (3737); Episode 61/100; Loss: 0.021451108157634735\n",
      "Step 12 (3738); Episode 61/100; Loss: 0.032156165689229965\n",
      "Step 13 (3739); Episode 61/100; Loss: 0.03789506480097771\n",
      "Step 14 (3740); Episode 61/100; Loss: 0.0028318280819803476\n",
      "Step 15 (3741); Episode 61/100; Loss: 0.003775753080844879\n",
      "Step 16 (3742); Episode 61/100; Loss: 0.011004585772752762\n",
      "Step 17 (3743); Episode 61/100; Loss: 0.04859466850757599\n",
      "Step 18 (3744); Episode 61/100; Loss: 0.02476697415113449\n",
      "Step 19 (3745); Episode 61/100; Loss: 0.042151063680648804\n",
      "Step 20 (3746); Episode 61/100; Loss: 0.06015029922127724\n",
      "Step 21 (3747); Episode 61/100; Loss: 0.037417855113744736\n",
      "Step 22 (3748); Episode 61/100; Loss: 0.009368553757667542\n",
      "Step 23 (3749); Episode 61/100; Loss: 0.038914259523153305\n",
      "Step 24 (3750); Episode 61/100; Loss: 0.004514325875788927\n",
      "Step 25 (3751); Episode 61/100; Loss: 0.00207646656781435\n",
      "Step 26 (3752); Episode 61/100; Loss: 0.0015249724965542555\n",
      "Step 27 (3753); Episode 61/100; Loss: 0.043811917304992676\n",
      "Step 28 (3754); Episode 61/100; Loss: 0.0013364962069317698\n",
      "Step 29 (3755); Episode 61/100; Loss: 0.07701007276773453\n",
      "Step 30 (3756); Episode 61/100; Loss: 0.060181211680173874\n",
      "Step 31 (3757); Episode 61/100; Loss: 0.020154161378741264\n",
      "Step 32 (3758); Episode 61/100; Loss: 0.13254350423812866\n",
      "Step 33 (3759); Episode 61/100; Loss: 0.000694903137627989\n",
      "Step 34 (3760); Episode 61/100; Loss: 0.02114330790936947\n",
      "Step 35 (3761); Episode 61/100; Loss: 0.05158638209104538\n",
      "Step 36 (3762); Episode 61/100; Loss: 0.0051940991543233395\n",
      "Step 37 (3763); Episode 61/100; Loss: 0.06684786081314087\n",
      "Step 38 (3764); Episode 61/100; Loss: 0.00240511866286397\n",
      "Step 39 (3765); Episode 61/100; Loss: 0.1571454107761383\n",
      "Step 40 (3766); Episode 61/100; Loss: 0.03539999946951866\n",
      "Step 41 (3767); Episode 61/100; Loss: 0.001766102621331811\n",
      "Step 42 (3768); Episode 61/100; Loss: 0.015335844829678535\n",
      "Step 43 (3769); Episode 61/100; Loss: 0.16294793784618378\n",
      "Step 44 (3770); Episode 61/100; Loss: 0.0020462372340261936\n",
      "Step 45 (3771); Episode 61/100; Loss: 0.04126356542110443\n",
      "Step 46 (3772); Episode 61/100; Loss: 0.07792708277702332\n",
      "Step 47 (3773); Episode 61/100; Loss: 0.09592366963624954\n",
      "Step 48 (3774); Episode 61/100; Loss: 0.06373961269855499\n",
      "Step 49 (3775); Episode 61/100; Loss: 0.03946542367339134\n",
      "Step 50 (3776); Episode 61/100; Loss: 0.03780097886919975\n",
      "Step 51 (3777); Episode 61/100; Loss: 0.0014432581374421716\n",
      "Step 52 (3778); Episode 61/100; Loss: 0.03829352185130119\n",
      "Step 53 (3779); Episode 61/100; Loss: 0.0026859226636588573\n",
      "Step 54 (3780); Episode 61/100; Loss: 0.00932309404015541\n",
      "Step 55 (3781); Episode 61/100; Loss: 0.014477499760687351\n",
      "Step 56 (3782); Episode 61/100; Loss: 0.001866542617790401\n",
      "Step 57 (3783); Episode 61/100; Loss: 0.030043965205550194\n",
      "Step 58 (3784); Episode 61/100; Loss: 0.035122111439704895\n",
      "Step 59 (3785); Episode 61/100; Loss: 0.002572478726506233\n",
      "Step 60 (3786); Episode 61/100; Loss: 0.0014635855332016945\n",
      "Step 61 (3787); Episode 61/100; Loss: 0.06493839621543884\n",
      "Step 62 (3788); Episode 61/100; Loss: 0.07287460565567017\n",
      "Step 63 (3789); Episode 61/100; Loss: 0.03943980857729912\n",
      "Step 64 (3790); Episode 61/100; Loss: 0.00216395384632051\n",
      "Step 65 (3791); Episode 61/100; Loss: 0.03888695687055588\n",
      "Step 66 (3792); Episode 61/100; Loss: 0.08246952295303345\n",
      "Step 67 (3793); Episode 61/100; Loss: 0.0016221344703808427\n",
      "Step 68 (3794); Episode 61/100; Loss: 0.001312664127908647\n",
      "Step 69 (3795); Episode 61/100; Loss: 0.08959636092185974\n",
      "Step 70 (3796); Episode 61/100; Loss: 0.058522917330265045\n",
      "Step 71 (3797); Episode 61/100; Loss: 0.0030571199022233486\n",
      "Step 72 (3798); Episode 61/100; Loss: 0.041206929832696915\n",
      "Step 73 (3799); Episode 61/100; Loss: 0.044105030596256256\n",
      "Step 74 (3800); Episode 61/100; Loss: 0.007573266513645649\n",
      "Step 75 (3801); Episode 61/100; Loss: 0.00647370982915163\n",
      "Step 76 (3802); Episode 61/100; Loss: 0.05251084268093109\n",
      "Step 77 (3803); Episode 61/100; Loss: 0.07649455964565277\n",
      "Step 78 (3804); Episode 61/100; Loss: 0.007295982446521521\n",
      "Step 79 (3805); Episode 61/100; Loss: 0.03837555646896362\n",
      "Step 80 (3806); Episode 61/100; Loss: 0.0017501454567536712\n",
      "Step 81 (3807); Episode 61/100; Loss: 0.0017952924827113748\n",
      "Step 82 (3808); Episode 61/100; Loss: 0.027033116668462753\n",
      "Step 83 (3809); Episode 61/100; Loss: 0.05908755213022232\n",
      "Step 84 (3810); Episode 61/100; Loss: 0.003638775087893009\n",
      "Step 85 (3811); Episode 61/100; Loss: 0.0029649208299815655\n",
      "Step 86 (3812); Episode 61/100; Loss: 0.05046549811959267\n",
      "Step 87 (3813); Episode 61/100; Loss: 0.0017480814130976796\n",
      "Step 88 (3814); Episode 61/100; Loss: 0.001148662413470447\n",
      "Step 89 (3815); Episode 61/100; Loss: 0.017362257465720177\n",
      "Step 90 (3816); Episode 61/100; Loss: 0.12550806999206543\n",
      "Step 91 (3817); Episode 61/100; Loss: 0.07247435301542282\n",
      "Step 92 (3818); Episode 61/100; Loss: 0.004716682713478804\n",
      "Step 93 (3819); Episode 61/100; Loss: 0.010395724326372147\n",
      "Step 94 (3820); Episode 61/100; Loss: 0.0825127363204956\n",
      "Step 95 (3821); Episode 61/100; Loss: 0.0019068814581260085\n",
      "Step 96 (3822); Episode 61/100; Loss: 0.017185024917125702\n",
      "Step 97 (3823); Episode 61/100; Loss: 0.003606446087360382\n",
      "Step 98 (3824); Episode 61/100; Loss: 0.0727754607796669\n",
      "Step 99 (3825); Episode 61/100; Loss: 0.045144326984882355\n",
      "Step 100 (3826); Episode 61/100; Loss: 0.08586960285902023\n",
      "Step 101 (3827); Episode 61/100; Loss: 0.0037602740339934826\n",
      "Step 102 (3828); Episode 61/100; Loss: 0.0008392799063585699\n",
      "Step 103 (3829); Episode 61/100; Loss: 0.03870933875441551\n",
      "Step 104 (3830); Episode 61/100; Loss: 0.0015931946691125631\n",
      "Step 105 (3831); Episode 61/100; Loss: 0.0058282590471208096\n",
      "Step 106 (3832); Episode 61/100; Loss: 0.0039648921228945255\n",
      "Step 107 (3833); Episode 61/100; Loss: 0.003181100357323885\n",
      "Step 108 (3834); Episode 61/100; Loss: 0.1154056042432785\n",
      "Step 109 (3835); Episode 61/100; Loss: 0.002831842051818967\n",
      "Step 110 (3836); Episode 61/100; Loss: 0.08538275957107544\n",
      "Step 111 (3837); Episode 61/100; Loss: 0.026430629193782806\n",
      "Step 112 (3838); Episode 61/100; Loss: 0.007963761687278748\n",
      "Step 113 (3839); Episode 61/100; Loss: 0.08409005403518677\n",
      "Step 114 (3840); Episode 61/100; Loss: 0.03323036804795265\n",
      "Step 115 (3841); Episode 61/100; Loss: 0.035965558141469955\n",
      "Step 116 (3842); Episode 61/100; Loss: 0.08994606137275696\n",
      "Step 117 (3843); Episode 61/100; Loss: 0.05596936121582985\n",
      "Step 118 (3844); Episode 61/100; Loss: 0.001922635012306273\n",
      "Step 119 (3845); Episode 61/100; Loss: 0.0032408437691628933\n",
      "Step 120 (3846); Episode 61/100; Loss: 0.0033558933064341545\n",
      "Step 121 (3847); Episode 61/100; Loss: 0.0019040917977690697\n",
      "Step 122 (3848); Episode 61/100; Loss: 0.06036680191755295\n",
      "Step 123 (3849); Episode 61/100; Loss: 0.028647907078266144\n",
      "Step 124 (3850); Episode 61/100; Loss: 0.0029142696876078844\n",
      "Step 125 (3851); Episode 61/100; Loss: 0.0021147720981389284\n",
      "Step 126 (3852); Episode 61/100; Loss: 0.003317992901429534\n",
      "Step 127 (3853); Episode 61/100; Loss: 0.0028724451549351215\n",
      "Step 128 (3854); Episode 61/100; Loss: 0.001315007684752345\n",
      "Step 129 (3855); Episode 61/100; Loss: 0.0011104927398264408\n",
      "Step 130 (3856); Episode 61/100; Loss: 0.0018054126994684339\n",
      "Step 131 (3857); Episode 61/100; Loss: 0.04370509833097458\n",
      "Step 132 (3858); Episode 61/100; Loss: 0.04876110330224037\n",
      "Step 133 (3859); Episode 61/100; Loss: 0.09395655244588852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 134 (3860); Episode 61/100; Loss: 0.032637324184179306\n",
      "Step 135 (3861); Episode 61/100; Loss: 0.0012326549040153623\n",
      "Step 136 (3862); Episode 61/100; Loss: 0.015895890071988106\n",
      "Step 137 (3863); Episode 61/100; Loss: 0.15075084567070007\n",
      "Step 138 (3864); Episode 61/100; Loss: 0.02624543197453022\n",
      "Step 139 (3865); Episode 61/100; Loss: 0.001901764771901071\n",
      "Step 140 (3866); Episode 61/100; Loss: 0.029018480330705643\n",
      "Step 141 (3867); Episode 61/100; Loss: 0.002929799724370241\n",
      "Step 142 (3868); Episode 61/100; Loss: 0.02859950065612793\n",
      "Step 143 (3869); Episode 61/100; Loss: 0.04904884472489357\n",
      "Step 144 (3870); Episode 61/100; Loss: 0.0955703929066658\n",
      "Step 145 (3871); Episode 61/100; Loss: 0.0022942458745092154\n",
      "Step 146 (3872); Episode 61/100; Loss: 0.04304400831460953\n",
      "Step 147 (3873); Episode 61/100; Loss: 0.024677297100424767\n",
      "Step 148 (3874); Episode 61/100; Loss: 0.02482689544558525\n",
      "Step 149 (3875); Episode 61/100; Loss: 0.016508396714925766\n",
      "Step 150 (3876); Episode 61/100; Loss: 0.042094022035598755\n",
      "Step 151 (3877); Episode 61/100; Loss: 0.0038268982898443937\n",
      "Step 152 (3878); Episode 61/100; Loss: 0.04726876690983772\n",
      "Step 153 (3879); Episode 61/100; Loss: 0.10871067643165588\n",
      "Step 154 (3880); Episode 61/100; Loss: 0.04071584716439247\n",
      "Step 155 (3881); Episode 61/100; Loss: 0.0033655136357992887\n",
      "Step 156 (3882); Episode 61/100; Loss: 0.048710260540246964\n",
      "Step 0 (3883); Episode 62/100; Loss: 0.053915902972221375\n",
      "Step 1 (3884); Episode 62/100; Loss: 0.0014788564294576645\n",
      "Step 2 (3885); Episode 62/100; Loss: 0.0018736120546236634\n",
      "Step 3 (3886); Episode 62/100; Loss: 0.0026469130534678698\n",
      "Step 4 (3887); Episode 62/100; Loss: 0.006341593340039253\n",
      "Step 5 (3888); Episode 62/100; Loss: 0.0028488740790635347\n",
      "Step 6 (3889); Episode 62/100; Loss: 0.027458317577838898\n",
      "Step 7 (3890); Episode 62/100; Loss: 0.04726530984044075\n",
      "Step 8 (3891); Episode 62/100; Loss: 0.03759770467877388\n",
      "Step 9 (3892); Episode 62/100; Loss: 0.03740742430090904\n",
      "Step 10 (3893); Episode 62/100; Loss: 0.07588760554790497\n",
      "Step 11 (3894); Episode 62/100; Loss: 0.07284702360630035\n",
      "Step 12 (3895); Episode 62/100; Loss: 0.041849132627248764\n",
      "Step 13 (3896); Episode 62/100; Loss: 0.003453264245763421\n",
      "Step 14 (3897); Episode 62/100; Loss: 0.05267005413770676\n",
      "Step 15 (3898); Episode 62/100; Loss: 0.0048513817600905895\n",
      "Step 16 (3899); Episode 62/100; Loss: 0.01989407278597355\n",
      "Step 17 (3900); Episode 62/100; Loss: 0.04914895072579384\n",
      "Step 18 (3901); Episode 62/100; Loss: 0.001433136872947216\n",
      "Step 19 (3902); Episode 62/100; Loss: 0.04831833392381668\n",
      "Step 20 (3903); Episode 62/100; Loss: 0.10179531574249268\n",
      "Step 21 (3904); Episode 62/100; Loss: 0.05035322532057762\n",
      "Step 22 (3905); Episode 62/100; Loss: 0.08045752346515656\n",
      "Step 23 (3906); Episode 62/100; Loss: 0.036755502223968506\n",
      "Step 24 (3907); Episode 62/100; Loss: 0.0017797076143324375\n",
      "Step 25 (3908); Episode 62/100; Loss: 0.001754732453264296\n",
      "Step 26 (3909); Episode 62/100; Loss: 0.04009786248207092\n",
      "Step 27 (3910); Episode 62/100; Loss: 0.0266596507281065\n",
      "Step 28 (3911); Episode 62/100; Loss: 0.08811777830123901\n",
      "Step 29 (3912); Episode 62/100; Loss: 0.0377841554582119\n",
      "Step 30 (3913); Episode 62/100; Loss: 0.0015150010585784912\n",
      "Step 31 (3914); Episode 62/100; Loss: 0.05783678591251373\n",
      "Step 32 (3915); Episode 62/100; Loss: 0.05357737839221954\n",
      "Step 33 (3916); Episode 62/100; Loss: 0.014939770102500916\n",
      "Step 34 (3917); Episode 62/100; Loss: 0.047713883221149445\n",
      "Step 35 (3918); Episode 62/100; Loss: 0.0017718729795888066\n",
      "Step 36 (3919); Episode 62/100; Loss: 0.049339767545461655\n",
      "Step 37 (3920); Episode 62/100; Loss: 0.004391264636069536\n",
      "Step 38 (3921); Episode 62/100; Loss: 0.0062444815412163734\n",
      "Step 39 (3922); Episode 62/100; Loss: 0.002526825526729226\n",
      "Step 40 (3923); Episode 62/100; Loss: 0.10853202641010284\n",
      "Step 41 (3924); Episode 62/100; Loss: 0.0028311712667346\n",
      "Step 42 (3925); Episode 62/100; Loss: 0.0834541916847229\n",
      "Step 43 (3926); Episode 62/100; Loss: 0.05399428680539131\n",
      "Step 44 (3927); Episode 62/100; Loss: 0.0037198998034000397\n",
      "Step 45 (3928); Episode 62/100; Loss: 0.03705569729208946\n",
      "Step 46 (3929); Episode 62/100; Loss: 0.004214115906506777\n",
      "Step 47 (3930); Episode 62/100; Loss: 0.00823232438415289\n",
      "Step 48 (3931); Episode 62/100; Loss: 0.003281989600509405\n",
      "Step 49 (3932); Episode 62/100; Loss: 0.05121229961514473\n",
      "Step 50 (3933); Episode 62/100; Loss: 0.05257386341691017\n",
      "Step 51 (3934); Episode 62/100; Loss: 0.11878664046525955\n",
      "Step 52 (3935); Episode 62/100; Loss: 0.002161134034395218\n",
      "Step 53 (3936); Episode 62/100; Loss: 0.005768096074461937\n",
      "Step 54 (3937); Episode 62/100; Loss: 0.04632161557674408\n",
      "Step 55 (3938); Episode 62/100; Loss: 0.047866109758615494\n",
      "Step 56 (3939); Episode 62/100; Loss: 0.11505859345197678\n",
      "Step 57 (3940); Episode 62/100; Loss: 0.0017566890455782413\n",
      "Step 58 (3941); Episode 62/100; Loss: 0.04300274699926376\n",
      "Step 59 (3942); Episode 62/100; Loss: 0.09924644231796265\n",
      "Step 60 (3943); Episode 62/100; Loss: 0.053840041160583496\n",
      "Step 61 (3944); Episode 62/100; Loss: 0.05695333704352379\n",
      "Step 62 (3945); Episode 62/100; Loss: 0.0017019618535414338\n",
      "Step 63 (3946); Episode 62/100; Loss: 0.00301989889703691\n",
      "Step 64 (3947); Episode 62/100; Loss: 0.0014842194505035877\n",
      "Step 65 (3948); Episode 62/100; Loss: 0.003838233882561326\n",
      "Step 66 (3949); Episode 62/100; Loss: 0.11931078881025314\n",
      "Step 67 (3950); Episode 62/100; Loss: 0.020912041887640953\n",
      "Step 68 (3951); Episode 62/100; Loss: 0.015386760234832764\n",
      "Step 69 (3952); Episode 62/100; Loss: 0.0020413442980498075\n",
      "Step 70 (3953); Episode 62/100; Loss: 0.047016944736242294\n",
      "Step 71 (3954); Episode 62/100; Loss: 0.03875849023461342\n",
      "Step 72 (3955); Episode 62/100; Loss: 0.0025746941100806\n",
      "Step 73 (3956); Episode 62/100; Loss: 0.01658809371292591\n",
      "Step 74 (3957); Episode 62/100; Loss: 0.05275868624448776\n",
      "Step 75 (3958); Episode 62/100; Loss: 0.003319226438179612\n",
      "Step 76 (3959); Episode 62/100; Loss: 0.06667612493038177\n",
      "Step 77 (3960); Episode 62/100; Loss: 0.04883222281932831\n",
      "Step 78 (3961); Episode 62/100; Loss: 0.045373693108558655\n",
      "Step 79 (3962); Episode 62/100; Loss: 0.002821451285853982\n",
      "Step 80 (3963); Episode 62/100; Loss: 0.056676071137189865\n",
      "Step 81 (3964); Episode 62/100; Loss: 0.006498340982943773\n",
      "Step 82 (3965); Episode 62/100; Loss: 0.027114974334836006\n",
      "Step 83 (3966); Episode 62/100; Loss: 0.046341124922037125\n",
      "Step 84 (3967); Episode 62/100; Loss: 0.027775291353464127\n",
      "Step 85 (3968); Episode 62/100; Loss: 0.006000762339681387\n",
      "Step 86 (3969); Episode 62/100; Loss: 0.0018824245780706406\n",
      "Step 87 (3970); Episode 62/100; Loss: 0.0757192000746727\n",
      "Step 88 (3971); Episode 62/100; Loss: 0.04274977743625641\n",
      "Step 89 (3972); Episode 62/100; Loss: 0.05776494741439819\n",
      "Step 90 (3973); Episode 62/100; Loss: 0.004898135084658861\n",
      "Step 91 (3974); Episode 62/100; Loss: 0.04756806045770645\n",
      "Step 92 (3975); Episode 62/100; Loss: 0.0020990518387407064\n",
      "Step 93 (3976); Episode 62/100; Loss: 0.07828184962272644\n",
      "Step 94 (3977); Episode 62/100; Loss: 0.0017293822020292282\n",
      "Step 95 (3978); Episode 62/100; Loss: 0.04785608872771263\n",
      "Step 96 (3979); Episode 62/100; Loss: 0.08455423265695572\n",
      "Step 97 (3980); Episode 62/100; Loss: 0.021472789347171783\n",
      "Step 98 (3981); Episode 62/100; Loss: 0.04179662466049194\n",
      "Step 99 (3982); Episode 62/100; Loss: 0.06286583095788956\n",
      "Step 100 (3983); Episode 62/100; Loss: 0.06139785796403885\n",
      "Step 101 (3984); Episode 62/100; Loss: 0.029071487486362457\n",
      "Step 102 (3985); Episode 62/100; Loss: 0.002343716798350215\n",
      "Step 103 (3986); Episode 62/100; Loss: 0.051746826618909836\n",
      "Step 104 (3987); Episode 62/100; Loss: 0.003125942312180996\n",
      "Step 105 (3988); Episode 62/100; Loss: 0.047320909798145294\n",
      "Step 106 (3989); Episode 62/100; Loss: 0.10071509331464767\n",
      "Step 107 (3990); Episode 62/100; Loss: 0.026197928935289383\n",
      "Step 108 (3991); Episode 62/100; Loss: 0.08455140888690948\n",
      "Step 109 (3992); Episode 62/100; Loss: 0.056892167776823044\n",
      "Step 110 (3993); Episode 62/100; Loss: 0.07763777673244476\n",
      "Step 111 (3994); Episode 62/100; Loss: 0.12791575491428375\n",
      "Step 112 (3995); Episode 62/100; Loss: 0.135557621717453\n",
      "Step 113 (3996); Episode 62/100; Loss: 0.03250856697559357\n",
      "Step 114 (3997); Episode 62/100; Loss: 0.044466085731983185\n",
      "Step 115 (3998); Episode 62/100; Loss: 0.005617521703243256\n",
      "Step 116 (3999); Episode 62/100; Loss: 0.0011965309968218207\n",
      "Step 117 (4000); Episode 62/100; Loss: 0.0015933890827000141\n",
      "Step 118 (4001); Episode 62/100; Loss: 0.0414753295481205\n",
      "Step 119 (4002); Episode 62/100; Loss: 0.025840813294053078\n",
      "Step 120 (4003); Episode 62/100; Loss: 0.0016317201079800725\n",
      "Step 121 (4004); Episode 62/100; Loss: 0.04409300163388252\n",
      "Step 122 (4005); Episode 62/100; Loss: 0.0035405526868999004\n",
      "Step 123 (4006); Episode 62/100; Loss: 0.00204209191724658\n",
      "Step 124 (4007); Episode 62/100; Loss: 0.020072530955076218\n",
      "Step 125 (4008); Episode 62/100; Loss: 0.032269250601530075\n",
      "Step 126 (4009); Episode 62/100; Loss: 0.04311779513955116\n",
      "Step 127 (4010); Episode 62/100; Loss: 0.0997161790728569\n",
      "Step 128 (4011); Episode 62/100; Loss: 0.056606072932481766\n",
      "Step 129 (4012); Episode 62/100; Loss: 0.022329578176140785\n",
      "Step 130 (4013); Episode 62/100; Loss: 0.0054925475269556046\n",
      "Step 131 (4014); Episode 62/100; Loss: 0.11699089407920837\n",
      "Step 132 (4015); Episode 62/100; Loss: 0.0012855847598984838\n",
      "Step 133 (4016); Episode 62/100; Loss: 0.08510901033878326\n",
      "Step 134 (4017); Episode 62/100; Loss: 0.021558545529842377\n",
      "Step 135 (4018); Episode 62/100; Loss: 0.001652049133554101\n",
      "Step 136 (4019); Episode 62/100; Loss: 0.004163285251706839\n",
      "Step 137 (4020); Episode 62/100; Loss: 0.15281738340854645\n",
      "Step 138 (4021); Episode 62/100; Loss: 0.003213955322280526\n",
      "Step 139 (4022); Episode 62/100; Loss: 0.046458691358566284\n",
      "Step 140 (4023); Episode 62/100; Loss: 0.012272958643734455\n",
      "Step 141 (4024); Episode 62/100; Loss: 0.0031316853128373623\n",
      "Step 142 (4025); Episode 62/100; Loss: 0.07007812708616257\n",
      "Step 143 (4026); Episode 62/100; Loss: 0.004208181984722614\n",
      "Step 144 (4027); Episode 62/100; Loss: 0.0017536580562591553\n",
      "Step 145 (4028); Episode 62/100; Loss: 0.06620805710554123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 146 (4029); Episode 62/100; Loss: 0.001921233139000833\n",
      "Step 147 (4030); Episode 62/100; Loss: 0.006212899461388588\n",
      "Step 148 (4031); Episode 62/100; Loss: 0.0012240351643413305\n",
      "Step 149 (4032); Episode 62/100; Loss: 0.024342376738786697\n",
      "Step 150 (4033); Episode 62/100; Loss: 0.014196363277733326\n",
      "Step 151 (4034); Episode 62/100; Loss: 0.003784834174439311\n",
      "Step 152 (4035); Episode 62/100; Loss: 0.09937462955713272\n",
      "Step 153 (4036); Episode 62/100; Loss: 0.09839506447315216\n",
      "Step 154 (4037); Episode 62/100; Loss: 0.028628217056393623\n",
      "Step 155 (4038); Episode 62/100; Loss: 0.003392465878278017\n",
      "Step 156 (4039); Episode 62/100; Loss: 0.0009946199133992195\n",
      "Step 157 (4040); Episode 62/100; Loss: 0.011756930500268936\n",
      "Step 158 (4041); Episode 62/100; Loss: 0.043514907360076904\n",
      "Step 159 (4042); Episode 62/100; Loss: 0.0017397146439179778\n",
      "Step 160 (4043); Episode 62/100; Loss: 0.001472520176321268\n",
      "Step 161 (4044); Episode 62/100; Loss: 0.0032048968132585287\n",
      "Step 162 (4045); Episode 62/100; Loss: 0.06454093754291534\n",
      "Step 163 (4046); Episode 62/100; Loss: 0.028523027896881104\n",
      "Step 164 (4047); Episode 62/100; Loss: 0.0038506994023919106\n",
      "Step 165 (4048); Episode 62/100; Loss: 0.057055216282606125\n",
      "Step 166 (4049); Episode 62/100; Loss: 0.0016966243274509907\n",
      "Step 167 (4050); Episode 62/100; Loss: 0.06388122588396072\n",
      "Step 168 (4051); Episode 62/100; Loss: 0.029836662113666534\n",
      "Step 169 (4052); Episode 62/100; Loss: 0.007998848333954811\n",
      "Step 170 (4053); Episode 62/100; Loss: 0.004471176769584417\n",
      "Step 0 (4054); Episode 63/100; Loss: 0.003309795167297125\n",
      "Step 1 (4055); Episode 63/100; Loss: 0.0024889798369258642\n",
      "Step 2 (4056); Episode 63/100; Loss: 0.050644613802433014\n",
      "Step 3 (4057); Episode 63/100; Loss: 0.0012122432235628366\n",
      "Step 4 (4058); Episode 63/100; Loss: 0.0017049774760380387\n",
      "Step 5 (4059); Episode 63/100; Loss: 0.014469827525317669\n",
      "Step 6 (4060); Episode 63/100; Loss: 0.001905166544020176\n",
      "Step 7 (4061); Episode 63/100; Loss: 0.05952724814414978\n",
      "Step 8 (4062); Episode 63/100; Loss: 0.003337359055876732\n",
      "Step 9 (4063); Episode 63/100; Loss: 0.045123644173145294\n",
      "Step 10 (4064); Episode 63/100; Loss: 0.051493775099515915\n",
      "Step 11 (4065); Episode 63/100; Loss: 0.004528148099780083\n",
      "Step 12 (4066); Episode 63/100; Loss: 0.0034008307848125696\n",
      "Step 13 (4067); Episode 63/100; Loss: 0.0842045322060585\n",
      "Step 14 (4068); Episode 63/100; Loss: 0.08104892075061798\n",
      "Step 15 (4069); Episode 63/100; Loss: 0.0025437765289098024\n",
      "Step 16 (4070); Episode 63/100; Loss: 0.12364228814840317\n",
      "Step 17 (4071); Episode 63/100; Loss: 0.03805268555879593\n",
      "Step 18 (4072); Episode 63/100; Loss: 0.049717023968696594\n",
      "Step 19 (4073); Episode 63/100; Loss: 0.03435955196619034\n",
      "Step 20 (4074); Episode 63/100; Loss: 0.04148126393556595\n",
      "Step 21 (4075); Episode 63/100; Loss: 0.042142052203416824\n",
      "Step 22 (4076); Episode 63/100; Loss: 0.003265740815550089\n",
      "Step 23 (4077); Episode 63/100; Loss: 0.049364957958459854\n",
      "Step 24 (4078); Episode 63/100; Loss: 0.014234954491257668\n",
      "Step 25 (4079); Episode 63/100; Loss: 0.002388057531788945\n",
      "Step 26 (4080); Episode 63/100; Loss: 0.02883693389594555\n",
      "Step 27 (4081); Episode 63/100; Loss: 0.06723485141992569\n",
      "Step 28 (4082); Episode 63/100; Loss: 0.04340419918298721\n",
      "Step 29 (4083); Episode 63/100; Loss: 0.05967062711715698\n",
      "Step 30 (4084); Episode 63/100; Loss: 0.026636207476258278\n",
      "Step 31 (4085); Episode 63/100; Loss: 0.1374272257089615\n",
      "Step 32 (4086); Episode 63/100; Loss: 0.0011412343010306358\n",
      "Step 33 (4087); Episode 63/100; Loss: 0.0470086969435215\n",
      "Step 34 (4088); Episode 63/100; Loss: 0.0034259867388755083\n",
      "Step 35 (4089); Episode 63/100; Loss: 0.04373861849308014\n",
      "Step 36 (4090); Episode 63/100; Loss: 0.02719300240278244\n",
      "Step 37 (4091); Episode 63/100; Loss: 0.0034403856843709946\n",
      "Step 38 (4092); Episode 63/100; Loss: 0.023019399493932724\n",
      "Step 39 (4093); Episode 63/100; Loss: 0.004870237782597542\n",
      "Step 40 (4094); Episode 63/100; Loss: 0.04055657610297203\n",
      "Step 41 (4095); Episode 63/100; Loss: 0.0015357075026258826\n",
      "Step 42 (4096); Episode 63/100; Loss: 0.005874414462596178\n",
      "Step 43 (4097); Episode 63/100; Loss: 0.052385516464710236\n",
      "Step 44 (4098); Episode 63/100; Loss: 0.018051818013191223\n",
      "Step 45 (4099); Episode 63/100; Loss: 0.061548225581645966\n",
      "Step 46 (4100); Episode 63/100; Loss: 0.020567016676068306\n",
      "Step 47 (4101); Episode 63/100; Loss: 0.0015975666465237737\n",
      "Step 48 (4102); Episode 63/100; Loss: 0.027028167620301247\n",
      "Step 49 (4103); Episode 63/100; Loss: 0.00346321240067482\n",
      "Step 50 (4104); Episode 63/100; Loss: 0.006838789209723473\n",
      "Step 51 (4105); Episode 63/100; Loss: 0.05865729972720146\n",
      "Step 52 (4106); Episode 63/100; Loss: 0.025072038173675537\n",
      "Step 53 (4107); Episode 63/100; Loss: 0.1481582075357437\n",
      "Step 54 (4108); Episode 63/100; Loss: 0.10498254001140594\n",
      "Step 55 (4109); Episode 63/100; Loss: 0.0422242134809494\n",
      "Step 56 (4110); Episode 63/100; Loss: 0.0019073541043326259\n",
      "Step 57 (4111); Episode 63/100; Loss: 0.01078714057803154\n",
      "Step 58 (4112); Episode 63/100; Loss: 0.0032912350725382566\n",
      "Step 59 (4113); Episode 63/100; Loss: 0.04054882377386093\n",
      "Step 60 (4114); Episode 63/100; Loss: 0.08025850355625153\n",
      "Step 61 (4115); Episode 63/100; Loss: 0.02497878111898899\n",
      "Step 62 (4116); Episode 63/100; Loss: 0.0054994081147015095\n",
      "Step 63 (4117); Episode 63/100; Loss: 0.0027232258580625057\n",
      "Step 64 (4118); Episode 63/100; Loss: 0.1398361623287201\n",
      "Step 65 (4119); Episode 63/100; Loss: 0.005211413837969303\n",
      "Step 66 (4120); Episode 63/100; Loss: 0.005103899631649256\n",
      "Step 67 (4121); Episode 63/100; Loss: 0.003026681486517191\n",
      "Step 68 (4122); Episode 63/100; Loss: 0.06460874527692795\n",
      "Step 69 (4123); Episode 63/100; Loss: 0.002344117034226656\n",
      "Step 70 (4124); Episode 63/100; Loss: 0.0056171598844230175\n",
      "Step 71 (4125); Episode 63/100; Loss: 0.017624350264668465\n",
      "Step 72 (4126); Episode 63/100; Loss: 0.05142699182033539\n",
      "Step 73 (4127); Episode 63/100; Loss: 0.03369367495179176\n",
      "Step 74 (4128); Episode 63/100; Loss: 0.07658123224973679\n",
      "Step 75 (4129); Episode 63/100; Loss: 0.01757345348596573\n",
      "Step 76 (4130); Episode 63/100; Loss: 0.004609560593962669\n",
      "Step 77 (4131); Episode 63/100; Loss: 0.03720283508300781\n",
      "Step 78 (4132); Episode 63/100; Loss: 0.07757241278886795\n",
      "Step 79 (4133); Episode 63/100; Loss: 0.018925396725535393\n",
      "Step 80 (4134); Episode 63/100; Loss: 0.04446844756603241\n",
      "Step 81 (4135); Episode 63/100; Loss: 0.08419115841388702\n",
      "Step 82 (4136); Episode 63/100; Loss: 0.05797937139868736\n",
      "Step 83 (4137); Episode 63/100; Loss: 0.047121986746788025\n",
      "Step 84 (4138); Episode 63/100; Loss: 0.003726406255736947\n",
      "Step 85 (4139); Episode 63/100; Loss: 0.0009855249663814902\n",
      "Step 86 (4140); Episode 63/100; Loss: 0.05628208816051483\n",
      "Step 87 (4141); Episode 63/100; Loss: 0.01857660338282585\n",
      "Step 88 (4142); Episode 63/100; Loss: 0.004065612331032753\n",
      "Step 89 (4143); Episode 63/100; Loss: 0.10120117664337158\n",
      "Step 90 (4144); Episode 63/100; Loss: 0.003058468457311392\n",
      "Step 91 (4145); Episode 63/100; Loss: 0.002493459964171052\n",
      "Step 92 (4146); Episode 63/100; Loss: 0.13475348055362701\n",
      "Step 93 (4147); Episode 63/100; Loss: 0.04193313419818878\n",
      "Step 94 (4148); Episode 63/100; Loss: 0.05366455763578415\n",
      "Step 95 (4149); Episode 63/100; Loss: 0.047812122851610184\n",
      "Step 96 (4150); Episode 63/100; Loss: 0.053030047565698624\n",
      "Step 97 (4151); Episode 63/100; Loss: 0.11771518737077713\n",
      "Step 98 (4152); Episode 63/100; Loss: 0.002257357584312558\n",
      "Step 99 (4153); Episode 63/100; Loss: 0.041552089154720306\n",
      "Step 100 (4154); Episode 63/100; Loss: 0.04201769456267357\n",
      "Step 101 (4155); Episode 63/100; Loss: 0.06053020432591438\n",
      "Step 102 (4156); Episode 63/100; Loss: 0.0023042878601700068\n",
      "Step 103 (4157); Episode 63/100; Loss: 0.0013513475423678756\n",
      "Step 104 (4158); Episode 63/100; Loss: 0.07353495061397552\n",
      "Step 105 (4159); Episode 63/100; Loss: 0.005643077660351992\n",
      "Step 106 (4160); Episode 63/100; Loss: 0.06369924545288086\n",
      "Step 107 (4161); Episode 63/100; Loss: 0.07591765373945236\n",
      "Step 108 (4162); Episode 63/100; Loss: 0.09810533374547958\n",
      "Step 109 (4163); Episode 63/100; Loss: 0.005249095614999533\n",
      "Step 110 (4164); Episode 63/100; Loss: 0.023670699447393417\n",
      "Step 111 (4165); Episode 63/100; Loss: 0.039146121591329575\n",
      "Step 112 (4166); Episode 63/100; Loss: 0.001423608628101647\n",
      "Step 113 (4167); Episode 63/100; Loss: 0.005073074717074633\n",
      "Step 114 (4168); Episode 63/100; Loss: 0.020981641486287117\n",
      "Step 115 (4169); Episode 63/100; Loss: 0.004921271000057459\n",
      "Step 116 (4170); Episode 63/100; Loss: 0.04716580733656883\n",
      "Step 117 (4171); Episode 63/100; Loss: 0.003981484565883875\n",
      "Step 118 (4172); Episode 63/100; Loss: 0.1005132794380188\n",
      "Step 119 (4173); Episode 63/100; Loss: 0.0901215523481369\n",
      "Step 120 (4174); Episode 63/100; Loss: 0.03953763097524643\n",
      "Step 121 (4175); Episode 63/100; Loss: 0.0024343000259250402\n",
      "Step 122 (4176); Episode 63/100; Loss: 0.021609056740999222\n",
      "Step 123 (4177); Episode 63/100; Loss: 0.01922111213207245\n",
      "Step 124 (4178); Episode 63/100; Loss: 0.001003300421871245\n",
      "Step 125 (4179); Episode 63/100; Loss: 0.001008539111353457\n",
      "Step 126 (4180); Episode 63/100; Loss: 0.0016879377653822303\n",
      "Step 127 (4181); Episode 63/100; Loss: 0.10237524658441544\n",
      "Step 128 (4182); Episode 63/100; Loss: 0.10518199950456619\n",
      "Step 129 (4183); Episode 63/100; Loss: 0.03234659880399704\n",
      "Step 130 (4184); Episode 63/100; Loss: 0.04329225793480873\n",
      "Step 131 (4185); Episode 63/100; Loss: 0.001709339558146894\n",
      "Step 132 (4186); Episode 63/100; Loss: 0.06375879049301147\n",
      "Step 133 (4187); Episode 63/100; Loss: 0.043909549713134766\n",
      "Step 134 (4188); Episode 63/100; Loss: 0.035443246364593506\n",
      "Step 135 (4189); Episode 63/100; Loss: 0.002557559637352824\n",
      "Step 136 (4190); Episode 63/100; Loss: 0.033826909959316254\n",
      "Step 137 (4191); Episode 63/100; Loss: 0.004964079707860947\n",
      "Step 138 (4192); Episode 63/100; Loss: 0.016472263261675835\n",
      "Step 139 (4193); Episode 63/100; Loss: 0.09137836843729019\n",
      "Step 140 (4194); Episode 63/100; Loss: 0.004277211148291826\n",
      "Step 141 (4195); Episode 63/100; Loss: 0.18202020227909088\n",
      "Step 142 (4196); Episode 63/100; Loss: 0.16049320995807648\n",
      "Step 143 (4197); Episode 63/100; Loss: 0.0417121946811676\n",
      "Step 144 (4198); Episode 63/100; Loss: 0.014172475785017014\n",
      "Step 0 (4199); Episode 64/100; Loss: 0.012431278824806213\n",
      "Step 1 (4200); Episode 64/100; Loss: 0.0020721631590276957\n",
      "Step 2 (4201); Episode 64/100; Loss: 0.12661013007164001\n",
      "Step 3 (4202); Episode 64/100; Loss: 0.08442846685647964\n",
      "Step 4 (4203); Episode 64/100; Loss: 0.059134818613529205\n",
      "Step 5 (4204); Episode 64/100; Loss: 0.007092049811035395\n",
      "Step 6 (4205); Episode 64/100; Loss: 0.04413597285747528\n",
      "Step 7 (4206); Episode 64/100; Loss: 0.042311009019613266\n",
      "Step 8 (4207); Episode 64/100; Loss: 0.0028565831016749144\n",
      "Step 9 (4208); Episode 64/100; Loss: 0.005360903684049845\n",
      "Step 10 (4209); Episode 64/100; Loss: 0.008115924894809723\n",
      "Step 11 (4210); Episode 64/100; Loss: 0.03699810057878494\n",
      "Step 12 (4211); Episode 64/100; Loss: 0.05308810994029045\n",
      "Step 13 (4212); Episode 64/100; Loss: 0.07434181123971939\n",
      "Step 14 (4213); Episode 64/100; Loss: 0.04407137632369995\n",
      "Step 15 (4214); Episode 64/100; Loss: 0.051999256014823914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 (4215); Episode 64/100; Loss: 0.05542733892798424\n",
      "Step 17 (4216); Episode 64/100; Loss: 0.015620360150933266\n",
      "Step 18 (4217); Episode 64/100; Loss: 0.0743379145860672\n",
      "Step 19 (4218); Episode 64/100; Loss: 0.04201028496026993\n",
      "Step 20 (4219); Episode 64/100; Loss: 0.03678722307085991\n",
      "Step 21 (4220); Episode 64/100; Loss: 0.04855532571673393\n",
      "Step 22 (4221); Episode 64/100; Loss: 0.019230075180530548\n",
      "Step 23 (4222); Episode 64/100; Loss: 0.07579559832811356\n",
      "Step 24 (4223); Episode 64/100; Loss: 0.06499193608760834\n",
      "Step 25 (4224); Episode 64/100; Loss: 0.0035240789875388145\n",
      "Step 26 (4225); Episode 64/100; Loss: 0.09151478111743927\n",
      "Step 27 (4226); Episode 64/100; Loss: 0.00542223546653986\n",
      "Step 28 (4227); Episode 64/100; Loss: 0.025316141545772552\n",
      "Step 29 (4228); Episode 64/100; Loss: 0.05495597422122955\n",
      "Step 30 (4229); Episode 64/100; Loss: 0.005456905346363783\n",
      "Step 31 (4230); Episode 64/100; Loss: 0.05024603381752968\n",
      "Step 32 (4231); Episode 64/100; Loss: 0.04258866608142853\n",
      "Step 33 (4232); Episode 64/100; Loss: 0.03260009363293648\n",
      "Step 34 (4233); Episode 64/100; Loss: 0.0473296195268631\n",
      "Step 35 (4234); Episode 64/100; Loss: 0.0758848562836647\n",
      "Step 36 (4235); Episode 64/100; Loss: 0.0018539780285209417\n",
      "Step 37 (4236); Episode 64/100; Loss: 0.027437672019004822\n",
      "Step 38 (4237); Episode 64/100; Loss: 0.06890096515417099\n",
      "Step 39 (4238); Episode 64/100; Loss: 0.003230613423511386\n",
      "Step 40 (4239); Episode 64/100; Loss: 0.0029114820063114166\n",
      "Step 41 (4240); Episode 64/100; Loss: 0.0026307255029678345\n",
      "Step 42 (4241); Episode 64/100; Loss: 0.004596512299031019\n",
      "Step 43 (4242); Episode 64/100; Loss: 0.012003450654447079\n",
      "Step 44 (4243); Episode 64/100; Loss: 0.004010737873613834\n",
      "Step 45 (4244); Episode 64/100; Loss: 0.10442890971899033\n",
      "Step 46 (4245); Episode 64/100; Loss: 0.04755295440554619\n",
      "Step 47 (4246); Episode 64/100; Loss: 0.11027853935956955\n",
      "Step 48 (4247); Episode 64/100; Loss: 0.00189574237447232\n",
      "Step 49 (4248); Episode 64/100; Loss: 0.028904125094413757\n",
      "Step 50 (4249); Episode 64/100; Loss: 0.041583504527807236\n",
      "Step 51 (4250); Episode 64/100; Loss: 0.039797231554985046\n",
      "Step 52 (4251); Episode 64/100; Loss: 0.001565046375617385\n",
      "Step 53 (4252); Episode 64/100; Loss: 0.049563437700271606\n",
      "Step 54 (4253); Episode 64/100; Loss: 0.0030101523734629154\n",
      "Step 55 (4254); Episode 64/100; Loss: 0.003620424075052142\n",
      "Step 56 (4255); Episode 64/100; Loss: 0.05876826494932175\n",
      "Step 57 (4256); Episode 64/100; Loss: 0.0014276228612288833\n",
      "Step 58 (4257); Episode 64/100; Loss: 0.04443654045462608\n",
      "Step 59 (4258); Episode 64/100; Loss: 0.0027275478933006525\n",
      "Step 60 (4259); Episode 64/100; Loss: 0.04439076781272888\n",
      "Step 61 (4260); Episode 64/100; Loss: 0.0013633474009111524\n",
      "Step 62 (4261); Episode 64/100; Loss: 0.12282472103834152\n",
      "Step 63 (4262); Episode 64/100; Loss: 0.04320937767624855\n",
      "Step 64 (4263); Episode 64/100; Loss: 0.004341850522905588\n",
      "Step 65 (4264); Episode 64/100; Loss: 0.041330430656671524\n",
      "Step 66 (4265); Episode 64/100; Loss: 0.03530889376997948\n",
      "Step 67 (4266); Episode 64/100; Loss: 0.06870249658823013\n",
      "Step 68 (4267); Episode 64/100; Loss: 0.0017493636114522815\n",
      "Step 69 (4268); Episode 64/100; Loss: 0.046546876430511475\n",
      "Step 70 (4269); Episode 64/100; Loss: 0.002245937241241336\n",
      "Step 71 (4270); Episode 64/100; Loss: 0.09303291887044907\n",
      "Step 72 (4271); Episode 64/100; Loss: 0.06286849081516266\n",
      "Step 73 (4272); Episode 64/100; Loss: 0.0020962536800652742\n",
      "Step 74 (4273); Episode 64/100; Loss: 0.02569679729640484\n",
      "Step 75 (4274); Episode 64/100; Loss: 0.022097088396549225\n",
      "Step 76 (4275); Episode 64/100; Loss: 0.002436187583953142\n",
      "Step 77 (4276); Episode 64/100; Loss: 0.1247992292046547\n",
      "Step 78 (4277); Episode 64/100; Loss: 0.03951757773756981\n",
      "Step 79 (4278); Episode 64/100; Loss: 0.002664002124220133\n",
      "Step 80 (4279); Episode 64/100; Loss: 0.05419151112437248\n",
      "Step 81 (4280); Episode 64/100; Loss: 0.047462042421102524\n",
      "Step 82 (4281); Episode 64/100; Loss: 0.0033672640565782785\n",
      "Step 83 (4282); Episode 64/100; Loss: 0.09804882854223251\n",
      "Step 84 (4283); Episode 64/100; Loss: 0.0018819071119651198\n",
      "Step 85 (4284); Episode 64/100; Loss: 0.192931666970253\n",
      "Step 86 (4285); Episode 64/100; Loss: 0.0971105545759201\n",
      "Step 87 (4286); Episode 64/100; Loss: 0.0019592156168073416\n",
      "Step 88 (4287); Episode 64/100; Loss: 0.04658350348472595\n",
      "Step 89 (4288); Episode 64/100; Loss: 0.004978986922651529\n",
      "Step 90 (4289); Episode 64/100; Loss: 0.002262416295707226\n",
      "Step 91 (4290); Episode 64/100; Loss: 0.0031950068660080433\n",
      "Step 92 (4291); Episode 64/100; Loss: 0.06830904632806778\n",
      "Step 93 (4292); Episode 64/100; Loss: 0.04741072654724121\n",
      "Step 94 (4293); Episode 64/100; Loss: 0.0023984969593584538\n",
      "Step 95 (4294); Episode 64/100; Loss: 0.050997719168663025\n",
      "Step 96 (4295); Episode 64/100; Loss: 0.0038346450310200453\n",
      "Step 97 (4296); Episode 64/100; Loss: 0.037224091589450836\n",
      "Step 98 (4297); Episode 64/100; Loss: 0.04252618923783302\n",
      "Step 99 (4298); Episode 64/100; Loss: 0.029596947133541107\n",
      "Step 100 (4299); Episode 64/100; Loss: 0.0045843604020774364\n",
      "Step 101 (4300); Episode 64/100; Loss: 0.06877809762954712\n",
      "Step 102 (4301); Episode 64/100; Loss: 0.06817417591810226\n",
      "Step 103 (4302); Episode 64/100; Loss: 0.05368549004197121\n",
      "Step 104 (4303); Episode 64/100; Loss: 0.07356713712215424\n",
      "Step 105 (4304); Episode 64/100; Loss: 0.002679050201550126\n",
      "Step 106 (4305); Episode 64/100; Loss: 0.03919069096446037\n",
      "Step 107 (4306); Episode 64/100; Loss: 0.0014934818027541041\n",
      "Step 108 (4307); Episode 64/100; Loss: 0.0015679332427680492\n",
      "Step 109 (4308); Episode 64/100; Loss: 0.016534889116883278\n",
      "Step 110 (4309); Episode 64/100; Loss: 0.04639515280723572\n",
      "Step 111 (4310); Episode 64/100; Loss: 0.0671081617474556\n",
      "Step 112 (4311); Episode 64/100; Loss: 0.01456580962985754\n",
      "Step 113 (4312); Episode 64/100; Loss: 0.017014430835843086\n",
      "Step 114 (4313); Episode 64/100; Loss: 0.004024224355816841\n",
      "Step 115 (4314); Episode 64/100; Loss: 0.017548568546772003\n",
      "Step 116 (4315); Episode 64/100; Loss: 0.04091794416308403\n",
      "Step 117 (4316); Episode 64/100; Loss: 0.0015048554632812738\n",
      "Step 118 (4317); Episode 64/100; Loss: 0.028823254629969597\n",
      "Step 119 (4318); Episode 64/100; Loss: 0.0010819343151524663\n",
      "Step 120 (4319); Episode 64/100; Loss: 0.031604915857315063\n",
      "Step 121 (4320); Episode 64/100; Loss: 0.0027873716317117214\n",
      "Step 122 (4321); Episode 64/100; Loss: 0.002972446382045746\n",
      "Step 123 (4322); Episode 64/100; Loss: 0.022511107847094536\n",
      "Step 124 (4323); Episode 64/100; Loss: 0.046665940433740616\n",
      "Step 125 (4324); Episode 64/100; Loss: 0.05792883783578873\n",
      "Step 126 (4325); Episode 64/100; Loss: 0.002601086860522628\n",
      "Step 127 (4326); Episode 64/100; Loss: 0.04005950689315796\n",
      "Step 128 (4327); Episode 64/100; Loss: 0.001345505123026669\n",
      "Step 129 (4328); Episode 64/100; Loss: 0.07577868551015854\n",
      "Step 130 (4329); Episode 64/100; Loss: 0.059536051005125046\n",
      "Step 131 (4330); Episode 64/100; Loss: 0.02929975837469101\n",
      "Step 132 (4331); Episode 64/100; Loss: 0.021750686690211296\n",
      "Step 133 (4332); Episode 64/100; Loss: 0.0027090844232589006\n",
      "Step 134 (4333); Episode 64/100; Loss: 0.004966920707374811\n",
      "Step 135 (4334); Episode 64/100; Loss: 0.037898704409599304\n",
      "Step 136 (4335); Episode 64/100; Loss: 0.058500174432992935\n",
      "Step 137 (4336); Episode 64/100; Loss: 0.042306844145059586\n",
      "Step 138 (4337); Episode 64/100; Loss: 0.027554525062441826\n",
      "Step 139 (4338); Episode 64/100; Loss: 0.015455675311386585\n",
      "Step 140 (4339); Episode 64/100; Loss: 0.02226443588733673\n",
      "Step 141 (4340); Episode 64/100; Loss: 0.00586365582421422\n",
      "Step 142 (4341); Episode 64/100; Loss: 0.0025537298060953617\n",
      "Step 143 (4342); Episode 64/100; Loss: 0.0020662969909608364\n",
      "Step 144 (4343); Episode 64/100; Loss: 0.03378986939787865\n",
      "Step 145 (4344); Episode 64/100; Loss: 0.0037062671035528183\n",
      "Step 146 (4345); Episode 64/100; Loss: 0.002533033723011613\n",
      "Step 147 (4346); Episode 64/100; Loss: 0.019094469025731087\n",
      "Step 148 (4347); Episode 64/100; Loss: 0.0357702299952507\n",
      "Step 149 (4348); Episode 64/100; Loss: 0.09410666674375534\n",
      "Step 150 (4349); Episode 64/100; Loss: 0.059330299496650696\n",
      "Step 151 (4350); Episode 64/100; Loss: 0.022371307015419006\n",
      "Step 152 (4351); Episode 64/100; Loss: 0.003954598680138588\n",
      "Step 153 (4352); Episode 64/100; Loss: 0.07704627513885498\n",
      "Step 154 (4353); Episode 64/100; Loss: 0.012062589637935162\n",
      "Step 155 (4354); Episode 64/100; Loss: 0.002430712105706334\n",
      "Step 156 (4355); Episode 64/100; Loss: 0.04873046651482582\n",
      "Step 157 (4356); Episode 64/100; Loss: 0.0016535237664356828\n",
      "Step 158 (4357); Episode 64/100; Loss: 0.08119546622037888\n",
      "Step 159 (4358); Episode 64/100; Loss: 0.04598095640540123\n",
      "Step 160 (4359); Episode 64/100; Loss: 0.01693080924451351\n",
      "Step 161 (4360); Episode 64/100; Loss: 0.04833701252937317\n",
      "Step 162 (4361); Episode 64/100; Loss: 0.13544592261314392\n",
      "Step 163 (4362); Episode 64/100; Loss: 0.03773307427763939\n",
      "Step 164 (4363); Episode 64/100; Loss: 0.004331744275987148\n",
      "Step 165 (4364); Episode 64/100; Loss: 0.07856851071119308\n",
      "Step 166 (4365); Episode 64/100; Loss: 0.036164700984954834\n",
      "Step 167 (4366); Episode 64/100; Loss: 0.0017705170903354883\n",
      "Step 168 (4367); Episode 64/100; Loss: 0.0962810143828392\n",
      "Step 169 (4368); Episode 64/100; Loss: 0.01709122397005558\n",
      "Step 170 (4369); Episode 64/100; Loss: 0.01013953611254692\n",
      "Step 0 (4370); Episode 65/100; Loss: 0.03698388859629631\n",
      "Step 1 (4371); Episode 65/100; Loss: 0.007533160969614983\n",
      "Step 2 (4372); Episode 65/100; Loss: 0.0015598749741911888\n",
      "Step 3 (4373); Episode 65/100; Loss: 0.004377111326903105\n",
      "Step 4 (4374); Episode 65/100; Loss: 0.0015667153056710958\n",
      "Step 5 (4375); Episode 65/100; Loss: 0.00830022431910038\n",
      "Step 6 (4376); Episode 65/100; Loss: 0.0013226408045738935\n",
      "Step 7 (4377); Episode 65/100; Loss: 0.10907148569822311\n",
      "Step 8 (4378); Episode 65/100; Loss: 0.04508790001273155\n",
      "Step 9 (4379); Episode 65/100; Loss: 0.05990169569849968\n",
      "Step 10 (4380); Episode 65/100; Loss: 0.031072961166501045\n",
      "Step 11 (4381); Episode 65/100; Loss: 0.0023055756464600563\n",
      "Step 12 (4382); Episode 65/100; Loss: 0.05222219601273537\n",
      "Step 13 (4383); Episode 65/100; Loss: 0.04230521619319916\n",
      "Step 14 (4384); Episode 65/100; Loss: 0.0030545422341674566\n",
      "Step 15 (4385); Episode 65/100; Loss: 0.0239785797894001\n",
      "Step 16 (4386); Episode 65/100; Loss: 0.05426394194364548\n",
      "Step 17 (4387); Episode 65/100; Loss: 0.012783304788172245\n",
      "Step 18 (4388); Episode 65/100; Loss: 0.11164917796850204\n",
      "Step 19 (4389); Episode 65/100; Loss: 0.051481325179338455\n",
      "Step 20 (4390); Episode 65/100; Loss: 0.0229922067373991\n",
      "Step 21 (4391); Episode 65/100; Loss: 0.0742216631770134\n",
      "Step 22 (4392); Episode 65/100; Loss: 0.0517144538462162\n",
      "Step 23 (4393); Episode 65/100; Loss: 0.0027343418914824724\n",
      "Step 24 (4394); Episode 65/100; Loss: 0.004605640657246113\n",
      "Step 25 (4395); Episode 65/100; Loss: 0.04971056431531906\n",
      "Step 26 (4396); Episode 65/100; Loss: 0.0024987540673464537\n",
      "Step 27 (4397); Episode 65/100; Loss: 0.002937149489298463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28 (4398); Episode 65/100; Loss: 0.015287963673472404\n",
      "Step 29 (4399); Episode 65/100; Loss: 0.10071252286434174\n",
      "Step 30 (4400); Episode 65/100; Loss: 0.047584254294633865\n",
      "Step 31 (4401); Episode 65/100; Loss: 0.004211804363876581\n",
      "Step 32 (4402); Episode 65/100; Loss: 0.0407722033560276\n",
      "Step 33 (4403); Episode 65/100; Loss: 0.04985058307647705\n",
      "Step 34 (4404); Episode 65/100; Loss: 0.03972666338086128\n",
      "Step 35 (4405); Episode 65/100; Loss: 0.04518122225999832\n",
      "Step 36 (4406); Episode 65/100; Loss: 0.007022263016551733\n",
      "Step 37 (4407); Episode 65/100; Loss: 0.005935307126492262\n",
      "Step 38 (4408); Episode 65/100; Loss: 0.04442358389496803\n",
      "Step 39 (4409); Episode 65/100; Loss: 0.06826652586460114\n",
      "Step 40 (4410); Episode 65/100; Loss: 0.048116739839315414\n",
      "Step 41 (4411); Episode 65/100; Loss: 0.030733788385987282\n",
      "Step 42 (4412); Episode 65/100; Loss: 0.0032836636528372765\n",
      "Step 43 (4413); Episode 65/100; Loss: 0.005140227265655994\n",
      "Step 44 (4414); Episode 65/100; Loss: 0.001484401524066925\n",
      "Step 45 (4415); Episode 65/100; Loss: 0.018547942861914635\n",
      "Step 46 (4416); Episode 65/100; Loss: 0.031879059970378876\n",
      "Step 47 (4417); Episode 65/100; Loss: 0.003683012444525957\n",
      "Step 48 (4418); Episode 65/100; Loss: 0.05645078420639038\n",
      "Step 49 (4419); Episode 65/100; Loss: 0.006177891511470079\n",
      "Step 50 (4420); Episode 65/100; Loss: 0.04679500684142113\n",
      "Step 51 (4421); Episode 65/100; Loss: 0.005130934063345194\n",
      "Step 52 (4422); Episode 65/100; Loss: 0.03171209990978241\n",
      "Step 53 (4423); Episode 65/100; Loss: 0.001517438911832869\n",
      "Step 54 (4424); Episode 65/100; Loss: 0.05159107595682144\n",
      "Step 55 (4425); Episode 65/100; Loss: 0.08665281534194946\n",
      "Step 56 (4426); Episode 65/100; Loss: 0.0035289591178297997\n",
      "Step 57 (4427); Episode 65/100; Loss: 0.09099874645471573\n",
      "Step 58 (4428); Episode 65/100; Loss: 0.00465482659637928\n",
      "Step 59 (4429); Episode 65/100; Loss: 0.030677219852805138\n",
      "Step 60 (4430); Episode 65/100; Loss: 0.004109453409910202\n",
      "Step 61 (4431); Episode 65/100; Loss: 0.005819385405629873\n",
      "Step 62 (4432); Episode 65/100; Loss: 0.04236794635653496\n",
      "Step 63 (4433); Episode 65/100; Loss: 0.0049021607264876366\n",
      "Step 64 (4434); Episode 65/100; Loss: 0.09297414124011993\n",
      "Step 65 (4435); Episode 65/100; Loss: 0.01765766367316246\n",
      "Step 66 (4436); Episode 65/100; Loss: 0.0017768967663869262\n",
      "Step 67 (4437); Episode 65/100; Loss: 0.08075018972158432\n",
      "Step 68 (4438); Episode 65/100; Loss: 0.03317692130804062\n",
      "Step 69 (4439); Episode 65/100; Loss: 0.0019754371605813503\n",
      "Step 70 (4440); Episode 65/100; Loss: 0.002125727478414774\n",
      "Step 71 (4441); Episode 65/100; Loss: 0.036593131721019745\n",
      "Step 72 (4442); Episode 65/100; Loss: 0.002193568740040064\n",
      "Step 73 (4443); Episode 65/100; Loss: 0.06789399683475494\n",
      "Step 74 (4444); Episode 65/100; Loss: 0.023506924510002136\n",
      "Step 75 (4445); Episode 65/100; Loss: 0.0611661933362484\n",
      "Step 76 (4446); Episode 65/100; Loss: 0.0013917532050982118\n",
      "Step 77 (4447); Episode 65/100; Loss: 0.007190287113189697\n",
      "Step 78 (4448); Episode 65/100; Loss: 0.002980572870001197\n",
      "Step 79 (4449); Episode 65/100; Loss: 0.022435717284679413\n",
      "Step 80 (4450); Episode 65/100; Loss: 0.026876483112573624\n",
      "Step 81 (4451); Episode 65/100; Loss: 0.05641448497772217\n",
      "Step 82 (4452); Episode 65/100; Loss: 0.11177404969930649\n",
      "Step 83 (4453); Episode 65/100; Loss: 0.011430127546191216\n",
      "Step 84 (4454); Episode 65/100; Loss: 0.0639539361000061\n",
      "Step 85 (4455); Episode 65/100; Loss: 0.006354139186441898\n",
      "Step 86 (4456); Episode 65/100; Loss: 0.00172383151948452\n",
      "Step 87 (4457); Episode 65/100; Loss: 0.0037216448690742254\n",
      "Step 88 (4458); Episode 65/100; Loss: 0.05533255264163017\n",
      "Step 89 (4459); Episode 65/100; Loss: 0.03818340227007866\n",
      "Step 90 (4460); Episode 65/100; Loss: 0.03702330216765404\n",
      "Step 91 (4461); Episode 65/100; Loss: 0.11042914539575577\n",
      "Step 92 (4462); Episode 65/100; Loss: 0.0014399446081370115\n",
      "Step 93 (4463); Episode 65/100; Loss: 0.018413616344332695\n",
      "Step 94 (4464); Episode 65/100; Loss: 0.020250428467988968\n",
      "Step 95 (4465); Episode 65/100; Loss: 0.052655935287475586\n",
      "Step 96 (4466); Episode 65/100; Loss: 0.005246264860033989\n",
      "Step 97 (4467); Episode 65/100; Loss: 0.0022735954262316227\n",
      "Step 98 (4468); Episode 65/100; Loss: 0.10637276619672775\n",
      "Step 99 (4469); Episode 65/100; Loss: 0.042951032519340515\n",
      "Step 100 (4470); Episode 65/100; Loss: 0.02421882376074791\n",
      "Step 101 (4471); Episode 65/100; Loss: 0.04065734148025513\n",
      "Step 102 (4472); Episode 65/100; Loss: 0.004349951632320881\n",
      "Step 103 (4473); Episode 65/100; Loss: 0.009715789929032326\n",
      "Step 104 (4474); Episode 65/100; Loss: 0.010855074971914291\n",
      "Step 105 (4475); Episode 65/100; Loss: 0.062815360724926\n",
      "Step 106 (4476); Episode 65/100; Loss: 0.0501759871840477\n",
      "Step 107 (4477); Episode 65/100; Loss: 0.004875386133790016\n",
      "Step 108 (4478); Episode 65/100; Loss: 0.04609004408121109\n",
      "Step 109 (4479); Episode 65/100; Loss: 0.004330926109105349\n",
      "Step 110 (4480); Episode 65/100; Loss: 0.04600727930665016\n",
      "Step 111 (4481); Episode 65/100; Loss: 0.01705961301922798\n",
      "Step 112 (4482); Episode 65/100; Loss: 0.03811660036444664\n",
      "Step 113 (4483); Episode 65/100; Loss: 0.05522920563817024\n",
      "Step 114 (4484); Episode 65/100; Loss: 0.006411128211766481\n",
      "Step 115 (4485); Episode 65/100; Loss: 0.04355192929506302\n",
      "Step 116 (4486); Episode 65/100; Loss: 0.04580901935696602\n",
      "Step 117 (4487); Episode 65/100; Loss: 0.002667586551979184\n",
      "Step 118 (4488); Episode 65/100; Loss: 0.1684795618057251\n",
      "Step 119 (4489); Episode 65/100; Loss: 0.058535728603601456\n",
      "Step 120 (4490); Episode 65/100; Loss: 0.14206187427043915\n",
      "Step 121 (4491); Episode 65/100; Loss: 0.004155976697802544\n",
      "Step 122 (4492); Episode 65/100; Loss: 0.04087494686245918\n",
      "Step 123 (4493); Episode 65/100; Loss: 0.046137675642967224\n",
      "Step 124 (4494); Episode 65/100; Loss: 0.005500658880919218\n",
      "Step 125 (4495); Episode 65/100; Loss: 0.0021573244594037533\n",
      "Step 126 (4496); Episode 65/100; Loss: 0.0038284496404230595\n",
      "Step 127 (4497); Episode 65/100; Loss: 0.03810643032193184\n",
      "Step 128 (4498); Episode 65/100; Loss: 0.0483330562710762\n",
      "Step 129 (4499); Episode 65/100; Loss: 0.026632478460669518\n",
      "Step 130 (4500); Episode 65/100; Loss: 0.1176794171333313\n",
      "Step 131 (4501); Episode 65/100; Loss: 0.0018111050594598055\n",
      "Step 132 (4502); Episode 65/100; Loss: 0.03850845992565155\n",
      "Step 133 (4503); Episode 65/100; Loss: 0.05958132445812225\n",
      "Step 134 (4504); Episode 65/100; Loss: 0.003122942754998803\n",
      "Step 0 (4505); Episode 66/100; Loss: 0.009555154480040073\n",
      "Step 1 (4506); Episode 66/100; Loss: 0.0460638664662838\n",
      "Step 2 (4507); Episode 66/100; Loss: 0.026049936190247536\n",
      "Step 3 (4508); Episode 66/100; Loss: 0.031169336289167404\n",
      "Step 4 (4509); Episode 66/100; Loss: 0.05613807961344719\n",
      "Step 5 (4510); Episode 66/100; Loss: 0.04349900409579277\n",
      "Step 6 (4511); Episode 66/100; Loss: 0.019188251346349716\n",
      "Step 7 (4512); Episode 66/100; Loss: 0.04031635820865631\n",
      "Step 8 (4513); Episode 66/100; Loss: 0.009214982390403748\n",
      "Step 9 (4514); Episode 66/100; Loss: 0.0033824369311332703\n",
      "Step 10 (4515); Episode 66/100; Loss: 0.0775170624256134\n",
      "Step 11 (4516); Episode 66/100; Loss: 0.051157910376787186\n",
      "Step 12 (4517); Episode 66/100; Loss: 0.003879242343828082\n",
      "Step 13 (4518); Episode 66/100; Loss: 0.04827870428562164\n",
      "Step 14 (4519); Episode 66/100; Loss: 0.08684752136468887\n",
      "Step 15 (4520); Episode 66/100; Loss: 0.019739322364330292\n",
      "Step 16 (4521); Episode 66/100; Loss: 0.04001213237643242\n",
      "Step 17 (4522); Episode 66/100; Loss: 0.005896530579775572\n",
      "Step 18 (4523); Episode 66/100; Loss: 0.10787174850702286\n",
      "Step 19 (4524); Episode 66/100; Loss: 0.004101664759218693\n",
      "Step 20 (4525); Episode 66/100; Loss: 0.08791295439004898\n",
      "Step 21 (4526); Episode 66/100; Loss: 0.003960539121180773\n",
      "Step 22 (4527); Episode 66/100; Loss: 0.038398656994104385\n",
      "Step 23 (4528); Episode 66/100; Loss: 0.03930630162358284\n",
      "Step 24 (4529); Episode 66/100; Loss: 0.06337018311023712\n",
      "Step 25 (4530); Episode 66/100; Loss: 0.001362908398732543\n",
      "Step 26 (4531); Episode 66/100; Loss: 0.004469499923288822\n",
      "Step 27 (4532); Episode 66/100; Loss: 0.08508536964654922\n",
      "Step 28 (4533); Episode 66/100; Loss: 0.0037891825195401907\n",
      "Step 29 (4534); Episode 66/100; Loss: 0.064791239798069\n",
      "Step 30 (4535); Episode 66/100; Loss: 0.003924352582544088\n",
      "Step 31 (4536); Episode 66/100; Loss: 0.06391534209251404\n",
      "Step 32 (4537); Episode 66/100; Loss: 0.08692401647567749\n",
      "Step 33 (4538); Episode 66/100; Loss: 0.055630914866924286\n",
      "Step 34 (4539); Episode 66/100; Loss: 0.001166097354143858\n",
      "Step 35 (4540); Episode 66/100; Loss: 0.040440138429403305\n",
      "Step 36 (4541); Episode 66/100; Loss: 0.03325018286705017\n",
      "Step 37 (4542); Episode 66/100; Loss: 0.05296817049384117\n",
      "Step 38 (4543); Episode 66/100; Loss: 0.004084563814103603\n",
      "Step 39 (4544); Episode 66/100; Loss: 0.0014520421391353011\n",
      "Step 40 (4545); Episode 66/100; Loss: 0.09904921799898148\n",
      "Step 41 (4546); Episode 66/100; Loss: 0.0632091537117958\n",
      "Step 42 (4547); Episode 66/100; Loss: 0.0018840826814994216\n",
      "Step 43 (4548); Episode 66/100; Loss: 0.031935252249240875\n",
      "Step 44 (4549); Episode 66/100; Loss: 0.07495910674333572\n",
      "Step 45 (4550); Episode 66/100; Loss: 0.05197494477033615\n",
      "Step 46 (4551); Episode 66/100; Loss: 0.05120134353637695\n",
      "Step 47 (4552); Episode 66/100; Loss: 0.07778461277484894\n",
      "Step 48 (4553); Episode 66/100; Loss: 0.0028096723835915327\n",
      "Step 49 (4554); Episode 66/100; Loss: 0.0019393425900489092\n",
      "Step 50 (4555); Episode 66/100; Loss: 0.07402823865413666\n",
      "Step 51 (4556); Episode 66/100; Loss: 0.015802109614014626\n",
      "Step 52 (4557); Episode 66/100; Loss: 0.042557548731565475\n",
      "Step 53 (4558); Episode 66/100; Loss: 0.09326429665088654\n",
      "Step 54 (4559); Episode 66/100; Loss: 0.06033335253596306\n",
      "Step 55 (4560); Episode 66/100; Loss: 0.004130986053496599\n",
      "Step 56 (4561); Episode 66/100; Loss: 0.032723572105169296\n",
      "Step 57 (4562); Episode 66/100; Loss: 0.0019298753468319774\n",
      "Step 58 (4563); Episode 66/100; Loss: 0.0028467762749642134\n",
      "Step 59 (4564); Episode 66/100; Loss: 0.039168354123830795\n",
      "Step 60 (4565); Episode 66/100; Loss: 0.030758086591959\n",
      "Step 61 (4566); Episode 66/100; Loss: 0.0010806163772940636\n",
      "Step 62 (4567); Episode 66/100; Loss: 0.06743830442428589\n",
      "Step 63 (4568); Episode 66/100; Loss: 0.03669341653585434\n",
      "Step 64 (4569); Episode 66/100; Loss: 0.0012773096095770597\n",
      "Step 65 (4570); Episode 66/100; Loss: 0.0009740198729559779\n",
      "Step 66 (4571); Episode 66/100; Loss: 0.042593203485012054\n",
      "Step 67 (4572); Episode 66/100; Loss: 0.08540607243776321\n",
      "Step 68 (4573); Episode 66/100; Loss: 0.001767441164702177\n",
      "Step 69 (4574); Episode 66/100; Loss: 0.003504799911752343\n",
      "Step 70 (4575); Episode 66/100; Loss: 0.05785197764635086\n",
      "Step 71 (4576); Episode 66/100; Loss: 0.0014422370586544275\n",
      "Step 72 (4577); Episode 66/100; Loss: 0.00927228294312954\n",
      "Step 73 (4578); Episode 66/100; Loss: 0.0034620631486177444\n",
      "Step 74 (4579); Episode 66/100; Loss: 0.001697040512226522\n",
      "Step 75 (4580); Episode 66/100; Loss: 0.04715214669704437\n",
      "Step 76 (4581); Episode 66/100; Loss: 0.052514925599098206\n",
      "Step 77 (4582); Episode 66/100; Loss: 0.04148648679256439\n",
      "Step 78 (4583); Episode 66/100; Loss: 0.0661177858710289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 79 (4584); Episode 66/100; Loss: 0.014998557977378368\n",
      "Step 80 (4585); Episode 66/100; Loss: 0.07598791271448135\n",
      "Step 81 (4586); Episode 66/100; Loss: 0.0010673473589122295\n",
      "Step 82 (4587); Episode 66/100; Loss: 0.006244472693651915\n",
      "Step 83 (4588); Episode 66/100; Loss: 0.003409525379538536\n",
      "Step 84 (4589); Episode 66/100; Loss: 0.002582641551271081\n",
      "Step 85 (4590); Episode 66/100; Loss: 0.07939053326845169\n",
      "Step 86 (4591); Episode 66/100; Loss: 0.05640142410993576\n",
      "Step 87 (4592); Episode 66/100; Loss: 0.07821113616228104\n",
      "Step 88 (4593); Episode 66/100; Loss: 0.0017994671361520886\n",
      "Step 89 (4594); Episode 66/100; Loss: 0.026578836143016815\n",
      "Step 90 (4595); Episode 66/100; Loss: 0.0042993188835680485\n",
      "Step 91 (4596); Episode 66/100; Loss: 0.08191532641649246\n",
      "Step 92 (4597); Episode 66/100; Loss: 0.029161175712943077\n",
      "Step 93 (4598); Episode 66/100; Loss: 0.04309089854359627\n",
      "Step 94 (4599); Episode 66/100; Loss: 0.04755544662475586\n",
      "Step 95 (4600); Episode 66/100; Loss: 0.003415792714804411\n",
      "Step 96 (4601); Episode 66/100; Loss: 0.005195619538426399\n",
      "Step 97 (4602); Episode 66/100; Loss: 0.008629539981484413\n",
      "Step 98 (4603); Episode 66/100; Loss: 0.006242032162845135\n",
      "Step 99 (4604); Episode 66/100; Loss: 0.04075121507048607\n",
      "Step 100 (4605); Episode 66/100; Loss: 0.03832586854696274\n",
      "Step 101 (4606); Episode 66/100; Loss: 0.048151835799217224\n",
      "Step 102 (4607); Episode 66/100; Loss: 0.04781857132911682\n",
      "Step 103 (4608); Episode 66/100; Loss: 0.05039137229323387\n",
      "Step 104 (4609); Episode 66/100; Loss: 0.0033673669677227736\n",
      "Step 105 (4610); Episode 66/100; Loss: 0.03897818177938461\n",
      "Step 106 (4611); Episode 66/100; Loss: 0.06101689860224724\n",
      "Step 107 (4612); Episode 66/100; Loss: 0.003039035014808178\n",
      "Step 108 (4613); Episode 66/100; Loss: 0.0020152737852185965\n",
      "Step 109 (4614); Episode 66/100; Loss: 0.06998012959957123\n",
      "Step 110 (4615); Episode 66/100; Loss: 0.020571548491716385\n",
      "Step 111 (4616); Episode 66/100; Loss: 0.0031597930938005447\n",
      "Step 112 (4617); Episode 66/100; Loss: 0.058680351823568344\n",
      "Step 113 (4618); Episode 66/100; Loss: 0.030440157279372215\n",
      "Step 114 (4619); Episode 66/100; Loss: 0.040763210505247116\n",
      "Step 115 (4620); Episode 66/100; Loss: 0.002021678490564227\n",
      "Step 116 (4621); Episode 66/100; Loss: 0.044504277408123016\n",
      "Step 117 (4622); Episode 66/100; Loss: 0.003113770391792059\n",
      "Step 118 (4623); Episode 66/100; Loss: 0.05587806925177574\n",
      "Step 119 (4624); Episode 66/100; Loss: 0.0017194167012348771\n",
      "Step 120 (4625); Episode 66/100; Loss: 0.0013825111091136932\n",
      "Step 121 (4626); Episode 66/100; Loss: 0.06023276224732399\n",
      "Step 122 (4627); Episode 66/100; Loss: 0.02550545148551464\n",
      "Step 123 (4628); Episode 66/100; Loss: 0.0936129242181778\n",
      "Step 124 (4629); Episode 66/100; Loss: 0.021690579131245613\n",
      "Step 125 (4630); Episode 66/100; Loss: 0.0037323336582630873\n",
      "Step 126 (4631); Episode 66/100; Loss: 0.01600193791091442\n",
      "Step 127 (4632); Episode 66/100; Loss: 0.0034459149464964867\n",
      "Step 128 (4633); Episode 66/100; Loss: 0.07575756311416626\n",
      "Step 129 (4634); Episode 66/100; Loss: 0.04592462256550789\n",
      "Step 130 (4635); Episode 66/100; Loss: 0.03782815858721733\n",
      "Step 131 (4636); Episode 66/100; Loss: 0.010862243361771107\n",
      "Step 132 (4637); Episode 66/100; Loss: 0.058456432074308395\n",
      "Step 133 (4638); Episode 66/100; Loss: 0.0017079721437767148\n",
      "Step 134 (4639); Episode 66/100; Loss: 0.044475868344306946\n",
      "Step 135 (4640); Episode 66/100; Loss: 0.0009073520195670426\n",
      "Step 136 (4641); Episode 66/100; Loss: 0.12025507539510727\n",
      "Step 137 (4642); Episode 66/100; Loss: 0.05090782791376114\n",
      "Step 138 (4643); Episode 66/100; Loss: 0.07550489157438278\n",
      "Step 139 (4644); Episode 66/100; Loss: 0.013310085982084274\n",
      "Step 140 (4645); Episode 66/100; Loss: 0.0028295591473579407\n",
      "Step 141 (4646); Episode 66/100; Loss: 0.006521627306938171\n",
      "Step 142 (4647); Episode 66/100; Loss: 0.06570208817720413\n",
      "Step 143 (4648); Episode 66/100; Loss: 0.028883444145321846\n",
      "Step 144 (4649); Episode 66/100; Loss: 0.0016079943161457777\n",
      "Step 145 (4650); Episode 66/100; Loss: 0.0050355009734630585\n",
      "Step 146 (4651); Episode 66/100; Loss: 0.0014081487897783518\n",
      "Step 147 (4652); Episode 66/100; Loss: 0.0021470701321959496\n",
      "Step 148 (4653); Episode 66/100; Loss: 0.0029391383286565542\n",
      "Step 149 (4654); Episode 66/100; Loss: 0.0013989503495395184\n",
      "Step 150 (4655); Episode 66/100; Loss: 0.002837566426023841\n",
      "Step 151 (4656); Episode 66/100; Loss: 0.001066847937181592\n",
      "Step 152 (4657); Episode 66/100; Loss: 0.0019638282246887684\n",
      "Step 153 (4658); Episode 66/100; Loss: 0.05038686469197273\n",
      "Step 154 (4659); Episode 66/100; Loss: 0.002899464685469866\n",
      "Step 155 (4660); Episode 66/100; Loss: 0.0020388218108564615\n",
      "Step 156 (4661); Episode 66/100; Loss: 0.0009215128957293928\n",
      "Step 157 (4662); Episode 66/100; Loss: 0.06171633303165436\n",
      "Step 158 (4663); Episode 66/100; Loss: 0.0017199215944856405\n",
      "Step 159 (4664); Episode 66/100; Loss: 0.040511295199394226\n",
      "Step 160 (4665); Episode 66/100; Loss: 0.03496265038847923\n",
      "Step 161 (4666); Episode 66/100; Loss: 0.002992805792018771\n",
      "Step 162 (4667); Episode 66/100; Loss: 0.1716887354850769\n",
      "Step 163 (4668); Episode 66/100; Loss: 0.00366739509627223\n",
      "Step 164 (4669); Episode 66/100; Loss: 0.00470111845061183\n",
      "Step 0 (4670); Episode 67/100; Loss: 0.0020990935154259205\n",
      "Step 1 (4671); Episode 67/100; Loss: 0.0009909250074997544\n",
      "Step 2 (4672); Episode 67/100; Loss: 0.01600894145667553\n",
      "Step 3 (4673); Episode 67/100; Loss: 0.11382656544446945\n",
      "Step 4 (4674); Episode 67/100; Loss: 0.0015516155399382114\n",
      "Step 5 (4675); Episode 67/100; Loss: 0.041198309510946274\n",
      "Step 6 (4676); Episode 67/100; Loss: 0.045973170548677444\n",
      "Step 7 (4677); Episode 67/100; Loss: 0.04465824365615845\n",
      "Step 8 (4678); Episode 67/100; Loss: 0.06958755850791931\n",
      "Step 9 (4679); Episode 67/100; Loss: 0.06166534870862961\n",
      "Step 10 (4680); Episode 67/100; Loss: 0.003290401306003332\n",
      "Step 11 (4681); Episode 67/100; Loss: 0.004876145161688328\n",
      "Step 12 (4682); Episode 67/100; Loss: 0.009309039451181889\n",
      "Step 13 (4683); Episode 67/100; Loss: 0.005130323115736246\n",
      "Step 14 (4684); Episode 67/100; Loss: 0.003015062538906932\n",
      "Step 15 (4685); Episode 67/100; Loss: 0.025583311915397644\n",
      "Step 16 (4686); Episode 67/100; Loss: 0.003145374823361635\n",
      "Step 17 (4687); Episode 67/100; Loss: 0.09675043076276779\n",
      "Step 18 (4688); Episode 67/100; Loss: 0.04320593923330307\n",
      "Step 19 (4689); Episode 67/100; Loss: 0.03818758577108383\n",
      "Step 20 (4690); Episode 67/100; Loss: 0.011700239963829517\n",
      "Step 21 (4691); Episode 67/100; Loss: 0.04299783334136009\n",
      "Step 22 (4692); Episode 67/100; Loss: 0.01870795525610447\n",
      "Step 23 (4693); Episode 67/100; Loss: 0.0014653627295047045\n",
      "Step 24 (4694); Episode 67/100; Loss: 0.07249224931001663\n",
      "Step 25 (4695); Episode 67/100; Loss: 0.0020454158075153828\n",
      "Step 26 (4696); Episode 67/100; Loss: 0.04220842197537422\n",
      "Step 27 (4697); Episode 67/100; Loss: 0.10252579301595688\n",
      "Step 28 (4698); Episode 67/100; Loss: 0.08998274058103561\n",
      "Step 29 (4699); Episode 67/100; Loss: 0.06319554895162582\n",
      "Step 30 (4700); Episode 67/100; Loss: 0.04695799946784973\n",
      "Step 31 (4701); Episode 67/100; Loss: 0.0016017780872061849\n",
      "Step 32 (4702); Episode 67/100; Loss: 0.14291024208068848\n",
      "Step 33 (4703); Episode 67/100; Loss: 0.003265642561018467\n",
      "Step 34 (4704); Episode 67/100; Loss: 0.06949594616889954\n",
      "Step 35 (4705); Episode 67/100; Loss: 0.0025592418387532234\n",
      "Step 36 (4706); Episode 67/100; Loss: 0.07171794772148132\n",
      "Step 37 (4707); Episode 67/100; Loss: 0.0017350277630612254\n",
      "Step 38 (4708); Episode 67/100; Loss: 0.0023981835693120956\n",
      "Step 39 (4709); Episode 67/100; Loss: 0.0027377265505492687\n",
      "Step 40 (4710); Episode 67/100; Loss: 0.046597521752119064\n",
      "Step 41 (4711); Episode 67/100; Loss: 0.0017342286882922053\n",
      "Step 42 (4712); Episode 67/100; Loss: 0.043729960918426514\n",
      "Step 43 (4713); Episode 67/100; Loss: 0.04296790435910225\n",
      "Step 44 (4714); Episode 67/100; Loss: 0.11787594854831696\n",
      "Step 45 (4715); Episode 67/100; Loss: 0.00193374278023839\n",
      "Step 46 (4716); Episode 67/100; Loss: 0.0023074159398674965\n",
      "Step 47 (4717); Episode 67/100; Loss: 0.002138359472155571\n",
      "Step 48 (4718); Episode 67/100; Loss: 0.0007809389499016106\n",
      "Step 49 (4719); Episode 67/100; Loss: 0.051214683800935745\n",
      "Step 50 (4720); Episode 67/100; Loss: 0.0025534825399518013\n",
      "Step 51 (4721); Episode 67/100; Loss: 0.040982913225889206\n",
      "Step 52 (4722); Episode 67/100; Loss: 0.04204485937952995\n",
      "Step 53 (4723); Episode 67/100; Loss: 0.00105532247107476\n",
      "Step 54 (4724); Episode 67/100; Loss: 0.004637629259377718\n",
      "Step 55 (4725); Episode 67/100; Loss: 0.042053475975990295\n",
      "Step 56 (4726); Episode 67/100; Loss: 0.004145269747823477\n",
      "Step 57 (4727); Episode 67/100; Loss: 0.0014587565092369914\n",
      "Step 58 (4728); Episode 67/100; Loss: 0.0023944510612636805\n",
      "Step 59 (4729); Episode 67/100; Loss: 0.002146487357094884\n",
      "Step 60 (4730); Episode 67/100; Loss: 0.12413864582777023\n",
      "Step 61 (4731); Episode 67/100; Loss: 0.0017534141661599278\n",
      "Step 62 (4732); Episode 67/100; Loss: 0.0020294268615543842\n",
      "Step 63 (4733); Episode 67/100; Loss: 0.08983305096626282\n",
      "Step 64 (4734); Episode 67/100; Loss: 0.05667192116379738\n",
      "Step 65 (4735); Episode 67/100; Loss: 0.004700086545199156\n",
      "Step 66 (4736); Episode 67/100; Loss: 0.07383687049150467\n",
      "Step 67 (4737); Episode 67/100; Loss: 0.02876961976289749\n",
      "Step 68 (4738); Episode 67/100; Loss: 0.05612931028008461\n",
      "Step 69 (4739); Episode 67/100; Loss: 0.0019860591273754835\n",
      "Step 70 (4740); Episode 67/100; Loss: 0.05066182091832161\n",
      "Step 71 (4741); Episode 67/100; Loss: 0.0828036218881607\n",
      "Step 72 (4742); Episode 67/100; Loss: 0.055304769426584244\n",
      "Step 73 (4743); Episode 67/100; Loss: 0.08195416629314423\n",
      "Step 74 (4744); Episode 67/100; Loss: 0.042465392500162125\n",
      "Step 75 (4745); Episode 67/100; Loss: 0.002886537229642272\n",
      "Step 76 (4746); Episode 67/100; Loss: 0.053842153400182724\n",
      "Step 77 (4747); Episode 67/100; Loss: 0.03136168792843819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 78 (4748); Episode 67/100; Loss: 0.05147996172308922\n",
      "Step 79 (4749); Episode 67/100; Loss: 0.0018816639203578234\n",
      "Step 80 (4750); Episode 67/100; Loss: 0.0011252214899286628\n",
      "Step 81 (4751); Episode 67/100; Loss: 0.08578483015298843\n",
      "Step 82 (4752); Episode 67/100; Loss: 0.0651109591126442\n",
      "Step 83 (4753); Episode 67/100; Loss: 0.0037606838159263134\n",
      "Step 84 (4754); Episode 67/100; Loss: 0.04289594292640686\n",
      "Step 85 (4755); Episode 67/100; Loss: 0.0023248614743351936\n",
      "Step 86 (4756); Episode 67/100; Loss: 0.002513377694413066\n",
      "Step 87 (4757); Episode 67/100; Loss: 0.08201265335083008\n",
      "Step 88 (4758); Episode 67/100; Loss: 0.0025721874553710222\n",
      "Step 89 (4759); Episode 67/100; Loss: 0.06447595357894897\n",
      "Step 90 (4760); Episode 67/100; Loss: 0.0033510385546833277\n",
      "Step 91 (4761); Episode 67/100; Loss: 0.002823039423674345\n",
      "Step 92 (4762); Episode 67/100; Loss: 0.003136119106784463\n",
      "Step 93 (4763); Episode 67/100; Loss: 0.0020759538747370243\n",
      "Step 94 (4764); Episode 67/100; Loss: 0.02611512318253517\n",
      "Step 95 (4765); Episode 67/100; Loss: 0.04770132526755333\n",
      "Step 96 (4766); Episode 67/100; Loss: 0.002962397411465645\n",
      "Step 97 (4767); Episode 67/100; Loss: 0.0011331494897603989\n",
      "Step 98 (4768); Episode 67/100; Loss: 0.09728928655385971\n",
      "Step 99 (4769); Episode 67/100; Loss: 0.0015148799866437912\n",
      "Step 100 (4770); Episode 67/100; Loss: 0.003510977141559124\n",
      "Step 101 (4771); Episode 67/100; Loss: 0.030536402016878128\n",
      "Step 102 (4772); Episode 67/100; Loss: 0.0009115856373682618\n",
      "Step 103 (4773); Episode 67/100; Loss: 0.0017454818589612842\n",
      "Step 104 (4774); Episode 67/100; Loss: 0.0028805225156247616\n",
      "Step 105 (4775); Episode 67/100; Loss: 0.0006663163076154888\n",
      "Step 106 (4776); Episode 67/100; Loss: 0.040928494185209274\n",
      "Step 107 (4777); Episode 67/100; Loss: 0.0008822219097055495\n",
      "Step 108 (4778); Episode 67/100; Loss: 0.0034781747963279486\n",
      "Step 109 (4779); Episode 67/100; Loss: 0.041085291653871536\n",
      "Step 110 (4780); Episode 67/100; Loss: 0.05251774936914444\n",
      "Step 111 (4781); Episode 67/100; Loss: 0.02710946463048458\n",
      "Step 112 (4782); Episode 67/100; Loss: 0.0022844355553388596\n",
      "Step 113 (4783); Episode 67/100; Loss: 0.03859636187553406\n",
      "Step 114 (4784); Episode 67/100; Loss: 0.08486125618219376\n",
      "Step 115 (4785); Episode 67/100; Loss: 0.0963427871465683\n",
      "Step 116 (4786); Episode 67/100; Loss: 0.01735186204314232\n",
      "Step 117 (4787); Episode 67/100; Loss: 0.03245693817734718\n",
      "Step 118 (4788); Episode 67/100; Loss: 0.003729254473000765\n",
      "Step 119 (4789); Episode 67/100; Loss: 0.08649364113807678\n",
      "Step 120 (4790); Episode 67/100; Loss: 0.07316143065690994\n",
      "Step 121 (4791); Episode 67/100; Loss: 0.04494856297969818\n",
      "Step 122 (4792); Episode 67/100; Loss: 0.046618811786174774\n",
      "Step 123 (4793); Episode 67/100; Loss: 0.0011455073254182935\n",
      "Step 124 (4794); Episode 67/100; Loss: 0.06595765054225922\n",
      "Step 125 (4795); Episode 67/100; Loss: 0.0015921740559861064\n",
      "Step 126 (4796); Episode 67/100; Loss: 0.017761388793587685\n",
      "Step 127 (4797); Episode 67/100; Loss: 0.0021348472218960524\n",
      "Step 128 (4798); Episode 67/100; Loss: 0.0191341619938612\n",
      "Step 129 (4799); Episode 67/100; Loss: 0.11063788831233978\n",
      "Step 130 (4800); Episode 67/100; Loss: 0.02677120268344879\n",
      "Step 131 (4801); Episode 67/100; Loss: 0.0028364434838294983\n",
      "Step 132 (4802); Episode 67/100; Loss: 0.0417955219745636\n",
      "Step 133 (4803); Episode 67/100; Loss: 0.05882476270198822\n",
      "Step 134 (4804); Episode 67/100; Loss: 0.003954110201448202\n",
      "Step 135 (4805); Episode 67/100; Loss: 0.002841319888830185\n",
      "Step 136 (4806); Episode 67/100; Loss: 0.09855734556913376\n",
      "Step 137 (4807); Episode 67/100; Loss: 0.0017733500571921468\n",
      "Step 138 (4808); Episode 67/100; Loss: 0.002883521607145667\n",
      "Step 139 (4809); Episode 67/100; Loss: 0.0281814057379961\n",
      "Step 140 (4810); Episode 67/100; Loss: 0.07611801475286484\n",
      "Step 141 (4811); Episode 67/100; Loss: 0.04846589267253876\n",
      "Step 142 (4812); Episode 67/100; Loss: 0.008868729695677757\n",
      "Step 143 (4813); Episode 67/100; Loss: 0.04947872832417488\n",
      "Step 144 (4814); Episode 67/100; Loss: 0.04763513058423996\n",
      "Step 145 (4815); Episode 67/100; Loss: 0.07868416607379913\n",
      "Step 146 (4816); Episode 67/100; Loss: 0.10359135270118713\n",
      "Step 147 (4817); Episode 67/100; Loss: 0.05541514232754707\n",
      "Step 148 (4818); Episode 67/100; Loss: 0.031104370951652527\n",
      "Step 149 (4819); Episode 67/100; Loss: 0.04678739234805107\n",
      "Step 150 (4820); Episode 67/100; Loss: 0.07270742952823639\n",
      "Step 151 (4821); Episode 67/100; Loss: 0.02431425452232361\n",
      "Step 152 (4822); Episode 67/100; Loss: 0.045237161219120026\n",
      "Step 153 (4823); Episode 67/100; Loss: 0.003116569248959422\n",
      "Step 154 (4824); Episode 67/100; Loss: 0.004451827611774206\n",
      "Step 155 (4825); Episode 67/100; Loss: 0.05336695909500122\n",
      "Step 156 (4826); Episode 67/100; Loss: 0.13849970698356628\n",
      "Step 157 (4827); Episode 67/100; Loss: 0.004344576969742775\n",
      "Step 158 (4828); Episode 67/100; Loss: 0.002082493156194687\n",
      "Step 159 (4829); Episode 67/100; Loss: 0.029769161716103554\n",
      "Step 160 (4830); Episode 67/100; Loss: 0.001623468124307692\n",
      "Step 161 (4831); Episode 67/100; Loss: 0.0015831902856007218\n",
      "Step 162 (4832); Episode 67/100; Loss: 0.006080389488488436\n",
      "Step 163 (4833); Episode 67/100; Loss: 0.04964280128479004\n",
      "Step 164 (4834); Episode 67/100; Loss: 0.0011287856614217162\n",
      "Step 165 (4835); Episode 67/100; Loss: 0.06499584019184113\n",
      "Step 166 (4836); Episode 67/100; Loss: 0.019219452515244484\n",
      "Step 167 (4837); Episode 67/100; Loss: 0.003306293161585927\n",
      "Step 168 (4838); Episode 67/100; Loss: 0.019098782911896706\n",
      "Step 169 (4839); Episode 67/100; Loss: 0.09544875472784042\n",
      "Step 170 (4840); Episode 67/100; Loss: 0.002044310327619314\n",
      "Step 171 (4841); Episode 67/100; Loss: 0.014449222944676876\n",
      "Step 172 (4842); Episode 67/100; Loss: 0.02479878067970276\n",
      "Step 173 (4843); Episode 67/100; Loss: 0.004539514891803265\n",
      "Step 174 (4844); Episode 67/100; Loss: 0.004172655288130045\n",
      "Step 175 (4845); Episode 67/100; Loss: 0.02224951982498169\n",
      "Step 176 (4846); Episode 67/100; Loss: 0.0015471864026039839\n",
      "Step 177 (4847); Episode 67/100; Loss: 0.04048340767621994\n",
      "Step 178 (4848); Episode 67/100; Loss: 0.08144943416118622\n",
      "Step 179 (4849); Episode 67/100; Loss: 0.0012650678399950266\n",
      "Step 180 (4850); Episode 67/100; Loss: 0.0008430166053585708\n",
      "Step 181 (4851); Episode 67/100; Loss: 0.0029558741953223944\n",
      "Step 182 (4852); Episode 67/100; Loss: 0.04448399692773819\n",
      "Step 183 (4853); Episode 67/100; Loss: 0.0548643097281456\n",
      "Step 184 (4854); Episode 67/100; Loss: 0.09321271628141403\n",
      "Step 185 (4855); Episode 67/100; Loss: 0.04832278564572334\n",
      "Step 186 (4856); Episode 67/100; Loss: 0.028860554099082947\n",
      "Step 187 (4857); Episode 67/100; Loss: 0.00225790380500257\n",
      "Step 188 (4858); Episode 67/100; Loss: 0.09950494766235352\n",
      "Step 0 (4859); Episode 68/100; Loss: 0.07615762948989868\n",
      "Step 1 (4860); Episode 68/100; Loss: 0.0016924092778936028\n",
      "Step 2 (4861); Episode 68/100; Loss: 0.05671565607190132\n",
      "Step 3 (4862); Episode 68/100; Loss: 0.0036112999077886343\n",
      "Step 4 (4863); Episode 68/100; Loss: 0.002485907170921564\n",
      "Step 5 (4864); Episode 68/100; Loss: 0.02922351472079754\n",
      "Step 6 (4865); Episode 68/100; Loss: 0.23190303146839142\n",
      "Step 7 (4866); Episode 68/100; Loss: 0.05203089118003845\n",
      "Step 8 (4867); Episode 68/100; Loss: 0.005880731623619795\n",
      "Step 9 (4868); Episode 68/100; Loss: 0.004118575248867273\n",
      "Step 10 (4869); Episode 68/100; Loss: 0.03934144228696823\n",
      "Step 11 (4870); Episode 68/100; Loss: 0.002095854375511408\n",
      "Step 12 (4871); Episode 68/100; Loss: 0.049830321222543716\n",
      "Step 13 (4872); Episode 68/100; Loss: 0.007257214281708002\n",
      "Step 14 (4873); Episode 68/100; Loss: 0.04035675525665283\n",
      "Step 15 (4874); Episode 68/100; Loss: 0.14325930178165436\n",
      "Step 16 (4875); Episode 68/100; Loss: 0.0533096119761467\n",
      "Step 17 (4876); Episode 68/100; Loss: 0.001573140500113368\n",
      "Step 18 (4877); Episode 68/100; Loss: 0.051164254546165466\n",
      "Step 19 (4878); Episode 68/100; Loss: 0.0018661923240870237\n",
      "Step 20 (4879); Episode 68/100; Loss: 0.05599820241332054\n",
      "Step 21 (4880); Episode 68/100; Loss: 0.07556172460317612\n",
      "Step 22 (4881); Episode 68/100; Loss: 0.0016309325583279133\n",
      "Step 23 (4882); Episode 68/100; Loss: 0.0015055163530632854\n",
      "Step 24 (4883); Episode 68/100; Loss: 0.09178313612937927\n",
      "Step 25 (4884); Episode 68/100; Loss: 0.046535756438970566\n",
      "Step 26 (4885); Episode 68/100; Loss: 0.043390825390815735\n",
      "Step 27 (4886); Episode 68/100; Loss: 0.02929060533642769\n",
      "Step 28 (4887); Episode 68/100; Loss: 0.005208497401326895\n",
      "Step 29 (4888); Episode 68/100; Loss: 0.002745183417573571\n",
      "Step 30 (4889); Episode 68/100; Loss: 0.03691492974758148\n",
      "Step 31 (4890); Episode 68/100; Loss: 0.11716295778751373\n",
      "Step 32 (4891); Episode 68/100; Loss: 0.0019455268047749996\n",
      "Step 33 (4892); Episode 68/100; Loss: 0.13542313873767853\n",
      "Step 34 (4893); Episode 68/100; Loss: 0.043832723051309586\n",
      "Step 35 (4894); Episode 68/100; Loss: 0.04317917302250862\n",
      "Step 36 (4895); Episode 68/100; Loss: 0.0021150654647499323\n",
      "Step 37 (4896); Episode 68/100; Loss: 0.004812547471374273\n",
      "Step 38 (4897); Episode 68/100; Loss: 0.04544847831130028\n",
      "Step 39 (4898); Episode 68/100; Loss: 0.0036482831928879023\n",
      "Step 40 (4899); Episode 68/100; Loss: 0.0019577424973249435\n",
      "Step 41 (4900); Episode 68/100; Loss: 0.019647935405373573\n",
      "Step 42 (4901); Episode 68/100; Loss: 0.06987594068050385\n",
      "Step 43 (4902); Episode 68/100; Loss: 0.021005667746067047\n",
      "Step 44 (4903); Episode 68/100; Loss: 0.007420247886329889\n",
      "Step 45 (4904); Episode 68/100; Loss: 0.07723826915025711\n",
      "Step 46 (4905); Episode 68/100; Loss: 0.0016438426682725549\n",
      "Step 47 (4906); Episode 68/100; Loss: 0.04487481713294983\n",
      "Step 48 (4907); Episode 68/100; Loss: 0.001260734279640019\n",
      "Step 49 (4908); Episode 68/100; Loss: 0.0025790652725845575\n",
      "Step 50 (4909); Episode 68/100; Loss: 0.04496701806783676\n",
      "Step 51 (4910); Episode 68/100; Loss: 0.001102685579098761\n",
      "Step 52 (4911); Episode 68/100; Loss: 0.0516965314745903\n",
      "Step 53 (4912); Episode 68/100; Loss: 0.04461023956537247\n",
      "Step 54 (4913); Episode 68/100; Loss: 0.0024722220841795206\n",
      "Step 55 (4914); Episode 68/100; Loss: 0.0038566316943615675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56 (4915); Episode 68/100; Loss: 0.04432152584195137\n",
      "Step 57 (4916); Episode 68/100; Loss: 0.0508313924074173\n",
      "Step 58 (4917); Episode 68/100; Loss: 0.0012748119188472629\n",
      "Step 59 (4918); Episode 68/100; Loss: 0.07483447343111038\n",
      "Step 60 (4919); Episode 68/100; Loss: 0.0010681734420359135\n",
      "Step 61 (4920); Episode 68/100; Loss: 0.0014345484087243676\n",
      "Step 62 (4921); Episode 68/100; Loss: 0.04750709980726242\n",
      "Step 63 (4922); Episode 68/100; Loss: 0.04511298984289169\n",
      "Step 64 (4923); Episode 68/100; Loss: 0.001942656235769391\n",
      "Step 65 (4924); Episode 68/100; Loss: 0.09393426775932312\n",
      "Step 66 (4925); Episode 68/100; Loss: 0.02652151696383953\n",
      "Step 67 (4926); Episode 68/100; Loss: 0.0034610831644386053\n",
      "Step 68 (4927); Episode 68/100; Loss: 0.002092339564114809\n",
      "Step 69 (4928); Episode 68/100; Loss: 0.10608143359422684\n",
      "Step 70 (4929); Episode 68/100; Loss: 0.046621378511190414\n",
      "Step 71 (4930); Episode 68/100; Loss: 0.0009934536647051573\n",
      "Step 72 (4931); Episode 68/100; Loss: 0.0016290796920657158\n",
      "Step 73 (4932); Episode 68/100; Loss: 0.002220442518591881\n",
      "Step 74 (4933); Episode 68/100; Loss: 0.03919047489762306\n",
      "Step 75 (4934); Episode 68/100; Loss: 0.002416699193418026\n",
      "Step 76 (4935); Episode 68/100; Loss: 0.0025055366568267345\n",
      "Step 77 (4936); Episode 68/100; Loss: 0.057208362966775894\n",
      "Step 78 (4937); Episode 68/100; Loss: 0.003014392452314496\n",
      "Step 79 (4938); Episode 68/100; Loss: 0.001758562051691115\n",
      "Step 80 (4939); Episode 68/100; Loss: 0.024571212008595467\n",
      "Step 81 (4940); Episode 68/100; Loss: 0.04742598533630371\n",
      "Step 82 (4941); Episode 68/100; Loss: 0.0013432695996016264\n",
      "Step 83 (4942); Episode 68/100; Loss: 0.0016393496189266443\n",
      "Step 84 (4943); Episode 68/100; Loss: 0.057758718729019165\n",
      "Step 85 (4944); Episode 68/100; Loss: 0.0693545937538147\n",
      "Step 86 (4945); Episode 68/100; Loss: 0.0026616554241627455\n",
      "Step 87 (4946); Episode 68/100; Loss: 0.0013291597133502364\n",
      "Step 88 (4947); Episode 68/100; Loss: 0.0043458784930408\n",
      "Step 89 (4948); Episode 68/100; Loss: 0.0695403516292572\n",
      "Step 90 (4949); Episode 68/100; Loss: 0.03970378264784813\n",
      "Step 91 (4950); Episode 68/100; Loss: 0.0025597389321774244\n",
      "Step 92 (4951); Episode 68/100; Loss: 0.0008360337233170867\n",
      "Step 93 (4952); Episode 68/100; Loss: 0.0030849098693579435\n",
      "Step 94 (4953); Episode 68/100; Loss: 0.0012841899879276752\n",
      "Step 95 (4954); Episode 68/100; Loss: 0.08051688224077225\n",
      "Step 96 (4955); Episode 68/100; Loss: 0.0007935143075883389\n",
      "Step 97 (4956); Episode 68/100; Loss: 0.002415170194581151\n",
      "Step 98 (4957); Episode 68/100; Loss: 0.09636612981557846\n",
      "Step 99 (4958); Episode 68/100; Loss: 0.04014774411916733\n",
      "Step 100 (4959); Episode 68/100; Loss: 0.038668178021907806\n",
      "Step 101 (4960); Episode 68/100; Loss: 0.06907224655151367\n",
      "Step 102 (4961); Episode 68/100; Loss: 0.02193913608789444\n",
      "Step 103 (4962); Episode 68/100; Loss: 0.00196134764701128\n",
      "Step 104 (4963); Episode 68/100; Loss: 0.05185627192258835\n",
      "Step 105 (4964); Episode 68/100; Loss: 0.02967282384634018\n",
      "Step 106 (4965); Episode 68/100; Loss: 0.024517549201846123\n",
      "Step 107 (4966); Episode 68/100; Loss: 0.0018771890318021178\n",
      "Step 108 (4967); Episode 68/100; Loss: 0.04615156725049019\n",
      "Step 109 (4968); Episode 68/100; Loss: 0.05223480239510536\n",
      "Step 110 (4969); Episode 68/100; Loss: 0.025460634380578995\n",
      "Step 111 (4970); Episode 68/100; Loss: 0.0511765331029892\n",
      "Step 112 (4971); Episode 68/100; Loss: 0.061122845858335495\n",
      "Step 113 (4972); Episode 68/100; Loss: 0.089044950902462\n",
      "Step 114 (4973); Episode 68/100; Loss: 0.0452631451189518\n",
      "Step 115 (4974); Episode 68/100; Loss: 0.03809697926044464\n",
      "Step 116 (4975); Episode 68/100; Loss: 0.07582688331604004\n",
      "Step 117 (4976); Episode 68/100; Loss: 0.0034703933633863926\n",
      "Step 118 (4977); Episode 68/100; Loss: 0.0013037625467404723\n",
      "Step 119 (4978); Episode 68/100; Loss: 0.0167625043541193\n",
      "Step 120 (4979); Episode 68/100; Loss: 0.010838354006409645\n",
      "Step 121 (4980); Episode 68/100; Loss: 0.0017028135480359197\n",
      "Step 122 (4981); Episode 68/100; Loss: 0.0022775724064558744\n",
      "Step 123 (4982); Episode 68/100; Loss: 0.04459650442004204\n",
      "Step 124 (4983); Episode 68/100; Loss: 0.09582965075969696\n",
      "Step 125 (4984); Episode 68/100; Loss: 0.0027339209336787462\n",
      "Step 126 (4985); Episode 68/100; Loss: 0.08212786167860031\n",
      "Step 127 (4986); Episode 68/100; Loss: 0.05336465314030647\n",
      "Step 128 (4987); Episode 68/100; Loss: 0.10148507356643677\n",
      "Step 129 (4988); Episode 68/100; Loss: 0.004499092698097229\n",
      "Step 130 (4989); Episode 68/100; Loss: 0.0026564588770270348\n",
      "Step 131 (4990); Episode 68/100; Loss: 0.013848177157342434\n",
      "Step 132 (4991); Episode 68/100; Loss: 0.04685576632618904\n",
      "Step 133 (4992); Episode 68/100; Loss: 0.020694473758339882\n",
      "Step 134 (4993); Episode 68/100; Loss: 0.005819409620016813\n",
      "Step 135 (4994); Episode 68/100; Loss: 0.21636444330215454\n",
      "Step 136 (4995); Episode 68/100; Loss: 0.05555912107229233\n",
      "Step 137 (4996); Episode 68/100; Loss: 0.00410881731659174\n",
      "Step 138 (4997); Episode 68/100; Loss: 0.002666399348527193\n",
      "Step 139 (4998); Episode 68/100; Loss: 0.0011194361140951514\n",
      "Step 140 (4999); Episode 68/100; Loss: 0.002779582981020212\n",
      "Step 141 (5000); Episode 68/100; Loss: 0.013966389931738377\n",
      "Step 142 (5001); Episode 68/100; Loss: 0.0016257269307971\n",
      "Step 143 (5002); Episode 68/100; Loss: 0.07749641686677933\n",
      "Step 144 (5003); Episode 68/100; Loss: 0.07027433812618256\n",
      "Step 145 (5004); Episode 68/100; Loss: 0.002305495087057352\n",
      "Step 146 (5005); Episode 68/100; Loss: 0.0015183236682787538\n",
      "Step 147 (5006); Episode 68/100; Loss: 0.04906197637319565\n",
      "Step 148 (5007); Episode 68/100; Loss: 0.0042619300074875355\n",
      "Step 149 (5008); Episode 68/100; Loss: 0.0387144610285759\n",
      "Step 150 (5009); Episode 68/100; Loss: 0.0022470965050160885\n",
      "Step 151 (5010); Episode 68/100; Loss: 0.0023128462489694357\n",
      "Step 152 (5011); Episode 68/100; Loss: 0.006833232007920742\n",
      "Step 153 (5012); Episode 68/100; Loss: 0.06034445762634277\n",
      "Step 154 (5013); Episode 68/100; Loss: 0.005292506422847509\n",
      "Step 155 (5014); Episode 68/100; Loss: 0.02654547616839409\n",
      "Step 156 (5015); Episode 68/100; Loss: 0.049021393060684204\n",
      "Step 157 (5016); Episode 68/100; Loss: 0.04572733864188194\n",
      "Step 158 (5017); Episode 68/100; Loss: 0.0011676608119159937\n",
      "Step 159 (5018); Episode 68/100; Loss: 0.0014321996131911874\n",
      "Step 160 (5019); Episode 68/100; Loss: 0.001460067112930119\n",
      "Step 161 (5020); Episode 68/100; Loss: 0.04614296555519104\n",
      "Step 162 (5021); Episode 68/100; Loss: 0.10665185004472733\n",
      "Step 163 (5022); Episode 68/100; Loss: 0.04571417719125748\n",
      "Step 164 (5023); Episode 68/100; Loss: 0.007095390930771828\n",
      "Step 165 (5024); Episode 68/100; Loss: 0.0010739733697846532\n",
      "Step 166 (5025); Episode 68/100; Loss: 0.001327721867710352\n",
      "Step 167 (5026); Episode 68/100; Loss: 0.09355326741933823\n",
      "Step 168 (5027); Episode 68/100; Loss: 0.043918851763010025\n",
      "Step 169 (5028); Episode 68/100; Loss: 0.012735698372125626\n",
      "Step 170 (5029); Episode 68/100; Loss: 0.038716886192560196\n",
      "Step 171 (5030); Episode 68/100; Loss: 0.00289140734821558\n",
      "Step 172 (5031); Episode 68/100; Loss: 0.08670099079608917\n",
      "Step 173 (5032); Episode 68/100; Loss: 0.051386408507823944\n",
      "Step 174 (5033); Episode 68/100; Loss: 0.053756069391965866\n",
      "Step 175 (5034); Episode 68/100; Loss: 0.04344923421740532\n",
      "Step 176 (5035); Episode 68/100; Loss: 0.0005567389307543635\n",
      "Step 177 (5036); Episode 68/100; Loss: 0.10752329230308533\n",
      "Step 178 (5037); Episode 68/100; Loss: 0.025334252044558525\n",
      "Step 179 (5038); Episode 68/100; Loss: 0.07296054065227509\n",
      "Step 180 (5039); Episode 68/100; Loss: 0.002344754757359624\n",
      "Step 181 (5040); Episode 68/100; Loss: 0.0017594740493223071\n",
      "Step 182 (5041); Episode 68/100; Loss: 0.041641078889369965\n",
      "Step 183 (5042); Episode 68/100; Loss: 0.0007282937876880169\n",
      "Step 184 (5043); Episode 68/100; Loss: 0.06439567357301712\n",
      "Step 185 (5044); Episode 68/100; Loss: 0.003546607680618763\n",
      "Step 186 (5045); Episode 68/100; Loss: 0.0021833963692188263\n",
      "Step 187 (5046); Episode 68/100; Loss: 0.046193838119506836\n",
      "Step 188 (5047); Episode 68/100; Loss: 0.040313225239515305\n",
      "Step 189 (5048); Episode 68/100; Loss: 0.08872254192829132\n",
      "Step 190 (5049); Episode 68/100; Loss: 0.07194370776414871\n",
      "Step 191 (5050); Episode 68/100; Loss: 0.04002755135297775\n",
      "Step 192 (5051); Episode 68/100; Loss: 0.0279457438737154\n",
      "Step 193 (5052); Episode 68/100; Loss: 0.045408058911561966\n",
      "Step 194 (5053); Episode 68/100; Loss: 0.04941284656524658\n",
      "Step 195 (5054); Episode 68/100; Loss: 0.06080019101500511\n",
      "Step 196 (5055); Episode 68/100; Loss: 0.0059847645461559296\n",
      "Step 197 (5056); Episode 68/100; Loss: 0.0018247251864522696\n",
      "Step 198 (5057); Episode 68/100; Loss: 0.07517966628074646\n",
      "Step 199 (5058); Episode 68/100; Loss: 0.04363624006509781\n",
      "Step 0 (5059); Episode 69/100; Loss: 0.003183235414326191\n",
      "Step 1 (5060); Episode 69/100; Loss: 0.012623493559658527\n",
      "Step 2 (5061); Episode 69/100; Loss: 0.0028406670317053795\n",
      "Step 3 (5062); Episode 69/100; Loss: 0.01860511675477028\n",
      "Step 4 (5063); Episode 69/100; Loss: 0.03516156226396561\n",
      "Step 5 (5064); Episode 69/100; Loss: 0.14014573395252228\n",
      "Step 6 (5065); Episode 69/100; Loss: 0.040631040930747986\n",
      "Step 7 (5066); Episode 69/100; Loss: 0.004483036696910858\n",
      "Step 8 (5067); Episode 69/100; Loss: 0.0018513113027438521\n",
      "Step 9 (5068); Episode 69/100; Loss: 0.001764087239280343\n",
      "Step 10 (5069); Episode 69/100; Loss: 0.0829375758767128\n",
      "Step 11 (5070); Episode 69/100; Loss: 0.0033802867401391268\n",
      "Step 12 (5071); Episode 69/100; Loss: 0.02234799787402153\n",
      "Step 13 (5072); Episode 69/100; Loss: 0.0013281700666993856\n",
      "Step 14 (5073); Episode 69/100; Loss: 0.0021281291265040636\n",
      "Step 15 (5074); Episode 69/100; Loss: 0.002613180549815297\n",
      "Step 16 (5075); Episode 69/100; Loss: 0.0020837681367993355\n",
      "Step 17 (5076); Episode 69/100; Loss: 0.01882745698094368\n",
      "Step 18 (5077); Episode 69/100; Loss: 0.04992307722568512\n",
      "Step 19 (5078); Episode 69/100; Loss: 0.0012771418550983071\n",
      "Step 20 (5079); Episode 69/100; Loss: 0.0028330939821898937\n",
      "Step 21 (5080); Episode 69/100; Loss: 0.004338942933827639\n",
      "Step 22 (5081); Episode 69/100; Loss: 0.0008446471183560789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23 (5082); Episode 69/100; Loss: 0.05591978505253792\n",
      "Step 24 (5083); Episode 69/100; Loss: 0.05532657727599144\n",
      "Step 25 (5084); Episode 69/100; Loss: 0.0017322811763733625\n",
      "Step 26 (5085); Episode 69/100; Loss: 0.0024667091201990843\n",
      "Step 27 (5086); Episode 69/100; Loss: 0.010634633712470531\n",
      "Step 28 (5087); Episode 69/100; Loss: 0.0009099958115257323\n",
      "Step 29 (5088); Episode 69/100; Loss: 0.0023520442191511393\n",
      "Step 30 (5089); Episode 69/100; Loss: 0.06547921150922775\n",
      "Step 31 (5090); Episode 69/100; Loss: 0.002516756532713771\n",
      "Step 32 (5091); Episode 69/100; Loss: 0.0644780769944191\n",
      "Step 33 (5092); Episode 69/100; Loss: 0.1407293826341629\n",
      "Step 34 (5093); Episode 69/100; Loss: 0.001313161221332848\n",
      "Step 35 (5094); Episode 69/100; Loss: 0.08421297371387482\n",
      "Step 36 (5095); Episode 69/100; Loss: 0.08079173415899277\n",
      "Step 37 (5096); Episode 69/100; Loss: 0.0450497567653656\n",
      "Step 38 (5097); Episode 69/100; Loss: 0.05417049676179886\n",
      "Step 39 (5098); Episode 69/100; Loss: 0.04938318952918053\n",
      "Step 40 (5099); Episode 69/100; Loss: 0.0012146849185228348\n",
      "Step 41 (5100); Episode 69/100; Loss: 0.04692964628338814\n",
      "Step 42 (5101); Episode 69/100; Loss: 0.000942008278798312\n",
      "Step 43 (5102); Episode 69/100; Loss: 0.00505481380969286\n",
      "Step 44 (5103); Episode 69/100; Loss: 0.0027462702710181475\n",
      "Step 45 (5104); Episode 69/100; Loss: 0.05849235877394676\n",
      "Step 46 (5105); Episode 69/100; Loss: 0.03191177919507027\n",
      "Step 47 (5106); Episode 69/100; Loss: 0.03890043497085571\n",
      "Step 48 (5107); Episode 69/100; Loss: 0.0020394253078848124\n",
      "Step 49 (5108); Episode 69/100; Loss: 0.04748241975903511\n",
      "Step 50 (5109); Episode 69/100; Loss: 0.10100981593132019\n",
      "Step 51 (5110); Episode 69/100; Loss: 0.003193391254171729\n",
      "Step 52 (5111); Episode 69/100; Loss: 0.005615051370114088\n",
      "Step 53 (5112); Episode 69/100; Loss: 0.05380956083536148\n",
      "Step 54 (5113); Episode 69/100; Loss: 0.046195510774850845\n",
      "Step 55 (5114); Episode 69/100; Loss: 0.08032689988613129\n",
      "Step 56 (5115); Episode 69/100; Loss: 0.060713622719049454\n",
      "Step 57 (5116); Episode 69/100; Loss: 0.07448974251747131\n",
      "Step 58 (5117); Episode 69/100; Loss: 0.08491997420787811\n",
      "Step 59 (5118); Episode 69/100; Loss: 0.06124376133084297\n",
      "Step 60 (5119); Episode 69/100; Loss: 0.017489483579993248\n",
      "Step 61 (5120); Episode 69/100; Loss: 0.003215266391634941\n",
      "Step 62 (5121); Episode 69/100; Loss: 0.019656410440802574\n",
      "Step 63 (5122); Episode 69/100; Loss: 0.0019596931524574757\n",
      "Step 64 (5123); Episode 69/100; Loss: 0.003308448940515518\n",
      "Step 65 (5124); Episode 69/100; Loss: 0.0010153275215998292\n",
      "Step 66 (5125); Episode 69/100; Loss: 0.004565584007650614\n",
      "Step 67 (5126); Episode 69/100; Loss: 0.037336718291044235\n",
      "Step 68 (5127); Episode 69/100; Loss: 0.002058320213109255\n",
      "Step 69 (5128); Episode 69/100; Loss: 0.0238412506878376\n",
      "Step 70 (5129); Episode 69/100; Loss: 0.06654152274131775\n",
      "Step 71 (5130); Episode 69/100; Loss: 0.001735785393975675\n",
      "Step 72 (5131); Episode 69/100; Loss: 0.0296282097697258\n",
      "Step 73 (5132); Episode 69/100; Loss: 0.03198089450597763\n",
      "Step 74 (5133); Episode 69/100; Loss: 0.08687989413738251\n",
      "Step 75 (5134); Episode 69/100; Loss: 0.06923721730709076\n",
      "Step 76 (5135); Episode 69/100; Loss: 0.0006949196103960276\n",
      "Step 77 (5136); Episode 69/100; Loss: 0.012674685567617416\n",
      "Step 78 (5137); Episode 69/100; Loss: 0.0006151297711767256\n",
      "Step 79 (5138); Episode 69/100; Loss: 0.0017404723912477493\n",
      "Step 80 (5139); Episode 69/100; Loss: 0.002288153162226081\n",
      "Step 81 (5140); Episode 69/100; Loss: 0.0012777268420904875\n",
      "Step 82 (5141); Episode 69/100; Loss: 0.076801598072052\n",
      "Step 83 (5142); Episode 69/100; Loss: 0.05183452367782593\n",
      "Step 84 (5143); Episode 69/100; Loss: 0.0020788139663636684\n",
      "Step 85 (5144); Episode 69/100; Loss: 0.05748778581619263\n",
      "Step 86 (5145); Episode 69/100; Loss: 0.05626727268099785\n",
      "Step 87 (5146); Episode 69/100; Loss: 0.00171920214779675\n",
      "Step 88 (5147); Episode 69/100; Loss: 0.1456449031829834\n",
      "Step 89 (5148); Episode 69/100; Loss: 0.022780239582061768\n",
      "Step 90 (5149); Episode 69/100; Loss: 0.020858915522694588\n",
      "Step 91 (5150); Episode 69/100; Loss: 0.05700354650616646\n",
      "Step 92 (5151); Episode 69/100; Loss: 0.004837059881538153\n",
      "Step 93 (5152); Episode 69/100; Loss: 0.0015147654339671135\n",
      "Step 94 (5153); Episode 69/100; Loss: 0.17307129502296448\n",
      "Step 95 (5154); Episode 69/100; Loss: 0.015226468443870544\n",
      "Step 96 (5155); Episode 69/100; Loss: 0.049804799258708954\n",
      "Step 97 (5156); Episode 69/100; Loss: 0.04972788318991661\n",
      "Step 98 (5157); Episode 69/100; Loss: 0.12129045277833939\n",
      "Step 99 (5158); Episode 69/100; Loss: 0.02544008195400238\n",
      "Step 100 (5159); Episode 69/100; Loss: 0.0033404517453163862\n",
      "Step 101 (5160); Episode 69/100; Loss: 0.04303110018372536\n",
      "Step 102 (5161); Episode 69/100; Loss: 0.0012088160729035735\n",
      "Step 103 (5162); Episode 69/100; Loss: 0.05972599610686302\n",
      "Step 104 (5163); Episode 69/100; Loss: 0.003757328726351261\n",
      "Step 105 (5164); Episode 69/100; Loss: 0.07339341193437576\n",
      "Step 106 (5165); Episode 69/100; Loss: 0.043820347636938095\n",
      "Step 107 (5166); Episode 69/100; Loss: 0.06331522017717361\n",
      "Step 108 (5167); Episode 69/100; Loss: 0.1023227646946907\n",
      "Step 109 (5168); Episode 69/100; Loss: 0.050292570143938065\n",
      "Step 110 (5169); Episode 69/100; Loss: 0.05953960120677948\n",
      "Step 111 (5170); Episode 69/100; Loss: 0.005270919296890497\n",
      "Step 112 (5171); Episode 69/100; Loss: 0.059458184987306595\n",
      "Step 113 (5172); Episode 69/100; Loss: 0.0014332284918054938\n",
      "Step 114 (5173); Episode 69/100; Loss: 0.03860759362578392\n",
      "Step 115 (5174); Episode 69/100; Loss: 0.035532575100660324\n",
      "Step 116 (5175); Episode 69/100; Loss: 0.0314762145280838\n",
      "Step 117 (5176); Episode 69/100; Loss: 0.09396430104970932\n",
      "Step 118 (5177); Episode 69/100; Loss: 0.0038571939803659916\n",
      "Step 119 (5178); Episode 69/100; Loss: 0.06377646327018738\n",
      "Step 120 (5179); Episode 69/100; Loss: 0.00255969469435513\n",
      "Step 121 (5180); Episode 69/100; Loss: 0.09937490522861481\n",
      "Step 122 (5181); Episode 69/100; Loss: 0.006133332848548889\n",
      "Step 123 (5182); Episode 69/100; Loss: 0.006606748327612877\n",
      "Step 124 (5183); Episode 69/100; Loss: 0.011045272462069988\n",
      "Step 125 (5184); Episode 69/100; Loss: 0.004064049571752548\n",
      "Step 126 (5185); Episode 69/100; Loss: 0.0022176075726747513\n",
      "Step 127 (5186); Episode 69/100; Loss: 0.04807010665535927\n",
      "Step 128 (5187); Episode 69/100; Loss: 0.03794766217470169\n",
      "Step 129 (5188); Episode 69/100; Loss: 0.003266858169808984\n",
      "Step 130 (5189); Episode 69/100; Loss: 0.0964035764336586\n",
      "Step 131 (5190); Episode 69/100; Loss: 0.004340238403528929\n",
      "Step 132 (5191); Episode 69/100; Loss: 0.05476890504360199\n",
      "Step 133 (5192); Episode 69/100; Loss: 0.004193814005702734\n",
      "Step 134 (5193); Episode 69/100; Loss: 0.018621748313307762\n",
      "Step 135 (5194); Episode 69/100; Loss: 0.0033569657243788242\n",
      "Step 136 (5195); Episode 69/100; Loss: 0.0018295150948688388\n",
      "Step 137 (5196); Episode 69/100; Loss: 0.0587930753827095\n",
      "Step 138 (5197); Episode 69/100; Loss: 0.002477189525961876\n",
      "Step 139 (5198); Episode 69/100; Loss: 0.002212527673691511\n",
      "Step 140 (5199); Episode 69/100; Loss: 0.15050138533115387\n",
      "Step 141 (5200); Episode 69/100; Loss: 0.001909901387989521\n",
      "Step 142 (5201); Episode 69/100; Loss: 0.04360439255833626\n",
      "Step 143 (5202); Episode 69/100; Loss: 0.0015731139574199915\n",
      "Step 144 (5203); Episode 69/100; Loss: 0.001005938509479165\n",
      "Step 145 (5204); Episode 69/100; Loss: 0.048559512943029404\n",
      "Step 146 (5205); Episode 69/100; Loss: 0.10733340680599213\n",
      "Step 147 (5206); Episode 69/100; Loss: 0.002060640836134553\n",
      "Step 148 (5207); Episode 69/100; Loss: 0.0008456335053779185\n",
      "Step 149 (5208); Episode 69/100; Loss: 0.001365827163681388\n",
      "Step 150 (5209); Episode 69/100; Loss: 0.047463346272706985\n",
      "Step 151 (5210); Episode 69/100; Loss: 0.0020588405895978212\n",
      "Step 152 (5211); Episode 69/100; Loss: 0.04669757932424545\n",
      "Step 153 (5212); Episode 69/100; Loss: 0.0034172371961176395\n",
      "Step 154 (5213); Episode 69/100; Loss: 0.001053232466802001\n",
      "Step 155 (5214); Episode 69/100; Loss: 0.0023949132300913334\n",
      "Step 156 (5215); Episode 69/100; Loss: 0.04539148136973381\n",
      "Step 157 (5216); Episode 69/100; Loss: 0.0853615552186966\n",
      "Step 158 (5217); Episode 69/100; Loss: 0.04511760175228119\n",
      "Step 159 (5218); Episode 69/100; Loss: 0.07119905948638916\n",
      "Step 160 (5219); Episode 69/100; Loss: 0.04692167416214943\n",
      "Step 161 (5220); Episode 69/100; Loss: 0.049816835671663284\n",
      "Step 162 (5221); Episode 69/100; Loss: 0.0013679687399417162\n",
      "Step 163 (5222); Episode 69/100; Loss: 0.0028460666071623564\n",
      "Step 164 (5223); Episode 69/100; Loss: 0.0015246524708345532\n",
      "Step 165 (5224); Episode 69/100; Loss: 0.0006994863506406546\n",
      "Step 166 (5225); Episode 69/100; Loss: 0.0025980565696954727\n",
      "Step 167 (5226); Episode 69/100; Loss: 0.04534107819199562\n",
      "Step 168 (5227); Episode 69/100; Loss: 0.0005768388509750366\n",
      "Step 169 (5228); Episode 69/100; Loss: 0.05662877857685089\n",
      "Step 170 (5229); Episode 69/100; Loss: 0.0014062895206734538\n",
      "Step 171 (5230); Episode 69/100; Loss: 0.06649112701416016\n",
      "Step 172 (5231); Episode 69/100; Loss: 0.004857583437114954\n",
      "Step 173 (5232); Episode 69/100; Loss: 0.04537716135382652\n",
      "Step 174 (5233); Episode 69/100; Loss: 0.0010567473946139216\n",
      "Step 175 (5234); Episode 69/100; Loss: 0.0006271536112762988\n",
      "Step 176 (5235); Episode 69/100; Loss: 0.0693429559469223\n",
      "Step 177 (5236); Episode 69/100; Loss: 0.04285027086734772\n",
      "Step 178 (5237); Episode 69/100; Loss: 0.024772394448518753\n",
      "Step 179 (5238); Episode 69/100; Loss: 0.04610663279891014\n",
      "Step 180 (5239); Episode 69/100; Loss: 0.003018053714185953\n",
      "Step 181 (5240); Episode 69/100; Loss: 0.001077948953025043\n",
      "Step 182 (5241); Episode 69/100; Loss: 0.08661344647407532\n",
      "Step 183 (5242); Episode 69/100; Loss: 0.04069473594427109\n",
      "Step 184 (5243); Episode 69/100; Loss: 0.004066902678459883\n",
      "Step 185 (5244); Episode 69/100; Loss: 0.002223558258265257\n",
      "Step 186 (5245); Episode 69/100; Loss: 0.04454942047595978\n",
      "Step 187 (5246); Episode 69/100; Loss: 0.0017613337840884924\n",
      "Step 188 (5247); Episode 69/100; Loss: 0.0008305745432153344\n",
      "Step 189 (5248); Episode 69/100; Loss: 0.0018329148879274726\n",
      "Step 190 (5249); Episode 69/100; Loss: 0.0029366938397288322\n",
      "Step 191 (5250); Episode 69/100; Loss: 0.001254879403859377\n",
      "Step 192 (5251); Episode 69/100; Loss: 0.021322207525372505\n",
      "Step 193 (5252); Episode 69/100; Loss: 0.0004904362722299993\n",
      "Step 194 (5253); Episode 69/100; Loss: 0.06679940968751907\n",
      "Step 195 (5254); Episode 69/100; Loss: 0.001169685274362564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 196 (5255); Episode 69/100; Loss: 0.034571267664432526\n",
      "Step 197 (5256); Episode 69/100; Loss: 0.0010659409454092383\n",
      "Step 198 (5257); Episode 69/100; Loss: 0.0015179846668615937\n",
      "Step 199 (5258); Episode 69/100; Loss: 0.09823096543550491\n",
      "Step 0 (5259); Episode 70/100; Loss: 0.02283567190170288\n",
      "Step 1 (5260); Episode 70/100; Loss: 0.000842785055283457\n",
      "Step 2 (5261); Episode 70/100; Loss: 0.0025451502297073603\n",
      "Step 3 (5262); Episode 70/100; Loss: 0.08036653697490692\n",
      "Step 4 (5263); Episode 70/100; Loss: 0.0030933974776417017\n",
      "Step 5 (5264); Episode 70/100; Loss: 0.0540861077606678\n",
      "Step 6 (5265); Episode 70/100; Loss: 0.0009772088378667831\n",
      "Step 7 (5266); Episode 70/100; Loss: 0.003781653707846999\n",
      "Step 8 (5267); Episode 70/100; Loss: 0.03638922795653343\n",
      "Step 9 (5268); Episode 70/100; Loss: 0.052049197256565094\n",
      "Step 10 (5269); Episode 70/100; Loss: 0.04486666992306709\n",
      "Step 11 (5270); Episode 70/100; Loss: 0.044774945825338364\n",
      "Step 12 (5271); Episode 70/100; Loss: 0.05202977731823921\n",
      "Step 13 (5272); Episode 70/100; Loss: 0.15392595529556274\n",
      "Step 14 (5273); Episode 70/100; Loss: 0.006695874966681004\n",
      "Step 15 (5274); Episode 70/100; Loss: 0.0024322648532688618\n",
      "Step 16 (5275); Episode 70/100; Loss: 0.0712892934679985\n",
      "Step 17 (5276); Episode 70/100; Loss: 0.03377719596028328\n",
      "Step 18 (5277); Episode 70/100; Loss: 0.004564397037029266\n",
      "Step 19 (5278); Episode 70/100; Loss: 0.054531436413526535\n",
      "Step 20 (5279); Episode 70/100; Loss: 0.07914279401302338\n",
      "Step 21 (5280); Episode 70/100; Loss: 0.08296815305948257\n",
      "Step 22 (5281); Episode 70/100; Loss: 0.0008530319319106638\n",
      "Step 23 (5282); Episode 70/100; Loss: 0.0428159199655056\n",
      "Step 24 (5283); Episode 70/100; Loss: 0.011411348357796669\n",
      "Step 25 (5284); Episode 70/100; Loss: 0.0014170225476846099\n",
      "Step 26 (5285); Episode 70/100; Loss: 0.0013532225275412202\n",
      "Step 27 (5286); Episode 70/100; Loss: 0.003207126632332802\n",
      "Step 28 (5287); Episode 70/100; Loss: 0.007802100852131844\n",
      "Step 29 (5288); Episode 70/100; Loss: 0.005530135240405798\n",
      "Step 30 (5289); Episode 70/100; Loss: 0.001235945150256157\n",
      "Step 31 (5290); Episode 70/100; Loss: 0.0015355314826592803\n",
      "Step 32 (5291); Episode 70/100; Loss: 0.0014695815043523908\n",
      "Step 33 (5292); Episode 70/100; Loss: 0.0012518656440079212\n",
      "Step 34 (5293); Episode 70/100; Loss: 0.05161675065755844\n",
      "Step 35 (5294); Episode 70/100; Loss: 0.09524278342723846\n",
      "Step 36 (5295); Episode 70/100; Loss: 0.0262324009090662\n",
      "Step 37 (5296); Episode 70/100; Loss: 0.05313833802938461\n",
      "Step 38 (5297); Episode 70/100; Loss: 0.09539084881544113\n",
      "Step 39 (5298); Episode 70/100; Loss: 0.030936352908611298\n",
      "Step 40 (5299); Episode 70/100; Loss: 0.003895646193996072\n",
      "Step 41 (5300); Episode 70/100; Loss: 0.04607142508029938\n",
      "Step 42 (5301); Episode 70/100; Loss: 0.001986081711947918\n",
      "Step 43 (5302); Episode 70/100; Loss: 0.10374098271131516\n",
      "Step 44 (5303); Episode 70/100; Loss: 0.0014047396834939718\n",
      "Step 45 (5304); Episode 70/100; Loss: 0.1365043669939041\n",
      "Step 46 (5305); Episode 70/100; Loss: 0.04207409918308258\n",
      "Step 47 (5306); Episode 70/100; Loss: 0.004454535897821188\n",
      "Step 48 (5307); Episode 70/100; Loss: 0.001455972669646144\n",
      "Step 49 (5308); Episode 70/100; Loss: 0.002735207788646221\n",
      "Step 50 (5309); Episode 70/100; Loss: 0.09859130531549454\n",
      "Step 51 (5310); Episode 70/100; Loss: 0.04264703765511513\n",
      "Step 52 (5311); Episode 70/100; Loss: 0.0039348541758954525\n",
      "Step 53 (5312); Episode 70/100; Loss: 0.003262365935370326\n",
      "Step 54 (5313); Episode 70/100; Loss: 0.0038395405281335115\n",
      "Step 55 (5314); Episode 70/100; Loss: 0.03969556465744972\n",
      "Step 56 (5315); Episode 70/100; Loss: 0.04270749166607857\n",
      "Step 57 (5316); Episode 70/100; Loss: 0.020060449838638306\n",
      "Step 58 (5317); Episode 70/100; Loss: 0.002433234825730324\n",
      "Step 59 (5318); Episode 70/100; Loss: 0.008335131220519543\n",
      "Step 60 (5319); Episode 70/100; Loss: 0.01836107298731804\n",
      "Step 61 (5320); Episode 70/100; Loss: 0.0027445072773844004\n",
      "Step 62 (5321); Episode 70/100; Loss: 0.0021551235113292933\n",
      "Step 63 (5322); Episode 70/100; Loss: 0.02032163366675377\n",
      "Step 64 (5323); Episode 70/100; Loss: 0.039088696241378784\n",
      "Step 65 (5324); Episode 70/100; Loss: 0.04117770493030548\n",
      "Step 66 (5325); Episode 70/100; Loss: 0.0850369930267334\n",
      "Step 67 (5326); Episode 70/100; Loss: 0.0339130200445652\n",
      "Step 68 (5327); Episode 70/100; Loss: 0.0021013435907661915\n",
      "Step 69 (5328); Episode 70/100; Loss: 0.027842918410897255\n",
      "Step 70 (5329); Episode 70/100; Loss: 0.014601505361497402\n",
      "Step 71 (5330); Episode 70/100; Loss: 0.0017697145231068134\n",
      "Step 72 (5331); Episode 70/100; Loss: 0.0016862328629940748\n",
      "Step 73 (5332); Episode 70/100; Loss: 0.047920823097229004\n",
      "Step 74 (5333); Episode 70/100; Loss: 0.004075748845934868\n",
      "Step 75 (5334); Episode 70/100; Loss: 0.0009625907987356186\n",
      "Step 76 (5335); Episode 70/100; Loss: 0.0029317394364625216\n",
      "Step 77 (5336); Episode 70/100; Loss: 0.001064319396391511\n",
      "Step 78 (5337); Episode 70/100; Loss: 0.040458887815475464\n",
      "Step 79 (5338); Episode 70/100; Loss: 0.0024826866574585438\n",
      "Step 80 (5339); Episode 70/100; Loss: 0.0696600154042244\n",
      "Step 81 (5340); Episode 70/100; Loss: 0.03363274410367012\n",
      "Step 82 (5341); Episode 70/100; Loss: 0.09707087278366089\n",
      "Step 83 (5342); Episode 70/100; Loss: 0.002192339627072215\n",
      "Step 84 (5343); Episode 70/100; Loss: 0.0036070251371711493\n",
      "Step 85 (5344); Episode 70/100; Loss: 0.040681205689907074\n",
      "Step 86 (5345); Episode 70/100; Loss: 0.001391360186971724\n",
      "Step 87 (5346); Episode 70/100; Loss: 0.03783658146858215\n",
      "Step 88 (5347); Episode 70/100; Loss: 0.040181126445531845\n",
      "Step 89 (5348); Episode 70/100; Loss: 0.002193138701841235\n",
      "Step 90 (5349); Episode 70/100; Loss: 0.04292703792452812\n",
      "Step 91 (5350); Episode 70/100; Loss: 0.07895935326814651\n",
      "Step 92 (5351); Episode 70/100; Loss: 0.0015175610315054655\n",
      "Step 93 (5352); Episode 70/100; Loss: 0.0013890225673094392\n",
      "Step 94 (5353); Episode 70/100; Loss: 0.05545365810394287\n",
      "Step 95 (5354); Episode 70/100; Loss: 0.03810032084584236\n",
      "Step 96 (5355); Episode 70/100; Loss: 0.0015931479865685105\n",
      "Step 97 (5356); Episode 70/100; Loss: 0.0011259691091254354\n",
      "Step 98 (5357); Episode 70/100; Loss: 0.057438336312770844\n",
      "Step 99 (5358); Episode 70/100; Loss: 0.06058390066027641\n",
      "Step 100 (5359); Episode 70/100; Loss: 0.06967299431562424\n",
      "Step 101 (5360); Episode 70/100; Loss: 0.040585607290267944\n",
      "Step 102 (5361); Episode 70/100; Loss: 0.040905509144067764\n",
      "Step 103 (5362); Episode 70/100; Loss: 0.009872212074697018\n",
      "Step 104 (5363); Episode 70/100; Loss: 0.0232878178358078\n",
      "Step 105 (5364); Episode 70/100; Loss: 0.0013794868718832731\n",
      "Step 106 (5365); Episode 70/100; Loss: 0.0033429241739213467\n",
      "Step 107 (5366); Episode 70/100; Loss: 0.024304067716002464\n",
      "Step 108 (5367); Episode 70/100; Loss: 0.002271592617034912\n",
      "Step 109 (5368); Episode 70/100; Loss: 0.00151034293230623\n",
      "Step 110 (5369); Episode 70/100; Loss: 0.001682742964476347\n",
      "Step 111 (5370); Episode 70/100; Loss: 0.0010089492425322533\n",
      "Step 112 (5371); Episode 70/100; Loss: 0.030739499256014824\n",
      "Step 113 (5372); Episode 70/100; Loss: 0.0012967285001650453\n",
      "Step 114 (5373); Episode 70/100; Loss: 0.0012432486983016133\n",
      "Step 115 (5374); Episode 70/100; Loss: 0.0016632996266707778\n",
      "Step 116 (5375); Episode 70/100; Loss: 0.0017942566191777587\n",
      "Step 117 (5376); Episode 70/100; Loss: 0.001737668295390904\n",
      "Step 118 (5377); Episode 70/100; Loss: 0.0014864739496260881\n",
      "Step 119 (5378); Episode 70/100; Loss: 0.0009788151364773512\n",
      "Step 120 (5379); Episode 70/100; Loss: 0.041744839400053024\n",
      "Step 121 (5380); Episode 70/100; Loss: 0.0013571104500442743\n",
      "Step 122 (5381); Episode 70/100; Loss: 0.0019860409665852785\n",
      "Step 123 (5382); Episode 70/100; Loss: 0.045590657740831375\n",
      "Step 124 (5383); Episode 70/100; Loss: 0.0939188227057457\n",
      "Step 125 (5384); Episode 70/100; Loss: 0.1436828225851059\n",
      "Step 126 (5385); Episode 70/100; Loss: 0.02492724172770977\n",
      "Step 127 (5386); Episode 70/100; Loss: 0.0034636680502444506\n",
      "Step 128 (5387); Episode 70/100; Loss: 0.0008759082993492484\n",
      "Step 129 (5388); Episode 70/100; Loss: 0.0017212678212672472\n",
      "Step 130 (5389); Episode 70/100; Loss: 0.04675019904971123\n",
      "Step 131 (5390); Episode 70/100; Loss: 0.09220542013645172\n",
      "Step 132 (5391); Episode 70/100; Loss: 0.003591256681829691\n",
      "Step 133 (5392); Episode 70/100; Loss: 0.0010223054559901357\n",
      "Step 134 (5393); Episode 70/100; Loss: 0.0028046639636158943\n",
      "Step 135 (5394); Episode 70/100; Loss: 0.03554364666342735\n",
      "Step 136 (5395); Episode 70/100; Loss: 0.005492138210684061\n",
      "Step 137 (5396); Episode 70/100; Loss: 0.08540154993534088\n",
      "Step 138 (5397); Episode 70/100; Loss: 0.001988351345062256\n",
      "Step 139 (5398); Episode 70/100; Loss: 0.048881761729717255\n",
      "Step 140 (5399); Episode 70/100; Loss: 0.057439401745796204\n",
      "Step 141 (5400); Episode 70/100; Loss: 0.001178382895886898\n",
      "Step 142 (5401); Episode 70/100; Loss: 0.09109747409820557\n",
      "Step 143 (5402); Episode 70/100; Loss: 0.001136859180405736\n",
      "Step 144 (5403); Episode 70/100; Loss: 0.03344464674592018\n",
      "Step 145 (5404); Episode 70/100; Loss: 0.0008728476241230965\n",
      "Step 146 (5405); Episode 70/100; Loss: 0.001680988585576415\n",
      "Step 147 (5406); Episode 70/100; Loss: 0.0013798599829897285\n",
      "Step 148 (5407); Episode 70/100; Loss: 0.040071599185466766\n",
      "Step 149 (5408); Episode 70/100; Loss: 0.09126513451337814\n",
      "Step 150 (5409); Episode 70/100; Loss: 0.016160428524017334\n",
      "Step 151 (5410); Episode 70/100; Loss: 0.058251045644283295\n",
      "Step 152 (5411); Episode 70/100; Loss: 0.044082850217819214\n",
      "Step 153 (5412); Episode 70/100; Loss: 0.0023409524001181126\n",
      "Step 154 (5413); Episode 70/100; Loss: 0.001449782052077353\n",
      "Step 155 (5414); Episode 70/100; Loss: 0.0018551030661910772\n",
      "Step 156 (5415); Episode 70/100; Loss: 0.05841973051428795\n",
      "Step 157 (5416); Episode 70/100; Loss: 0.04059521108865738\n",
      "Step 158 (5417); Episode 70/100; Loss: 0.049432069063186646\n",
      "Step 159 (5418); Episode 70/100; Loss: 0.0007098743808455765\n",
      "Step 160 (5419); Episode 70/100; Loss: 0.01866835355758667\n",
      "Step 161 (5420); Episode 70/100; Loss: 0.028755763545632362\n",
      "Step 162 (5421); Episode 70/100; Loss: 0.040713123977184296\n",
      "Step 163 (5422); Episode 70/100; Loss: 0.07750359177589417\n",
      "Step 164 (5423); Episode 70/100; Loss: 0.0044202678836882114\n",
      "Step 165 (5424); Episode 70/100; Loss: 0.00134305062238127\n",
      "Step 166 (5425); Episode 70/100; Loss: 0.01772194728255272\n",
      "Step 167 (5426); Episode 70/100; Loss: 0.00484269205480814\n",
      "Step 168 (5427); Episode 70/100; Loss: 0.0024440858978778124\n",
      "Step 169 (5428); Episode 70/100; Loss: 0.0021291940938681364\n",
      "Step 170 (5429); Episode 70/100; Loss: 0.0424487441778183\n",
      "Step 171 (5430); Episode 70/100; Loss: 0.09215350449085236\n",
      "Step 172 (5431); Episode 70/100; Loss: 0.02995605394244194\n",
      "Step 173 (5432); Episode 70/100; Loss: 0.056227490305900574\n",
      "Step 174 (5433); Episode 70/100; Loss: 0.04474746435880661\n",
      "Step 175 (5434); Episode 70/100; Loss: 0.002165239304304123\n",
      "Step 176 (5435); Episode 70/100; Loss: 0.002096987795084715\n",
      "Step 177 (5436); Episode 70/100; Loss: 0.0017657751450315118\n",
      "Step 178 (5437); Episode 70/100; Loss: 0.0436205230653286\n",
      "Step 179 (5438); Episode 70/100; Loss: 0.0007237857789732516\n",
      "Step 180 (5439); Episode 70/100; Loss: 0.054223764687776566\n",
      "Step 181 (5440); Episode 70/100; Loss: 0.0011132463114336133\n",
      "Step 182 (5441); Episode 70/100; Loss: 0.033126335591077805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 183 (5442); Episode 70/100; Loss: 0.0006315886857919395\n",
      "Step 184 (5443); Episode 70/100; Loss: 0.052752092480659485\n",
      "Step 185 (5444); Episode 70/100; Loss: 0.04097918048501015\n",
      "Step 186 (5445); Episode 70/100; Loss: 0.004970623645931482\n",
      "Step 187 (5446); Episode 70/100; Loss: 0.0014311798149719834\n",
      "Step 188 (5447); Episode 70/100; Loss: 0.0465395413339138\n",
      "Step 189 (5448); Episode 70/100; Loss: 0.0012052860110998154\n",
      "Step 190 (5449); Episode 70/100; Loss: 0.002455565147101879\n",
      "Step 191 (5450); Episode 70/100; Loss: 0.04619373381137848\n",
      "Step 192 (5451); Episode 70/100; Loss: 0.0021010329946875572\n",
      "Step 193 (5452); Episode 70/100; Loss: 0.060358889400959015\n",
      "Step 194 (5453); Episode 70/100; Loss: 0.08258280903100967\n",
      "Step 195 (5454); Episode 70/100; Loss: 0.04972774535417557\n",
      "Step 196 (5455); Episode 70/100; Loss: 0.0007172183832153678\n",
      "Step 197 (5456); Episode 70/100; Loss: 0.050014786422252655\n",
      "Step 198 (5457); Episode 70/100; Loss: 0.060066431760787964\n",
      "Step 199 (5458); Episode 70/100; Loss: 0.0014594730455428362\n",
      "Step 0 (5459); Episode 71/100; Loss: 0.043292928487062454\n",
      "Step 1 (5460); Episode 71/100; Loss: 0.005212240852415562\n",
      "Step 2 (5461); Episode 71/100; Loss: 0.040197357535362244\n",
      "Step 3 (5462); Episode 71/100; Loss: 0.00088507030159235\n",
      "Step 4 (5463); Episode 71/100; Loss: 0.0013389716623350978\n",
      "Step 5 (5464); Episode 71/100; Loss: 0.0021411303896456957\n",
      "Step 6 (5465); Episode 71/100; Loss: 0.03462850674986839\n",
      "Step 7 (5466); Episode 71/100; Loss: 0.058349356055259705\n",
      "Step 8 (5467); Episode 71/100; Loss: 0.08068295568227768\n",
      "Step 9 (5468); Episode 71/100; Loss: 0.005930490326136351\n",
      "Step 10 (5469); Episode 71/100; Loss: 0.0008450617315247655\n",
      "Step 11 (5470); Episode 71/100; Loss: 0.0012028433848172426\n",
      "Step 12 (5471); Episode 71/100; Loss: 0.054741520434617996\n",
      "Step 13 (5472); Episode 71/100; Loss: 0.044840048998594284\n",
      "Step 14 (5473); Episode 71/100; Loss: 0.0010554908076301217\n",
      "Step 15 (5474); Episode 71/100; Loss: 0.04561995342373848\n",
      "Step 16 (5475); Episode 71/100; Loss: 0.10957062989473343\n",
      "Step 17 (5476); Episode 71/100; Loss: 0.1037507951259613\n",
      "Step 18 (5477); Episode 71/100; Loss: 0.029433516785502434\n",
      "Step 19 (5478); Episode 71/100; Loss: 0.05493195354938507\n",
      "Step 20 (5479); Episode 71/100; Loss: 0.002532815095037222\n",
      "Step 21 (5480); Episode 71/100; Loss: 0.0013856920413672924\n",
      "Step 22 (5481); Episode 71/100; Loss: 0.0007722034933976829\n",
      "Step 23 (5482); Episode 71/100; Loss: 0.0018445632886141539\n",
      "Step 24 (5483); Episode 71/100; Loss: 0.0026450001168996096\n",
      "Step 25 (5484); Episode 71/100; Loss: 0.002552997088059783\n",
      "Step 26 (5485); Episode 71/100; Loss: 0.0009081562166102231\n",
      "Step 27 (5486); Episode 71/100; Loss: 0.04121965169906616\n",
      "Step 28 (5487); Episode 71/100; Loss: 0.07502620667219162\n",
      "Step 29 (5488); Episode 71/100; Loss: 0.03118470311164856\n",
      "Step 30 (5489); Episode 71/100; Loss: 0.02583746612071991\n",
      "Step 31 (5490); Episode 71/100; Loss: 0.0017235887935385108\n",
      "Step 32 (5491); Episode 71/100; Loss: 0.0026496611535549164\n",
      "Step 33 (5492); Episode 71/100; Loss: 0.0015774710336700082\n",
      "Step 34 (5493); Episode 71/100; Loss: 0.0021309421863406897\n",
      "Step 35 (5494); Episode 71/100; Loss: 0.03694084659218788\n",
      "Step 36 (5495); Episode 71/100; Loss: 0.0008285553776659071\n",
      "Step 37 (5496); Episode 71/100; Loss: 0.001898000598885119\n",
      "Step 38 (5497); Episode 71/100; Loss: 0.0006321934633888304\n",
      "Step 39 (5498); Episode 71/100; Loss: 0.0026827461551874876\n",
      "Step 40 (5499); Episode 71/100; Loss: 0.0009197607869282365\n",
      "Step 41 (5500); Episode 71/100; Loss: 0.0005602774326689541\n",
      "Step 42 (5501); Episode 71/100; Loss: 0.04400096461176872\n",
      "Step 43 (5502); Episode 71/100; Loss: 0.0012165370862931013\n",
      "Step 44 (5503); Episode 71/100; Loss: 0.053786471486091614\n",
      "Step 45 (5504); Episode 71/100; Loss: 0.043574489653110504\n",
      "Step 46 (5505); Episode 71/100; Loss: 0.0014683336485177279\n",
      "Step 47 (5506); Episode 71/100; Loss: 0.030823489651083946\n",
      "Step 48 (5507); Episode 71/100; Loss: 0.03515045344829559\n",
      "Step 49 (5508); Episode 71/100; Loss: 0.00217730482108891\n",
      "Step 50 (5509); Episode 71/100; Loss: 0.0007915372843854129\n",
      "Step 51 (5510); Episode 71/100; Loss: 0.000568302464671433\n",
      "Step 52 (5511); Episode 71/100; Loss: 0.0026943546254187822\n",
      "Step 53 (5512); Episode 71/100; Loss: 0.10817387700080872\n",
      "Step 54 (5513); Episode 71/100; Loss: 0.0403667651116848\n",
      "Step 55 (5514); Episode 71/100; Loss: 0.05124347284436226\n",
      "Step 56 (5515); Episode 71/100; Loss: 0.02193424291908741\n",
      "Step 57 (5516); Episode 71/100; Loss: 0.059538133442401886\n",
      "Step 58 (5517); Episode 71/100; Loss: 0.030402887612581253\n",
      "Step 59 (5518); Episode 71/100; Loss: 0.051620740443468094\n",
      "Step 60 (5519); Episode 71/100; Loss: 0.033225685358047485\n",
      "Step 61 (5520); Episode 71/100; Loss: 0.0014601342845708132\n",
      "Step 62 (5521); Episode 71/100; Loss: 0.0016742771258577704\n",
      "Step 63 (5522); Episode 71/100; Loss: 0.001034957473166287\n",
      "Step 64 (5523); Episode 71/100; Loss: 0.0016390401870012283\n",
      "Step 65 (5524); Episode 71/100; Loss: 0.0006287484429776669\n",
      "Step 66 (5525); Episode 71/100; Loss: 0.002361516235396266\n",
      "Step 67 (5526); Episode 71/100; Loss: 0.027253808453679085\n",
      "Step 68 (5527); Episode 71/100; Loss: 0.09788278490304947\n",
      "Step 69 (5528); Episode 71/100; Loss: 0.03485426306724548\n",
      "Step 70 (5529); Episode 71/100; Loss: 0.04629761725664139\n",
      "Step 71 (5530); Episode 71/100; Loss: 0.04614599794149399\n",
      "Step 72 (5531); Episode 71/100; Loss: 0.0013091627042740583\n",
      "Step 73 (5532); Episode 71/100; Loss: 0.0018819400575011969\n",
      "Step 74 (5533); Episode 71/100; Loss: 0.0008819250506348908\n",
      "Step 75 (5534); Episode 71/100; Loss: 0.09068925678730011\n",
      "Step 76 (5535); Episode 71/100; Loss: 0.0013328740606084466\n",
      "Step 77 (5536); Episode 71/100; Loss: 0.0007053834269754589\n",
      "Step 78 (5537); Episode 71/100; Loss: 0.0577457956969738\n",
      "Step 79 (5538); Episode 71/100; Loss: 0.08216957002878189\n",
      "Step 80 (5539); Episode 71/100; Loss: 0.001251115114428103\n",
      "Step 81 (5540); Episode 71/100; Loss: 0.0011916115181520581\n",
      "Step 82 (5541); Episode 71/100; Loss: 0.0019071116112172604\n",
      "Step 83 (5542); Episode 71/100; Loss: 0.001046863617375493\n",
      "Step 84 (5543); Episode 71/100; Loss: 0.06646743416786194\n",
      "Step 85 (5544); Episode 71/100; Loss: 0.044246427714824677\n",
      "Step 86 (5545); Episode 71/100; Loss: 0.010812221094965935\n",
      "Step 87 (5546); Episode 71/100; Loss: 0.0016578182112425566\n",
      "Step 88 (5547); Episode 71/100; Loss: 0.08754912763834\n",
      "Step 89 (5548); Episode 71/100; Loss: 0.026998421177268028\n",
      "Step 90 (5549); Episode 71/100; Loss: 0.0025557088665664196\n",
      "Step 91 (5550); Episode 71/100; Loss: 0.0520295612514019\n",
      "Step 92 (5551); Episode 71/100; Loss: 0.10434489697217941\n",
      "Step 93 (5552); Episode 71/100; Loss: 0.0027554675471037626\n",
      "Step 94 (5553); Episode 71/100; Loss: 0.0028967775870114565\n",
      "Step 95 (5554); Episode 71/100; Loss: 0.0018768914742395282\n",
      "Step 96 (5555); Episode 71/100; Loss: 0.02188849076628685\n",
      "Step 97 (5556); Episode 71/100; Loss: 0.05288856849074364\n",
      "Step 98 (5557); Episode 71/100; Loss: 0.04950765520334244\n",
      "Step 99 (5558); Episode 71/100; Loss: 0.01678331382572651\n",
      "Step 100 (5559); Episode 71/100; Loss: 0.0010282867588102818\n",
      "Step 101 (5560); Episode 71/100; Loss: 0.01896173506975174\n",
      "Step 102 (5561); Episode 71/100; Loss: 0.0012756347423419356\n",
      "Step 103 (5562); Episode 71/100; Loss: 0.08615417033433914\n",
      "Step 104 (5563); Episode 71/100; Loss: 0.028748514130711555\n",
      "Step 105 (5564); Episode 71/100; Loss: 0.14684893190860748\n",
      "Step 106 (5565); Episode 71/100; Loss: 0.026780063286423683\n",
      "Step 107 (5566); Episode 71/100; Loss: 0.031618718057870865\n",
      "Step 108 (5567); Episode 71/100; Loss: 0.0020172058138996363\n",
      "Step 109 (5568); Episode 71/100; Loss: 0.0024197290185838938\n",
      "Step 110 (5569); Episode 71/100; Loss: 0.04323706403374672\n",
      "Step 111 (5570); Episode 71/100; Loss: 0.09206047654151917\n",
      "Step 112 (5571); Episode 71/100; Loss: 0.015056587755680084\n",
      "Step 113 (5572); Episode 71/100; Loss: 0.0011052752379328012\n",
      "Step 114 (5573); Episode 71/100; Loss: 0.0730823427438736\n",
      "Step 115 (5574); Episode 71/100; Loss: 0.009788908064365387\n",
      "Step 116 (5575); Episode 71/100; Loss: 0.027957849204540253\n",
      "Step 117 (5576); Episode 71/100; Loss: 0.09257209300994873\n",
      "Step 118 (5577); Episode 71/100; Loss: 0.016288159415125847\n",
      "Step 119 (5578); Episode 71/100; Loss: 0.004884906578809023\n",
      "Step 120 (5579); Episode 71/100; Loss: 0.039581261575222015\n",
      "Step 121 (5580); Episode 71/100; Loss: 0.0038945411797612906\n",
      "Step 122 (5581); Episode 71/100; Loss: 0.0009175748564302921\n",
      "Step 123 (5582); Episode 71/100; Loss: 0.0942312702536583\n",
      "Step 124 (5583); Episode 71/100; Loss: 0.003753052093088627\n",
      "Step 125 (5584); Episode 71/100; Loss: 0.04951661825180054\n",
      "Step 126 (5585); Episode 71/100; Loss: 0.04621986672282219\n",
      "Step 127 (5586); Episode 71/100; Loss: 0.0014091851189732552\n",
      "Step 128 (5587); Episode 71/100; Loss: 0.0847056433558464\n",
      "Step 129 (5588); Episode 71/100; Loss: 0.0015870389761403203\n",
      "Step 130 (5589); Episode 71/100; Loss: 0.13536448776721954\n",
      "Step 131 (5590); Episode 71/100; Loss: 0.0048722922801971436\n",
      "Step 132 (5591); Episode 71/100; Loss: 0.01763896644115448\n",
      "Step 133 (5592); Episode 71/100; Loss: 0.043034207075834274\n",
      "Step 134 (5593); Episode 71/100; Loss: 0.03169695660471916\n",
      "Step 135 (5594); Episode 71/100; Loss: 0.04529774188995361\n",
      "Step 136 (5595); Episode 71/100; Loss: 0.0032435725443065166\n",
      "Step 137 (5596); Episode 71/100; Loss: 0.0035379899200052023\n",
      "Step 138 (5597); Episode 71/100; Loss: 0.042585209012031555\n",
      "Step 139 (5598); Episode 71/100; Loss: 0.005302509758621454\n",
      "Step 140 (5599); Episode 71/100; Loss: 0.04018668085336685\n",
      "Step 141 (5600); Episode 71/100; Loss: 0.022767651826143265\n",
      "Step 142 (5601); Episode 71/100; Loss: 0.003319435054436326\n",
      "Step 143 (5602); Episode 71/100; Loss: 0.13506357371807098\n",
      "Step 144 (5603); Episode 71/100; Loss: 0.029940344393253326\n",
      "Step 145 (5604); Episode 71/100; Loss: 0.005331413820385933\n",
      "Step 146 (5605); Episode 71/100; Loss: 0.0016468792455270886\n",
      "Step 147 (5606); Episode 71/100; Loss: 0.017018819227814674\n",
      "Step 148 (5607); Episode 71/100; Loss: 0.018437841907143593\n",
      "Step 149 (5608); Episode 71/100; Loss: 0.0011393174063414335\n",
      "Step 150 (5609); Episode 71/100; Loss: 0.04469236731529236\n",
      "Step 151 (5610); Episode 71/100; Loss: 0.004852748941630125\n",
      "Step 152 (5611); Episode 71/100; Loss: 0.0015731310704723\n",
      "Step 153 (5612); Episode 71/100; Loss: 0.0015773106133565307\n",
      "Step 154 (5613); Episode 71/100; Loss: 0.0015243608504533768\n",
      "Step 155 (5614); Episode 71/100; Loss: 0.04548940062522888\n",
      "Step 156 (5615); Episode 71/100; Loss: 0.0014918064698576927\n",
      "Step 157 (5616); Episode 71/100; Loss: 0.056618642061948776\n",
      "Step 0 (5617); Episode 72/100; Loss: 0.003345990087836981\n",
      "Step 1 (5618); Episode 72/100; Loss: 0.04988076165318489\n",
      "Step 2 (5619); Episode 72/100; Loss: 0.0036820184905081987\n",
      "Step 3 (5620); Episode 72/100; Loss: 0.012562599964439869\n",
      "Step 4 (5621); Episode 72/100; Loss: 0.03745952248573303\n",
      "Step 5 (5622); Episode 72/100; Loss: 0.0014411703450605273\n",
      "Step 6 (5623); Episode 72/100; Loss: 0.047076936811208725\n",
      "Step 7 (5624); Episode 72/100; Loss: 0.002451433800160885\n",
      "Step 8 (5625); Episode 72/100; Loss: 0.07728463411331177\n",
      "Step 9 (5626); Episode 72/100; Loss: 0.0015499372966587543\n",
      "Step 10 (5627); Episode 72/100; Loss: 0.08531761914491653\n",
      "Step 11 (5628); Episode 72/100; Loss: 0.0031506528612226248\n",
      "Step 12 (5629); Episode 72/100; Loss: 0.09678218513727188\n",
      "Step 13 (5630); Episode 72/100; Loss: 0.004209702368825674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14 (5631); Episode 72/100; Loss: 0.002458519535139203\n",
      "Step 15 (5632); Episode 72/100; Loss: 0.0014202245511114597\n",
      "Step 16 (5633); Episode 72/100; Loss: 0.003474361030384898\n",
      "Step 17 (5634); Episode 72/100; Loss: 0.0743986964225769\n",
      "Step 18 (5635); Episode 72/100; Loss: 0.007676203269511461\n",
      "Step 19 (5636); Episode 72/100; Loss: 0.001396632636897266\n",
      "Step 20 (5637); Episode 72/100; Loss: 0.0031274561770260334\n",
      "Step 21 (5638); Episode 72/100; Loss: 0.040049728006124496\n",
      "Step 22 (5639); Episode 72/100; Loss: 0.04706001654267311\n",
      "Step 23 (5640); Episode 72/100; Loss: 0.04830614849925041\n",
      "Step 24 (5641); Episode 72/100; Loss: 0.046944864094257355\n",
      "Step 25 (5642); Episode 72/100; Loss: 0.0943891629576683\n",
      "Step 26 (5643); Episode 72/100; Loss: 0.08720768988132477\n",
      "Step 27 (5644); Episode 72/100; Loss: 0.09798579663038254\n",
      "Step 28 (5645); Episode 72/100; Loss: 0.03392975404858589\n",
      "Step 29 (5646); Episode 72/100; Loss: 0.14992249011993408\n",
      "Step 30 (5647); Episode 72/100; Loss: 0.005708902608603239\n",
      "Step 31 (5648); Episode 72/100; Loss: 0.03065497800707817\n",
      "Step 32 (5649); Episode 72/100; Loss: 0.000537162646651268\n",
      "Step 33 (5650); Episode 72/100; Loss: 0.001282546785660088\n",
      "Step 34 (5651); Episode 72/100; Loss: 0.0015824487200006843\n",
      "Step 35 (5652); Episode 72/100; Loss: 0.1141953393816948\n",
      "Step 36 (5653); Episode 72/100; Loss: 0.05382693558931351\n",
      "Step 37 (5654); Episode 72/100; Loss: 0.026771338656544685\n",
      "Step 38 (5655); Episode 72/100; Loss: 0.027299914509058\n",
      "Step 39 (5656); Episode 72/100; Loss: 0.032262105494737625\n",
      "Step 40 (5657); Episode 72/100; Loss: 0.0042184581980109215\n",
      "Step 41 (5658); Episode 72/100; Loss: 0.03472185134887695\n",
      "Step 42 (5659); Episode 72/100; Loss: 0.05925745144486427\n",
      "Step 43 (5660); Episode 72/100; Loss: 0.006027460563927889\n",
      "Step 44 (5661); Episode 72/100; Loss: 0.0015604814980179071\n",
      "Step 45 (5662); Episode 72/100; Loss: 0.037446971982717514\n",
      "Step 46 (5663); Episode 72/100; Loss: 0.0696035623550415\n",
      "Step 47 (5664); Episode 72/100; Loss: 0.0017903329571709037\n",
      "Step 48 (5665); Episode 72/100; Loss: 0.00656408816576004\n",
      "Step 49 (5666); Episode 72/100; Loss: 0.047678131610155106\n",
      "Step 50 (5667); Episode 72/100; Loss: 0.09335730224847794\n",
      "Step 51 (5668); Episode 72/100; Loss: 0.064243845641613\n",
      "Step 52 (5669); Episode 72/100; Loss: 0.04092548415064812\n",
      "Step 53 (5670); Episode 72/100; Loss: 0.003288805251941085\n",
      "Step 54 (5671); Episode 72/100; Loss: 0.06397039443254471\n",
      "Step 55 (5672); Episode 72/100; Loss: 0.049348436295986176\n",
      "Step 56 (5673); Episode 72/100; Loss: 0.001937509747222066\n",
      "Step 57 (5674); Episode 72/100; Loss: 0.001755407894961536\n",
      "Step 58 (5675); Episode 72/100; Loss: 0.014454553835093975\n",
      "Step 59 (5676); Episode 72/100; Loss: 0.09062345325946808\n",
      "Step 60 (5677); Episode 72/100; Loss: 0.034190475940704346\n",
      "Step 61 (5678); Episode 72/100; Loss: 0.0021172312553972006\n",
      "Step 62 (5679); Episode 72/100; Loss: 0.03754108399152756\n",
      "Step 63 (5680); Episode 72/100; Loss: 0.004017046187072992\n",
      "Step 64 (5681); Episode 72/100; Loss: 0.058339741080999374\n",
      "Step 65 (5682); Episode 72/100; Loss: 0.030894285067915916\n",
      "Step 66 (5683); Episode 72/100; Loss: 0.03752686455845833\n",
      "Step 67 (5684); Episode 72/100; Loss: 0.002971853129565716\n",
      "Step 68 (5685); Episode 72/100; Loss: 0.0030333739705383778\n",
      "Step 69 (5686); Episode 72/100; Loss: 0.08080139756202698\n",
      "Step 70 (5687); Episode 72/100; Loss: 0.06755110621452332\n",
      "Step 71 (5688); Episode 72/100; Loss: 0.05262364074587822\n",
      "Step 72 (5689); Episode 72/100; Loss: 0.0010028532706201077\n",
      "Step 73 (5690); Episode 72/100; Loss: 0.019414011389017105\n",
      "Step 74 (5691); Episode 72/100; Loss: 0.0686379075050354\n",
      "Step 75 (5692); Episode 72/100; Loss: 0.0023827441036701202\n",
      "Step 76 (5693); Episode 72/100; Loss: 0.03431734815239906\n",
      "Step 77 (5694); Episode 72/100; Loss: 0.005619036965072155\n",
      "Step 78 (5695); Episode 72/100; Loss: 0.0010263414587825537\n",
      "Step 79 (5696); Episode 72/100; Loss: 0.05022747442126274\n",
      "Step 80 (5697); Episode 72/100; Loss: 0.0023138525430113077\n",
      "Step 81 (5698); Episode 72/100; Loss: 0.02501954510807991\n",
      "Step 82 (5699); Episode 72/100; Loss: 0.061076533049345016\n",
      "Step 83 (5700); Episode 72/100; Loss: 0.040464457124471664\n",
      "Step 84 (5701); Episode 72/100; Loss: 0.04689662158489227\n",
      "Step 85 (5702); Episode 72/100; Loss: 0.0022110817953944206\n",
      "Step 86 (5703); Episode 72/100; Loss: 0.11784298717975616\n",
      "Step 87 (5704); Episode 72/100; Loss: 0.021515605971217155\n",
      "Step 88 (5705); Episode 72/100; Loss: 0.0022851729299873114\n",
      "Step 89 (5706); Episode 72/100; Loss: 0.0475701205432415\n",
      "Step 90 (5707); Episode 72/100; Loss: 0.07398343831300735\n",
      "Step 91 (5708); Episode 72/100; Loss: 0.018537694588303566\n",
      "Step 92 (5709); Episode 72/100; Loss: 0.003406933043152094\n",
      "Step 93 (5710); Episode 72/100; Loss: 0.09430893510580063\n",
      "Step 94 (5711); Episode 72/100; Loss: 0.025996733456850052\n",
      "Step 95 (5712); Episode 72/100; Loss: 0.005650532431900501\n",
      "Step 96 (5713); Episode 72/100; Loss: 0.0026025378610938787\n",
      "Step 97 (5714); Episode 72/100; Loss: 0.0018037144327536225\n",
      "Step 98 (5715); Episode 72/100; Loss: 0.003041301853954792\n",
      "Step 99 (5716); Episode 72/100; Loss: 0.0607466846704483\n",
      "Step 100 (5717); Episode 72/100; Loss: 0.07770856469869614\n",
      "Step 101 (5718); Episode 72/100; Loss: 0.002297461498528719\n",
      "Step 102 (5719); Episode 72/100; Loss: 0.09570326656103134\n",
      "Step 103 (5720); Episode 72/100; Loss: 0.0020567921455949545\n",
      "Step 104 (5721); Episode 72/100; Loss: 0.00230977451428771\n",
      "Step 105 (5722); Episode 72/100; Loss: 0.013227281160652637\n",
      "Step 106 (5723); Episode 72/100; Loss: 0.0009747245931066573\n",
      "Step 107 (5724); Episode 72/100; Loss: 0.0030304724350571632\n",
      "Step 108 (5725); Episode 72/100; Loss: 0.04764164239168167\n",
      "Step 109 (5726); Episode 72/100; Loss: 0.05626809969544411\n",
      "Step 110 (5727); Episode 72/100; Loss: 0.095081627368927\n",
      "Step 111 (5728); Episode 72/100; Loss: 0.04540137201547623\n",
      "Step 112 (5729); Episode 72/100; Loss: 0.0013398590963333845\n",
      "Step 113 (5730); Episode 72/100; Loss: 0.01621917448937893\n",
      "Step 114 (5731); Episode 72/100; Loss: 0.09272480756044388\n",
      "Step 115 (5732); Episode 72/100; Loss: 0.055011481046676636\n",
      "Step 116 (5733); Episode 72/100; Loss: 0.002104894956573844\n",
      "Step 117 (5734); Episode 72/100; Loss: 0.09652768820524216\n",
      "Step 118 (5735); Episode 72/100; Loss: 0.004175887443125248\n",
      "Step 119 (5736); Episode 72/100; Loss: 0.04758872091770172\n",
      "Step 120 (5737); Episode 72/100; Loss: 0.04370581731200218\n",
      "Step 121 (5738); Episode 72/100; Loss: 0.0016947297845035791\n",
      "Step 122 (5739); Episode 72/100; Loss: 0.0019845610950142145\n",
      "Step 123 (5740); Episode 72/100; Loss: 0.0017679237062111497\n",
      "Step 124 (5741); Episode 72/100; Loss: 0.0014763021608814597\n",
      "Step 125 (5742); Episode 72/100; Loss: 0.05724995955824852\n",
      "Step 126 (5743); Episode 72/100; Loss: 0.08967218548059464\n",
      "Step 127 (5744); Episode 72/100; Loss: 0.0033572539687156677\n",
      "Step 128 (5745); Episode 72/100; Loss: 0.043821293860673904\n",
      "Step 129 (5746); Episode 72/100; Loss: 0.0804947018623352\n",
      "Step 130 (5747); Episode 72/100; Loss: 0.02814248763024807\n",
      "Step 131 (5748); Episode 72/100; Loss: 0.0012606235686689615\n",
      "Step 132 (5749); Episode 72/100; Loss: 0.044600579887628555\n",
      "Step 133 (5750); Episode 72/100; Loss: 0.02992718666791916\n",
      "Step 134 (5751); Episode 72/100; Loss: 0.057435449212789536\n",
      "Step 135 (5752); Episode 72/100; Loss: 0.2110009789466858\n",
      "Step 136 (5753); Episode 72/100; Loss: 0.029380351305007935\n",
      "Step 137 (5754); Episode 72/100; Loss: 0.08421047776937485\n",
      "Step 138 (5755); Episode 72/100; Loss: 0.002194395288825035\n",
      "Step 139 (5756); Episode 72/100; Loss: 0.04078354686498642\n",
      "Step 140 (5757); Episode 72/100; Loss: 0.007929808460175991\n",
      "Step 141 (5758); Episode 72/100; Loss: 0.03314322233200073\n",
      "Step 142 (5759); Episode 72/100; Loss: 0.07123661786317825\n",
      "Step 143 (5760); Episode 72/100; Loss: 0.0028824496548622847\n",
      "Step 144 (5761); Episode 72/100; Loss: 0.031321778893470764\n",
      "Step 145 (5762); Episode 72/100; Loss: 0.1499355137348175\n",
      "Step 146 (5763); Episode 72/100; Loss: 0.0010510412976145744\n",
      "Step 147 (5764); Episode 72/100; Loss: 0.009057490155100822\n",
      "Step 148 (5765); Episode 72/100; Loss: 0.04535480961203575\n",
      "Step 149 (5766); Episode 72/100; Loss: 0.0025820962619036436\n",
      "Step 150 (5767); Episode 72/100; Loss: 0.07049909234046936\n",
      "Step 151 (5768); Episode 72/100; Loss: 0.0030298293568193913\n",
      "Step 152 (5769); Episode 72/100; Loss: 0.002627675188705325\n",
      "Step 153 (5770); Episode 72/100; Loss: 0.0022122820373624563\n",
      "Step 154 (5771); Episode 72/100; Loss: 0.005268063861876726\n",
      "Step 155 (5772); Episode 72/100; Loss: 0.00845551211386919\n",
      "Step 156 (5773); Episode 72/100; Loss: 0.0011478518135845661\n",
      "Step 157 (5774); Episode 72/100; Loss: 0.010462081991136074\n",
      "Step 158 (5775); Episode 72/100; Loss: 0.07801520824432373\n",
      "Step 159 (5776); Episode 72/100; Loss: 0.00654063792899251\n",
      "Step 160 (5777); Episode 72/100; Loss: 0.0014448700239881873\n",
      "Step 161 (5778); Episode 72/100; Loss: 0.0016744522145017982\n",
      "Step 162 (5779); Episode 72/100; Loss: 0.0015157313318923116\n",
      "Step 163 (5780); Episode 72/100; Loss: 0.0028373892419040203\n",
      "Step 164 (5781); Episode 72/100; Loss: 0.0016485134838148952\n",
      "Step 165 (5782); Episode 72/100; Loss: 0.0031704874709248543\n",
      "Step 166 (5783); Episode 72/100; Loss: 0.0018709306605160236\n",
      "Step 167 (5784); Episode 72/100; Loss: 0.004839307628571987\n",
      "Step 168 (5785); Episode 72/100; Loss: 0.00391645822674036\n",
      "Step 169 (5786); Episode 72/100; Loss: 0.07573004066944122\n",
      "Step 170 (5787); Episode 72/100; Loss: 0.0019534549210220575\n",
      "Step 171 (5788); Episode 72/100; Loss: 0.04954509437084198\n",
      "Step 172 (5789); Episode 72/100; Loss: 0.002178990049287677\n",
      "Step 173 (5790); Episode 72/100; Loss: 0.09682628512382507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 174 (5791); Episode 72/100; Loss: 0.04372434690594673\n",
      "Step 175 (5792); Episode 72/100; Loss: 0.05931421369314194\n",
      "Step 176 (5793); Episode 72/100; Loss: 0.010165226645767689\n",
      "Step 177 (5794); Episode 72/100; Loss: 0.0017181592993438244\n",
      "Step 178 (5795); Episode 72/100; Loss: 0.07647034525871277\n",
      "Step 179 (5796); Episode 72/100; Loss: 0.1273747980594635\n",
      "Step 180 (5797); Episode 72/100; Loss: 0.00828566588461399\n",
      "Step 181 (5798); Episode 72/100; Loss: 0.002803378738462925\n",
      "Step 182 (5799); Episode 72/100; Loss: 0.0010378285078331828\n",
      "Step 183 (5800); Episode 72/100; Loss: 0.05963285267353058\n",
      "Step 184 (5801); Episode 72/100; Loss: 0.0012638299958780408\n",
      "Step 185 (5802); Episode 72/100; Loss: 0.0006700780941173434\n",
      "Step 186 (5803); Episode 72/100; Loss: 0.0016744483727961779\n",
      "Step 187 (5804); Episode 72/100; Loss: 0.0012315564090386033\n",
      "Step 188 (5805); Episode 72/100; Loss: 0.07296130061149597\n",
      "Step 189 (5806); Episode 72/100; Loss: 0.02642098255455494\n",
      "Step 190 (5807); Episode 72/100; Loss: 0.07918056845664978\n",
      "Step 191 (5808); Episode 72/100; Loss: 0.04933536425232887\n",
      "Step 192 (5809); Episode 72/100; Loss: 0.002590079791843891\n",
      "Step 193 (5810); Episode 72/100; Loss: 0.0015309668378904462\n",
      "Step 194 (5811); Episode 72/100; Loss: 0.001928243087604642\n",
      "Step 195 (5812); Episode 72/100; Loss: 0.012477000243961811\n",
      "Step 196 (5813); Episode 72/100; Loss: 0.002304461318999529\n",
      "Step 197 (5814); Episode 72/100; Loss: 0.06817544251680374\n",
      "Step 198 (5815); Episode 72/100; Loss: 0.001475699944421649\n",
      "Step 199 (5816); Episode 72/100; Loss: 0.0007791414391249418\n",
      "Step 0 (5817); Episode 73/100; Loss: 0.08469675481319427\n",
      "Step 1 (5818); Episode 73/100; Loss: 0.0010097791673615575\n",
      "Step 2 (5819); Episode 73/100; Loss: 0.0016571672167629004\n",
      "Step 3 (5820); Episode 73/100; Loss: 0.05118173733353615\n",
      "Step 4 (5821); Episode 73/100; Loss: 0.04545852541923523\n",
      "Step 5 (5822); Episode 73/100; Loss: 0.0027816258370876312\n",
      "Step 6 (5823); Episode 73/100; Loss: 0.08160492032766342\n",
      "Step 7 (5824); Episode 73/100; Loss: 0.0034932310227304697\n",
      "Step 8 (5825); Episode 73/100; Loss: 0.09938792139291763\n",
      "Step 9 (5826); Episode 73/100; Loss: 0.11174087226390839\n",
      "Step 10 (5827); Episode 73/100; Loss: 0.08219284564256668\n",
      "Step 11 (5828); Episode 73/100; Loss: 0.0025258134119212627\n",
      "Step 12 (5829); Episode 73/100; Loss: 0.09019535779953003\n",
      "Step 13 (5830); Episode 73/100; Loss: 0.047501035034656525\n",
      "Step 14 (5831); Episode 73/100; Loss: 0.05041230469942093\n",
      "Step 15 (5832); Episode 73/100; Loss: 0.057764142751693726\n",
      "Step 16 (5833); Episode 73/100; Loss: 0.057750899344682693\n",
      "Step 17 (5834); Episode 73/100; Loss: 0.05611570551991463\n",
      "Step 18 (5835); Episode 73/100; Loss: 0.03634199872612953\n",
      "Step 19 (5836); Episode 73/100; Loss: 0.01241022814065218\n",
      "Step 20 (5837); Episode 73/100; Loss: 0.002303638029843569\n",
      "Step 21 (5838); Episode 73/100; Loss: 0.0033954086247831583\n",
      "Step 22 (5839); Episode 73/100; Loss: 0.0932210311293602\n",
      "Step 23 (5840); Episode 73/100; Loss: 0.038476958870887756\n",
      "Step 24 (5841); Episode 73/100; Loss: 0.0938199907541275\n",
      "Step 25 (5842); Episode 73/100; Loss: 0.0012950529344379902\n",
      "Step 26 (5843); Episode 73/100; Loss: 0.0014429979491978884\n",
      "Step 27 (5844); Episode 73/100; Loss: 0.002551706274971366\n",
      "Step 28 (5845); Episode 73/100; Loss: 0.0016224575228989124\n",
      "Step 29 (5846); Episode 73/100; Loss: 0.0023232505191117525\n",
      "Step 30 (5847); Episode 73/100; Loss: 0.1493365317583084\n",
      "Step 31 (5848); Episode 73/100; Loss: 0.002707354724407196\n",
      "Step 32 (5849); Episode 73/100; Loss: 0.04230974614620209\n",
      "Step 33 (5850); Episode 73/100; Loss: 0.0024020036216825247\n",
      "Step 34 (5851); Episode 73/100; Loss: 0.058873746544122696\n",
      "Step 35 (5852); Episode 73/100; Loss: 0.005751572083681822\n",
      "Step 36 (5853); Episode 73/100; Loss: 0.055730193853378296\n",
      "Step 37 (5854); Episode 73/100; Loss: 0.05125334858894348\n",
      "Step 38 (5855); Episode 73/100; Loss: 0.05157218500971794\n",
      "Step 39 (5856); Episode 73/100; Loss: 0.06822137534618378\n",
      "Step 40 (5857); Episode 73/100; Loss: 0.001417609746567905\n",
      "Step 41 (5858); Episode 73/100; Loss: 0.039505355060100555\n",
      "Step 42 (5859); Episode 73/100; Loss: 0.044048942625522614\n",
      "Step 43 (5860); Episode 73/100; Loss: 0.03676382079720497\n",
      "Step 44 (5861); Episode 73/100; Loss: 0.0014768021646887064\n",
      "Step 45 (5862); Episode 73/100; Loss: 0.00429175328463316\n",
      "Step 46 (5863); Episode 73/100; Loss: 0.0020936194341629744\n",
      "Step 47 (5864); Episode 73/100; Loss: 0.05885905399918556\n",
      "Step 48 (5865); Episode 73/100; Loss: 0.04271998628973961\n",
      "Step 49 (5866); Episode 73/100; Loss: 0.0470946729183197\n",
      "Step 50 (5867); Episode 73/100; Loss: 0.002118389355018735\n",
      "Step 51 (5868); Episode 73/100; Loss: 0.07397990673780441\n",
      "Step 52 (5869); Episode 73/100; Loss: 0.1127220094203949\n",
      "Step 53 (5870); Episode 73/100; Loss: 0.04241635277867317\n",
      "Step 54 (5871); Episode 73/100; Loss: 0.04020218551158905\n",
      "Step 55 (5872); Episode 73/100; Loss: 0.0335204042494297\n",
      "Step 56 (5873); Episode 73/100; Loss: 0.0022333485540002584\n",
      "Step 57 (5874); Episode 73/100; Loss: 0.0010838008020073175\n",
      "Step 58 (5875); Episode 73/100; Loss: 0.044480983167886734\n",
      "Step 59 (5876); Episode 73/100; Loss: 0.0026633450761437416\n",
      "Step 60 (5877); Episode 73/100; Loss: 0.0022787400521337986\n",
      "Step 61 (5878); Episode 73/100; Loss: 0.14494267106056213\n",
      "Step 62 (5879); Episode 73/100; Loss: 0.001959865679964423\n",
      "Step 63 (5880); Episode 73/100; Loss: 0.06314344704151154\n",
      "Step 64 (5881); Episode 73/100; Loss: 0.002699419856071472\n",
      "Step 65 (5882); Episode 73/100; Loss: 0.04902997985482216\n",
      "Step 66 (5883); Episode 73/100; Loss: 0.001534423790872097\n",
      "Step 67 (5884); Episode 73/100; Loss: 0.010931551456451416\n",
      "Step 68 (5885); Episode 73/100; Loss: 0.013551279902458191\n",
      "Step 69 (5886); Episode 73/100; Loss: 0.0044197761453688145\n",
      "Step 70 (5887); Episode 73/100; Loss: 0.10320359468460083\n",
      "Step 71 (5888); Episode 73/100; Loss: 0.0452297143638134\n",
      "Step 72 (5889); Episode 73/100; Loss: 0.0017289369134232402\n",
      "Step 73 (5890); Episode 73/100; Loss: 0.0025045534130185843\n",
      "Step 74 (5891); Episode 73/100; Loss: 0.046674538403749466\n",
      "Step 75 (5892); Episode 73/100; Loss: 0.0038163766730576754\n",
      "Step 76 (5893); Episode 73/100; Loss: 0.00962704885751009\n",
      "Step 77 (5894); Episode 73/100; Loss: 0.001650442136451602\n",
      "Step 78 (5895); Episode 73/100; Loss: 0.04523405060172081\n",
      "Step 79 (5896); Episode 73/100; Loss: 0.05505487695336342\n",
      "Step 80 (5897); Episode 73/100; Loss: 0.0011251326650381088\n",
      "Step 81 (5898); Episode 73/100; Loss: 0.0882909819483757\n",
      "Step 82 (5899); Episode 73/100; Loss: 0.0022349569480866194\n",
      "Step 83 (5900); Episode 73/100; Loss: 0.004926549270749092\n",
      "Step 84 (5901); Episode 73/100; Loss: 0.002066379878669977\n",
      "Step 85 (5902); Episode 73/100; Loss: 0.10300535708665848\n",
      "Step 86 (5903); Episode 73/100; Loss: 0.04659094288945198\n",
      "Step 87 (5904); Episode 73/100; Loss: 0.0038744204211980104\n",
      "Step 88 (5905); Episode 73/100; Loss: 0.0015578378224745393\n",
      "Step 89 (5906); Episode 73/100; Loss: 0.0011990745551884174\n",
      "Step 90 (5907); Episode 73/100; Loss: 0.053762827068567276\n",
      "Step 91 (5908); Episode 73/100; Loss: 0.02490629069507122\n",
      "Step 92 (5909); Episode 73/100; Loss: 0.003220728598535061\n",
      "Step 93 (5910); Episode 73/100; Loss: 0.04857592284679413\n",
      "Step 94 (5911); Episode 73/100; Loss: 0.1095457524061203\n",
      "Step 95 (5912); Episode 73/100; Loss: 0.0036748549900949\n",
      "Step 96 (5913); Episode 73/100; Loss: 0.0022217724472284317\n",
      "Step 97 (5914); Episode 73/100; Loss: 0.004270927514880896\n",
      "Step 98 (5915); Episode 73/100; Loss: 0.0025079380720853806\n",
      "Step 99 (5916); Episode 73/100; Loss: 0.001898937625810504\n",
      "Step 100 (5917); Episode 73/100; Loss: 0.0019447605591267347\n",
      "Step 101 (5918); Episode 73/100; Loss: 0.05398789420723915\n",
      "Step 102 (5919); Episode 73/100; Loss: 0.059421002864837646\n",
      "Step 103 (5920); Episode 73/100; Loss: 0.04912547022104263\n",
      "Step 104 (5921); Episode 73/100; Loss: 0.0005436702049337327\n",
      "Step 105 (5922); Episode 73/100; Loss: 0.0009379664552398026\n",
      "Step 106 (5923); Episode 73/100; Loss: 0.05465821921825409\n",
      "Step 107 (5924); Episode 73/100; Loss: 0.03804684802889824\n",
      "Step 108 (5925); Episode 73/100; Loss: 0.003199287923052907\n",
      "Step 109 (5926); Episode 73/100; Loss: 0.04998914897441864\n",
      "Step 110 (5927); Episode 73/100; Loss: 0.006556842476129532\n",
      "Step 111 (5928); Episode 73/100; Loss: 0.054999567568302155\n",
      "Step 112 (5929); Episode 73/100; Loss: 0.0008724173530936241\n",
      "Step 113 (5930); Episode 73/100; Loss: 0.04007461667060852\n",
      "Step 114 (5931); Episode 73/100; Loss: 0.0027920398861169815\n",
      "Step 115 (5932); Episode 73/100; Loss: 0.0022315823007375\n",
      "Step 116 (5933); Episode 73/100; Loss: 0.0960741639137268\n",
      "Step 117 (5934); Episode 73/100; Loss: 0.004468094557523727\n",
      "Step 118 (5935); Episode 73/100; Loss: 0.0019315950339660048\n",
      "Step 119 (5936); Episode 73/100; Loss: 0.0011915690265595913\n",
      "Step 120 (5937); Episode 73/100; Loss: 0.0011108035687357187\n",
      "Step 121 (5938); Episode 73/100; Loss: 0.001748266164213419\n",
      "Step 122 (5939); Episode 73/100; Loss: 0.0007760207518003881\n",
      "Step 123 (5940); Episode 73/100; Loss: 0.0015350672183558345\n",
      "Step 124 (5941); Episode 73/100; Loss: 0.0020297763403505087\n",
      "Step 125 (5942); Episode 73/100; Loss: 0.002511280355975032\n",
      "Step 126 (5943); Episode 73/100; Loss: 0.0019248331664130092\n",
      "Step 127 (5944); Episode 73/100; Loss: 0.0015594223514199257\n",
      "Step 128 (5945); Episode 73/100; Loss: 0.04095291346311569\n",
      "Step 129 (5946); Episode 73/100; Loss: 0.06155674159526825\n",
      "Step 130 (5947); Episode 73/100; Loss: 0.0015650660498067737\n",
      "Step 131 (5948); Episode 73/100; Loss: 0.000784408301115036\n",
      "Step 132 (5949); Episode 73/100; Loss: 0.0014131182106211782\n",
      "Step 133 (5950); Episode 73/100; Loss: 0.01755988784134388\n",
      "Step 134 (5951); Episode 73/100; Loss: 0.15147057175636292\n",
      "Step 135 (5952); Episode 73/100; Loss: 0.05436651036143303\n",
      "Step 136 (5953); Episode 73/100; Loss: 0.0012358539970591664\n",
      "Step 137 (5954); Episode 73/100; Loss: 0.0022610819432884455\n",
      "Step 138 (5955); Episode 73/100; Loss: 0.03439008444547653\n",
      "Step 139 (5956); Episode 73/100; Loss: 0.0013202375266700983\n",
      "Step 140 (5957); Episode 73/100; Loss: 0.028357286006212234\n",
      "Step 141 (5958); Episode 73/100; Loss: 0.0027085489127784967\n",
      "Step 142 (5959); Episode 73/100; Loss: 0.02555963210761547\n",
      "Step 143 (5960); Episode 73/100; Loss: 0.07337111979722977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 144 (5961); Episode 73/100; Loss: 0.0014618966961279511\n",
      "Step 145 (5962); Episode 73/100; Loss: 0.012352253310382366\n",
      "Step 146 (5963); Episode 73/100; Loss: 0.020219463855028152\n",
      "Step 147 (5964); Episode 73/100; Loss: 0.10366089642047882\n",
      "Step 148 (5965); Episode 73/100; Loss: 0.0011454132618382573\n",
      "Step 149 (5966); Episode 73/100; Loss: 0.13448989391326904\n",
      "Step 150 (5967); Episode 73/100; Loss: 0.05374756082892418\n",
      "Step 151 (5968); Episode 73/100; Loss: 0.002436200389638543\n",
      "Step 152 (5969); Episode 73/100; Loss: 0.0008094427757896483\n",
      "Step 153 (5970); Episode 73/100; Loss: 0.0041249459609389305\n",
      "Step 154 (5971); Episode 73/100; Loss: 0.0009058981668204069\n",
      "Step 155 (5972); Episode 73/100; Loss: 0.0016627864679321647\n",
      "Step 156 (5973); Episode 73/100; Loss: 0.0005014297203160822\n",
      "Step 157 (5974); Episode 73/100; Loss: 0.0014758585020899773\n",
      "Step 158 (5975); Episode 73/100; Loss: 0.06300387531518936\n",
      "Step 159 (5976); Episode 73/100; Loss: 0.024206776171922684\n",
      "Step 160 (5977); Episode 73/100; Loss: 0.022166486829519272\n",
      "Step 161 (5978); Episode 73/100; Loss: 0.003773753298446536\n",
      "Step 162 (5979); Episode 73/100; Loss: 0.04907150939106941\n",
      "Step 163 (5980); Episode 73/100; Loss: 0.003717644838616252\n",
      "Step 164 (5981); Episode 73/100; Loss: 0.0020248787477612495\n",
      "Step 165 (5982); Episode 73/100; Loss: 0.06039309874176979\n",
      "Step 166 (5983); Episode 73/100; Loss: 0.0013772441307082772\n",
      "Step 167 (5984); Episode 73/100; Loss: 0.052169982343912125\n",
      "Step 168 (5985); Episode 73/100; Loss: 0.054996609687805176\n",
      "Step 169 (5986); Episode 73/100; Loss: 0.04958237707614899\n",
      "Step 170 (5987); Episode 73/100; Loss: 0.0009535375866107643\n",
      "Step 171 (5988); Episode 73/100; Loss: 0.05142557621002197\n",
      "Step 172 (5989); Episode 73/100; Loss: 0.08948468416929245\n",
      "Step 173 (5990); Episode 73/100; Loss: 0.0009792918572202325\n",
      "Step 174 (5991); Episode 73/100; Loss: 0.0014172546798363328\n",
      "Step 175 (5992); Episode 73/100; Loss: 0.0015242955414578319\n",
      "Step 176 (5993); Episode 73/100; Loss: 0.0021342956461012363\n",
      "Step 177 (5994); Episode 73/100; Loss: 0.0006892068195156753\n",
      "Step 178 (5995); Episode 73/100; Loss: 0.003476248122751713\n",
      "Step 179 (5996); Episode 73/100; Loss: 0.0011418878566473722\n",
      "Step 180 (5997); Episode 73/100; Loss: 0.04533825069665909\n",
      "Step 181 (5998); Episode 73/100; Loss: 0.04751357436180115\n",
      "Step 182 (5999); Episode 73/100; Loss: 0.011621478945016861\n",
      "Step 183 (6000); Episode 73/100; Loss: 0.06122899800539017\n",
      "Step 184 (6001); Episode 73/100; Loss: 0.0007503307424485683\n",
      "Step 185 (6002); Episode 73/100; Loss: 0.0025354253593832254\n",
      "Step 186 (6003); Episode 73/100; Loss: 0.044467777013778687\n",
      "Step 187 (6004); Episode 73/100; Loss: 0.0813099816441536\n",
      "Step 188 (6005); Episode 73/100; Loss: 0.002608884358778596\n",
      "Step 189 (6006); Episode 73/100; Loss: 0.0015758785884827375\n",
      "Step 190 (6007); Episode 73/100; Loss: 0.05652114748954773\n",
      "Step 191 (6008); Episode 73/100; Loss: 0.001931075006723404\n",
      "Step 192 (6009); Episode 73/100; Loss: 0.0013604342238977551\n",
      "Step 193 (6010); Episode 73/100; Loss: 0.020396307110786438\n",
      "Step 194 (6011); Episode 73/100; Loss: 0.04982969909906387\n",
      "Step 195 (6012); Episode 73/100; Loss: 0.056528158485889435\n",
      "Step 196 (6013); Episode 73/100; Loss: 0.005571519490331411\n",
      "Step 197 (6014); Episode 73/100; Loss: 0.05348401516675949\n",
      "Step 198 (6015); Episode 73/100; Loss: 0.0012022568844258785\n",
      "Step 199 (6016); Episode 73/100; Loss: 0.04830025136470795\n",
      "Step 0 (6017); Episode 74/100; Loss: 0.002647352172061801\n",
      "Step 1 (6018); Episode 74/100; Loss: 0.0016457242891192436\n",
      "Step 2 (6019); Episode 74/100; Loss: 0.0037210294976830482\n",
      "Step 3 (6020); Episode 74/100; Loss: 0.0943160429596901\n",
      "Step 4 (6021); Episode 74/100; Loss: 0.03269185125827789\n",
      "Step 5 (6022); Episode 74/100; Loss: 0.004627919290214777\n",
      "Step 6 (6023); Episode 74/100; Loss: 0.04996338114142418\n",
      "Step 7 (6024); Episode 74/100; Loss: 0.0020340054761618376\n",
      "Step 8 (6025); Episode 74/100; Loss: 0.004257506225258112\n",
      "Step 9 (6026); Episode 74/100; Loss: 0.07288717478513718\n",
      "Step 10 (6027); Episode 74/100; Loss: 0.003400537883862853\n",
      "Step 11 (6028); Episode 74/100; Loss: 0.0019822902977466583\n",
      "Step 12 (6029); Episode 74/100; Loss: 0.0015789868775755167\n",
      "Step 13 (6030); Episode 74/100; Loss: 0.001498909667134285\n",
      "Step 14 (6031); Episode 74/100; Loss: 0.045938439667224884\n",
      "Step 15 (6032); Episode 74/100; Loss: 0.0010036980966106057\n",
      "Step 16 (6033); Episode 74/100; Loss: 0.037674739956855774\n",
      "Step 17 (6034); Episode 74/100; Loss: 0.0013051932910457253\n",
      "Step 18 (6035); Episode 74/100; Loss: 0.057678479701280594\n",
      "Step 19 (6036); Episode 74/100; Loss: 0.013912211172282696\n",
      "Step 20 (6037); Episode 74/100; Loss: 0.0019300957210361958\n",
      "Step 21 (6038); Episode 74/100; Loss: 0.05940592288970947\n",
      "Step 22 (6039); Episode 74/100; Loss: 0.0016499710036441684\n",
      "Step 23 (6040); Episode 74/100; Loss: 0.028927255421876907\n",
      "Step 24 (6041); Episode 74/100; Loss: 0.010416455566883087\n",
      "Step 25 (6042); Episode 74/100; Loss: 0.00045850183232687414\n",
      "Step 26 (6043); Episode 74/100; Loss: 0.0014284946955740452\n",
      "Step 27 (6044); Episode 74/100; Loss: 0.09746894240379333\n",
      "Step 28 (6045); Episode 74/100; Loss: 0.08368822187185287\n",
      "Step 29 (6046); Episode 74/100; Loss: 0.0010528985876590014\n",
      "Step 30 (6047); Episode 74/100; Loss: 0.12514497339725494\n",
      "Step 31 (6048); Episode 74/100; Loss: 0.015588897280395031\n",
      "Step 32 (6049); Episode 74/100; Loss: 0.04968757554888725\n",
      "Step 33 (6050); Episode 74/100; Loss: 0.0017074465285986662\n",
      "Step 34 (6051); Episode 74/100; Loss: 0.0008714325958862901\n",
      "Step 35 (6052); Episode 74/100; Loss: 0.04804733395576477\n",
      "Step 36 (6053); Episode 74/100; Loss: 0.002590758027508855\n",
      "Step 37 (6054); Episode 74/100; Loss: 0.0394037589430809\n",
      "Step 38 (6055); Episode 74/100; Loss: 0.0016831294633448124\n",
      "Step 39 (6056); Episode 74/100; Loss: 0.02485235035419464\n",
      "Step 40 (6057); Episode 74/100; Loss: 0.022272145375609398\n",
      "Step 41 (6058); Episode 74/100; Loss: 0.001111885765567422\n",
      "Step 42 (6059); Episode 74/100; Loss: 0.002701155375689268\n",
      "Step 43 (6060); Episode 74/100; Loss: 0.0010610587196424603\n",
      "Step 44 (6061); Episode 74/100; Loss: 0.0011240909807384014\n",
      "Step 45 (6062); Episode 74/100; Loss: 0.03475957363843918\n",
      "Step 46 (6063); Episode 74/100; Loss: 0.0025215772911906242\n",
      "Step 47 (6064); Episode 74/100; Loss: 0.05204816162586212\n",
      "Step 48 (6065); Episode 74/100; Loss: 0.043166011571884155\n",
      "Step 49 (6066); Episode 74/100; Loss: 0.0014141835272312164\n",
      "Step 50 (6067); Episode 74/100; Loss: 0.0970328226685524\n",
      "Step 51 (6068); Episode 74/100; Loss: 0.004829828627407551\n",
      "Step 52 (6069); Episode 74/100; Loss: 0.0018320033559575677\n",
      "Step 53 (6070); Episode 74/100; Loss: 0.0006940661696717143\n",
      "Step 54 (6071); Episode 74/100; Loss: 0.00045152445090934634\n",
      "Step 55 (6072); Episode 74/100; Loss: 0.03884091600775719\n",
      "Step 56 (6073); Episode 74/100; Loss: 0.04086047783493996\n",
      "Step 57 (6074); Episode 74/100; Loss: 0.0399836041033268\n",
      "Step 58 (6075); Episode 74/100; Loss: 0.001100343419238925\n",
      "Step 59 (6076); Episode 74/100; Loss: 0.08368787169456482\n",
      "Step 60 (6077); Episode 74/100; Loss: 0.05200432240962982\n",
      "Step 61 (6078); Episode 74/100; Loss: 0.05709113925695419\n",
      "Step 62 (6079); Episode 74/100; Loss: 0.0019128667190670967\n",
      "Step 63 (6080); Episode 74/100; Loss: 0.03538777306675911\n",
      "Step 64 (6081); Episode 74/100; Loss: 0.01031662616878748\n",
      "Step 65 (6082); Episode 74/100; Loss: 0.016125211492180824\n",
      "Step 66 (6083); Episode 74/100; Loss: 0.04684935882687569\n",
      "Step 67 (6084); Episode 74/100; Loss: 0.009687710553407669\n",
      "Step 68 (6085); Episode 74/100; Loss: 0.0010380677413195372\n",
      "Step 69 (6086); Episode 74/100; Loss: 0.04612705484032631\n",
      "Step 70 (6087); Episode 74/100; Loss: 0.003271152265369892\n",
      "Step 71 (6088); Episode 74/100; Loss: 0.08630664646625519\n",
      "Step 72 (6089); Episode 74/100; Loss: 0.05027231201529503\n",
      "Step 73 (6090); Episode 74/100; Loss: 0.0015781328547745943\n",
      "Step 74 (6091); Episode 74/100; Loss: 0.0012053310638293624\n",
      "Step 75 (6092); Episode 74/100; Loss: 0.14671330153942108\n",
      "Step 76 (6093); Episode 74/100; Loss: 0.041899386793375015\n",
      "Step 77 (6094); Episode 74/100; Loss: 0.02782129868865013\n",
      "Step 78 (6095); Episode 74/100; Loss: 0.0027005658484995365\n",
      "Step 79 (6096); Episode 74/100; Loss: 0.12448180466890335\n",
      "Step 80 (6097); Episode 74/100; Loss: 0.026158306747674942\n",
      "Step 81 (6098); Episode 74/100; Loss: 0.0020295775029808283\n",
      "Step 82 (6099); Episode 74/100; Loss: 0.09050159156322479\n",
      "Step 83 (6100); Episode 74/100; Loss: 0.04323556646704674\n",
      "Step 84 (6101); Episode 74/100; Loss: 0.002722226083278656\n",
      "Step 85 (6102); Episode 74/100; Loss: 0.04477173462510109\n",
      "Step 86 (6103); Episode 74/100; Loss: 0.04712047800421715\n",
      "Step 87 (6104); Episode 74/100; Loss: 0.0052501303143799305\n",
      "Step 88 (6105); Episode 74/100; Loss: 0.048198599368333817\n",
      "Step 89 (6106); Episode 74/100; Loss: 0.0026316028088331223\n",
      "Step 90 (6107); Episode 74/100; Loss: 0.08197635412216187\n",
      "Step 91 (6108); Episode 74/100; Loss: 0.0024757154751569033\n",
      "Step 92 (6109); Episode 74/100; Loss: 0.05643412843346596\n",
      "Step 93 (6110); Episode 74/100; Loss: 0.10765697807073593\n",
      "Step 94 (6111); Episode 74/100; Loss: 0.033243097364902496\n",
      "Step 95 (6112); Episode 74/100; Loss: 0.055762775242328644\n",
      "Step 96 (6113); Episode 74/100; Loss: 0.001927158678881824\n",
      "Step 97 (6114); Episode 74/100; Loss: 0.0015843600267544389\n",
      "Step 98 (6115); Episode 74/100; Loss: 0.0014649743679910898\n",
      "Step 99 (6116); Episode 74/100; Loss: 0.0010944660753011703\n",
      "Step 100 (6117); Episode 74/100; Loss: 0.0014209256041795015\n",
      "Step 101 (6118); Episode 74/100; Loss: 0.04603040963411331\n",
      "Step 102 (6119); Episode 74/100; Loss: 0.005970837082713842\n",
      "Step 103 (6120); Episode 74/100; Loss: 0.001429798430763185\n",
      "Step 104 (6121); Episode 74/100; Loss: 0.0014807527186349034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 105 (6122); Episode 74/100; Loss: 0.0008533052750863135\n",
      "Step 106 (6123); Episode 74/100; Loss: 0.001141973421908915\n",
      "Step 107 (6124); Episode 74/100; Loss: 0.0022498697508126497\n",
      "Step 108 (6125); Episode 74/100; Loss: 0.0012429633643478155\n",
      "Step 109 (6126); Episode 74/100; Loss: 0.0015000615967437625\n",
      "Step 110 (6127); Episode 74/100; Loss: 0.0013008833630010486\n",
      "Step 111 (6128); Episode 74/100; Loss: 0.0017552456120029092\n",
      "Step 112 (6129); Episode 74/100; Loss: 0.0009017318370752037\n",
      "Step 113 (6130); Episode 74/100; Loss: 0.10928875207901001\n",
      "Step 114 (6131); Episode 74/100; Loss: 0.0014570875791832805\n",
      "Step 115 (6132); Episode 74/100; Loss: 0.0015192386927083135\n",
      "Step 116 (6133); Episode 74/100; Loss: 0.00043870892841368914\n",
      "Step 117 (6134); Episode 74/100; Loss: 0.05393323674798012\n",
      "Step 118 (6135); Episode 74/100; Loss: 0.0016068174736574292\n",
      "Step 119 (6136); Episode 74/100; Loss: 0.03819245845079422\n",
      "Step 120 (6137); Episode 74/100; Loss: 0.00038820336339995265\n",
      "Step 121 (6138); Episode 74/100; Loss: 0.046187013387680054\n",
      "Step 122 (6139); Episode 74/100; Loss: 0.04578155651688576\n",
      "Step 123 (6140); Episode 74/100; Loss: 0.0006957720033824444\n",
      "Step 124 (6141); Episode 74/100; Loss: 0.009891737252473831\n",
      "Step 125 (6142); Episode 74/100; Loss: 0.0032259724102914333\n",
      "Step 126 (6143); Episode 74/100; Loss: 0.0006132262060418725\n",
      "Step 127 (6144); Episode 74/100; Loss: 0.05505288392305374\n",
      "Step 128 (6145); Episode 74/100; Loss: 0.0014933330239728093\n",
      "Step 129 (6146); Episode 74/100; Loss: 0.04097127541899681\n",
      "Step 130 (6147); Episode 74/100; Loss: 0.0006444074097089469\n",
      "Step 131 (6148); Episode 74/100; Loss: 0.04837477207183838\n",
      "Step 132 (6149); Episode 74/100; Loss: 0.0007197397062554955\n",
      "Step 133 (6150); Episode 74/100; Loss: 0.0032171830534934998\n",
      "Step 134 (6151); Episode 74/100; Loss: 0.09371604025363922\n",
      "Step 135 (6152); Episode 74/100; Loss: 0.049706146121025085\n",
      "Step 136 (6153); Episode 74/100; Loss: 0.000948082422837615\n",
      "Step 137 (6154); Episode 74/100; Loss: 0.001363848801702261\n",
      "Step 138 (6155); Episode 74/100; Loss: 0.0026122990529984236\n",
      "Step 139 (6156); Episode 74/100; Loss: 0.0017750938422977924\n",
      "Step 140 (6157); Episode 74/100; Loss: 0.04119965061545372\n",
      "Step 141 (6158); Episode 74/100; Loss: 0.004038337618112564\n",
      "Step 142 (6159); Episode 74/100; Loss: 0.0014596113469451666\n",
      "Step 143 (6160); Episode 74/100; Loss: 0.0005679579335264862\n",
      "Step 144 (6161); Episode 74/100; Loss: 0.0016443897038698196\n",
      "Step 145 (6162); Episode 74/100; Loss: 0.0038593285717070103\n",
      "Step 146 (6163); Episode 74/100; Loss: 0.0025625682901591063\n",
      "Step 147 (6164); Episode 74/100; Loss: 0.05085160210728645\n",
      "Step 148 (6165); Episode 74/100; Loss: 0.07916136831045151\n",
      "Step 149 (6166); Episode 74/100; Loss: 0.08218427002429962\n",
      "Step 150 (6167); Episode 74/100; Loss: 0.04864877462387085\n",
      "Step 151 (6168); Episode 74/100; Loss: 0.09612184017896652\n",
      "Step 152 (6169); Episode 74/100; Loss: 0.04109593480825424\n",
      "Step 153 (6170); Episode 74/100; Loss: 0.07321270555257797\n",
      "Step 154 (6171); Episode 74/100; Loss: 0.03448168933391571\n",
      "Step 155 (6172); Episode 74/100; Loss: 0.002691107802093029\n",
      "Step 156 (6173); Episode 74/100; Loss: 0.0017482978291809559\n",
      "Step 157 (6174); Episode 74/100; Loss: 0.0037833142559975386\n",
      "Step 158 (6175); Episode 74/100; Loss: 0.0008159789722412825\n",
      "Step 159 (6176); Episode 74/100; Loss: 0.012088408693671227\n",
      "Step 160 (6177); Episode 74/100; Loss: 0.0028208144940435886\n",
      "Step 161 (6178); Episode 74/100; Loss: 0.04470205679535866\n",
      "Step 162 (6179); Episode 74/100; Loss: 0.02748989686369896\n",
      "Step 163 (6180); Episode 74/100; Loss: 0.0006966966902837157\n",
      "Step 164 (6181); Episode 74/100; Loss: 0.0006152968853712082\n",
      "Step 165 (6182); Episode 74/100; Loss: 0.0035187515895813704\n",
      "Step 166 (6183); Episode 74/100; Loss: 0.003753972239792347\n",
      "Step 167 (6184); Episode 74/100; Loss: 0.0026206483598798513\n",
      "Step 168 (6185); Episode 74/100; Loss: 0.0020038546063005924\n",
      "Step 169 (6186); Episode 74/100; Loss: 0.08156684786081314\n",
      "Step 170 (6187); Episode 74/100; Loss: 0.05004173517227173\n",
      "Step 171 (6188); Episode 74/100; Loss: 0.029214276000857353\n",
      "Step 172 (6189); Episode 74/100; Loss: 0.044453684240579605\n",
      "Step 173 (6190); Episode 74/100; Loss: 0.0019600121304392815\n",
      "Step 174 (6191); Episode 74/100; Loss: 0.0028125518001616\n",
      "Step 175 (6192); Episode 74/100; Loss: 0.001458097598515451\n",
      "Step 176 (6193); Episode 74/100; Loss: 0.0008905252325348556\n",
      "Step 177 (6194); Episode 74/100; Loss: 0.0019362906459718943\n",
      "Step 178 (6195); Episode 74/100; Loss: 0.0016996306367218494\n",
      "Step 179 (6196); Episode 74/100; Loss: 0.0008855084888637066\n",
      "Step 180 (6197); Episode 74/100; Loss: 0.03031240403652191\n",
      "Step 181 (6198); Episode 74/100; Loss: 0.05096503347158432\n",
      "Step 182 (6199); Episode 74/100; Loss: 0.0014251039829105139\n",
      "Step 183 (6200); Episode 74/100; Loss: 0.0014182096347212791\n",
      "Step 184 (6201); Episode 74/100; Loss: 0.02301698550581932\n",
      "Step 185 (6202); Episode 74/100; Loss: 0.028253301978111267\n",
      "Step 186 (6203); Episode 74/100; Loss: 0.013692828826606274\n",
      "Step 187 (6204); Episode 74/100; Loss: 0.0008443376864306629\n",
      "Step 188 (6205); Episode 74/100; Loss: 0.0023029716685414314\n",
      "Step 189 (6206); Episode 74/100; Loss: 0.001562289660796523\n",
      "Step 190 (6207); Episode 74/100; Loss: 0.04291915521025658\n",
      "Step 191 (6208); Episode 74/100; Loss: 0.036580029875040054\n",
      "Step 192 (6209); Episode 74/100; Loss: 0.017973316833376884\n",
      "Step 193 (6210); Episode 74/100; Loss: 0.02234017848968506\n",
      "Step 194 (6211); Episode 74/100; Loss: 0.0010739917634055018\n",
      "Step 195 (6212); Episode 74/100; Loss: 0.0010943911038339138\n",
      "Step 196 (6213); Episode 74/100; Loss: 0.0047830273397266865\n",
      "Step 197 (6214); Episode 74/100; Loss: 0.043845340609550476\n",
      "Step 198 (6215); Episode 74/100; Loss: 0.04704836755990982\n",
      "Step 199 (6216); Episode 74/100; Loss: 0.0035583695862442255\n",
      "Step 0 (6217); Episode 75/100; Loss: 0.004088420886546373\n",
      "Step 1 (6218); Episode 75/100; Loss: 0.0008400370716117322\n",
      "Step 2 (6219); Episode 75/100; Loss: 0.003135716775432229\n",
      "Step 3 (6220); Episode 75/100; Loss: 0.002555923303589225\n",
      "Step 4 (6221); Episode 75/100; Loss: 0.0010767108760774136\n",
      "Step 5 (6222); Episode 75/100; Loss: 0.01312352903187275\n",
      "Step 6 (6223); Episode 75/100; Loss: 0.04184514284133911\n",
      "Step 7 (6224); Episode 75/100; Loss: 0.0008992565562948585\n",
      "Step 8 (6225); Episode 75/100; Loss: 0.015232368372380733\n",
      "Step 9 (6226); Episode 75/100; Loss: 0.05961010605096817\n",
      "Step 10 (6227); Episode 75/100; Loss: 0.05853996425867081\n",
      "Step 11 (6228); Episode 75/100; Loss: 0.001680669724009931\n",
      "Step 12 (6229); Episode 75/100; Loss: 0.04678180441260338\n",
      "Step 13 (6230); Episode 75/100; Loss: 0.0847957655787468\n",
      "Step 14 (6231); Episode 75/100; Loss: 0.0014379631029441953\n",
      "Step 15 (6232); Episode 75/100; Loss: 0.052919644862413406\n",
      "Step 16 (6233); Episode 75/100; Loss: 0.04546012729406357\n",
      "Step 17 (6234); Episode 75/100; Loss: 0.04440060257911682\n",
      "Step 18 (6235); Episode 75/100; Loss: 0.06710461527109146\n",
      "Step 19 (6236); Episode 75/100; Loss: 0.0006705453270114958\n",
      "Step 20 (6237); Episode 75/100; Loss: 0.0010129958391189575\n",
      "Step 21 (6238); Episode 75/100; Loss: 0.03672024980187416\n",
      "Step 22 (6239); Episode 75/100; Loss: 0.035331591963768005\n",
      "Step 23 (6240); Episode 75/100; Loss: 0.03599788248538971\n",
      "Step 24 (6241); Episode 75/100; Loss: 0.003927033860236406\n",
      "Step 25 (6242); Episode 75/100; Loss: 0.00127212586812675\n",
      "Step 26 (6243); Episode 75/100; Loss: 0.00366744352504611\n",
      "Step 27 (6244); Episode 75/100; Loss: 0.0011406004196032882\n",
      "Step 28 (6245); Episode 75/100; Loss: 0.0007558111101388931\n",
      "Step 29 (6246); Episode 75/100; Loss: 0.025050116702914238\n",
      "Step 30 (6247); Episode 75/100; Loss: 0.04510711506009102\n",
      "Step 31 (6248); Episode 75/100; Loss: 0.0011711565311998129\n",
      "Step 32 (6249); Episode 75/100; Loss: 0.026605254039168358\n",
      "Step 33 (6250); Episode 75/100; Loss: 0.048698242753744125\n",
      "Step 34 (6251); Episode 75/100; Loss: 0.0011956380913034081\n",
      "Step 35 (6252); Episode 75/100; Loss: 0.06950018554925919\n",
      "Step 36 (6253); Episode 75/100; Loss: 0.055812984704971313\n",
      "Step 37 (6254); Episode 75/100; Loss: 0.0013239486142992973\n",
      "Step 38 (6255); Episode 75/100; Loss: 0.0029760769102722406\n",
      "Step 39 (6256); Episode 75/100; Loss: 0.0014243621844798326\n",
      "Step 40 (6257); Episode 75/100; Loss: 0.0005992907099425793\n",
      "Step 41 (6258); Episode 75/100; Loss: 0.0030139645095914602\n",
      "Step 42 (6259); Episode 75/100; Loss: 0.004241710528731346\n",
      "Step 43 (6260); Episode 75/100; Loss: 0.004791280720382929\n",
      "Step 44 (6261); Episode 75/100; Loss: 0.0024630229454487562\n",
      "Step 45 (6262); Episode 75/100; Loss: 0.0007352083339355886\n",
      "Step 46 (6263); Episode 75/100; Loss: 0.0009316697251051664\n",
      "Step 47 (6264); Episode 75/100; Loss: 0.20612089335918427\n",
      "Step 48 (6265); Episode 75/100; Loss: 0.10325683653354645\n",
      "Step 49 (6266); Episode 75/100; Loss: 0.0010783537290990353\n",
      "Step 50 (6267); Episode 75/100; Loss: 0.0015250655123963952\n",
      "Step 51 (6268); Episode 75/100; Loss: 0.0468045212328434\n",
      "Step 52 (6269); Episode 75/100; Loss: 0.0018953969702124596\n",
      "Step 53 (6270); Episode 75/100; Loss: 0.001354580745100975\n",
      "Step 54 (6271); Episode 75/100; Loss: 0.18653465807437897\n",
      "Step 55 (6272); Episode 75/100; Loss: 0.0014121168060228229\n",
      "Step 56 (6273); Episode 75/100; Loss: 0.00260491156950593\n",
      "Step 57 (6274); Episode 75/100; Loss: 0.06813029199838638\n",
      "Step 58 (6275); Episode 75/100; Loss: 0.04704485461115837\n",
      "Step 59 (6276); Episode 75/100; Loss: 0.04350048676133156\n",
      "Step 60 (6277); Episode 75/100; Loss: 0.002566099865362048\n",
      "Step 61 (6278); Episode 75/100; Loss: 0.002568080322816968\n",
      "Step 62 (6279); Episode 75/100; Loss: 0.0010218600509688258\n",
      "Step 63 (6280); Episode 75/100; Loss: 0.0013890842674300075\n",
      "Step 64 (6281); Episode 75/100; Loss: 0.04601789265871048\n",
      "Step 65 (6282); Episode 75/100; Loss: 0.10724171996116638\n",
      "Step 66 (6283); Episode 75/100; Loss: 0.00372847868129611\n",
      "Step 67 (6284); Episode 75/100; Loss: 0.0013894940493628383\n",
      "Step 68 (6285); Episode 75/100; Loss: 0.025854652747511864\n",
      "Step 69 (6286); Episode 75/100; Loss: 0.0019255573861300945\n",
      "Step 70 (6287); Episode 75/100; Loss: 0.001708898926153779\n",
      "Step 71 (6288); Episode 75/100; Loss: 0.020934857428073883\n",
      "Step 72 (6289); Episode 75/100; Loss: 0.02302633412182331\n",
      "Step 73 (6290); Episode 75/100; Loss: 0.0008577379630878568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 74 (6291); Episode 75/100; Loss: 0.0009826123714447021\n",
      "Step 75 (6292); Episode 75/100; Loss: 0.0025754449889063835\n",
      "Step 76 (6293); Episode 75/100; Loss: 0.0012030614307150245\n",
      "Step 77 (6294); Episode 75/100; Loss: 0.07495446503162384\n",
      "Step 78 (6295); Episode 75/100; Loss: 0.05114276707172394\n",
      "Step 79 (6296); Episode 75/100; Loss: 0.028116853907704353\n",
      "Step 80 (6297); Episode 75/100; Loss: 0.10177016258239746\n",
      "Step 81 (6298); Episode 75/100; Loss: 0.0031864894554018974\n",
      "Step 82 (6299); Episode 75/100; Loss: 0.0008665375062264502\n",
      "Step 83 (6300); Episode 75/100; Loss: 0.000912543386220932\n",
      "Step 84 (6301); Episode 75/100; Loss: 0.04091613367199898\n",
      "Step 85 (6302); Episode 75/100; Loss: 0.04690899699926376\n",
      "Step 86 (6303); Episode 75/100; Loss: 0.002101288177073002\n",
      "Step 87 (6304); Episode 75/100; Loss: 0.06590607017278671\n",
      "Step 88 (6305); Episode 75/100; Loss: 0.0006508444203063846\n",
      "Step 89 (6306); Episode 75/100; Loss: 0.0010894956067204475\n",
      "Step 90 (6307); Episode 75/100; Loss: 0.14193041622638702\n",
      "Step 91 (6308); Episode 75/100; Loss: 0.0009040212607942522\n",
      "Step 92 (6309); Episode 75/100; Loss: 0.028317317366600037\n",
      "Step 93 (6310); Episode 75/100; Loss: 0.013315108604729176\n",
      "Step 94 (6311); Episode 75/100; Loss: 0.04567267373204231\n",
      "Step 95 (6312); Episode 75/100; Loss: 0.0015186751261353493\n",
      "Step 96 (6313); Episode 75/100; Loss: 0.047578755766153336\n",
      "Step 97 (6314); Episode 75/100; Loss: 0.0011553578078746796\n",
      "Step 98 (6315); Episode 75/100; Loss: 0.003567734034731984\n",
      "Step 99 (6316); Episode 75/100; Loss: 0.001500240177847445\n",
      "Step 100 (6317); Episode 75/100; Loss: 0.04515877366065979\n",
      "Step 101 (6318); Episode 75/100; Loss: 0.02618798427283764\n",
      "Step 102 (6319); Episode 75/100; Loss: 0.011748398654162884\n",
      "Step 103 (6320); Episode 75/100; Loss: 0.0045457761734724045\n",
      "Step 104 (6321); Episode 75/100; Loss: 0.0020416793413460255\n",
      "Step 105 (6322); Episode 75/100; Loss: 0.001434791716746986\n",
      "Step 106 (6323); Episode 75/100; Loss: 0.001516945892944932\n",
      "Step 107 (6324); Episode 75/100; Loss: 0.04439491033554077\n",
      "Step 108 (6325); Episode 75/100; Loss: 0.05709325149655342\n",
      "Step 109 (6326); Episode 75/100; Loss: 0.0007159686647355556\n",
      "Step 110 (6327); Episode 75/100; Loss: 0.0009140814654529095\n",
      "Step 111 (6328); Episode 75/100; Loss: 0.10523059964179993\n",
      "Step 112 (6329); Episode 75/100; Loss: 0.0008603838505223393\n",
      "Step 113 (6330); Episode 75/100; Loss: 0.0045288740657269955\n",
      "Step 114 (6331); Episode 75/100; Loss: 0.001513522700406611\n",
      "Step 115 (6332); Episode 75/100; Loss: 0.05545446276664734\n",
      "Step 116 (6333); Episode 75/100; Loss: 0.06130107492208481\n",
      "Step 117 (6334); Episode 75/100; Loss: 0.04417800530791283\n",
      "Step 118 (6335); Episode 75/100; Loss: 0.04359990358352661\n",
      "Step 119 (6336); Episode 75/100; Loss: 0.005004531238228083\n",
      "Step 120 (6337); Episode 75/100; Loss: 0.004492332227528095\n",
      "Step 121 (6338); Episode 75/100; Loss: 0.045468829572200775\n",
      "Step 122 (6339); Episode 75/100; Loss: 0.004182387143373489\n",
      "Step 123 (6340); Episode 75/100; Loss: 0.1050756424665451\n",
      "Step 124 (6341); Episode 75/100; Loss: 0.03375385329127312\n",
      "Step 125 (6342); Episode 75/100; Loss: 0.0009707699064165354\n",
      "Step 126 (6343); Episode 75/100; Loss: 0.000599087739828974\n",
      "Step 127 (6344); Episode 75/100; Loss: 0.0019138797651976347\n",
      "Step 128 (6345); Episode 75/100; Loss: 0.04287039488554001\n",
      "Step 129 (6346); Episode 75/100; Loss: 0.0006622588261961937\n",
      "Step 130 (6347); Episode 75/100; Loss: 0.049248091876506805\n",
      "Step 131 (6348); Episode 75/100; Loss: 0.0033779998775571585\n",
      "Step 132 (6349); Episode 75/100; Loss: 0.0005462091648951173\n",
      "Step 133 (6350); Episode 75/100; Loss: 0.04606137424707413\n",
      "Step 134 (6351); Episode 75/100; Loss: 0.05065613612532616\n",
      "Step 135 (6352); Episode 75/100; Loss: 0.015146087855100632\n",
      "Step 136 (6353); Episode 75/100; Loss: 0.04886872321367264\n",
      "Step 137 (6354); Episode 75/100; Loss: 0.048611339181661606\n",
      "Step 138 (6355); Episode 75/100; Loss: 0.025519702583551407\n",
      "Step 139 (6356); Episode 75/100; Loss: 0.042063143104314804\n",
      "Step 140 (6357); Episode 75/100; Loss: 0.0007929998100735247\n",
      "Step 141 (6358); Episode 75/100; Loss: 0.04813015088438988\n",
      "Step 142 (6359); Episode 75/100; Loss: 0.0004715445975307375\n",
      "Step 143 (6360); Episode 75/100; Loss: 0.0010721092112362385\n",
      "Step 144 (6361); Episode 75/100; Loss: 0.05537639185786247\n",
      "Step 145 (6362); Episode 75/100; Loss: 0.0019898496102541685\n",
      "Step 146 (6363); Episode 75/100; Loss: 0.03763365373015404\n",
      "Step 147 (6364); Episode 75/100; Loss: 0.0027723803650587797\n",
      "Step 148 (6365); Episode 75/100; Loss: 0.000935432268306613\n",
      "Step 149 (6366); Episode 75/100; Loss: 0.0006068499060347676\n",
      "Step 150 (6367); Episode 75/100; Loss: 0.04290218651294708\n",
      "Step 151 (6368); Episode 75/100; Loss: 0.054223060607910156\n",
      "Step 152 (6369); Episode 75/100; Loss: 0.0011879712110385299\n",
      "Step 153 (6370); Episode 75/100; Loss: 0.002078094519674778\n",
      "Step 154 (6371); Episode 75/100; Loss: 0.001140317297540605\n",
      "Step 155 (6372); Episode 75/100; Loss: 0.0017182553419843316\n",
      "Step 156 (6373); Episode 75/100; Loss: 0.09314941614866257\n",
      "Step 157 (6374); Episode 75/100; Loss: 0.12300446629524231\n",
      "Step 158 (6375); Episode 75/100; Loss: 0.003975212574005127\n",
      "Step 159 (6376); Episode 75/100; Loss: 0.004157535266131163\n",
      "Step 160 (6377); Episode 75/100; Loss: 0.014539930038154125\n",
      "Step 161 (6378); Episode 75/100; Loss: 0.0007566247950308025\n",
      "Step 162 (6379); Episode 75/100; Loss: 0.0035336092114448547\n",
      "Step 163 (6380); Episode 75/100; Loss: 0.004471963737159967\n",
      "Step 164 (6381); Episode 75/100; Loss: 0.0050089797005057335\n",
      "Step 165 (6382); Episode 75/100; Loss: 0.0006446016486734152\n",
      "Step 166 (6383); Episode 75/100; Loss: 0.08764361590147018\n",
      "Step 167 (6384); Episode 75/100; Loss: 0.04732422158122063\n",
      "Step 168 (6385); Episode 75/100; Loss: 0.04003692418336868\n",
      "Step 169 (6386); Episode 75/100; Loss: 0.05537432059645653\n",
      "Step 170 (6387); Episode 75/100; Loss: 0.10250817984342575\n",
      "Step 171 (6388); Episode 75/100; Loss: 0.0033192248083651066\n",
      "Step 172 (6389); Episode 75/100; Loss: 0.052098724991083145\n",
      "Step 0 (6390); Episode 76/100; Loss: 0.07667481154203415\n",
      "Step 1 (6391); Episode 76/100; Loss: 0.03602340444922447\n",
      "Step 2 (6392); Episode 76/100; Loss: 0.05639311671257019\n",
      "Step 3 (6393); Episode 76/100; Loss: 0.04507505148649216\n",
      "Step 4 (6394); Episode 76/100; Loss: 0.0024307819548994303\n",
      "Step 5 (6395); Episode 76/100; Loss: 0.04746650531888008\n",
      "Step 6 (6396); Episode 76/100; Loss: 0.002444551093503833\n",
      "Step 7 (6397); Episode 76/100; Loss: 0.002607635222375393\n",
      "Step 8 (6398); Episode 76/100; Loss: 0.008967685513198376\n",
      "Step 9 (6399); Episode 76/100; Loss: 0.045397739857435226\n",
      "Step 10 (6400); Episode 76/100; Loss: 0.017676424235105515\n",
      "Step 11 (6401); Episode 76/100; Loss: 0.0014236156130209565\n",
      "Step 12 (6402); Episode 76/100; Loss: 0.027066996321082115\n",
      "Step 13 (6403); Episode 76/100; Loss: 0.0018023269949480891\n",
      "Step 14 (6404); Episode 76/100; Loss: 0.0006893163081258535\n",
      "Step 15 (6405); Episode 76/100; Loss: 0.0005582096055150032\n",
      "Step 16 (6406); Episode 76/100; Loss: 0.05828249827027321\n",
      "Step 17 (6407); Episode 76/100; Loss: 0.016924643889069557\n",
      "Step 18 (6408); Episode 76/100; Loss: 0.0031506677623838186\n",
      "Step 19 (6409); Episode 76/100; Loss: 0.0409681610763073\n",
      "Step 20 (6410); Episode 76/100; Loss: 0.0013929888373240829\n",
      "Step 21 (6411); Episode 76/100; Loss: 0.06611042469739914\n",
      "Step 22 (6412); Episode 76/100; Loss: 0.07251743972301483\n",
      "Step 23 (6413); Episode 76/100; Loss: 0.0031023346818983555\n",
      "Step 24 (6414); Episode 76/100; Loss: 0.0027136914432048798\n",
      "Step 25 (6415); Episode 76/100; Loss: 0.0025329662021249533\n",
      "Step 26 (6416); Episode 76/100; Loss: 0.002910644980147481\n",
      "Step 27 (6417); Episode 76/100; Loss: 0.002848800038918853\n",
      "Step 28 (6418); Episode 76/100; Loss: 0.003515324555337429\n",
      "Step 29 (6419); Episode 76/100; Loss: 0.003009833861142397\n",
      "Step 30 (6420); Episode 76/100; Loss: 0.0028506345115602016\n",
      "Step 31 (6421); Episode 76/100; Loss: 0.031170984730124474\n",
      "Step 32 (6422); Episode 76/100; Loss: 0.061464808881282806\n",
      "Step 33 (6423); Episode 76/100; Loss: 0.003994388971477747\n",
      "Step 34 (6424); Episode 76/100; Loss: 0.0010783534962683916\n",
      "Step 35 (6425); Episode 76/100; Loss: 0.0018291851738467813\n",
      "Step 36 (6426); Episode 76/100; Loss: 0.058430809527635574\n",
      "Step 37 (6427); Episode 76/100; Loss: 0.04915038123726845\n",
      "Step 38 (6428); Episode 76/100; Loss: 0.0570707768201828\n",
      "Step 39 (6429); Episode 76/100; Loss: 0.04988585412502289\n",
      "Step 40 (6430); Episode 76/100; Loss: 0.0010648976312950253\n",
      "Step 41 (6431); Episode 76/100; Loss: 0.0010757625568658113\n",
      "Step 42 (6432); Episode 76/100; Loss: 0.001383941969834268\n",
      "Step 43 (6433); Episode 76/100; Loss: 0.000543676782399416\n",
      "Step 44 (6434); Episode 76/100; Loss: 0.0020777571480721235\n",
      "Step 45 (6435); Episode 76/100; Loss: 0.001392486272379756\n",
      "Step 46 (6436); Episode 76/100; Loss: 0.0012246643891558051\n",
      "Step 47 (6437); Episode 76/100; Loss: 0.06081782653927803\n",
      "Step 48 (6438); Episode 76/100; Loss: 0.050821702927351\n",
      "Step 49 (6439); Episode 76/100; Loss: 0.013159980066120625\n",
      "Step 50 (6440); Episode 76/100; Loss: 0.05235381796956062\n",
      "Step 51 (6441); Episode 76/100; Loss: 0.05460460111498833\n",
      "Step 52 (6442); Episode 76/100; Loss: 0.001438181265257299\n",
      "Step 53 (6443); Episode 76/100; Loss: 0.01997709833085537\n",
      "Step 54 (6444); Episode 76/100; Loss: 0.0015891741495579481\n",
      "Step 55 (6445); Episode 76/100; Loss: 0.0008980449638329446\n",
      "Step 56 (6446); Episode 76/100; Loss: 0.0016844274941831827\n",
      "Step 57 (6447); Episode 76/100; Loss: 0.0020576019305735826\n",
      "Step 58 (6448); Episode 76/100; Loss: 0.0007785208290442824\n",
      "Step 59 (6449); Episode 76/100; Loss: 0.0011618457501754165\n",
      "Step 60 (6450); Episode 76/100; Loss: 0.002387609798461199\n",
      "Step 61 (6451); Episode 76/100; Loss: 0.08460672944784164\n",
      "Step 62 (6452); Episode 76/100; Loss: 0.0015724352560937405\n",
      "Step 63 (6453); Episode 76/100; Loss: 0.0010888339020311832\n",
      "Step 64 (6454); Episode 76/100; Loss: 0.0031701314728707075\n",
      "Step 65 (6455); Episode 76/100; Loss: 0.0011500547407194972\n",
      "Step 66 (6456); Episode 76/100; Loss: 0.0011770192068070173\n",
      "Step 67 (6457); Episode 76/100; Loss: 0.033707380294799805\n",
      "Step 68 (6458); Episode 76/100; Loss: 0.07658373564481735\n",
      "Step 69 (6459); Episode 76/100; Loss: 0.0016622229013592005\n",
      "Step 70 (6460); Episode 76/100; Loss: 0.011119126342236996\n",
      "Step 71 (6461); Episode 76/100; Loss: 0.0013340029399842024\n",
      "Step 72 (6462); Episode 76/100; Loss: 0.04828602075576782\n",
      "Step 73 (6463); Episode 76/100; Loss: 0.09760437160730362\n",
      "Step 74 (6464); Episode 76/100; Loss: 0.04459380730986595\n",
      "Step 75 (6465); Episode 76/100; Loss: 0.045313552021980286\n",
      "Step 76 (6466); Episode 76/100; Loss: 0.02820422127842903\n",
      "Step 77 (6467); Episode 76/100; Loss: 0.05465853586792946\n",
      "Step 78 (6468); Episode 76/100; Loss: 0.0026320002507418394\n",
      "Step 79 (6469); Episode 76/100; Loss: 0.1336132138967514\n",
      "Step 80 (6470); Episode 76/100; Loss: 0.0016228694003075361\n",
      "Step 81 (6471); Episode 76/100; Loss: 0.011707533150911331\n",
      "Step 82 (6472); Episode 76/100; Loss: 0.05417270213365555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 83 (6473); Episode 76/100; Loss: 0.00415900070220232\n",
      "Step 84 (6474); Episode 76/100; Loss: 0.0018051671795547009\n",
      "Step 85 (6475); Episode 76/100; Loss: 0.056927669793367386\n",
      "Step 86 (6476); Episode 76/100; Loss: 0.057723335921764374\n",
      "Step 87 (6477); Episode 76/100; Loss: 0.001524886698462069\n",
      "Step 88 (6478); Episode 76/100; Loss: 0.003995327278971672\n",
      "Step 89 (6479); Episode 76/100; Loss: 0.03939387947320938\n",
      "Step 90 (6480); Episode 76/100; Loss: 0.001893884502351284\n",
      "Step 91 (6481); Episode 76/100; Loss: 0.12851642072200775\n",
      "Step 92 (6482); Episode 76/100; Loss: 0.0011752863647416234\n",
      "Step 93 (6483); Episode 76/100; Loss: 0.06641733646392822\n",
      "Step 94 (6484); Episode 76/100; Loss: 0.011942505836486816\n",
      "Step 95 (6485); Episode 76/100; Loss: 0.056561850011348724\n",
      "Step 96 (6486); Episode 76/100; Loss: 0.0015568048693239689\n",
      "Step 97 (6487); Episode 76/100; Loss: 0.001397380605340004\n",
      "Step 98 (6488); Episode 76/100; Loss: 0.023967744782567024\n",
      "Step 99 (6489); Episode 76/100; Loss: 0.001824352890253067\n",
      "Step 100 (6490); Episode 76/100; Loss: 0.04896464943885803\n",
      "Step 101 (6491); Episode 76/100; Loss: 0.000865744601469487\n",
      "Step 102 (6492); Episode 76/100; Loss: 0.010276001878082752\n",
      "Step 103 (6493); Episode 76/100; Loss: 0.0018602118361741304\n",
      "Step 104 (6494); Episode 76/100; Loss: 0.006205800920724869\n",
      "Step 105 (6495); Episode 76/100; Loss: 0.002628871938213706\n",
      "Step 106 (6496); Episode 76/100; Loss: 0.09165279567241669\n",
      "Step 107 (6497); Episode 76/100; Loss: 0.0014176000840961933\n",
      "Step 108 (6498); Episode 76/100; Loss: 0.047629743814468384\n",
      "Step 109 (6499); Episode 76/100; Loss: 0.04344433546066284\n",
      "Step 110 (6500); Episode 76/100; Loss: 0.044030752032995224\n",
      "Step 111 (6501); Episode 76/100; Loss: 0.0006881175213493407\n",
      "Step 112 (6502); Episode 76/100; Loss: 0.003197216661646962\n",
      "Step 113 (6503); Episode 76/100; Loss: 0.05006888881325722\n",
      "Step 114 (6504); Episode 76/100; Loss: 0.0025497034657746553\n",
      "Step 115 (6505); Episode 76/100; Loss: 0.045575469732284546\n",
      "Step 116 (6506); Episode 76/100; Loss: 0.0024885423481464386\n",
      "Step 117 (6507); Episode 76/100; Loss: 0.0021159383468329906\n",
      "Step 118 (6508); Episode 76/100; Loss: 0.07720310986042023\n",
      "Step 119 (6509); Episode 76/100; Loss: 0.0033331059385091066\n",
      "Step 120 (6510); Episode 76/100; Loss: 0.003198120044544339\n",
      "Step 121 (6511); Episode 76/100; Loss: 0.0006779373507015407\n",
      "Step 122 (6512); Episode 76/100; Loss: 0.047965288162231445\n",
      "Step 123 (6513); Episode 76/100; Loss: 0.002028833841904998\n",
      "Step 124 (6514); Episode 76/100; Loss: 0.030130509287118912\n",
      "Step 125 (6515); Episode 76/100; Loss: 0.06658947467803955\n",
      "Step 126 (6516); Episode 76/100; Loss: 0.05902750417590141\n",
      "Step 127 (6517); Episode 76/100; Loss: 0.06482769548892975\n",
      "Step 128 (6518); Episode 76/100; Loss: 0.05134761705994606\n",
      "Step 129 (6519); Episode 76/100; Loss: 0.0006590803386643529\n",
      "Step 130 (6520); Episode 76/100; Loss: 0.0006225028191693127\n",
      "Step 131 (6521); Episode 76/100; Loss: 0.002007711213082075\n",
      "Step 132 (6522); Episode 76/100; Loss: 0.0007190489559434354\n",
      "Step 133 (6523); Episode 76/100; Loss: 0.03347295895218849\n",
      "Step 134 (6524); Episode 76/100; Loss: 0.028819290921092033\n",
      "Step 135 (6525); Episode 76/100; Loss: 0.03965401649475098\n",
      "Step 136 (6526); Episode 76/100; Loss: 0.04636049643158913\n",
      "Step 137 (6527); Episode 76/100; Loss: 0.04697646573185921\n",
      "Step 138 (6528); Episode 76/100; Loss: 0.0022202436812222004\n",
      "Step 139 (6529); Episode 76/100; Loss: 0.05724415183067322\n",
      "Step 140 (6530); Episode 76/100; Loss: 0.08681914210319519\n",
      "Step 141 (6531); Episode 76/100; Loss: 0.002699731383472681\n",
      "Step 142 (6532); Episode 76/100; Loss: 0.052244633436203\n",
      "Step 143 (6533); Episode 76/100; Loss: 0.003912805113941431\n",
      "Step 144 (6534); Episode 76/100; Loss: 0.029913466423749924\n",
      "Step 145 (6535); Episode 76/100; Loss: 0.0014126318274065852\n",
      "Step 146 (6536); Episode 76/100; Loss: 0.0018777466611936688\n",
      "Step 147 (6537); Episode 76/100; Loss: 0.057898275554180145\n",
      "Step 148 (6538); Episode 76/100; Loss: 0.0029241943266242743\n",
      "Step 149 (6539); Episode 76/100; Loss: 0.0013960313517600298\n",
      "Step 150 (6540); Episode 76/100; Loss: 0.04968680068850517\n",
      "Step 151 (6541); Episode 76/100; Loss: 0.0010083257220685482\n",
      "Step 152 (6542); Episode 76/100; Loss: 0.0012502081226557493\n",
      "Step 153 (6543); Episode 76/100; Loss: 0.003385447897017002\n",
      "Step 154 (6544); Episode 76/100; Loss: 0.002008151961490512\n",
      "Step 155 (6545); Episode 76/100; Loss: 0.001221293001435697\n",
      "Step 156 (6546); Episode 76/100; Loss: 0.001083500450477004\n",
      "Step 157 (6547); Episode 76/100; Loss: 0.011656939052045345\n",
      "Step 158 (6548); Episode 76/100; Loss: 0.0018611372215673327\n",
      "Step 159 (6549); Episode 76/100; Loss: 0.004891322925686836\n",
      "Step 160 (6550); Episode 76/100; Loss: 0.0023227622732520103\n",
      "Step 161 (6551); Episode 76/100; Loss: 0.016969328746199608\n",
      "Step 162 (6552); Episode 76/100; Loss: 0.0026262165047228336\n",
      "Step 163 (6553); Episode 76/100; Loss: 0.029179172590374947\n",
      "Step 164 (6554); Episode 76/100; Loss: 0.0011723965872079134\n",
      "Step 165 (6555); Episode 76/100; Loss: 0.05509839579463005\n",
      "Step 166 (6556); Episode 76/100; Loss: 0.0012751894537359476\n",
      "Step 167 (6557); Episode 76/100; Loss: 0.05615512281656265\n",
      "Step 168 (6558); Episode 76/100; Loss: 0.0013888824032619596\n",
      "Step 169 (6559); Episode 76/100; Loss: 0.058529701083898544\n",
      "Step 170 (6560); Episode 76/100; Loss: 0.0012335442006587982\n",
      "Step 171 (6561); Episode 76/100; Loss: 0.040740977972745895\n",
      "Step 172 (6562); Episode 76/100; Loss: 0.05247795954346657\n",
      "Step 173 (6563); Episode 76/100; Loss: 0.0008178319549188018\n",
      "Step 174 (6564); Episode 76/100; Loss: 0.003361352253705263\n",
      "Step 175 (6565); Episode 76/100; Loss: 0.0007949628634378314\n",
      "Step 176 (6566); Episode 76/100; Loss: 0.0005096981767565012\n",
      "Step 177 (6567); Episode 76/100; Loss: 0.04659591242671013\n",
      "Step 178 (6568); Episode 76/100; Loss: 0.001635806169360876\n",
      "Step 179 (6569); Episode 76/100; Loss: 0.0012470500078052282\n",
      "Step 180 (6570); Episode 76/100; Loss: 0.050757430493831635\n",
      "Step 181 (6571); Episode 76/100; Loss: 0.049168024212121964\n",
      "Step 182 (6572); Episode 76/100; Loss: 0.10428039729595184\n",
      "Step 183 (6573); Episode 76/100; Loss: 0.0009406826575286686\n",
      "Step 184 (6574); Episode 76/100; Loss: 0.07042346149682999\n",
      "Step 185 (6575); Episode 76/100; Loss: 0.06486421823501587\n",
      "Step 186 (6576); Episode 76/100; Loss: 0.003749322844669223\n",
      "Step 187 (6577); Episode 76/100; Loss: 0.0013309622881934047\n",
      "Step 188 (6578); Episode 76/100; Loss: 0.003991562407463789\n",
      "Step 189 (6579); Episode 76/100; Loss: 0.0011963160941377282\n",
      "Step 190 (6580); Episode 76/100; Loss: 0.002948397770524025\n",
      "Step 191 (6581); Episode 76/100; Loss: 0.0021334639750421047\n",
      "Step 192 (6582); Episode 76/100; Loss: 0.06540226936340332\n",
      "Step 193 (6583); Episode 76/100; Loss: 0.0017328803660348058\n",
      "Step 194 (6584); Episode 76/100; Loss: 0.04467082768678665\n",
      "Step 195 (6585); Episode 76/100; Loss: 0.04249292612075806\n",
      "Step 196 (6586); Episode 76/100; Loss: 0.0011742927599698305\n",
      "Step 197 (6587); Episode 76/100; Loss: 0.0010428882669657469\n",
      "Step 198 (6588); Episode 76/100; Loss: 0.0008308099932037294\n",
      "Step 199 (6589); Episode 76/100; Loss: 0.057519689202308655\n",
      "Step 0 (6590); Episode 77/100; Loss: 0.002333646873012185\n",
      "Step 1 (6591); Episode 77/100; Loss: 0.0010205727303400636\n",
      "Step 2 (6592); Episode 77/100; Loss: 0.0826302319765091\n",
      "Step 3 (6593); Episode 77/100; Loss: 0.0924038514494896\n",
      "Step 4 (6594); Episode 77/100; Loss: 0.0559086948633194\n",
      "Step 5 (6595); Episode 77/100; Loss: 0.0021430898923426867\n",
      "Step 6 (6596); Episode 77/100; Loss: 0.0023855376057326794\n",
      "Step 7 (6597); Episode 77/100; Loss: 0.0019206488505005836\n",
      "Step 8 (6598); Episode 77/100; Loss: 0.07376691699028015\n",
      "Step 9 (6599); Episode 77/100; Loss: 0.018533797934651375\n",
      "Step 10 (6600); Episode 77/100; Loss: 0.0007518315687775612\n",
      "Step 11 (6601); Episode 77/100; Loss: 0.05383578687906265\n",
      "Step 12 (6602); Episode 77/100; Loss: 0.0006306315772235394\n",
      "Step 13 (6603); Episode 77/100; Loss: 0.003705262439325452\n",
      "Step 14 (6604); Episode 77/100; Loss: 0.14355769753456116\n",
      "Step 15 (6605); Episode 77/100; Loss: 0.0011326965177431703\n",
      "Step 16 (6606); Episode 77/100; Loss: 0.0012090452946722507\n",
      "Step 17 (6607); Episode 77/100; Loss: 0.05741429328918457\n",
      "Step 18 (6608); Episode 77/100; Loss: 0.09688631445169449\n",
      "Step 19 (6609); Episode 77/100; Loss: 0.0006483299657702446\n",
      "Step 20 (6610); Episode 77/100; Loss: 0.002316608326509595\n",
      "Step 21 (6611); Episode 77/100; Loss: 0.03021657094359398\n",
      "Step 22 (6612); Episode 77/100; Loss: 0.015297247096896172\n",
      "Step 23 (6613); Episode 77/100; Loss: 0.000894862983841449\n",
      "Step 24 (6614); Episode 77/100; Loss: 0.04846100136637688\n",
      "Step 25 (6615); Episode 77/100; Loss: 0.04893626645207405\n",
      "Step 26 (6616); Episode 77/100; Loss: 0.05344386398792267\n",
      "Step 27 (6617); Episode 77/100; Loss: 0.017314035445451736\n",
      "Step 28 (6618); Episode 77/100; Loss: 0.0024444772861897945\n",
      "Step 29 (6619); Episode 77/100; Loss: 0.0008932976052165031\n",
      "Step 30 (6620); Episode 77/100; Loss: 0.001574229565449059\n",
      "Step 31 (6621); Episode 77/100; Loss: 0.011015896685421467\n",
      "Step 32 (6622); Episode 77/100; Loss: 0.0010536211775615811\n",
      "Step 33 (6623); Episode 77/100; Loss: 0.000961995858233422\n",
      "Step 34 (6624); Episode 77/100; Loss: 0.0007304383907467127\n",
      "Step 35 (6625); Episode 77/100; Loss: 0.049762025475502014\n",
      "Step 36 (6626); Episode 77/100; Loss: 0.02914518676698208\n",
      "Step 37 (6627); Episode 77/100; Loss: 0.0040287962183356285\n",
      "Step 38 (6628); Episode 77/100; Loss: 0.001140983891673386\n",
      "Step 39 (6629); Episode 77/100; Loss: 0.05224905535578728\n",
      "Step 40 (6630); Episode 77/100; Loss: 0.0011060526594519615\n",
      "Step 41 (6631); Episode 77/100; Loss: 0.0031334038358181715\n",
      "Step 42 (6632); Episode 77/100; Loss: 0.002139071235433221\n",
      "Step 43 (6633); Episode 77/100; Loss: 0.003417444881051779\n",
      "Step 44 (6634); Episode 77/100; Loss: 0.049948133528232574\n",
      "Step 45 (6635); Episode 77/100; Loss: 0.0017499985406175256\n",
      "Step 46 (6636); Episode 77/100; Loss: 0.026140963658690453\n",
      "Step 47 (6637); Episode 77/100; Loss: 0.08912405371665955\n",
      "Step 48 (6638); Episode 77/100; Loss: 0.0455796904861927\n",
      "Step 49 (6639); Episode 77/100; Loss: 0.0012812044005841017\n",
      "Step 50 (6640); Episode 77/100; Loss: 0.03301265835762024\n",
      "Step 51 (6641); Episode 77/100; Loss: 0.09804833680391312\n",
      "Step 52 (6642); Episode 77/100; Loss: 0.0161038376390934\n",
      "Step 53 (6643); Episode 77/100; Loss: 0.03594968840479851\n",
      "Step 54 (6644); Episode 77/100; Loss: 0.037569671869277954\n",
      "Step 55 (6645); Episode 77/100; Loss: 0.0014018877409398556\n",
      "Step 56 (6646); Episode 77/100; Loss: 0.003639825386926532\n",
      "Step 57 (6647); Episode 77/100; Loss: 0.0021716689225286245\n",
      "Step 58 (6648); Episode 77/100; Loss: 0.09038244932889938\n",
      "Step 59 (6649); Episode 77/100; Loss: 0.012603790499269962\n",
      "Step 60 (6650); Episode 77/100; Loss: 0.0009442585287615657\n",
      "Step 61 (6651); Episode 77/100; Loss: 0.018249886110424995\n",
      "Step 62 (6652); Episode 77/100; Loss: 0.042979784309864044\n",
      "Step 63 (6653); Episode 77/100; Loss: 0.052504997700452805\n",
      "Step 64 (6654); Episode 77/100; Loss: 0.0670732632279396\n",
      "Step 65 (6655); Episode 77/100; Loss: 0.005928938742727041\n",
      "Step 66 (6656); Episode 77/100; Loss: 0.002034461125731468\n",
      "Step 67 (6657); Episode 77/100; Loss: 0.0008204652112908661\n",
      "Step 68 (6658); Episode 77/100; Loss: 0.004797833040356636\n",
      "Step 69 (6659); Episode 77/100; Loss: 0.0015154235297814012\n",
      "Step 70 (6660); Episode 77/100; Loss: 0.07515424489974976\n",
      "Step 71 (6661); Episode 77/100; Loss: 0.001855607726611197\n",
      "Step 72 (6662); Episode 77/100; Loss: 0.0399487130343914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 73 (6663); Episode 77/100; Loss: 0.044621918350458145\n",
      "Step 74 (6664); Episode 77/100; Loss: 0.0527082122862339\n",
      "Step 75 (6665); Episode 77/100; Loss: 0.02648824080824852\n",
      "Step 76 (6666); Episode 77/100; Loss: 0.021200697869062424\n",
      "Step 77 (6667); Episode 77/100; Loss: 0.1125551238656044\n",
      "Step 78 (6668); Episode 77/100; Loss: 0.0012929936638101935\n",
      "Step 79 (6669); Episode 77/100; Loss: 0.037846118211746216\n",
      "Step 80 (6670); Episode 77/100; Loss: 0.0011198491556569934\n",
      "Step 81 (6671); Episode 77/100; Loss: 0.001092181890271604\n",
      "Step 82 (6672); Episode 77/100; Loss: 0.046250928193330765\n",
      "Step 83 (6673); Episode 77/100; Loss: 0.09644048660993576\n",
      "Step 84 (6674); Episode 77/100; Loss: 0.05638328194618225\n",
      "Step 85 (6675); Episode 77/100; Loss: 0.0012346806470304728\n",
      "Step 86 (6676); Episode 77/100; Loss: 0.10076633095741272\n",
      "Step 87 (6677); Episode 77/100; Loss: 0.0025332944933325052\n",
      "Step 88 (6678); Episode 77/100; Loss: 0.09051541984081268\n",
      "Step 89 (6679); Episode 77/100; Loss: 0.04291250929236412\n",
      "Step 90 (6680); Episode 77/100; Loss: 0.0022676996886730194\n",
      "Step 91 (6681); Episode 77/100; Loss: 0.00980076752603054\n",
      "Step 92 (6682); Episode 77/100; Loss: 0.005475143902003765\n",
      "Step 93 (6683); Episode 77/100; Loss: 0.029616769403219223\n",
      "Step 94 (6684); Episode 77/100; Loss: 0.06661307066679001\n",
      "Step 95 (6685); Episode 77/100; Loss: 0.04548705741763115\n",
      "Step 96 (6686); Episode 77/100; Loss: 0.0017161668511107564\n",
      "Step 97 (6687); Episode 77/100; Loss: 0.0709017887711525\n",
      "Step 98 (6688); Episode 77/100; Loss: 0.0032356607262045145\n",
      "Step 99 (6689); Episode 77/100; Loss: 0.003063549753278494\n",
      "Step 100 (6690); Episode 77/100; Loss: 0.0011323236394673586\n",
      "Step 101 (6691); Episode 77/100; Loss: 0.004379897844046354\n",
      "Step 102 (6692); Episode 77/100; Loss: 0.107905313372612\n",
      "Step 103 (6693); Episode 77/100; Loss: 0.05579441413283348\n",
      "Step 104 (6694); Episode 77/100; Loss: 0.0021450023632496595\n",
      "Step 105 (6695); Episode 77/100; Loss: 0.046951696276664734\n",
      "Step 106 (6696); Episode 77/100; Loss: 0.0397307425737381\n",
      "Step 107 (6697); Episode 77/100; Loss: 0.001170301460660994\n",
      "Step 108 (6698); Episode 77/100; Loss: 0.0025932095013558865\n",
      "Step 109 (6699); Episode 77/100; Loss: 0.001722740358673036\n",
      "Step 110 (6700); Episode 77/100; Loss: 0.0030504269525408745\n",
      "Step 111 (6701); Episode 77/100; Loss: 0.032267533242702484\n",
      "Step 112 (6702); Episode 77/100; Loss: 0.05918915569782257\n",
      "Step 113 (6703); Episode 77/100; Loss: 0.0021616017911583185\n",
      "Step 114 (6704); Episode 77/100; Loss: 0.0035390318371355534\n",
      "Step 115 (6705); Episode 77/100; Loss: 0.029315510764718056\n",
      "Step 116 (6706); Episode 77/100; Loss: 0.0013336915289983153\n",
      "Step 117 (6707); Episode 77/100; Loss: 0.003332977183163166\n",
      "Step 118 (6708); Episode 77/100; Loss: 0.0012509479420259595\n",
      "Step 119 (6709); Episode 77/100; Loss: 0.08452651649713516\n",
      "Step 120 (6710); Episode 77/100; Loss: 0.001935511827468872\n",
      "Step 121 (6711); Episode 77/100; Loss: 0.002649962203577161\n",
      "Step 122 (6712); Episode 77/100; Loss: 0.02462584152817726\n",
      "Step 123 (6713); Episode 77/100; Loss: 0.014108679257333279\n",
      "Step 124 (6714); Episode 77/100; Loss: 0.007989649660885334\n",
      "Step 125 (6715); Episode 77/100; Loss: 0.04987816512584686\n",
      "Step 126 (6716); Episode 77/100; Loss: 0.036014799028635025\n",
      "Step 127 (6717); Episode 77/100; Loss: 0.002359071047976613\n",
      "Step 128 (6718); Episode 77/100; Loss: 0.05012080818414688\n",
      "Step 129 (6719); Episode 77/100; Loss: 0.0022843852639198303\n",
      "Step 130 (6720); Episode 77/100; Loss: 0.0013964410172775388\n",
      "Step 131 (6721); Episode 77/100; Loss: 0.0011549158953130245\n",
      "Step 132 (6722); Episode 77/100; Loss: 0.0396757498383522\n",
      "Step 133 (6723); Episode 77/100; Loss: 0.07173561304807663\n",
      "Step 134 (6724); Episode 77/100; Loss: 0.0014303099596872926\n",
      "Step 135 (6725); Episode 77/100; Loss: 0.001487702364102006\n",
      "Step 0 (6726); Episode 78/100; Loss: 0.05710219591856003\n",
      "Step 1 (6727); Episode 78/100; Loss: 0.04749969765543938\n",
      "Step 2 (6728); Episode 78/100; Loss: 0.05867798253893852\n",
      "Step 3 (6729); Episode 78/100; Loss: 0.076333187520504\n",
      "Step 4 (6730); Episode 78/100; Loss: 0.04356589540839195\n",
      "Step 5 (6731); Episode 78/100; Loss: 0.0024059503339231014\n",
      "Step 6 (6732); Episode 78/100; Loss: 0.09717573970556259\n",
      "Step 7 (6733); Episode 78/100; Loss: 0.005865106359124184\n",
      "Step 8 (6734); Episode 78/100; Loss: 0.000986343715339899\n",
      "Step 9 (6735); Episode 78/100; Loss: 0.0020988760516047478\n",
      "Step 10 (6736); Episode 78/100; Loss: 0.005429850425571203\n",
      "Step 11 (6737); Episode 78/100; Loss: 0.0446850024163723\n",
      "Step 12 (6738); Episode 78/100; Loss: 0.0031548338010907173\n",
      "Step 13 (6739); Episode 78/100; Loss: 0.0030260442290455103\n",
      "Step 14 (6740); Episode 78/100; Loss: 0.08950774371623993\n",
      "Step 15 (6741); Episode 78/100; Loss: 0.05417225882411003\n",
      "Step 16 (6742); Episode 78/100; Loss: 0.016303211450576782\n",
      "Step 17 (6743); Episode 78/100; Loss: 0.02440319024026394\n",
      "Step 18 (6744); Episode 78/100; Loss: 0.017137443646788597\n",
      "Step 19 (6745); Episode 78/100; Loss: 0.0015670839929953218\n",
      "Step 20 (6746); Episode 78/100; Loss: 0.0020202568266540766\n",
      "Step 21 (6747); Episode 78/100; Loss: 0.0005987735348753631\n",
      "Step 22 (6748); Episode 78/100; Loss: 0.0025971876457333565\n",
      "Step 23 (6749); Episode 78/100; Loss: 0.053290385752916336\n",
      "Step 24 (6750); Episode 78/100; Loss: 0.049207430332899094\n",
      "Step 25 (6751); Episode 78/100; Loss: 0.0030604994390159845\n",
      "Step 26 (6752); Episode 78/100; Loss: 0.035235945135354996\n",
      "Step 27 (6753); Episode 78/100; Loss: 0.024564901366829872\n",
      "Step 28 (6754); Episode 78/100; Loss: 0.0033403614070266485\n",
      "Step 29 (6755); Episode 78/100; Loss: 0.05277049541473389\n",
      "Step 30 (6756); Episode 78/100; Loss: 0.0017716322327032685\n",
      "Step 31 (6757); Episode 78/100; Loss: 0.010827898047864437\n",
      "Step 32 (6758); Episode 78/100; Loss: 0.04820920526981354\n",
      "Step 33 (6759); Episode 78/100; Loss: 0.02453048713505268\n",
      "Step 34 (6760); Episode 78/100; Loss: 0.003601740812882781\n",
      "Step 35 (6761); Episode 78/100; Loss: 0.004516602493822575\n",
      "Step 36 (6762); Episode 78/100; Loss: 0.0013980746734887362\n",
      "Step 37 (6763); Episode 78/100; Loss: 0.10788512974977493\n",
      "Step 38 (6764); Episode 78/100; Loss: 0.044104430824518204\n",
      "Step 39 (6765); Episode 78/100; Loss: 0.0010756937554106116\n",
      "Step 40 (6766); Episode 78/100; Loss: 0.0009074673871509731\n",
      "Step 41 (6767); Episode 78/100; Loss: 0.0021396006923168898\n",
      "Step 42 (6768); Episode 78/100; Loss: 0.0009314322378486395\n",
      "Step 43 (6769); Episode 78/100; Loss: 0.0047308700159192085\n",
      "Step 44 (6770); Episode 78/100; Loss: 0.001483879634179175\n",
      "Step 45 (6771); Episode 78/100; Loss: 0.025484999641776085\n",
      "Step 46 (6772); Episode 78/100; Loss: 0.057426854968070984\n",
      "Step 47 (6773); Episode 78/100; Loss: 0.05797841399908066\n",
      "Step 48 (6774); Episode 78/100; Loss: 0.04984242469072342\n",
      "Step 49 (6775); Episode 78/100; Loss: 0.001152984332293272\n",
      "Step 50 (6776); Episode 78/100; Loss: 0.0013020620681345463\n",
      "Step 51 (6777); Episode 78/100; Loss: 0.014405992813408375\n",
      "Step 52 (6778); Episode 78/100; Loss: 0.04580225795507431\n",
      "Step 53 (6779); Episode 78/100; Loss: 0.02325599454343319\n",
      "Step 54 (6780); Episode 78/100; Loss: 0.052994076162576675\n",
      "Step 55 (6781); Episode 78/100; Loss: 0.05176033079624176\n",
      "Step 56 (6782); Episode 78/100; Loss: 0.0014749361434951425\n",
      "Step 57 (6783); Episode 78/100; Loss: 0.0615232028067112\n",
      "Step 58 (6784); Episode 78/100; Loss: 0.005324173253029585\n",
      "Step 59 (6785); Episode 78/100; Loss: 0.0009297653450630605\n",
      "Step 60 (6786); Episode 78/100; Loss: 0.0018338601803407073\n",
      "Step 61 (6787); Episode 78/100; Loss: 0.04990407079458237\n",
      "Step 62 (6788); Episode 78/100; Loss: 0.05274757742881775\n",
      "Step 63 (6789); Episode 78/100; Loss: 0.002016304526478052\n",
      "Step 64 (6790); Episode 78/100; Loss: 0.057544782757759094\n",
      "Step 65 (6791); Episode 78/100; Loss: 0.0011586308246478438\n",
      "Step 66 (6792); Episode 78/100; Loss: 0.0009257550700567663\n",
      "Step 67 (6793); Episode 78/100; Loss: 0.05549786984920502\n",
      "Step 68 (6794); Episode 78/100; Loss: 0.000723057659342885\n",
      "Step 69 (6795); Episode 78/100; Loss: 0.044945187866687775\n",
      "Step 70 (6796); Episode 78/100; Loss: 0.05436583235859871\n",
      "Step 71 (6797); Episode 78/100; Loss: 0.037987202405929565\n",
      "Step 72 (6798); Episode 78/100; Loss: 0.037801068276166916\n",
      "Step 73 (6799); Episode 78/100; Loss: 0.001614092499949038\n",
      "Step 74 (6800); Episode 78/100; Loss: 0.08983229845762253\n",
      "Step 75 (6801); Episode 78/100; Loss: 0.035181786864995956\n",
      "Step 76 (6802); Episode 78/100; Loss: 0.0038125375285744667\n",
      "Step 77 (6803); Episode 78/100; Loss: 0.0017616718541830778\n",
      "Step 78 (6804); Episode 78/100; Loss: 0.0030706687830388546\n",
      "Step 79 (6805); Episode 78/100; Loss: 0.001857974217273295\n",
      "Step 80 (6806); Episode 78/100; Loss: 0.0006434653187170625\n",
      "Step 81 (6807); Episode 78/100; Loss: 0.045739706605672836\n",
      "Step 82 (6808); Episode 78/100; Loss: 0.0037002882454544306\n",
      "Step 83 (6809); Episode 78/100; Loss: 0.012428883463144302\n",
      "Step 84 (6810); Episode 78/100; Loss: 0.001155031961388886\n",
      "Step 85 (6811); Episode 78/100; Loss: 0.051724907010793686\n",
      "Step 86 (6812); Episode 78/100; Loss: 0.0006622172077186406\n",
      "Step 87 (6813); Episode 78/100; Loss: 0.0015419569099321961\n",
      "Step 88 (6814); Episode 78/100; Loss: 0.05089423432946205\n",
      "Step 89 (6815); Episode 78/100; Loss: 0.000961356156039983\n",
      "Step 90 (6816); Episode 78/100; Loss: 0.056971628218889236\n",
      "Step 91 (6817); Episode 78/100; Loss: 0.001730093965306878\n",
      "Step 92 (6818); Episode 78/100; Loss: 0.0015265437541529536\n",
      "Step 93 (6819); Episode 78/100; Loss: 0.10609892010688782\n",
      "Step 94 (6820); Episode 78/100; Loss: 0.02570359781384468\n",
      "Step 95 (6821); Episode 78/100; Loss: 0.001503641833551228\n",
      "Step 96 (6822); Episode 78/100; Loss: 0.0010638710809871554\n",
      "Step 97 (6823); Episode 78/100; Loss: 0.000644868181552738\n",
      "Step 98 (6824); Episode 78/100; Loss: 0.05490635335445404\n",
      "Step 99 (6825); Episode 78/100; Loss: 0.06765129417181015\n",
      "Step 100 (6826); Episode 78/100; Loss: 0.0013327879132702947\n",
      "Step 101 (6827); Episode 78/100; Loss: 0.0015227390686050057\n",
      "Step 102 (6828); Episode 78/100; Loss: 0.054375164210796356\n",
      "Step 103 (6829); Episode 78/100; Loss: 0.07921027392148972\n",
      "Step 104 (6830); Episode 78/100; Loss: 0.001867198385298252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 105 (6831); Episode 78/100; Loss: 0.03569679707288742\n",
      "Step 106 (6832); Episode 78/100; Loss: 0.05694589391350746\n",
      "Step 107 (6833); Episode 78/100; Loss: 0.003267185529693961\n",
      "Step 108 (6834); Episode 78/100; Loss: 0.002672824775800109\n",
      "Step 109 (6835); Episode 78/100; Loss: 0.0012353525962680578\n",
      "Step 110 (6836); Episode 78/100; Loss: 0.06982849538326263\n",
      "Step 111 (6837); Episode 78/100; Loss: 0.0010618118103593588\n",
      "Step 112 (6838); Episode 78/100; Loss: 0.02045188844203949\n",
      "Step 113 (6839); Episode 78/100; Loss: 0.08721401542425156\n",
      "Step 114 (6840); Episode 78/100; Loss: 0.0018705057445913553\n",
      "Step 115 (6841); Episode 78/100; Loss: 0.0023355591110885143\n",
      "Step 116 (6842); Episode 78/100; Loss: 0.05281023681163788\n",
      "Step 117 (6843); Episode 78/100; Loss: 0.049704477190971375\n",
      "Step 118 (6844); Episode 78/100; Loss: 0.03516213223338127\n",
      "Step 119 (6845); Episode 78/100; Loss: 0.004498898517340422\n",
      "Step 120 (6846); Episode 78/100; Loss: 0.0110658910125494\n",
      "Step 121 (6847); Episode 78/100; Loss: 0.09046551585197449\n",
      "Step 122 (6848); Episode 78/100; Loss: 0.0527912899851799\n",
      "Step 123 (6849); Episode 78/100; Loss: 0.001243165461346507\n",
      "Step 124 (6850); Episode 78/100; Loss: 0.0017340717604383826\n",
      "Step 125 (6851); Episode 78/100; Loss: 0.003612198168411851\n",
      "Step 126 (6852); Episode 78/100; Loss: 0.0012252626474946737\n",
      "Step 127 (6853); Episode 78/100; Loss: 0.0009404198499396443\n",
      "Step 128 (6854); Episode 78/100; Loss: 0.002448098501190543\n",
      "Step 129 (6855); Episode 78/100; Loss: 0.02045346051454544\n",
      "Step 130 (6856); Episode 78/100; Loss: 0.0034944487269967794\n",
      "Step 131 (6857); Episode 78/100; Loss: 0.049668408930301666\n",
      "Step 132 (6858); Episode 78/100; Loss: 0.05139641836285591\n",
      "Step 133 (6859); Episode 78/100; Loss: 0.008205657824873924\n",
      "Step 134 (6860); Episode 78/100; Loss: 0.009828341193497181\n",
      "Step 135 (6861); Episode 78/100; Loss: 0.010808916762471199\n",
      "Step 136 (6862); Episode 78/100; Loss: 0.01686760038137436\n",
      "Step 137 (6863); Episode 78/100; Loss: 0.0007272996008396149\n",
      "Step 138 (6864); Episode 78/100; Loss: 0.003787762252613902\n",
      "Step 0 (6865); Episode 79/100; Loss: 0.04309074208140373\n",
      "Step 1 (6866); Episode 79/100; Loss: 0.002066045068204403\n",
      "Step 2 (6867); Episode 79/100; Loss: 0.0019007143564522266\n",
      "Step 3 (6868); Episode 79/100; Loss: 0.0009449805365875363\n",
      "Step 4 (6869); Episode 79/100; Loss: 0.0011926869628950953\n",
      "Step 5 (6870); Episode 79/100; Loss: 0.07289397716522217\n",
      "Step 6 (6871); Episode 79/100; Loss: 0.0009374615037813783\n",
      "Step 7 (6872); Episode 79/100; Loss: 0.0009431866346858442\n",
      "Step 8 (6873); Episode 79/100; Loss: 0.0024898999836295843\n",
      "Step 9 (6874); Episode 79/100; Loss: 0.053863395005464554\n",
      "Step 10 (6875); Episode 79/100; Loss: 0.10014720261096954\n",
      "Step 11 (6876); Episode 79/100; Loss: 0.005090178456157446\n",
      "Step 12 (6877); Episode 79/100; Loss: 0.0011359874624758959\n",
      "Step 13 (6878); Episode 79/100; Loss: 0.0010451428825035691\n",
      "Step 14 (6879); Episode 79/100; Loss: 0.08950263261795044\n",
      "Step 15 (6880); Episode 79/100; Loss: 0.10362829267978668\n",
      "Step 16 (6881); Episode 79/100; Loss: 0.0029147975146770477\n",
      "Step 17 (6882); Episode 79/100; Loss: 0.05238506942987442\n",
      "Step 18 (6883); Episode 79/100; Loss: 0.15132679045200348\n",
      "Step 19 (6884); Episode 79/100; Loss: 0.006890618242323399\n",
      "Step 20 (6885); Episode 79/100; Loss: 0.001881033182144165\n",
      "Step 21 (6886); Episode 79/100; Loss: 0.0008179661817848682\n",
      "Step 22 (6887); Episode 79/100; Loss: 0.0012268138816580176\n",
      "Step 23 (6888); Episode 79/100; Loss: 0.0467865951359272\n",
      "Step 24 (6889); Episode 79/100; Loss: 0.095482237637043\n",
      "Step 25 (6890); Episode 79/100; Loss: 0.0011296240845695138\n",
      "Step 26 (6891); Episode 79/100; Loss: 0.0009525476489216089\n",
      "Step 27 (6892); Episode 79/100; Loss: 0.0459844172000885\n",
      "Step 28 (6893); Episode 79/100; Loss: 0.06017988920211792\n",
      "Step 29 (6894); Episode 79/100; Loss: 0.0014644161565229297\n",
      "Step 30 (6895); Episode 79/100; Loss: 0.001359872636385262\n",
      "Step 31 (6896); Episode 79/100; Loss: 0.022779718041419983\n",
      "Step 32 (6897); Episode 79/100; Loss: 0.0013028664980083704\n",
      "Step 33 (6898); Episode 79/100; Loss: 0.058832284063100815\n",
      "Step 34 (6899); Episode 79/100; Loss: 0.0017140553100034595\n",
      "Step 35 (6900); Episode 79/100; Loss: 0.0024115920532494783\n",
      "Step 36 (6901); Episode 79/100; Loss: 0.005285433027893305\n",
      "Step 37 (6902); Episode 79/100; Loss: 0.0014796192990615964\n",
      "Step 38 (6903); Episode 79/100; Loss: 0.0019492800347507\n",
      "Step 39 (6904); Episode 79/100; Loss: 0.04819079861044884\n",
      "Step 40 (6905); Episode 79/100; Loss: 0.0052248043939471245\n",
      "Step 41 (6906); Episode 79/100; Loss: 0.04465937241911888\n",
      "Step 42 (6907); Episode 79/100; Loss: 0.002381719183176756\n",
      "Step 43 (6908); Episode 79/100; Loss: 0.034468501806259155\n",
      "Step 44 (6909); Episode 79/100; Loss: 0.004128698725253344\n",
      "Step 45 (6910); Episode 79/100; Loss: 0.0036873153876513243\n",
      "Step 46 (6911); Episode 79/100; Loss: 0.0027823832351714373\n",
      "Step 47 (6912); Episode 79/100; Loss: 0.004120177589356899\n",
      "Step 48 (6913); Episode 79/100; Loss: 0.0017131698550656438\n",
      "Step 49 (6914); Episode 79/100; Loss: 0.07646447420120239\n",
      "Step 50 (6915); Episode 79/100; Loss: 0.0010904701193794608\n",
      "Step 51 (6916); Episode 79/100; Loss: 0.04775189980864525\n",
      "Step 52 (6917); Episode 79/100; Loss: 0.0037450005766004324\n",
      "Step 53 (6918); Episode 79/100; Loss: 0.0014487606240436435\n",
      "Step 54 (6919); Episode 79/100; Loss: 0.04053205996751785\n",
      "Step 55 (6920); Episode 79/100; Loss: 0.0013281573774293065\n",
      "Step 56 (6921); Episode 79/100; Loss: 0.001256930292584002\n",
      "Step 57 (6922); Episode 79/100; Loss: 0.05730012431740761\n",
      "Step 58 (6923); Episode 79/100; Loss: 0.04582696780562401\n",
      "Step 59 (6924); Episode 79/100; Loss: 0.0006221124203875661\n",
      "Step 60 (6925); Episode 79/100; Loss: 0.0020529194734990597\n",
      "Step 61 (6926); Episode 79/100; Loss: 0.0036117895506322384\n",
      "Step 62 (6927); Episode 79/100; Loss: 0.054299496114254\n",
      "Step 63 (6928); Episode 79/100; Loss: 0.021835172548890114\n",
      "Step 64 (6929); Episode 79/100; Loss: 0.0014205952174961567\n",
      "Step 65 (6930); Episode 79/100; Loss: 0.001790395355783403\n",
      "Step 66 (6931); Episode 79/100; Loss: 0.018470024690032005\n",
      "Step 67 (6932); Episode 79/100; Loss: 0.07999447733163834\n",
      "Step 68 (6933); Episode 79/100; Loss: 0.019678274169564247\n",
      "Step 69 (6934); Episode 79/100; Loss: 0.0015026119071990252\n",
      "Step 70 (6935); Episode 79/100; Loss: 0.15457673370838165\n",
      "Step 71 (6936); Episode 79/100; Loss: 0.11512269079685211\n",
      "Step 72 (6937); Episode 79/100; Loss: 0.0013079690979793668\n",
      "Step 73 (6938); Episode 79/100; Loss: 0.0011124240700155497\n",
      "Step 74 (6939); Episode 79/100; Loss: 0.0009708760771900415\n",
      "Step 75 (6940); Episode 79/100; Loss: 0.046630293130874634\n",
      "Step 76 (6941); Episode 79/100; Loss: 0.0011715552536770701\n",
      "Step 77 (6942); Episode 79/100; Loss: 0.0009291540482081473\n",
      "Step 78 (6943); Episode 79/100; Loss: 0.10119310021400452\n",
      "Step 79 (6944); Episode 79/100; Loss: 0.0011802788358181715\n",
      "Step 80 (6945); Episode 79/100; Loss: 0.0004968072171323001\n",
      "Step 81 (6946); Episode 79/100; Loss: 0.059299394488334656\n",
      "Step 82 (6947); Episode 79/100; Loss: 0.0020329889375716448\n",
      "Step 83 (6948); Episode 79/100; Loss: 0.0011844401014968753\n",
      "Step 84 (6949); Episode 79/100; Loss: 0.11080903559923172\n",
      "Step 85 (6950); Episode 79/100; Loss: 0.002148337895050645\n",
      "Step 86 (6951); Episode 79/100; Loss: 0.04229454696178436\n",
      "Step 87 (6952); Episode 79/100; Loss: 0.014566173776984215\n",
      "Step 88 (6953); Episode 79/100; Loss: 0.0010124340187758207\n",
      "Step 89 (6954); Episode 79/100; Loss: 0.09993426501750946\n",
      "Step 90 (6955); Episode 79/100; Loss: 0.046253498643636703\n",
      "Step 91 (6956); Episode 79/100; Loss: 0.0012762604746967554\n",
      "Step 92 (6957); Episode 79/100; Loss: 0.026752471923828125\n",
      "Step 93 (6958); Episode 79/100; Loss: 0.12831644713878632\n",
      "Step 94 (6959); Episode 79/100; Loss: 0.0019705062732100487\n",
      "Step 95 (6960); Episode 79/100; Loss: 0.04148680716753006\n",
      "Step 96 (6961); Episode 79/100; Loss: 0.018365437164902687\n",
      "Step 97 (6962); Episode 79/100; Loss: 0.10106249153614044\n",
      "Step 98 (6963); Episode 79/100; Loss: 0.0996323972940445\n",
      "Step 99 (6964); Episode 79/100; Loss: 0.0030669511761516333\n",
      "Step 100 (6965); Episode 79/100; Loss: 0.06787900626659393\n",
      "Step 101 (6966); Episode 79/100; Loss: 0.0549604706466198\n",
      "Step 102 (6967); Episode 79/100; Loss: 0.000962086662184447\n",
      "Step 103 (6968); Episode 79/100; Loss: 0.031453490257263184\n",
      "Step 104 (6969); Episode 79/100; Loss: 0.12238385528326035\n",
      "Step 105 (6970); Episode 79/100; Loss: 0.01446069311350584\n",
      "Step 106 (6971); Episode 79/100; Loss: 0.0041670482605695724\n",
      "Step 107 (6972); Episode 79/100; Loss: 0.0033900481648743153\n",
      "Step 108 (6973); Episode 79/100; Loss: 0.0039108144119381905\n",
      "Step 109 (6974); Episode 79/100; Loss: 0.0018703944515436888\n",
      "Step 110 (6975); Episode 79/100; Loss: 0.0024035705719143152\n",
      "Step 111 (6976); Episode 79/100; Loss: 0.06715025007724762\n",
      "Step 112 (6977); Episode 79/100; Loss: 0.0017862692475318909\n",
      "Step 113 (6978); Episode 79/100; Loss: 0.07121800631284714\n",
      "Step 114 (6979); Episode 79/100; Loss: 0.0011980447452515364\n",
      "Step 115 (6980); Episode 79/100; Loss: 0.05042225122451782\n",
      "Step 116 (6981); Episode 79/100; Loss: 0.002413688926026225\n",
      "Step 117 (6982); Episode 79/100; Loss: 0.009519055485725403\n",
      "Step 118 (6983); Episode 79/100; Loss: 0.0596022829413414\n",
      "Step 119 (6984); Episode 79/100; Loss: 0.04576057195663452\n",
      "Step 120 (6985); Episode 79/100; Loss: 0.0017339782789349556\n",
      "Step 121 (6986); Episode 79/100; Loss: 0.06025766208767891\n",
      "Step 122 (6987); Episode 79/100; Loss: 0.09798545390367508\n",
      "Step 123 (6988); Episode 79/100; Loss: 0.014546122401952744\n",
      "Step 124 (6989); Episode 79/100; Loss: 0.002317376434803009\n",
      "Step 125 (6990); Episode 79/100; Loss: 0.04261142387986183\n",
      "Step 126 (6991); Episode 79/100; Loss: 0.0026067157741636038\n",
      "Step 127 (6992); Episode 79/100; Loss: 0.061360571533441544\n",
      "Step 128 (6993); Episode 79/100; Loss: 0.0017285434296354651\n",
      "Step 129 (6994); Episode 79/100; Loss: 0.0018101949244737625\n",
      "Step 130 (6995); Episode 79/100; Loss: 0.018748991191387177\n",
      "Step 131 (6996); Episode 79/100; Loss: 0.056890688836574554\n",
      "Step 132 (6997); Episode 79/100; Loss: 0.007870232686400414\n",
      "Step 133 (6998); Episode 79/100; Loss: 0.0011755868326872587\n",
      "Step 134 (6999); Episode 79/100; Loss: 0.0012897143606096506\n",
      "Step 135 (7000); Episode 79/100; Loss: 0.14089109003543854\n",
      "Step 136 (7001); Episode 79/100; Loss: 0.07104407250881195\n",
      "Step 137 (7002); Episode 79/100; Loss: 0.055338457226753235\n",
      "Step 138 (7003); Episode 79/100; Loss: 0.05252184718847275\n",
      "Step 139 (7004); Episode 79/100; Loss: 0.0031774681992828846\n",
      "Step 140 (7005); Episode 79/100; Loss: 0.001294225687161088\n",
      "Step 141 (7006); Episode 79/100; Loss: 0.053361356258392334\n",
      "Step 142 (7007); Episode 79/100; Loss: 0.0010644610738381743\n",
      "Step 143 (7008); Episode 79/100; Loss: 0.010399062186479568\n",
      "Step 144 (7009); Episode 79/100; Loss: 0.05744703486561775\n",
      "Step 145 (7010); Episode 79/100; Loss: 0.09131461381912231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 146 (7011); Episode 79/100; Loss: 0.0015070560621097684\n",
      "Step 147 (7012); Episode 79/100; Loss: 0.002726213075220585\n",
      "Step 148 (7013); Episode 79/100; Loss: 0.0013169407611712813\n",
      "Step 149 (7014); Episode 79/100; Loss: 0.02952457033097744\n",
      "Step 150 (7015); Episode 79/100; Loss: 0.0013463428476825356\n",
      "Step 151 (7016); Episode 79/100; Loss: 0.0011107220780104399\n",
      "Step 152 (7017); Episode 79/100; Loss: 0.0005091927596367896\n",
      "Step 153 (7018); Episode 79/100; Loss: 0.0009066828060895205\n",
      "Step 154 (7019); Episode 79/100; Loss: 0.0019846863579005003\n",
      "Step 155 (7020); Episode 79/100; Loss: 0.012094897218048573\n",
      "Step 156 (7021); Episode 79/100; Loss: 0.07242769747972488\n",
      "Step 157 (7022); Episode 79/100; Loss: 0.0005352721782401204\n",
      "Step 158 (7023); Episode 79/100; Loss: 0.0009985894430428743\n",
      "Step 159 (7024); Episode 79/100; Loss: 0.0017068864544853568\n",
      "Step 160 (7025); Episode 79/100; Loss: 0.0013071143766865134\n",
      "Step 161 (7026); Episode 79/100; Loss: 0.0012050868244841695\n",
      "Step 162 (7027); Episode 79/100; Loss: 0.08447213470935822\n",
      "Step 163 (7028); Episode 79/100; Loss: 0.0010434722062200308\n",
      "Step 164 (7029); Episode 79/100; Loss: 0.0012916410341858864\n",
      "Step 165 (7030); Episode 79/100; Loss: 0.09292015433311462\n",
      "Step 166 (7031); Episode 79/100; Loss: 0.000879851751960814\n",
      "Step 167 (7032); Episode 79/100; Loss: 0.0006867526681162417\n",
      "Step 168 (7033); Episode 79/100; Loss: 0.0009682606323622167\n",
      "Step 169 (7034); Episode 79/100; Loss: 0.005523271393030882\n",
      "Step 170 (7035); Episode 79/100; Loss: 0.0004088716523256153\n",
      "Step 171 (7036); Episode 79/100; Loss: 0.001403091591782868\n",
      "Step 172 (7037); Episode 79/100; Loss: 0.003478881437331438\n",
      "Step 173 (7038); Episode 79/100; Loss: 0.0012640792410820723\n",
      "Step 174 (7039); Episode 79/100; Loss: 0.04668867588043213\n",
      "Step 175 (7040); Episode 79/100; Loss: 0.05181135609745979\n",
      "Step 176 (7041); Episode 79/100; Loss: 0.0019150373991578817\n",
      "Step 177 (7042); Episode 79/100; Loss: 0.0021395094227045774\n",
      "Step 178 (7043); Episode 79/100; Loss: 0.0015241617802530527\n",
      "Step 179 (7044); Episode 79/100; Loss: 0.04902249202132225\n",
      "Step 180 (7045); Episode 79/100; Loss: 0.0011740437475964427\n",
      "Step 181 (7046); Episode 79/100; Loss: 0.080202616751194\n",
      "Step 182 (7047); Episode 79/100; Loss: 0.04687592759728432\n",
      "Step 183 (7048); Episode 79/100; Loss: 0.04514331370592117\n",
      "Step 184 (7049); Episode 79/100; Loss: 0.0008663975750096142\n",
      "Step 185 (7050); Episode 79/100; Loss: 0.17944666743278503\n",
      "Step 186 (7051); Episode 79/100; Loss: 0.017088863998651505\n",
      "Step 187 (7052); Episode 79/100; Loss: 0.0006485346821136773\n",
      "Step 188 (7053); Episode 79/100; Loss: 0.0035474137403070927\n",
      "Step 189 (7054); Episode 79/100; Loss: 0.0018143680645152926\n",
      "Step 190 (7055); Episode 79/100; Loss: 0.04995257779955864\n",
      "Step 191 (7056); Episode 79/100; Loss: 0.019200477749109268\n",
      "Step 192 (7057); Episode 79/100; Loss: 0.05587772652506828\n",
      "Step 193 (7058); Episode 79/100; Loss: 0.0007032676949165761\n",
      "Step 194 (7059); Episode 79/100; Loss: 0.05063483864068985\n",
      "Step 195 (7060); Episode 79/100; Loss: 0.004766702651977539\n",
      "Step 196 (7061); Episode 79/100; Loss: 0.015523015521466732\n",
      "Step 197 (7062); Episode 79/100; Loss: 0.001265086350031197\n",
      "Step 198 (7063); Episode 79/100; Loss: 0.001260615885257721\n",
      "Step 199 (7064); Episode 79/100; Loss: 0.0009099128656089306\n",
      "Step 0 (7065); Episode 80/100; Loss: 0.0008236924768425524\n",
      "Step 1 (7066); Episode 80/100; Loss: 0.09465930610895157\n",
      "Step 2 (7067); Episode 80/100; Loss: 0.0568629615008831\n",
      "Step 3 (7068); Episode 80/100; Loss: 0.055328574031591415\n",
      "Step 4 (7069); Episode 80/100; Loss: 0.001153775374405086\n",
      "Step 5 (7070); Episode 80/100; Loss: 0.03810463473200798\n",
      "Step 6 (7071); Episode 80/100; Loss: 0.0017698733136057854\n",
      "Step 7 (7072); Episode 80/100; Loss: 0.10385539382696152\n",
      "Step 8 (7073); Episode 80/100; Loss: 0.0015707769198343158\n",
      "Step 9 (7074); Episode 80/100; Loss: 0.0031209553126245737\n",
      "Step 10 (7075); Episode 80/100; Loss: 0.0014563705772161484\n",
      "Step 11 (7076); Episode 80/100; Loss: 0.0015255700564011931\n",
      "Step 12 (7077); Episode 80/100; Loss: 0.0016520116478204727\n",
      "Step 13 (7078); Episode 80/100; Loss: 0.04697339981794357\n",
      "Step 14 (7079); Episode 80/100; Loss: 0.05268722400069237\n",
      "Step 15 (7080); Episode 80/100; Loss: 0.0018370451871305704\n",
      "Step 16 (7081); Episode 80/100; Loss: 0.040076397359371185\n",
      "Step 17 (7082); Episode 80/100; Loss: 0.021891281008720398\n",
      "Step 18 (7083); Episode 80/100; Loss: 0.055773552507162094\n",
      "Step 19 (7084); Episode 80/100; Loss: 0.10353749990463257\n",
      "Step 20 (7085); Episode 80/100; Loss: 0.0605512298643589\n",
      "Step 21 (7086); Episode 80/100; Loss: 0.09574828296899796\n",
      "Step 22 (7087); Episode 80/100; Loss: 0.09891384094953537\n",
      "Step 23 (7088); Episode 80/100; Loss: 0.00117914704605937\n",
      "Step 24 (7089); Episode 80/100; Loss: 0.0005265856743790209\n",
      "Step 25 (7090); Episode 80/100; Loss: 0.0556233748793602\n",
      "Step 26 (7091); Episode 80/100; Loss: 0.0072833760641515255\n",
      "Step 27 (7092); Episode 80/100; Loss: 0.002897802274674177\n",
      "Step 28 (7093); Episode 80/100; Loss: 0.07807550579309464\n",
      "Step 29 (7094); Episode 80/100; Loss: 0.046744879335165024\n",
      "Step 30 (7095); Episode 80/100; Loss: 0.0018340866081416607\n",
      "Step 31 (7096); Episode 80/100; Loss: 0.0016743566375225782\n",
      "Step 32 (7097); Episode 80/100; Loss: 0.0014430549927055836\n",
      "Step 33 (7098); Episode 80/100; Loss: 0.1471148431301117\n",
      "Step 34 (7099); Episode 80/100; Loss: 0.0015082965837791562\n",
      "Step 35 (7100); Episode 80/100; Loss: 0.0012761405669152737\n",
      "Step 36 (7101); Episode 80/100; Loss: 0.046641867607831955\n",
      "Step 37 (7102); Episode 80/100; Loss: 0.0009859534911811352\n",
      "Step 38 (7103); Episode 80/100; Loss: 0.0019619097001850605\n",
      "Step 39 (7104); Episode 80/100; Loss: 0.0021324213594198227\n",
      "Step 40 (7105); Episode 80/100; Loss: 0.0008797803311608732\n",
      "Step 41 (7106); Episode 80/100; Loss: 0.05530292168259621\n",
      "Step 42 (7107); Episode 80/100; Loss: 0.037935078144073486\n",
      "Step 43 (7108); Episode 80/100; Loss: 0.0011161982547491789\n",
      "Step 44 (7109); Episode 80/100; Loss: 0.05465928092598915\n",
      "Step 45 (7110); Episode 80/100; Loss: 0.039476726204156876\n",
      "Step 46 (7111); Episode 80/100; Loss: 0.0007214430952444673\n",
      "Step 47 (7112); Episode 80/100; Loss: 0.036677200347185135\n",
      "Step 48 (7113); Episode 80/100; Loss: 0.05257127061486244\n",
      "Step 49 (7114); Episode 80/100; Loss: 0.0008783363737165928\n",
      "Step 50 (7115); Episode 80/100; Loss: 0.08446096628904343\n",
      "Step 51 (7116); Episode 80/100; Loss: 0.05256025493144989\n",
      "Step 52 (7117); Episode 80/100; Loss: 0.0006085236673243344\n",
      "Step 53 (7118); Episode 80/100; Loss: 0.0006454315735027194\n",
      "Step 54 (7119); Episode 80/100; Loss: 0.000592729018535465\n",
      "Step 55 (7120); Episode 80/100; Loss: 0.06014496460556984\n",
      "Step 56 (7121); Episode 80/100; Loss: 0.06302990019321442\n",
      "Step 57 (7122); Episode 80/100; Loss: 0.06345536559820175\n",
      "Step 58 (7123); Episode 80/100; Loss: 0.0875534862279892\n",
      "Step 59 (7124); Episode 80/100; Loss: 0.06673818826675415\n",
      "Step 60 (7125); Episode 80/100; Loss: 0.03725672513246536\n",
      "Step 61 (7126); Episode 80/100; Loss: 0.04122922942042351\n",
      "Step 62 (7127); Episode 80/100; Loss: 0.11039389669895172\n",
      "Step 63 (7128); Episode 80/100; Loss: 0.05104658007621765\n",
      "Step 64 (7129); Episode 80/100; Loss: 0.09552229940891266\n",
      "Step 65 (7130); Episode 80/100; Loss: 0.04378277808427811\n",
      "Step 66 (7131); Episode 80/100; Loss: 0.001352919265627861\n",
      "Step 67 (7132); Episode 80/100; Loss: 0.003477745456621051\n",
      "Step 68 (7133); Episode 80/100; Loss: 0.0028079862240701914\n",
      "Step 69 (7134); Episode 80/100; Loss: 0.04959994554519653\n",
      "Step 70 (7135); Episode 80/100; Loss: 0.002278967760503292\n",
      "Step 71 (7136); Episode 80/100; Loss: 0.08422710746526718\n",
      "Step 72 (7137); Episode 80/100; Loss: 0.001632370287552476\n",
      "Step 73 (7138); Episode 80/100; Loss: 0.04823862016201019\n",
      "Step 74 (7139); Episode 80/100; Loss: 0.008374856784939766\n",
      "Step 75 (7140); Episode 80/100; Loss: 0.001072413637302816\n",
      "Step 76 (7141); Episode 80/100; Loss: 0.000933609320782125\n",
      "Step 77 (7142); Episode 80/100; Loss: 0.0006814995431341231\n",
      "Step 78 (7143); Episode 80/100; Loss: 0.05417466536164284\n",
      "Step 79 (7144); Episode 80/100; Loss: 0.03356684744358063\n",
      "Step 80 (7145); Episode 80/100; Loss: 0.0015784803545102477\n",
      "Step 81 (7146); Episode 80/100; Loss: 0.00038529926678165793\n",
      "Step 82 (7147); Episode 80/100; Loss: 0.00582653796300292\n",
      "Step 83 (7148); Episode 80/100; Loss: 0.0015886261826381087\n",
      "Step 84 (7149); Episode 80/100; Loss: 0.051779840141534805\n",
      "Step 85 (7150); Episode 80/100; Loss: 0.0018278455827385187\n",
      "Step 86 (7151); Episode 80/100; Loss: 0.0010414357529953122\n",
      "Step 87 (7152); Episode 80/100; Loss: 0.016938261687755585\n",
      "Step 88 (7153); Episode 80/100; Loss: 0.18784545361995697\n",
      "Step 89 (7154); Episode 80/100; Loss: 0.09884650260210037\n",
      "Step 90 (7155); Episode 80/100; Loss: 0.05927051603794098\n",
      "Step 91 (7156); Episode 80/100; Loss: 0.003544612554833293\n",
      "Step 92 (7157); Episode 80/100; Loss: 0.07974071800708771\n",
      "Step 93 (7158); Episode 80/100; Loss: 0.0034063817001879215\n",
      "Step 94 (7159); Episode 80/100; Loss: 0.0022787388879805803\n",
      "Step 95 (7160); Episode 80/100; Loss: 0.04279817268252373\n",
      "Step 96 (7161); Episode 80/100; Loss: 0.05057927966117859\n",
      "Step 97 (7162); Episode 80/100; Loss: 0.0019859836902469397\n",
      "Step 98 (7163); Episode 80/100; Loss: 0.04311468079686165\n",
      "Step 99 (7164); Episode 80/100; Loss: 0.001576053793542087\n",
      "Step 100 (7165); Episode 80/100; Loss: 0.04781044274568558\n",
      "Step 101 (7166); Episode 80/100; Loss: 0.002579794032499194\n",
      "Step 102 (7167); Episode 80/100; Loss: 0.004006659612059593\n",
      "Step 103 (7168); Episode 80/100; Loss: 0.0024023624137043953\n",
      "Step 104 (7169); Episode 80/100; Loss: 0.003250094596296549\n",
      "Step 105 (7170); Episode 80/100; Loss: 0.07130701839923859\n",
      "Step 106 (7171); Episode 80/100; Loss: 0.0947968140244484\n",
      "Step 107 (7172); Episode 80/100; Loss: 0.001223925850354135\n",
      "Step 108 (7173); Episode 80/100; Loss: 0.05238672345876694\n",
      "Step 109 (7174); Episode 80/100; Loss: 0.0016373799880966544\n",
      "Step 110 (7175); Episode 80/100; Loss: 0.0013198043452575803\n",
      "Step 111 (7176); Episode 80/100; Loss: 0.04755597189068794\n",
      "Step 112 (7177); Episode 80/100; Loss: 0.0016208187444135547\n",
      "Step 113 (7178); Episode 80/100; Loss: 0.0008353418670594692\n",
      "Step 114 (7179); Episode 80/100; Loss: 0.15257039666175842\n",
      "Step 115 (7180); Episode 80/100; Loss: 0.053400490432977676\n",
      "Step 116 (7181); Episode 80/100; Loss: 0.0245126411318779\n",
      "Step 117 (7182); Episode 80/100; Loss: 0.028035754337906837\n",
      "Step 118 (7183); Episode 80/100; Loss: 0.045557115226984024\n",
      "Step 119 (7184); Episode 80/100; Loss: 0.002509442623704672\n",
      "Step 120 (7185); Episode 80/100; Loss: 0.05518791452050209\n",
      "Step 121 (7186); Episode 80/100; Loss: 0.0049067712388932705\n",
      "Step 122 (7187); Episode 80/100; Loss: 0.047017768025398254\n",
      "Step 123 (7188); Episode 80/100; Loss: 0.00368872401304543\n",
      "Step 124 (7189); Episode 80/100; Loss: 0.04250733181834221\n",
      "Step 0 (7190); Episode 81/100; Loss: 0.0025381746236234903\n",
      "Step 1 (7191); Episode 81/100; Loss: 0.001494278316386044\n",
      "Step 2 (7192); Episode 81/100; Loss: 0.0013613748596981168\n",
      "Step 3 (7193); Episode 81/100; Loss: 0.05771816521883011\n",
      "Step 4 (7194); Episode 81/100; Loss: 0.001830756082199514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 (7195); Episode 81/100; Loss: 0.0013118577189743519\n",
      "Step 6 (7196); Episode 81/100; Loss: 0.03055056743323803\n",
      "Step 7 (7197); Episode 81/100; Loss: 0.0014393397141247988\n",
      "Step 8 (7198); Episode 81/100; Loss: 0.047962650656700134\n",
      "Step 9 (7199); Episode 81/100; Loss: 0.0013739923015236855\n",
      "Step 10 (7200); Episode 81/100; Loss: 0.005906006321310997\n",
      "Step 11 (7201); Episode 81/100; Loss: 0.041669540107250214\n",
      "Step 12 (7202); Episode 81/100; Loss: 0.016168678179383278\n",
      "Step 13 (7203); Episode 81/100; Loss: 0.0015962938778102398\n",
      "Step 14 (7204); Episode 81/100; Loss: 0.057447560131549835\n",
      "Step 15 (7205); Episode 81/100; Loss: 0.0031230777967721224\n",
      "Step 16 (7206); Episode 81/100; Loss: 0.05607425048947334\n",
      "Step 17 (7207); Episode 81/100; Loss: 0.0026029942091554403\n",
      "Step 18 (7208); Episode 81/100; Loss: 0.06056411191821098\n",
      "Step 19 (7209); Episode 81/100; Loss: 0.000752375228330493\n",
      "Step 20 (7210); Episode 81/100; Loss: 0.09161324799060822\n",
      "Step 21 (7211); Episode 81/100; Loss: 0.019402138888835907\n",
      "Step 22 (7212); Episode 81/100; Loss: 0.0010296448599547148\n",
      "Step 23 (7213); Episode 81/100; Loss: 0.04647909104824066\n",
      "Step 24 (7214); Episode 81/100; Loss: 0.05088939145207405\n",
      "Step 25 (7215); Episode 81/100; Loss: 0.03501522168517113\n",
      "Step 26 (7216); Episode 81/100; Loss: 0.0008612442179583013\n",
      "Step 27 (7217); Episode 81/100; Loss: 0.0013876324519515038\n",
      "Step 28 (7218); Episode 81/100; Loss: 0.0015955815324559808\n",
      "Step 29 (7219); Episode 81/100; Loss: 0.0026013769675046206\n",
      "Step 30 (7220); Episode 81/100; Loss: 0.0009168374817818403\n",
      "Step 31 (7221); Episode 81/100; Loss: 0.0011451494647189975\n",
      "Step 32 (7222); Episode 81/100; Loss: 0.0050257910043001175\n",
      "Step 33 (7223); Episode 81/100; Loss: 0.1406325250864029\n",
      "Step 34 (7224); Episode 81/100; Loss: 0.0042727673426270485\n",
      "Step 35 (7225); Episode 81/100; Loss: 0.0038434809539467096\n",
      "Step 36 (7226); Episode 81/100; Loss: 0.0033899256959557533\n",
      "Step 37 (7227); Episode 81/100; Loss: 0.05660182610154152\n",
      "Step 38 (7228); Episode 81/100; Loss: 0.0008697515586391091\n",
      "Step 39 (7229); Episode 81/100; Loss: 0.08466343581676483\n",
      "Step 40 (7230); Episode 81/100; Loss: 0.0010472184512764215\n",
      "Step 41 (7231); Episode 81/100; Loss: 0.050393205136060715\n",
      "Step 42 (7232); Episode 81/100; Loss: 0.002029403578490019\n",
      "Step 43 (7233); Episode 81/100; Loss: 0.05089129880070686\n",
      "Step 44 (7234); Episode 81/100; Loss: 0.002676169155165553\n",
      "Step 45 (7235); Episode 81/100; Loss: 0.14318342506885529\n",
      "Step 46 (7236); Episode 81/100; Loss: 0.03808508813381195\n",
      "Step 47 (7237); Episode 81/100; Loss: 0.0018056872067973018\n",
      "Step 48 (7238); Episode 81/100; Loss: 0.020390478894114494\n",
      "Step 49 (7239); Episode 81/100; Loss: 0.0013938203919678926\n",
      "Step 50 (7240); Episode 81/100; Loss: 0.004006584174931049\n",
      "Step 51 (7241); Episode 81/100; Loss: 0.0007435426232405007\n",
      "Step 52 (7242); Episode 81/100; Loss: 0.0009537930600345135\n",
      "Step 53 (7243); Episode 81/100; Loss: 0.003530260641127825\n",
      "Step 54 (7244); Episode 81/100; Loss: 0.04863585904240608\n",
      "Step 55 (7245); Episode 81/100; Loss: 0.03808978945016861\n",
      "Step 56 (7246); Episode 81/100; Loss: 0.002026599831879139\n",
      "Step 57 (7247); Episode 81/100; Loss: 0.04545750468969345\n",
      "Step 58 (7248); Episode 81/100; Loss: 0.0019320682622492313\n",
      "Step 59 (7249); Episode 81/100; Loss: 0.014252852648496628\n",
      "Step 60 (7250); Episode 81/100; Loss: 0.041230179369449615\n",
      "Step 61 (7251); Episode 81/100; Loss: 0.05701389163732529\n",
      "Step 62 (7252); Episode 81/100; Loss: 0.056831248104572296\n",
      "Step 63 (7253); Episode 81/100; Loss: 0.0012889675563201308\n",
      "Step 64 (7254); Episode 81/100; Loss: 0.0008527509053237736\n",
      "Step 65 (7255); Episode 81/100; Loss: 0.0019301519496366382\n",
      "Step 66 (7256); Episode 81/100; Loss: 0.06357720494270325\n",
      "Step 67 (7257); Episode 81/100; Loss: 0.001939561334438622\n",
      "Step 68 (7258); Episode 81/100; Loss: 0.0028575947508215904\n",
      "Step 69 (7259); Episode 81/100; Loss: 0.002804620424285531\n",
      "Step 70 (7260); Episode 81/100; Loss: 0.004011711571365595\n",
      "Step 71 (7261); Episode 81/100; Loss: 0.050600118935108185\n",
      "Step 72 (7262); Episode 81/100; Loss: 0.048515431582927704\n",
      "Step 73 (7263); Episode 81/100; Loss: 0.011673643253743649\n",
      "Step 74 (7264); Episode 81/100; Loss: 0.05636467784643173\n",
      "Step 75 (7265); Episode 81/100; Loss: 0.05530347675085068\n",
      "Step 76 (7266); Episode 81/100; Loss: 0.10743837058544159\n",
      "Step 77 (7267); Episode 81/100; Loss: 0.0008288933313451707\n",
      "Step 78 (7268); Episode 81/100; Loss: 0.0007711254875175655\n",
      "Step 79 (7269); Episode 81/100; Loss: 0.04589911922812462\n",
      "Step 80 (7270); Episode 81/100; Loss: 0.0043336632661521435\n",
      "Step 81 (7271); Episode 81/100; Loss: 0.033577047288417816\n",
      "Step 82 (7272); Episode 81/100; Loss: 0.0535011813044548\n",
      "Step 83 (7273); Episode 81/100; Loss: 0.07941365987062454\n",
      "Step 84 (7274); Episode 81/100; Loss: 0.003497531870380044\n",
      "Step 85 (7275); Episode 81/100; Loss: 0.05543294548988342\n",
      "Step 86 (7276); Episode 81/100; Loss: 0.002259637927636504\n",
      "Step 87 (7277); Episode 81/100; Loss: 0.0030020377598702908\n",
      "Step 88 (7278); Episode 81/100; Loss: 0.051563847810029984\n",
      "Step 89 (7279); Episode 81/100; Loss: 0.0012477029813453555\n",
      "Step 90 (7280); Episode 81/100; Loss: 0.04325699433684349\n",
      "Step 91 (7281); Episode 81/100; Loss: 0.007118795067071915\n",
      "Step 92 (7282); Episode 81/100; Loss: 0.0021290648728609085\n",
      "Step 93 (7283); Episode 81/100; Loss: 0.002501665148884058\n",
      "Step 94 (7284); Episode 81/100; Loss: 0.0464065782725811\n",
      "Step 95 (7285); Episode 81/100; Loss: 0.0015335909556597471\n",
      "Step 96 (7286); Episode 81/100; Loss: 0.046722590923309326\n",
      "Step 97 (7287); Episode 81/100; Loss: 0.08816955238580704\n",
      "Step 98 (7288); Episode 81/100; Loss: 0.03893022611737251\n",
      "Step 99 (7289); Episode 81/100; Loss: 0.0014940393157303333\n",
      "Step 100 (7290); Episode 81/100; Loss: 0.049522314220666885\n",
      "Step 101 (7291); Episode 81/100; Loss: 0.0013303369050845504\n",
      "Step 102 (7292); Episode 81/100; Loss: 0.0010603152913972735\n",
      "Step 103 (7293); Episode 81/100; Loss: 0.015837790444493294\n",
      "Step 104 (7294); Episode 81/100; Loss: 0.051551077514886856\n",
      "Step 105 (7295); Episode 81/100; Loss: 0.047525737434625626\n",
      "Step 106 (7296); Episode 81/100; Loss: 0.002567468909546733\n",
      "Step 107 (7297); Episode 81/100; Loss: 0.0012245430843904614\n",
      "Step 108 (7298); Episode 81/100; Loss: 0.002404663944616914\n",
      "Step 109 (7299); Episode 81/100; Loss: 0.04231172055006027\n",
      "Step 110 (7300); Episode 81/100; Loss: 0.05765846371650696\n",
      "Step 111 (7301); Episode 81/100; Loss: 0.002896852558478713\n",
      "Step 112 (7302); Episode 81/100; Loss: 0.0018591571133583784\n",
      "Step 113 (7303); Episode 81/100; Loss: 0.08197566121816635\n",
      "Step 114 (7304); Episode 81/100; Loss: 0.024387363344430923\n",
      "Step 115 (7305); Episode 81/100; Loss: 0.0011299647158011794\n",
      "Step 116 (7306); Episode 81/100; Loss: 0.05456256493926048\n",
      "Step 117 (7307); Episode 81/100; Loss: 0.0009289251174777746\n",
      "Step 118 (7308); Episode 81/100; Loss: 0.0012521801982074976\n",
      "Step 119 (7309); Episode 81/100; Loss: 0.02864416502416134\n",
      "Step 120 (7310); Episode 81/100; Loss: 0.04667823389172554\n",
      "Step 121 (7311); Episode 81/100; Loss: 0.0025175907649099827\n",
      "Step 122 (7312); Episode 81/100; Loss: 0.046548161655664444\n",
      "Step 123 (7313); Episode 81/100; Loss: 0.0011132336221635342\n",
      "Step 124 (7314); Episode 81/100; Loss: 0.04847419261932373\n",
      "Step 125 (7315); Episode 81/100; Loss: 0.05765827000141144\n",
      "Step 126 (7316); Episode 81/100; Loss: 0.08644240349531174\n",
      "Step 127 (7317); Episode 81/100; Loss: 0.07674998790025711\n",
      "Step 128 (7318); Episode 81/100; Loss: 0.027767501771450043\n",
      "Step 129 (7319); Episode 81/100; Loss: 0.0010307764168828726\n",
      "Step 130 (7320); Episode 81/100; Loss: 0.001963456626981497\n",
      "Step 131 (7321); Episode 81/100; Loss: 0.053487397730350494\n",
      "Step 132 (7322); Episode 81/100; Loss: 0.027501555159687996\n",
      "Step 133 (7323); Episode 81/100; Loss: 0.004202691372483969\n",
      "Step 134 (7324); Episode 81/100; Loss: 0.0017323988722637296\n",
      "Step 135 (7325); Episode 81/100; Loss: 0.05367202311754227\n",
      "Step 136 (7326); Episode 81/100; Loss: 0.06423476338386536\n",
      "Step 137 (7327); Episode 81/100; Loss: 0.03719475865364075\n",
      "Step 138 (7328); Episode 81/100; Loss: 0.0063242120668292046\n",
      "Step 139 (7329); Episode 81/100; Loss: 0.048073939979076385\n",
      "Step 140 (7330); Episode 81/100; Loss: 0.0010400706669315696\n",
      "Step 141 (7331); Episode 81/100; Loss: 0.0012819941621273756\n",
      "Step 142 (7332); Episode 81/100; Loss: 0.002695657778531313\n",
      "Step 143 (7333); Episode 81/100; Loss: 0.000507999851834029\n",
      "Step 144 (7334); Episode 81/100; Loss: 0.02595500461757183\n",
      "Step 145 (7335); Episode 81/100; Loss: 0.002654115203768015\n",
      "Step 146 (7336); Episode 81/100; Loss: 0.04621262848377228\n",
      "Step 147 (7337); Episode 81/100; Loss: 0.05363103374838829\n",
      "Step 148 (7338); Episode 81/100; Loss: 0.005127284210175276\n",
      "Step 149 (7339); Episode 81/100; Loss: 0.0006056507700122893\n",
      "Step 150 (7340); Episode 81/100; Loss: 0.026230137795209885\n",
      "Step 151 (7341); Episode 81/100; Loss: 0.09912403672933578\n",
      "Step 152 (7342); Episode 81/100; Loss: 0.0006993850111030042\n",
      "Step 153 (7343); Episode 81/100; Loss: 0.0017444479744881392\n",
      "Step 154 (7344); Episode 81/100; Loss: 0.0024783683475106955\n",
      "Step 155 (7345); Episode 81/100; Loss: 0.0005461835535243154\n",
      "Step 156 (7346); Episode 81/100; Loss: 0.0444137342274189\n",
      "Step 157 (7347); Episode 81/100; Loss: 0.04724982753396034\n",
      "Step 158 (7348); Episode 81/100; Loss: 0.04657629132270813\n",
      "Step 159 (7349); Episode 81/100; Loss: 0.0010643660789355636\n",
      "Step 160 (7350); Episode 81/100; Loss: 0.0756913423538208\n",
      "Step 161 (7351); Episode 81/100; Loss: 0.0007211430347524583\n",
      "Step 162 (7352); Episode 81/100; Loss: 0.01750023476779461\n",
      "Step 163 (7353); Episode 81/100; Loss: 0.005540691316127777\n",
      "Step 164 (7354); Episode 81/100; Loss: 0.012740889564156532\n",
      "Step 165 (7355); Episode 81/100; Loss: 0.041091881692409515\n",
      "Step 166 (7356); Episode 81/100; Loss: 0.0016342517919838428\n",
      "Step 167 (7357); Episode 81/100; Loss: 0.001789582660421729\n",
      "Step 168 (7358); Episode 81/100; Loss: 0.0009695771150290966\n",
      "Step 169 (7359); Episode 81/100; Loss: 0.003383099799975753\n",
      "Step 170 (7360); Episode 81/100; Loss: 0.0569213442504406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 171 (7361); Episode 81/100; Loss: 0.035768602043390274\n",
      "Step 172 (7362); Episode 81/100; Loss: 0.10263203084468842\n",
      "Step 173 (7363); Episode 81/100; Loss: 0.06768234819173813\n",
      "Step 174 (7364); Episode 81/100; Loss: 0.0752398669719696\n",
      "Step 175 (7365); Episode 81/100; Loss: 0.001209109090268612\n",
      "Step 176 (7366); Episode 81/100; Loss: 0.04541062191128731\n",
      "Step 177 (7367); Episode 81/100; Loss: 0.03048139624297619\n",
      "Step 178 (7368); Episode 81/100; Loss: 0.0036349662113934755\n",
      "Step 179 (7369); Episode 81/100; Loss: 0.04532940313220024\n",
      "Step 180 (7370); Episode 81/100; Loss: 0.0032172054052352905\n",
      "Step 181 (7371); Episode 81/100; Loss: 0.00114202790427953\n",
      "Step 182 (7372); Episode 81/100; Loss: 0.03374244272708893\n",
      "Step 183 (7373); Episode 81/100; Loss: 0.0014806154649704695\n",
      "Step 184 (7374); Episode 81/100; Loss: 0.02398068830370903\n",
      "Step 185 (7375); Episode 81/100; Loss: 0.0016498086042702198\n",
      "Step 186 (7376); Episode 81/100; Loss: 0.05067111924290657\n",
      "Step 187 (7377); Episode 81/100; Loss: 0.0018900512950494885\n",
      "Step 188 (7378); Episode 81/100; Loss: 0.001776067423634231\n",
      "Step 189 (7379); Episode 81/100; Loss: 0.0009013866656459868\n",
      "Step 190 (7380); Episode 81/100; Loss: 0.057813145220279694\n",
      "Step 191 (7381); Episode 81/100; Loss: 0.0495465025305748\n",
      "Step 192 (7382); Episode 81/100; Loss: 0.08135809004306793\n",
      "Step 193 (7383); Episode 81/100; Loss: 0.0015078418655321002\n",
      "Step 194 (7384); Episode 81/100; Loss: 0.003394975559785962\n",
      "Step 195 (7385); Episode 81/100; Loss: 0.007221282925456762\n",
      "Step 196 (7386); Episode 81/100; Loss: 0.0017950038891285658\n",
      "Step 197 (7387); Episode 81/100; Loss: 0.013598279096186161\n",
      "Step 198 (7388); Episode 81/100; Loss: 0.0009510889067314565\n",
      "Step 199 (7389); Episode 81/100; Loss: 0.06714852899312973\n",
      "Step 0 (7390); Episode 82/100; Loss: 0.04531469941139221\n",
      "Step 1 (7391); Episode 82/100; Loss: 0.05888718366622925\n",
      "Step 2 (7392); Episode 82/100; Loss: 0.002644541673362255\n",
      "Step 3 (7393); Episode 82/100; Loss: 0.04446757957339287\n",
      "Step 4 (7394); Episode 82/100; Loss: 0.04340123012661934\n",
      "Step 5 (7395); Episode 82/100; Loss: 0.02646355889737606\n",
      "Step 6 (7396); Episode 82/100; Loss: 0.09383375942707062\n",
      "Step 7 (7397); Episode 82/100; Loss: 0.0023887658026069403\n",
      "Step 8 (7398); Episode 82/100; Loss: 0.006376561708748341\n",
      "Step 9 (7399); Episode 82/100; Loss: 0.0023510693572461605\n",
      "Step 10 (7400); Episode 82/100; Loss: 0.0023738080635666847\n",
      "Step 11 (7401); Episode 82/100; Loss: 0.051644135266542435\n",
      "Step 12 (7402); Episode 82/100; Loss: 0.05419353395700455\n",
      "Step 13 (7403); Episode 82/100; Loss: 0.002766077406704426\n",
      "Step 14 (7404); Episode 82/100; Loss: 0.0019035148434340954\n",
      "Step 15 (7405); Episode 82/100; Loss: 0.002791387727484107\n",
      "Step 16 (7406); Episode 82/100; Loss: 0.0032804037909954786\n",
      "Step 17 (7407); Episode 82/100; Loss: 0.02565618045628071\n",
      "Step 18 (7408); Episode 82/100; Loss: 0.04611552134156227\n",
      "Step 19 (7409); Episode 82/100; Loss: 0.08753909170627594\n",
      "Step 20 (7410); Episode 82/100; Loss: 0.0023073742631822824\n",
      "Step 21 (7411); Episode 82/100; Loss: 0.04873587563633919\n",
      "Step 22 (7412); Episode 82/100; Loss: 0.06963707506656647\n",
      "Step 23 (7413); Episode 82/100; Loss: 0.0371076725423336\n",
      "Step 24 (7414); Episode 82/100; Loss: 0.001307416008785367\n",
      "Step 25 (7415); Episode 82/100; Loss: 0.008866175077855587\n",
      "Step 26 (7416); Episode 82/100; Loss: 0.013377233408391476\n",
      "Step 27 (7417); Episode 82/100; Loss: 0.0021187455859035254\n",
      "Step 28 (7418); Episode 82/100; Loss: 0.04703972116112709\n",
      "Step 29 (7419); Episode 82/100; Loss: 0.002288889605551958\n",
      "Step 30 (7420); Episode 82/100; Loss: 0.007406036369502544\n",
      "Step 31 (7421); Episode 82/100; Loss: 0.039389025419950485\n",
      "Step 32 (7422); Episode 82/100; Loss: 0.000791294383816421\n",
      "Step 33 (7423); Episode 82/100; Loss: 0.05875527113676071\n",
      "Step 34 (7424); Episode 82/100; Loss: 0.0016227421583607793\n",
      "Step 35 (7425); Episode 82/100; Loss: 0.055161960422992706\n",
      "Step 36 (7426); Episode 82/100; Loss: 0.003120848210528493\n",
      "Step 37 (7427); Episode 82/100; Loss: 0.043030980974435806\n",
      "Step 38 (7428); Episode 82/100; Loss: 0.0016439969185739756\n",
      "Step 39 (7429); Episode 82/100; Loss: 0.002252225298434496\n",
      "Step 40 (7430); Episode 82/100; Loss: 0.1401136964559555\n",
      "Step 41 (7431); Episode 82/100; Loss: 0.038012418895959854\n",
      "Step 42 (7432); Episode 82/100; Loss: 0.001044523436576128\n",
      "Step 43 (7433); Episode 82/100; Loss: 0.04946942627429962\n",
      "Step 44 (7434); Episode 82/100; Loss: 0.020392026752233505\n",
      "Step 45 (7435); Episode 82/100; Loss: 0.13452543318271637\n",
      "Step 46 (7436); Episode 82/100; Loss: 0.0013235631631687284\n",
      "Step 47 (7437); Episode 82/100; Loss: 0.0020641887094825506\n",
      "Step 48 (7438); Episode 82/100; Loss: 0.0020342408679425716\n",
      "Step 49 (7439); Episode 82/100; Loss: 0.0017168477643281221\n",
      "Step 50 (7440); Episode 82/100; Loss: 0.0031169322319328785\n",
      "Step 51 (7441); Episode 82/100; Loss: 0.04388278350234032\n",
      "Step 52 (7442); Episode 82/100; Loss: 0.002862147754058242\n",
      "Step 53 (7443); Episode 82/100; Loss: 0.05505876615643501\n",
      "Step 54 (7444); Episode 82/100; Loss: 0.02981375902891159\n",
      "Step 55 (7445); Episode 82/100; Loss: 0.006602073088288307\n",
      "Step 56 (7446); Episode 82/100; Loss: 0.001139558502472937\n",
      "Step 57 (7447); Episode 82/100; Loss: 0.09621290862560272\n",
      "Step 58 (7448); Episode 82/100; Loss: 0.052185751497745514\n",
      "Step 59 (7449); Episode 82/100; Loss: 0.0005138174165040255\n",
      "Step 60 (7450); Episode 82/100; Loss: 0.03473687171936035\n",
      "Step 61 (7451); Episode 82/100; Loss: 0.0019699668046087027\n",
      "Step 62 (7452); Episode 82/100; Loss: 0.0013183712726458907\n",
      "Step 63 (7453); Episode 82/100; Loss: 0.030210023745894432\n",
      "Step 64 (7454); Episode 82/100; Loss: 0.05525609478354454\n",
      "Step 65 (7455); Episode 82/100; Loss: 0.001195832621306181\n",
      "Step 66 (7456); Episode 82/100; Loss: 0.0007213292992673814\n",
      "Step 67 (7457); Episode 82/100; Loss: 0.0506402887403965\n",
      "Step 68 (7458); Episode 82/100; Loss: 0.022210372611880302\n",
      "Step 69 (7459); Episode 82/100; Loss: 0.01059385109692812\n",
      "Step 70 (7460); Episode 82/100; Loss: 0.0010168502340093255\n",
      "Step 71 (7461); Episode 82/100; Loss: 0.029989635571837425\n",
      "Step 72 (7462); Episode 82/100; Loss: 0.0013395127607509494\n",
      "Step 73 (7463); Episode 82/100; Loss: 0.0007970337173901498\n",
      "Step 74 (7464); Episode 82/100; Loss: 0.056512609124183655\n",
      "Step 75 (7465); Episode 82/100; Loss: 0.049104150384664536\n",
      "Step 76 (7466); Episode 82/100; Loss: 0.03558029234409332\n",
      "Step 77 (7467); Episode 82/100; Loss: 0.002074058633297682\n",
      "Step 78 (7468); Episode 82/100; Loss: 0.02861376851797104\n",
      "Step 79 (7469); Episode 82/100; Loss: 0.0014309659600257874\n",
      "Step 80 (7470); Episode 82/100; Loss: 0.0020602962467819452\n",
      "Step 81 (7471); Episode 82/100; Loss: 0.0036014080978929996\n",
      "Step 82 (7472); Episode 82/100; Loss: 0.049410589039325714\n",
      "Step 83 (7473); Episode 82/100; Loss: 0.0008735100855119526\n",
      "Step 84 (7474); Episode 82/100; Loss: 0.002764446660876274\n",
      "Step 85 (7475); Episode 82/100; Loss: 0.0009560181642882526\n",
      "Step 86 (7476); Episode 82/100; Loss: 0.05766035243868828\n",
      "Step 87 (7477); Episode 82/100; Loss: 0.05708448216319084\n",
      "Step 88 (7478); Episode 82/100; Loss: 0.05124375969171524\n",
      "Step 89 (7479); Episode 82/100; Loss: 0.02341875620186329\n",
      "Step 90 (7480); Episode 82/100; Loss: 0.05503225326538086\n",
      "Step 91 (7481); Episode 82/100; Loss: 0.05007767677307129\n",
      "Step 92 (7482); Episode 82/100; Loss: 0.009855234995484352\n",
      "Step 93 (7483); Episode 82/100; Loss: 0.0016318862326443195\n",
      "Step 94 (7484); Episode 82/100; Loss: 0.003394847037270665\n",
      "Step 95 (7485); Episode 82/100; Loss: 0.000345170235959813\n",
      "Step 96 (7486); Episode 82/100; Loss: 0.05200667306780815\n",
      "Step 97 (7487); Episode 82/100; Loss: 0.055407192558050156\n",
      "Step 98 (7488); Episode 82/100; Loss: 0.10332712531089783\n",
      "Step 99 (7489); Episode 82/100; Loss: 0.0014883211115375161\n",
      "Step 100 (7490); Episode 82/100; Loss: 0.000993914669379592\n",
      "Step 101 (7491); Episode 82/100; Loss: 0.04828112944960594\n",
      "Step 102 (7492); Episode 82/100; Loss: 0.0016146701527759433\n",
      "Step 103 (7493); Episode 82/100; Loss: 0.0008513468783348799\n",
      "Step 104 (7494); Episode 82/100; Loss: 0.05060383677482605\n",
      "Step 105 (7495); Episode 82/100; Loss: 0.006673227064311504\n",
      "Step 106 (7496); Episode 82/100; Loss: 0.0012210210552439094\n",
      "Step 107 (7497); Episode 82/100; Loss: 0.0013411000836640596\n",
      "Step 108 (7498); Episode 82/100; Loss: 0.06286900490522385\n",
      "Step 109 (7499); Episode 82/100; Loss: 0.047817472368478775\n",
      "Step 110 (7500); Episode 82/100; Loss: 0.001499611884355545\n",
      "Step 111 (7501); Episode 82/100; Loss: 0.0006206436082720757\n",
      "Step 112 (7502); Episode 82/100; Loss: 0.04713037610054016\n",
      "Step 113 (7503); Episode 82/100; Loss: 0.0020396604668349028\n",
      "Step 114 (7504); Episode 82/100; Loss: 0.09105748683214188\n",
      "Step 115 (7505); Episode 82/100; Loss: 0.04228166490793228\n",
      "Step 116 (7506); Episode 82/100; Loss: 0.0008211508975364268\n",
      "Step 117 (7507); Episode 82/100; Loss: 0.0030204837676137686\n",
      "Step 118 (7508); Episode 82/100; Loss: 0.046000368893146515\n",
      "Step 119 (7509); Episode 82/100; Loss: 0.0016766143962740898\n",
      "Step 120 (7510); Episode 82/100; Loss: 0.056471891701221466\n",
      "Step 121 (7511); Episode 82/100; Loss: 0.06576195359230042\n",
      "Step 122 (7512); Episode 82/100; Loss: 0.04354667291045189\n",
      "Step 123 (7513); Episode 82/100; Loss: 0.013394257985055447\n",
      "Step 124 (7514); Episode 82/100; Loss: 0.001974107464775443\n",
      "Step 125 (7515); Episode 82/100; Loss: 0.048781346529722214\n",
      "Step 126 (7516); Episode 82/100; Loss: 0.04703274741768837\n",
      "Step 127 (7517); Episode 82/100; Loss: 0.0018246095860376954\n",
      "Step 128 (7518); Episode 82/100; Loss: 0.0015960527816787362\n",
      "Step 129 (7519); Episode 82/100; Loss: 0.03935772553086281\n",
      "Step 130 (7520); Episode 82/100; Loss: 0.025338293984532356\n",
      "Step 131 (7521); Episode 82/100; Loss: 0.000417069299146533\n",
      "Step 132 (7522); Episode 82/100; Loss: 0.0013418055605143309\n",
      "Step 133 (7523); Episode 82/100; Loss: 0.059569697827100754\n",
      "Step 134 (7524); Episode 82/100; Loss: 0.002006054390221834\n",
      "Step 135 (7525); Episode 82/100; Loss: 0.07379457354545593\n",
      "Step 136 (7526); Episode 82/100; Loss: 0.03659871593117714\n",
      "Step 137 (7527); Episode 82/100; Loss: 0.0007226492743939161\n",
      "Step 138 (7528); Episode 82/100; Loss: 0.0006705615669488907\n",
      "Step 139 (7529); Episode 82/100; Loss: 0.0005484626162797213\n",
      "Step 140 (7530); Episode 82/100; Loss: 0.046069201081991196\n",
      "Step 141 (7531); Episode 82/100; Loss: 0.04928627610206604\n",
      "Step 142 (7532); Episode 82/100; Loss: 0.048212338238954544\n",
      "Step 143 (7533); Episode 82/100; Loss: 0.004417513031512499\n",
      "Step 144 (7534); Episode 82/100; Loss: 0.001589988823980093\n",
      "Step 145 (7535); Episode 82/100; Loss: 0.0562770776450634\n",
      "Step 146 (7536); Episode 82/100; Loss: 0.03986217826604843\n",
      "Step 147 (7537); Episode 82/100; Loss: 0.002539948560297489\n",
      "Step 148 (7538); Episode 82/100; Loss: 0.006887913215905428\n",
      "Step 149 (7539); Episode 82/100; Loss: 0.003828376764431596\n",
      "Step 150 (7540); Episode 82/100; Loss: 0.04752032086253166\n",
      "Step 151 (7541); Episode 82/100; Loss: 0.00250411918386817\n",
      "Step 152 (7542); Episode 82/100; Loss: 0.03491130471229553\n",
      "Step 153 (7543); Episode 82/100; Loss: 0.0009458950371481478\n",
      "Step 154 (7544); Episode 82/100; Loss: 0.0006244339747354388\n",
      "Step 155 (7545); Episode 82/100; Loss: 0.05176108330488205\n",
      "Step 156 (7546); Episode 82/100; Loss: 0.0005441720131784678\n",
      "Step 157 (7547); Episode 82/100; Loss: 0.04509611800312996\n",
      "Step 158 (7548); Episode 82/100; Loss: 0.000824393006041646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 159 (7549); Episode 82/100; Loss: 0.07777107506990433\n",
      "Step 160 (7550); Episode 82/100; Loss: 0.002663248684257269\n",
      "Step 161 (7551); Episode 82/100; Loss: 0.002104684477671981\n",
      "Step 162 (7552); Episode 82/100; Loss: 0.05588330700993538\n",
      "Step 163 (7553); Episode 82/100; Loss: 0.010031433776021004\n",
      "Step 164 (7554); Episode 82/100; Loss: 0.0035982003901153803\n",
      "Step 165 (7555); Episode 82/100; Loss: 0.02616637945175171\n",
      "Step 0 (7556); Episode 83/100; Loss: 0.056546714156866074\n",
      "Step 1 (7557); Episode 83/100; Loss: 0.0014746022643521428\n",
      "Step 2 (7558); Episode 83/100; Loss: 0.0016335988184437156\n",
      "Step 3 (7559); Episode 83/100; Loss: 0.0494711808860302\n",
      "Step 4 (7560); Episode 83/100; Loss: 0.0016345796175301075\n",
      "Step 5 (7561); Episode 83/100; Loss: 0.001571301487274468\n",
      "Step 6 (7562); Episode 83/100; Loss: 0.009368662722408772\n",
      "Step 7 (7563); Episode 83/100; Loss: 0.04277649149298668\n",
      "Step 8 (7564); Episode 83/100; Loss: 0.051387328654527664\n",
      "Step 9 (7565); Episode 83/100; Loss: 0.03993034362792969\n",
      "Step 10 (7566); Episode 83/100; Loss: 0.07300194352865219\n",
      "Step 11 (7567); Episode 83/100; Loss: 0.0011258574668318033\n",
      "Step 12 (7568); Episode 83/100; Loss: 0.04986114054918289\n",
      "Step 13 (7569); Episode 83/100; Loss: 0.005920439958572388\n",
      "Step 14 (7570); Episode 83/100; Loss: 0.002397873904556036\n",
      "Step 15 (7571); Episode 83/100; Loss: 0.002030547009781003\n",
      "Step 16 (7572); Episode 83/100; Loss: 0.04744784161448479\n",
      "Step 17 (7573); Episode 83/100; Loss: 0.05135958641767502\n",
      "Step 18 (7574); Episode 83/100; Loss: 0.05266730114817619\n",
      "Step 19 (7575); Episode 83/100; Loss: 0.0019892877899110317\n",
      "Step 20 (7576); Episode 83/100; Loss: 0.036090414971113205\n",
      "Step 21 (7577); Episode 83/100; Loss: 0.010923531837761402\n",
      "Step 22 (7578); Episode 83/100; Loss: 0.058253973722457886\n",
      "Step 23 (7579); Episode 83/100; Loss: 0.024630585685372353\n",
      "Step 24 (7580); Episode 83/100; Loss: 0.03379841148853302\n",
      "Step 25 (7581); Episode 83/100; Loss: 0.0015780437970533967\n",
      "Step 26 (7582); Episode 83/100; Loss: 0.04554668441414833\n",
      "Step 27 (7583); Episode 83/100; Loss: 0.05263704061508179\n",
      "Step 28 (7584); Episode 83/100; Loss: 0.04952820762991905\n",
      "Step 29 (7585); Episode 83/100; Loss: 0.005487440619617701\n",
      "Step 30 (7586); Episode 83/100; Loss: 0.025423262268304825\n",
      "Step 31 (7587); Episode 83/100; Loss: 0.00251780916005373\n",
      "Step 32 (7588); Episode 83/100; Loss: 0.0016613299958407879\n",
      "Step 33 (7589); Episode 83/100; Loss: 0.001824862789362669\n",
      "Step 34 (7590); Episode 83/100; Loss: 0.0010176795767620206\n",
      "Step 35 (7591); Episode 83/100; Loss: 0.00035441716318018734\n",
      "Step 36 (7592); Episode 83/100; Loss: 0.0009372993954457343\n",
      "Step 37 (7593); Episode 83/100; Loss: 0.0008490737527608871\n",
      "Step 38 (7594); Episode 83/100; Loss: 0.053175851702690125\n",
      "Step 39 (7595); Episode 83/100; Loss: 0.000930009875446558\n",
      "Step 40 (7596); Episode 83/100; Loss: 0.04953218996524811\n",
      "Step 41 (7597); Episode 83/100; Loss: 0.0011153374798595905\n",
      "Step 42 (7598); Episode 83/100; Loss: 0.0016700286651030183\n",
      "Step 43 (7599); Episode 83/100; Loss: 0.028464846312999725\n",
      "Step 44 (7600); Episode 83/100; Loss: 0.0018800391117110848\n",
      "Step 45 (7601); Episode 83/100; Loss: 0.012583679519593716\n",
      "Step 46 (7602); Episode 83/100; Loss: 0.0012186510721221566\n",
      "Step 47 (7603); Episode 83/100; Loss: 0.0021975631825625896\n",
      "Step 48 (7604); Episode 83/100; Loss: 0.05891381576657295\n",
      "Step 49 (7605); Episode 83/100; Loss: 0.10227255523204803\n",
      "Step 50 (7606); Episode 83/100; Loss: 0.017707154154777527\n",
      "Step 51 (7607); Episode 83/100; Loss: 0.07559617608785629\n",
      "Step 52 (7608); Episode 83/100; Loss: 0.0009362861164845526\n",
      "Step 53 (7609); Episode 83/100; Loss: 0.0014286792138591409\n",
      "Step 54 (7610); Episode 83/100; Loss: 0.001348154037259519\n",
      "Step 55 (7611); Episode 83/100; Loss: 0.001010505948215723\n",
      "Step 56 (7612); Episode 83/100; Loss: 0.001775834010913968\n",
      "Step 57 (7613); Episode 83/100; Loss: 0.048069365322589874\n",
      "Step 58 (7614); Episode 83/100; Loss: 0.0009444725583307445\n",
      "Step 59 (7615); Episode 83/100; Loss: 0.0012253359891474247\n",
      "Step 60 (7616); Episode 83/100; Loss: 0.09905044734477997\n",
      "Step 61 (7617); Episode 83/100; Loss: 0.0013830154202878475\n",
      "Step 62 (7618); Episode 83/100; Loss: 0.08673479408025742\n",
      "Step 63 (7619); Episode 83/100; Loss: 0.09032201766967773\n",
      "Step 64 (7620); Episode 83/100; Loss: 0.038865987211465836\n",
      "Step 65 (7621); Episode 83/100; Loss: 0.0460011251270771\n",
      "Step 66 (7622); Episode 83/100; Loss: 0.09882790595293045\n",
      "Step 67 (7623); Episode 83/100; Loss: 0.0012181006604805589\n",
      "Step 68 (7624); Episode 83/100; Loss: 0.001349739613942802\n",
      "Step 69 (7625); Episode 83/100; Loss: 0.0036186871584504843\n",
      "Step 70 (7626); Episode 83/100; Loss: 0.0013949987478554249\n",
      "Step 71 (7627); Episode 83/100; Loss: 0.05267199128866196\n",
      "Step 72 (7628); Episode 83/100; Loss: 0.04478466883301735\n",
      "Step 73 (7629); Episode 83/100; Loss: 0.04787798598408699\n",
      "Step 74 (7630); Episode 83/100; Loss: 0.06813554465770721\n",
      "Step 75 (7631); Episode 83/100; Loss: 0.002219897462055087\n",
      "Step 76 (7632); Episode 83/100; Loss: 0.00153557606972754\n",
      "Step 77 (7633); Episode 83/100; Loss: 0.006578151602298021\n",
      "Step 78 (7634); Episode 83/100; Loss: 0.0015890939394012094\n",
      "Step 79 (7635); Episode 83/100; Loss: 0.0021413564682006836\n",
      "Step 80 (7636); Episode 83/100; Loss: 0.04752717539668083\n",
      "Step 81 (7637); Episode 83/100; Loss: 0.0031659312080591917\n",
      "Step 82 (7638); Episode 83/100; Loss: 0.03874252736568451\n",
      "Step 83 (7639); Episode 83/100; Loss: 0.0007953488384373486\n",
      "Step 84 (7640); Episode 83/100; Loss: 0.0017479019006714225\n",
      "Step 85 (7641); Episode 83/100; Loss: 0.11155355721712112\n",
      "Step 86 (7642); Episode 83/100; Loss: 0.05942295491695404\n",
      "Step 87 (7643); Episode 83/100; Loss: 0.0739869624376297\n",
      "Step 88 (7644); Episode 83/100; Loss: 0.0010402967454865575\n",
      "Step 89 (7645); Episode 83/100; Loss: 0.0007168939337134361\n",
      "Step 90 (7646); Episode 83/100; Loss: 0.0069410833530128\n",
      "Step 91 (7647); Episode 83/100; Loss: 0.0005777667392976582\n",
      "Step 92 (7648); Episode 83/100; Loss: 0.0003728702722582966\n",
      "Step 93 (7649); Episode 83/100; Loss: 0.0016561599913984537\n",
      "Step 94 (7650); Episode 83/100; Loss: 0.04570716246962547\n",
      "Step 95 (7651); Episode 83/100; Loss: 0.005248825531452894\n",
      "Step 96 (7652); Episode 83/100; Loss: 0.052368320524692535\n",
      "Step 97 (7653); Episode 83/100; Loss: 0.0011554211378097534\n",
      "Step 98 (7654); Episode 83/100; Loss: 0.001841468852944672\n",
      "Step 99 (7655); Episode 83/100; Loss: 0.029225356876850128\n",
      "Step 100 (7656); Episode 83/100; Loss: 0.013488936237990856\n",
      "Step 101 (7657); Episode 83/100; Loss: 0.14663684368133545\n",
      "Step 102 (7658); Episode 83/100; Loss: 0.0022621387615799904\n",
      "Step 103 (7659); Episode 83/100; Loss: 0.05125847086310387\n",
      "Step 104 (7660); Episode 83/100; Loss: 0.04525415599346161\n",
      "Step 105 (7661); Episode 83/100; Loss: 0.044672392308712006\n",
      "Step 106 (7662); Episode 83/100; Loss: 0.05417529121041298\n",
      "Step 107 (7663); Episode 83/100; Loss: 0.001794668729417026\n",
      "Step 108 (7664); Episode 83/100; Loss: 0.0012316802749410272\n",
      "Step 109 (7665); Episode 83/100; Loss: 0.00020713070989586413\n",
      "Step 110 (7666); Episode 83/100; Loss: 0.0010873452993109822\n",
      "Step 111 (7667); Episode 83/100; Loss: 0.03739180415868759\n",
      "Step 112 (7668); Episode 83/100; Loss: 0.001563096884638071\n",
      "Step 113 (7669); Episode 83/100; Loss: 0.05090567469596863\n",
      "Step 114 (7670); Episode 83/100; Loss: 0.0007944959797896445\n",
      "Step 115 (7671); Episode 83/100; Loss: 0.052101727575063705\n",
      "Step 116 (7672); Episode 83/100; Loss: 0.04823119938373566\n",
      "Step 117 (7673); Episode 83/100; Loss: 0.05701804906129837\n",
      "Step 118 (7674); Episode 83/100; Loss: 0.05462726578116417\n",
      "Step 119 (7675); Episode 83/100; Loss: 0.020595677196979523\n",
      "Step 120 (7676); Episode 83/100; Loss: 0.0006001443252898753\n",
      "Step 121 (7677); Episode 83/100; Loss: 0.01512904092669487\n",
      "Step 122 (7678); Episode 83/100; Loss: 0.0009031881345435977\n",
      "Step 123 (7679); Episode 83/100; Loss: 0.0029985804576426744\n",
      "Step 124 (7680); Episode 83/100; Loss: 0.0037951022386550903\n",
      "Step 125 (7681); Episode 83/100; Loss: 0.03781236335635185\n",
      "Step 126 (7682); Episode 83/100; Loss: 0.0009996213484555483\n",
      "Step 127 (7683); Episode 83/100; Loss: 0.00280974549241364\n",
      "Step 128 (7684); Episode 83/100; Loss: 0.0594698004424572\n",
      "Step 129 (7685); Episode 83/100; Loss: 0.06053277105093002\n",
      "Step 130 (7686); Episode 83/100; Loss: 0.0015959716401994228\n",
      "Step 131 (7687); Episode 83/100; Loss: 0.04828488081693649\n",
      "Step 132 (7688); Episode 83/100; Loss: 0.02193780243396759\n",
      "Step 133 (7689); Episode 83/100; Loss: 0.05145396292209625\n",
      "Step 134 (7690); Episode 83/100; Loss: 0.09805141389369965\n",
      "Step 135 (7691); Episode 83/100; Loss: 0.04955398291349411\n",
      "Step 136 (7692); Episode 83/100; Loss: 0.04693828523159027\n",
      "Step 137 (7693); Episode 83/100; Loss: 0.0015823771245777607\n",
      "Step 138 (7694); Episode 83/100; Loss: 0.0016889299731701612\n",
      "Step 139 (7695); Episode 83/100; Loss: 0.11040196567773819\n",
      "Step 140 (7696); Episode 83/100; Loss: 0.020350264385342598\n",
      "Step 141 (7697); Episode 83/100; Loss: 0.01100884284824133\n",
      "Step 142 (7698); Episode 83/100; Loss: 0.05359845981001854\n",
      "Step 143 (7699); Episode 83/100; Loss: 0.04247952997684479\n",
      "Step 144 (7700); Episode 83/100; Loss: 0.0017968310276046395\n",
      "Step 145 (7701); Episode 83/100; Loss: 0.0023312228731811047\n",
      "Step 146 (7702); Episode 83/100; Loss: 0.007983902469277382\n",
      "Step 147 (7703); Episode 83/100; Loss: 0.0016600555973127484\n",
      "Step 148 (7704); Episode 83/100; Loss: 0.002500751754269004\n",
      "Step 149 (7705); Episode 83/100; Loss: 0.04528913274407387\n",
      "Step 150 (7706); Episode 83/100; Loss: 0.04599715769290924\n",
      "Step 151 (7707); Episode 83/100; Loss: 0.001906449324451387\n",
      "Step 152 (7708); Episode 83/100; Loss: 0.0025941752828657627\n",
      "Step 153 (7709); Episode 83/100; Loss: 0.001734084915369749\n",
      "Step 0 (7710); Episode 84/100; Loss: 0.004419439937919378\n",
      "Step 1 (7711); Episode 84/100; Loss: 0.001559884869493544\n",
      "Step 2 (7712); Episode 84/100; Loss: 0.018145162612199783\n",
      "Step 3 (7713); Episode 84/100; Loss: 0.12725424766540527\n",
      "Step 4 (7714); Episode 84/100; Loss: 0.0007104966789484024\n",
      "Step 5 (7715); Episode 84/100; Loss: 0.0031355025712400675\n",
      "Step 6 (7716); Episode 84/100; Loss: 0.04751601442694664\n",
      "Step 7 (7717); Episode 84/100; Loss: 0.05706203728914261\n",
      "Step 8 (7718); Episode 84/100; Loss: 0.05723639950156212\n",
      "Step 9 (7719); Episode 84/100; Loss: 0.10090498626232147\n",
      "Step 10 (7720); Episode 84/100; Loss: 0.0012014086823910475\n",
      "Step 11 (7721); Episode 84/100; Loss: 0.00049741001566872\n",
      "Step 12 (7722); Episode 84/100; Loss: 0.048362020403146744\n",
      "Step 13 (7723); Episode 84/100; Loss: 0.0006828391924500465\n",
      "Step 14 (7724); Episode 84/100; Loss: 0.05064227432012558\n",
      "Step 15 (7725); Episode 84/100; Loss: 0.03881751000881195\n",
      "Step 16 (7726); Episode 84/100; Loss: 0.04268234595656395\n",
      "Step 17 (7727); Episode 84/100; Loss: 0.003604486119002104\n",
      "Step 18 (7728); Episode 84/100; Loss: 0.05492071807384491\n",
      "Step 19 (7729); Episode 84/100; Loss: 0.045302681624889374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 (7730); Episode 84/100; Loss: 0.0775885060429573\n",
      "Step 21 (7731); Episode 84/100; Loss: 0.001206922926940024\n",
      "Step 22 (7732); Episode 84/100; Loss: 0.05227737873792648\n",
      "Step 23 (7733); Episode 84/100; Loss: 0.053445879369974136\n",
      "Step 24 (7734); Episode 84/100; Loss: 0.05056743696331978\n",
      "Step 25 (7735); Episode 84/100; Loss: 0.0022988796699792147\n",
      "Step 26 (7736); Episode 84/100; Loss: 0.0030899194534868\n",
      "Step 27 (7737); Episode 84/100; Loss: 0.0013063240330666304\n",
      "Step 28 (7738); Episode 84/100; Loss: 0.054545219987630844\n",
      "Step 29 (7739); Episode 84/100; Loss: 0.05231102183461189\n",
      "Step 30 (7740); Episode 84/100; Loss: 0.047762833535671234\n",
      "Step 31 (7741); Episode 84/100; Loss: 0.0008876380743458867\n",
      "Step 32 (7742); Episode 84/100; Loss: 0.0005705005023628473\n",
      "Step 33 (7743); Episode 84/100; Loss: 0.00328431255184114\n",
      "Step 34 (7744); Episode 84/100; Loss: 0.001420873450115323\n",
      "Step 35 (7745); Episode 84/100; Loss: 0.09750182181596756\n",
      "Step 36 (7746); Episode 84/100; Loss: 0.02568158134818077\n",
      "Step 37 (7747); Episode 84/100; Loss: 0.0006517141009680927\n",
      "Step 38 (7748); Episode 84/100; Loss: 0.002362349536269903\n",
      "Step 39 (7749); Episode 84/100; Loss: 0.0007739994907751679\n",
      "Step 40 (7750); Episode 84/100; Loss: 0.0007587912841700017\n",
      "Step 41 (7751); Episode 84/100; Loss: 0.0014683639165014029\n",
      "Step 42 (7752); Episode 84/100; Loss: 0.05698716640472412\n",
      "Step 43 (7753); Episode 84/100; Loss: 0.0010219281539320946\n",
      "Step 44 (7754); Episode 84/100; Loss: 0.0006064389599487185\n",
      "Step 45 (7755); Episode 84/100; Loss: 0.0005292132846079767\n",
      "Step 46 (7756); Episode 84/100; Loss: 0.027663882821798325\n",
      "Step 47 (7757); Episode 84/100; Loss: 0.0009101914474740624\n",
      "Step 48 (7758); Episode 84/100; Loss: 0.0014388601994141936\n",
      "Step 49 (7759); Episode 84/100; Loss: 0.1901678442955017\n",
      "Step 50 (7760); Episode 84/100; Loss: 0.0014771063579246402\n",
      "Step 51 (7761); Episode 84/100; Loss: 0.04549993574619293\n",
      "Step 52 (7762); Episode 84/100; Loss: 0.05777638778090477\n",
      "Step 53 (7763); Episode 84/100; Loss: 0.04830939322710037\n",
      "Step 54 (7764); Episode 84/100; Loss: 0.0026851296424865723\n",
      "Step 55 (7765); Episode 84/100; Loss: 0.0007547531276941299\n",
      "Step 56 (7766); Episode 84/100; Loss: 0.0017098919488489628\n",
      "Step 57 (7767); Episode 84/100; Loss: 0.03525746241211891\n",
      "Step 58 (7768); Episode 84/100; Loss: 0.0011700569884851575\n",
      "Step 59 (7769); Episode 84/100; Loss: 0.06487417221069336\n",
      "Step 60 (7770); Episode 84/100; Loss: 0.0014598044799640775\n",
      "Step 61 (7771); Episode 84/100; Loss: 0.049511708319187164\n",
      "Step 62 (7772); Episode 84/100; Loss: 0.0018508235225453973\n",
      "Step 63 (7773); Episode 84/100; Loss: 0.04970875382423401\n",
      "Step 64 (7774); Episode 84/100; Loss: 0.08656807243824005\n",
      "Step 65 (7775); Episode 84/100; Loss: 0.00045961193973198533\n",
      "Step 66 (7776); Episode 84/100; Loss: 0.04394509643316269\n",
      "Step 67 (7777); Episode 84/100; Loss: 0.05218850448727608\n",
      "Step 68 (7778); Episode 84/100; Loss: 0.09632284939289093\n",
      "Step 69 (7779); Episode 84/100; Loss: 0.03548799455165863\n",
      "Step 70 (7780); Episode 84/100; Loss: 0.008170192129909992\n",
      "Step 71 (7781); Episode 84/100; Loss: 0.022566774860024452\n",
      "Step 72 (7782); Episode 84/100; Loss: 0.04546980932354927\n",
      "Step 73 (7783); Episode 84/100; Loss: 0.10556821525096893\n",
      "Step 74 (7784); Episode 84/100; Loss: 0.05741240456700325\n",
      "Step 75 (7785); Episode 84/100; Loss: 0.06996916234493256\n",
      "Step 76 (7786); Episode 84/100; Loss: 0.1035555824637413\n",
      "Step 77 (7787); Episode 84/100; Loss: 0.056448545306921005\n",
      "Step 78 (7788); Episode 84/100; Loss: 0.0020401852671056986\n",
      "Step 79 (7789); Episode 84/100; Loss: 0.04104366898536682\n",
      "Step 80 (7790); Episode 84/100; Loss: 0.0017299411119893193\n",
      "Step 81 (7791); Episode 84/100; Loss: 0.0014613146195188165\n",
      "Step 82 (7792); Episode 84/100; Loss: 0.04409380257129669\n",
      "Step 83 (7793); Episode 84/100; Loss: 0.0029686959460377693\n",
      "Step 84 (7794); Episode 84/100; Loss: 0.0022979085333645344\n",
      "Step 85 (7795); Episode 84/100; Loss: 0.04989005625247955\n",
      "Step 86 (7796); Episode 84/100; Loss: 0.09236989915370941\n",
      "Step 87 (7797); Episode 84/100; Loss: 0.001744697568938136\n",
      "Step 88 (7798); Episode 84/100; Loss: 0.0020226144697517157\n",
      "Step 89 (7799); Episode 84/100; Loss: 0.05338328331708908\n",
      "Step 90 (7800); Episode 84/100; Loss: 0.002107847249135375\n",
      "Step 91 (7801); Episode 84/100; Loss: 0.0018807306187227368\n",
      "Step 92 (7802); Episode 84/100; Loss: 0.0009697577334009111\n",
      "Step 93 (7803); Episode 84/100; Loss: 0.010685902088880539\n",
      "Step 94 (7804); Episode 84/100; Loss: 0.019015828147530556\n",
      "Step 95 (7805); Episode 84/100; Loss: 0.0015034312382340431\n",
      "Step 96 (7806); Episode 84/100; Loss: 0.0023994818329811096\n",
      "Step 97 (7807); Episode 84/100; Loss: 0.04262436553835869\n",
      "Step 98 (7808); Episode 84/100; Loss: 0.0031840489245951176\n",
      "Step 99 (7809); Episode 84/100; Loss: 0.051598794758319855\n",
      "Step 100 (7810); Episode 84/100; Loss: 0.0070570604875683784\n",
      "Step 101 (7811); Episode 84/100; Loss: 0.10368102043867111\n",
      "Step 102 (7812); Episode 84/100; Loss: 0.06595473736524582\n",
      "Step 103 (7813); Episode 84/100; Loss: 0.0036876427475363016\n",
      "Step 104 (7814); Episode 84/100; Loss: 0.05738309770822525\n",
      "Step 105 (7815); Episode 84/100; Loss: 0.05286534130573273\n",
      "Step 106 (7816); Episode 84/100; Loss: 0.11639183014631271\n",
      "Step 107 (7817); Episode 84/100; Loss: 0.05271800607442856\n",
      "Step 108 (7818); Episode 84/100; Loss: 0.0005389426369220018\n",
      "Step 109 (7819); Episode 84/100; Loss: 0.04302193969488144\n",
      "Step 110 (7820); Episode 84/100; Loss: 0.05511000379920006\n",
      "Step 111 (7821); Episode 84/100; Loss: 0.0017086940351873636\n",
      "Step 112 (7822); Episode 84/100; Loss: 0.00214180164039135\n",
      "Step 113 (7823); Episode 84/100; Loss: 0.001454130164347589\n",
      "Step 114 (7824); Episode 84/100; Loss: 0.001121785957366228\n",
      "Step 115 (7825); Episode 84/100; Loss: 0.04501371085643768\n",
      "Step 116 (7826); Episode 84/100; Loss: 0.00201599788852036\n",
      "Step 117 (7827); Episode 84/100; Loss: 0.0010605381103232503\n",
      "Step 118 (7828); Episode 84/100; Loss: 0.024044295772910118\n",
      "Step 119 (7829); Episode 84/100; Loss: 0.05350160598754883\n",
      "Step 120 (7830); Episode 84/100; Loss: 0.016959594562649727\n",
      "Step 121 (7831); Episode 84/100; Loss: 0.001959563232958317\n",
      "Step 122 (7832); Episode 84/100; Loss: 0.0007842903723940253\n",
      "Step 123 (7833); Episode 84/100; Loss: 0.04437295347452164\n",
      "Step 124 (7834); Episode 84/100; Loss: 0.0007231801864691079\n",
      "Step 125 (7835); Episode 84/100; Loss: 0.052923865616321564\n",
      "Step 126 (7836); Episode 84/100; Loss: 0.00180036888923496\n",
      "Step 127 (7837); Episode 84/100; Loss: 0.0015214190352708101\n",
      "Step 128 (7838); Episode 84/100; Loss: 0.0018825434381142259\n",
      "Step 129 (7839); Episode 84/100; Loss: 0.0010005065705627203\n",
      "Step 130 (7840); Episode 84/100; Loss: 0.001197566045448184\n",
      "Step 131 (7841); Episode 84/100; Loss: 0.000914892356377095\n",
      "Step 132 (7842); Episode 84/100; Loss: 0.025264082476496696\n",
      "Step 133 (7843); Episode 84/100; Loss: 0.009004408493638039\n",
      "Step 134 (7844); Episode 84/100; Loss: 0.0008356158505193889\n",
      "Step 135 (7845); Episode 84/100; Loss: 0.0015631463611498475\n",
      "Step 136 (7846); Episode 84/100; Loss: 0.0010435752337798476\n",
      "Step 137 (7847); Episode 84/100; Loss: 0.002782757394015789\n",
      "Step 138 (7848); Episode 84/100; Loss: 0.04781080037355423\n",
      "Step 139 (7849); Episode 84/100; Loss: 0.051463305950164795\n",
      "Step 140 (7850); Episode 84/100; Loss: 0.0019456729060038924\n",
      "Step 141 (7851); Episode 84/100; Loss: 0.000509260396938771\n",
      "Step 142 (7852); Episode 84/100; Loss: 0.049949854612350464\n",
      "Step 143 (7853); Episode 84/100; Loss: 0.09072214365005493\n",
      "Step 144 (7854); Episode 84/100; Loss: 0.059821195900440216\n",
      "Step 145 (7855); Episode 84/100; Loss: 0.04923852160573006\n",
      "Step 146 (7856); Episode 84/100; Loss: 0.0014087966410443187\n",
      "Step 147 (7857); Episode 84/100; Loss: 0.05181380361318588\n",
      "Step 148 (7858); Episode 84/100; Loss: 0.04961583390831947\n",
      "Step 149 (7859); Episode 84/100; Loss: 0.039809536188840866\n",
      "Step 150 (7860); Episode 84/100; Loss: 0.000935457122977823\n",
      "Step 151 (7861); Episode 84/100; Loss: 0.04742303863167763\n",
      "Step 152 (7862); Episode 84/100; Loss: 0.03651757165789604\n",
      "Step 153 (7863); Episode 84/100; Loss: 0.14075490832328796\n",
      "Step 154 (7864); Episode 84/100; Loss: 0.0018749521113932133\n",
      "Step 155 (7865); Episode 84/100; Loss: 0.0594647116959095\n",
      "Step 156 (7866); Episode 84/100; Loss: 0.07655395567417145\n",
      "Step 157 (7867); Episode 84/100; Loss: 0.0019049191614612937\n",
      "Step 158 (7868); Episode 84/100; Loss: 0.001405975897796452\n",
      "Step 159 (7869); Episode 84/100; Loss: 0.00929401908069849\n",
      "Step 160 (7870); Episode 84/100; Loss: 0.09260457009077072\n",
      "Step 161 (7871); Episode 84/100; Loss: 0.0026714419946074486\n",
      "Step 162 (7872); Episode 84/100; Loss: 0.0036352425813674927\n",
      "Step 163 (7873); Episode 84/100; Loss: 0.002294621430337429\n",
      "Step 164 (7874); Episode 84/100; Loss: 0.0008525378070771694\n",
      "Step 165 (7875); Episode 84/100; Loss: 0.002658757148310542\n",
      "Step 166 (7876); Episode 84/100; Loss: 0.004024002235382795\n",
      "Step 167 (7877); Episode 84/100; Loss: 0.0011564159067347646\n",
      "Step 168 (7878); Episode 84/100; Loss: 0.0015751501778140664\n",
      "Step 169 (7879); Episode 84/100; Loss: 0.001457707490772009\n",
      "Step 170 (7880); Episode 84/100; Loss: 0.0006915993290022016\n",
      "Step 171 (7881); Episode 84/100; Loss: 0.0427340529859066\n",
      "Step 172 (7882); Episode 84/100; Loss: 0.0014214932452887297\n",
      "Step 173 (7883); Episode 84/100; Loss: 0.045526839792728424\n",
      "Step 174 (7884); Episode 84/100; Loss: 0.030762460082769394\n",
      "Step 175 (7885); Episode 84/100; Loss: 0.001294588320888579\n",
      "Step 176 (7886); Episode 84/100; Loss: 0.05596102774143219\n",
      "Step 177 (7887); Episode 84/100; Loss: 0.057230908423662186\n",
      "Step 178 (7888); Episode 84/100; Loss: 0.09360398352146149\n",
      "Step 179 (7889); Episode 84/100; Loss: 0.003183910856023431\n",
      "Step 180 (7890); Episode 84/100; Loss: 0.00854802131652832\n",
      "Step 181 (7891); Episode 84/100; Loss: 0.006040595471858978\n",
      "Step 182 (7892); Episode 84/100; Loss: 0.00134470802731812\n",
      "Step 183 (7893); Episode 84/100; Loss: 0.002322789281606674\n",
      "Step 184 (7894); Episode 84/100; Loss: 0.0014237705618143082\n",
      "Step 185 (7895); Episode 84/100; Loss: 0.04831883683800697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 186 (7896); Episode 84/100; Loss: 0.052870217710733414\n",
      "Step 187 (7897); Episode 84/100; Loss: 0.0019464442739263177\n",
      "Step 188 (7898); Episode 84/100; Loss: 0.046457041054964066\n",
      "Step 189 (7899); Episode 84/100; Loss: 0.0013466348173096776\n",
      "Step 190 (7900); Episode 84/100; Loss: 0.0011465686839073896\n",
      "Step 191 (7901); Episode 84/100; Loss: 0.08006024360656738\n",
      "Step 192 (7902); Episode 84/100; Loss: 0.0020804316736757755\n",
      "Step 193 (7903); Episode 84/100; Loss: 0.0015856210375204682\n",
      "Step 194 (7904); Episode 84/100; Loss: 0.05540488660335541\n",
      "Step 195 (7905); Episode 84/100; Loss: 0.08755842596292496\n",
      "Step 196 (7906); Episode 84/100; Loss: 0.0009116227156482637\n",
      "Step 197 (7907); Episode 84/100; Loss: 0.046045538038015366\n",
      "Step 198 (7908); Episode 84/100; Loss: 0.013912510126829147\n",
      "Step 199 (7909); Episode 84/100; Loss: 0.0026487470604479313\n",
      "Step 0 (7910); Episode 85/100; Loss: 0.024354031309485435\n",
      "Step 1 (7911); Episode 85/100; Loss: 0.0315815694630146\n",
      "Step 2 (7912); Episode 85/100; Loss: 0.0029234595131129026\n",
      "Step 3 (7913); Episode 85/100; Loss: 0.0008657672442495823\n",
      "Step 4 (7914); Episode 85/100; Loss: 0.0496528185904026\n",
      "Step 5 (7915); Episode 85/100; Loss: 0.0032659093849360943\n",
      "Step 6 (7916); Episode 85/100; Loss: 0.029143312945961952\n",
      "Step 7 (7917); Episode 85/100; Loss: 0.0036170468665659428\n",
      "Step 8 (7918); Episode 85/100; Loss: 0.02915647253394127\n",
      "Step 9 (7919); Episode 85/100; Loss: 0.044752903282642365\n",
      "Step 10 (7920); Episode 85/100; Loss: 0.0010070095304399729\n",
      "Step 11 (7921); Episode 85/100; Loss: 0.0353495217859745\n",
      "Step 12 (7922); Episode 85/100; Loss: 0.00224682642146945\n",
      "Step 13 (7923); Episode 85/100; Loss: 0.0010371645912528038\n",
      "Step 14 (7924); Episode 85/100; Loss: 0.0006520696333609521\n",
      "Step 15 (7925); Episode 85/100; Loss: 0.001089664758183062\n",
      "Step 16 (7926); Episode 85/100; Loss: 0.04579731449484825\n",
      "Step 17 (7927); Episode 85/100; Loss: 0.0006762616685591638\n",
      "Step 18 (7928); Episode 85/100; Loss: 0.05040683224797249\n",
      "Step 19 (7929); Episode 85/100; Loss: 0.09666536003351212\n",
      "Step 20 (7930); Episode 85/100; Loss: 0.001996132545173168\n",
      "Step 21 (7931); Episode 85/100; Loss: 0.016279559582471848\n",
      "Step 22 (7932); Episode 85/100; Loss: 0.0022349879145622253\n",
      "Step 23 (7933); Episode 85/100; Loss: 0.002444076118990779\n",
      "Step 24 (7934); Episode 85/100; Loss: 0.0008438833756372333\n",
      "Step 25 (7935); Episode 85/100; Loss: 0.00037661753594875336\n",
      "Step 26 (7936); Episode 85/100; Loss: 0.0721544474363327\n",
      "Step 27 (7937); Episode 85/100; Loss: 0.0015886236215010285\n",
      "Step 28 (7938); Episode 85/100; Loss: 0.0941772535443306\n",
      "Step 29 (7939); Episode 85/100; Loss: 0.0012457129778340459\n",
      "Step 30 (7940); Episode 85/100; Loss: 0.0013041500933468342\n",
      "Step 31 (7941); Episode 85/100; Loss: 0.04597818851470947\n",
      "Step 32 (7942); Episode 85/100; Loss: 0.094600148499012\n",
      "Step 33 (7943); Episode 85/100; Loss: 0.009445791132748127\n",
      "Step 34 (7944); Episode 85/100; Loss: 0.0014771511778235435\n",
      "Step 35 (7945); Episode 85/100; Loss: 0.0062648034654557705\n",
      "Step 36 (7946); Episode 85/100; Loss: 0.014765343628823757\n",
      "Step 37 (7947); Episode 85/100; Loss: 0.0999021902680397\n",
      "Step 38 (7948); Episode 85/100; Loss: 0.0029159574769437313\n",
      "Step 39 (7949); Episode 85/100; Loss: 0.002521913731470704\n",
      "Step 40 (7950); Episode 85/100; Loss: 0.04829491674900055\n",
      "Step 41 (7951); Episode 85/100; Loss: 0.06134143844246864\n",
      "Step 42 (7952); Episode 85/100; Loss: 0.044975850731134415\n",
      "Step 43 (7953); Episode 85/100; Loss: 0.0013438524911180139\n",
      "Step 44 (7954); Episode 85/100; Loss: 0.0023205378092825413\n",
      "Step 45 (7955); Episode 85/100; Loss: 0.050373487174510956\n",
      "Step 46 (7956); Episode 85/100; Loss: 0.049946628510951996\n",
      "Step 47 (7957); Episode 85/100; Loss: 0.038747530430555344\n",
      "Step 48 (7958); Episode 85/100; Loss: 0.043872810900211334\n",
      "Step 49 (7959); Episode 85/100; Loss: 0.002143623773008585\n",
      "Step 50 (7960); Episode 85/100; Loss: 0.07244452834129333\n",
      "Step 51 (7961); Episode 85/100; Loss: 0.07871826738119125\n",
      "Step 52 (7962); Episode 85/100; Loss: 0.03774794191122055\n",
      "Step 53 (7963); Episode 85/100; Loss: 0.003965418320149183\n",
      "Step 54 (7964); Episode 85/100; Loss: 0.0013865988003090024\n",
      "Step 55 (7965); Episode 85/100; Loss: 0.001930932397954166\n",
      "Step 56 (7966); Episode 85/100; Loss: 0.04571656510233879\n",
      "Step 57 (7967); Episode 85/100; Loss: 0.035413529723882675\n",
      "Step 58 (7968); Episode 85/100; Loss: 0.0019149569561704993\n",
      "Step 59 (7969); Episode 85/100; Loss: 0.02540985494852066\n",
      "Step 60 (7970); Episode 85/100; Loss: 0.04281181842088699\n",
      "Step 61 (7971); Episode 85/100; Loss: 0.051031600683927536\n",
      "Step 62 (7972); Episode 85/100; Loss: 0.0018992641707882285\n",
      "Step 63 (7973); Episode 85/100; Loss: 0.046174027025699615\n",
      "Step 64 (7974); Episode 85/100; Loss: 0.04451042413711548\n",
      "Step 65 (7975); Episode 85/100; Loss: 0.0007254708907566965\n",
      "Step 66 (7976); Episode 85/100; Loss: 0.002070498885586858\n",
      "Step 67 (7977); Episode 85/100; Loss: 0.0025590870063751936\n",
      "Step 68 (7978); Episode 85/100; Loss: 0.07221926748752594\n",
      "Step 69 (7979); Episode 85/100; Loss: 0.003146065166220069\n",
      "Step 70 (7980); Episode 85/100; Loss: 0.004135425668209791\n",
      "Step 71 (7981); Episode 85/100; Loss: 0.05167009308934212\n",
      "Step 72 (7982); Episode 85/100; Loss: 0.0020231178496032953\n",
      "Step 73 (7983); Episode 85/100; Loss: 0.0024871083442121744\n",
      "Step 74 (7984); Episode 85/100; Loss: 0.0016785230254754424\n",
      "Step 75 (7985); Episode 85/100; Loss: 0.0008490596665069461\n",
      "Step 76 (7986); Episode 85/100; Loss: 0.0012843463337048888\n",
      "Step 77 (7987); Episode 85/100; Loss: 0.001978888176381588\n",
      "Step 78 (7988); Episode 85/100; Loss: 0.04486855864524841\n",
      "Step 79 (7989); Episode 85/100; Loss: 0.0011745436349883676\n",
      "Step 80 (7990); Episode 85/100; Loss: 0.09802521020174026\n",
      "Step 81 (7991); Episode 85/100; Loss: 0.0013619714882224798\n",
      "Step 82 (7992); Episode 85/100; Loss: 0.014220899902284145\n",
      "Step 83 (7993); Episode 85/100; Loss: 0.0009586011292412877\n",
      "Step 84 (7994); Episode 85/100; Loss: 0.0012748026056215167\n",
      "Step 85 (7995); Episode 85/100; Loss: 0.0015056427801027894\n",
      "Step 86 (7996); Episode 85/100; Loss: 0.055391695350408554\n",
      "Step 87 (7997); Episode 85/100; Loss: 0.04721614718437195\n",
      "Step 88 (7998); Episode 85/100; Loss: 0.002783619798719883\n",
      "Step 89 (7999); Episode 85/100; Loss: 0.0012931018136441708\n",
      "Step 90 (8000); Episode 85/100; Loss: 0.15450477600097656\n",
      "Step 91 (8001); Episode 85/100; Loss: 0.03561246395111084\n",
      "Step 92 (8002); Episode 85/100; Loss: 0.029465515166521072\n",
      "Step 93 (8003); Episode 85/100; Loss: 0.007694138213992119\n",
      "Step 94 (8004); Episode 85/100; Loss: 0.028785398229956627\n",
      "Step 95 (8005); Episode 85/100; Loss: 0.001552555593661964\n",
      "Step 96 (8006); Episode 85/100; Loss: 0.0032537560909986496\n",
      "Step 97 (8007); Episode 85/100; Loss: 0.04463721439242363\n",
      "Step 98 (8008); Episode 85/100; Loss: 0.07476099580526352\n",
      "Step 99 (8009); Episode 85/100; Loss: 0.09218538552522659\n",
      "Step 100 (8010); Episode 85/100; Loss: 0.08498552441596985\n",
      "Step 101 (8011); Episode 85/100; Loss: 0.04852493852376938\n",
      "Step 102 (8012); Episode 85/100; Loss: 0.038051292300224304\n",
      "Step 103 (8013); Episode 85/100; Loss: 0.05911862477660179\n",
      "Step 104 (8014); Episode 85/100; Loss: 0.047058649361133575\n",
      "Step 105 (8015); Episode 85/100; Loss: 0.0015653377631679177\n",
      "Step 106 (8016); Episode 85/100; Loss: 0.0007261784048750997\n",
      "Step 107 (8017); Episode 85/100; Loss: 0.0016619315138086677\n",
      "Step 108 (8018); Episode 85/100; Loss: 0.022104371339082718\n",
      "Step 109 (8019); Episode 85/100; Loss: 0.00273283664137125\n",
      "Step 110 (8020); Episode 85/100; Loss: 0.0007748212083242834\n",
      "Step 111 (8021); Episode 85/100; Loss: 0.03968353196978569\n",
      "Step 112 (8022); Episode 85/100; Loss: 0.0018126432551071048\n",
      "Step 113 (8023); Episode 85/100; Loss: 0.0028004685882478952\n",
      "Step 114 (8024); Episode 85/100; Loss: 0.002620463026687503\n",
      "Step 115 (8025); Episode 85/100; Loss: 0.000588643888477236\n",
      "Step 116 (8026); Episode 85/100; Loss: 0.0014507591258734465\n",
      "Step 117 (8027); Episode 85/100; Loss: 0.0037934291176497936\n",
      "Step 118 (8028); Episode 85/100; Loss: 0.001086738076992333\n",
      "Step 119 (8029); Episode 85/100; Loss: 0.0021854876540601254\n",
      "Step 120 (8030); Episode 85/100; Loss: 0.004988730885088444\n",
      "Step 121 (8031); Episode 85/100; Loss: 0.001491994597017765\n",
      "Step 122 (8032); Episode 85/100; Loss: 0.041026681661605835\n",
      "Step 123 (8033); Episode 85/100; Loss: 0.001991996308788657\n",
      "Step 124 (8034); Episode 85/100; Loss: 0.00446771876886487\n",
      "Step 125 (8035); Episode 85/100; Loss: 0.0017981298733502626\n",
      "Step 126 (8036); Episode 85/100; Loss: 0.0012716864002868533\n",
      "Step 127 (8037); Episode 85/100; Loss: 0.05853613466024399\n",
      "Step 128 (8038); Episode 85/100; Loss: 0.0011549313785508275\n",
      "Step 129 (8039); Episode 85/100; Loss: 0.10412904620170593\n",
      "Step 130 (8040); Episode 85/100; Loss: 0.08735767751932144\n",
      "Step 0 (8041); Episode 86/100; Loss: 0.052456121891736984\n",
      "Step 1 (8042); Episode 86/100; Loss: 0.10939402133226395\n",
      "Step 2 (8043); Episode 86/100; Loss: 0.0032362493220716715\n",
      "Step 3 (8044); Episode 86/100; Loss: 0.0012861964059993625\n",
      "Step 4 (8045); Episode 86/100; Loss: 0.03607496991753578\n",
      "Step 5 (8046); Episode 86/100; Loss: 0.0018224648665636778\n",
      "Step 6 (8047); Episode 86/100; Loss: 0.0005365224205888808\n",
      "Step 7 (8048); Episode 86/100; Loss: 0.0550055168569088\n",
      "Step 8 (8049); Episode 86/100; Loss: 0.0015897485427558422\n",
      "Step 9 (8050); Episode 86/100; Loss: 0.0022085784003138542\n",
      "Step 10 (8051); Episode 86/100; Loss: 0.07830585539340973\n",
      "Step 11 (8052); Episode 86/100; Loss: 0.07683786749839783\n",
      "Step 12 (8053); Episode 86/100; Loss: 0.0022339411079883575\n",
      "Step 13 (8054); Episode 86/100; Loss: 0.0011961862910538912\n",
      "Step 14 (8055); Episode 86/100; Loss: 0.0013509875861927867\n",
      "Step 15 (8056); Episode 86/100; Loss: 0.0004492364823818207\n",
      "Step 16 (8057); Episode 86/100; Loss: 0.05730113759636879\n",
      "Step 17 (8058); Episode 86/100; Loss: 0.05039456859230995\n",
      "Step 18 (8059); Episode 86/100; Loss: 0.001682791393250227\n",
      "Step 19 (8060); Episode 86/100; Loss: 0.04544522985816002\n",
      "Step 20 (8061); Episode 86/100; Loss: 0.0485488623380661\n",
      "Step 21 (8062); Episode 86/100; Loss: 0.07803983986377716\n",
      "Step 22 (8063); Episode 86/100; Loss: 0.000993223860859871\n",
      "Step 23 (8064); Episode 86/100; Loss: 0.05255456268787384\n",
      "Step 24 (8065); Episode 86/100; Loss: 0.03994603827595711\n",
      "Step 25 (8066); Episode 86/100; Loss: 0.004805122036486864\n",
      "Step 26 (8067); Episode 86/100; Loss: 0.0012045704061165452\n",
      "Step 27 (8068); Episode 86/100; Loss: 0.0017339165788143873\n",
      "Step 28 (8069); Episode 86/100; Loss: 0.0008444630657322705\n",
      "Step 29 (8070); Episode 86/100; Loss: 0.0014120209962129593\n",
      "Step 30 (8071); Episode 86/100; Loss: 0.07377931475639343\n",
      "Step 31 (8072); Episode 86/100; Loss: 0.0013495037565007806\n",
      "Step 32 (8073); Episode 86/100; Loss: 0.0020375470630824566\n",
      "Step 33 (8074); Episode 86/100; Loss: 0.009434082545340061\n",
      "Step 34 (8075); Episode 86/100; Loss: 0.04743243008852005\n",
      "Step 35 (8076); Episode 86/100; Loss: 0.0010523429373279214\n",
      "Step 36 (8077); Episode 86/100; Loss: 0.0004878955951426178\n",
      "Step 37 (8078); Episode 86/100; Loss: 0.001568454084917903\n",
      "Step 38 (8079); Episode 86/100; Loss: 0.042326029390096664\n",
      "Step 39 (8080); Episode 86/100; Loss: 0.0013796731363981962\n",
      "Step 40 (8081); Episode 86/100; Loss: 0.0010990699520334601\n",
      "Step 41 (8082); Episode 86/100; Loss: 0.04359881579875946\n",
      "Step 42 (8083); Episode 86/100; Loss: 0.0025392998941242695\n",
      "Step 43 (8084); Episode 86/100; Loss: 0.052360836416482925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44 (8085); Episode 86/100; Loss: 0.019580774009227753\n",
      "Step 45 (8086); Episode 86/100; Loss: 0.04766688495874405\n",
      "Step 46 (8087); Episode 86/100; Loss: 0.00361084402538836\n",
      "Step 47 (8088); Episode 86/100; Loss: 0.04592416435480118\n",
      "Step 48 (8089); Episode 86/100; Loss: 0.09886733442544937\n",
      "Step 49 (8090); Episode 86/100; Loss: 0.046822886914014816\n",
      "Step 50 (8091); Episode 86/100; Loss: 0.0494280643761158\n",
      "Step 51 (8092); Episode 86/100; Loss: 0.13647502660751343\n",
      "Step 52 (8093); Episode 86/100; Loss: 0.052310794591903687\n",
      "Step 53 (8094); Episode 86/100; Loss: 0.0015350185567513108\n",
      "Step 54 (8095); Episode 86/100; Loss: 0.002935628639534116\n",
      "Step 55 (8096); Episode 86/100; Loss: 0.05332117900252342\n",
      "Step 56 (8097); Episode 86/100; Loss: 0.05483780428767204\n",
      "Step 57 (8098); Episode 86/100; Loss: 0.001434855512343347\n",
      "Step 58 (8099); Episode 86/100; Loss: 0.03738537058234215\n",
      "Step 59 (8100); Episode 86/100; Loss: 0.0033245303202420473\n",
      "Step 60 (8101); Episode 86/100; Loss: 0.0014754305593669415\n",
      "Step 61 (8102); Episode 86/100; Loss: 0.05524632707238197\n",
      "Step 62 (8103); Episode 86/100; Loss: 0.001601594383828342\n",
      "Step 63 (8104); Episode 86/100; Loss: 0.0005298531032167375\n",
      "Step 64 (8105); Episode 86/100; Loss: 0.05450744554400444\n",
      "Step 65 (8106); Episode 86/100; Loss: 0.04739151895046234\n",
      "Step 66 (8107); Episode 86/100; Loss: 0.0356781892478466\n",
      "Step 67 (8108); Episode 86/100; Loss: 0.09361335635185242\n",
      "Step 68 (8109); Episode 86/100; Loss: 0.047294434159994125\n",
      "Step 69 (8110); Episode 86/100; Loss: 0.04791780188679695\n",
      "Step 70 (8111); Episode 86/100; Loss: 0.0011149004567414522\n",
      "Step 71 (8112); Episode 86/100; Loss: 0.0018008582992479205\n",
      "Step 72 (8113); Episode 86/100; Loss: 0.04774293303489685\n",
      "Step 73 (8114); Episode 86/100; Loss: 0.0010272316867485642\n",
      "Step 74 (8115); Episode 86/100; Loss: 0.002819398418068886\n",
      "Step 75 (8116); Episode 86/100; Loss: 0.0014065657742321491\n",
      "Step 76 (8117); Episode 86/100; Loss: 0.0011573212686926126\n",
      "Step 77 (8118); Episode 86/100; Loss: 0.0007967521669343114\n",
      "Step 78 (8119); Episode 86/100; Loss: 0.08342432975769043\n",
      "Step 79 (8120); Episode 86/100; Loss: 0.02151888608932495\n",
      "Step 80 (8121); Episode 86/100; Loss: 0.0004960560472682118\n",
      "Step 81 (8122); Episode 86/100; Loss: 0.017744703218340874\n",
      "Step 82 (8123); Episode 86/100; Loss: 0.002087055006995797\n",
      "Step 83 (8124); Episode 86/100; Loss: 0.04335585609078407\n",
      "Step 84 (8125); Episode 86/100; Loss: 0.0033492413349449635\n",
      "Step 85 (8126); Episode 86/100; Loss: 0.04654572531580925\n",
      "Step 86 (8127); Episode 86/100; Loss: 0.0007022221689112484\n",
      "Step 87 (8128); Episode 86/100; Loss: 0.002641635248437524\n",
      "Step 88 (8129); Episode 86/100; Loss: 0.05538693442940712\n",
      "Step 89 (8130); Episode 86/100; Loss: 0.00221815868280828\n",
      "Step 90 (8131); Episode 86/100; Loss: 0.0007539878715761006\n",
      "Step 91 (8132); Episode 86/100; Loss: 0.0007396700675599277\n",
      "Step 92 (8133); Episode 86/100; Loss: 0.042441707104444504\n",
      "Step 93 (8134); Episode 86/100; Loss: 0.018276790156960487\n",
      "Step 94 (8135); Episode 86/100; Loss: 0.067528635263443\n",
      "Step 95 (8136); Episode 86/100; Loss: 0.05255249887704849\n",
      "Step 96 (8137); Episode 86/100; Loss: 0.0006969607202336192\n",
      "Step 97 (8138); Episode 86/100; Loss: 0.001343534211628139\n",
      "Step 98 (8139); Episode 86/100; Loss: 0.0012042763410136104\n",
      "Step 99 (8140); Episode 86/100; Loss: 0.0826682448387146\n",
      "Step 100 (8141); Episode 86/100; Loss: 0.001967780292034149\n",
      "Step 101 (8142); Episode 86/100; Loss: 0.0020054986234754324\n",
      "Step 102 (8143); Episode 86/100; Loss: 0.042973462492227554\n",
      "Step 103 (8144); Episode 86/100; Loss: 0.07051616907119751\n",
      "Step 104 (8145); Episode 86/100; Loss: 0.0641244649887085\n",
      "Step 105 (8146); Episode 86/100; Loss: 0.0029265540651977062\n",
      "Step 106 (8147); Episode 86/100; Loss: 0.04571458324790001\n",
      "Step 107 (8148); Episode 86/100; Loss: 0.0007368389051407576\n",
      "Step 108 (8149); Episode 86/100; Loss: 0.0022969632409512997\n",
      "Step 109 (8150); Episode 86/100; Loss: 0.0006580749759450555\n",
      "Step 110 (8151); Episode 86/100; Loss: 0.002549753524363041\n",
      "Step 111 (8152); Episode 86/100; Loss: 0.0008642106549814343\n",
      "Step 112 (8153); Episode 86/100; Loss: 0.001566525548696518\n",
      "Step 113 (8154); Episode 86/100; Loss: 0.0010923038935288787\n",
      "Step 114 (8155); Episode 86/100; Loss: 0.05165459215641022\n",
      "Step 115 (8156); Episode 86/100; Loss: 0.04620470479130745\n",
      "Step 116 (8157); Episode 86/100; Loss: 0.0005690731923095882\n",
      "Step 117 (8158); Episode 86/100; Loss: 0.09105201810598373\n",
      "Step 118 (8159); Episode 86/100; Loss: 0.0009214756428264081\n",
      "Step 119 (8160); Episode 86/100; Loss: 0.0008815889596007764\n",
      "Step 120 (8161); Episode 86/100; Loss: 0.0025528951082378626\n",
      "Step 121 (8162); Episode 86/100; Loss: 0.0026694778352975845\n",
      "Step 122 (8163); Episode 86/100; Loss: 0.047354958951473236\n",
      "Step 123 (8164); Episode 86/100; Loss: 0.05363566800951958\n",
      "Step 124 (8165); Episode 86/100; Loss: 0.044529203325510025\n",
      "Step 125 (8166); Episode 86/100; Loss: 0.07540036737918854\n",
      "Step 126 (8167); Episode 86/100; Loss: 0.0008453856571577489\n",
      "Step 127 (8168); Episode 86/100; Loss: 0.0012203726219013333\n",
      "Step 128 (8169); Episode 86/100; Loss: 0.05566128343343735\n",
      "Step 129 (8170); Episode 86/100; Loss: 0.0034177806228399277\n",
      "Step 130 (8171); Episode 86/100; Loss: 0.09467049688100815\n",
      "Step 131 (8172); Episode 86/100; Loss: 0.002291120355948806\n",
      "Step 132 (8173); Episode 86/100; Loss: 0.0014615999534726143\n",
      "Step 133 (8174); Episode 86/100; Loss: 0.0974927470088005\n",
      "Step 134 (8175); Episode 86/100; Loss: 0.004901677370071411\n",
      "Step 135 (8176); Episode 86/100; Loss: 0.05702777951955795\n",
      "Step 136 (8177); Episode 86/100; Loss: 0.04966423660516739\n",
      "Step 137 (8178); Episode 86/100; Loss: 0.024928120896220207\n",
      "Step 138 (8179); Episode 86/100; Loss: 0.04029575362801552\n",
      "Step 139 (8180); Episode 86/100; Loss: 0.0008211661479435861\n",
      "Step 140 (8181); Episode 86/100; Loss: 0.001259184442460537\n",
      "Step 141 (8182); Episode 86/100; Loss: 0.0007152448524720967\n",
      "Step 142 (8183); Episode 86/100; Loss: 0.02123350463807583\n",
      "Step 143 (8184); Episode 86/100; Loss: 0.05613891780376434\n",
      "Step 144 (8185); Episode 86/100; Loss: 0.001049272483214736\n",
      "Step 145 (8186); Episode 86/100; Loss: 0.08136720955371857\n",
      "Step 146 (8187); Episode 86/100; Loss: 0.006467406637966633\n",
      "Step 147 (8188); Episode 86/100; Loss: 0.0467701330780983\n",
      "Step 148 (8189); Episode 86/100; Loss: 0.021627161651849747\n",
      "Step 149 (8190); Episode 86/100; Loss: 0.0007823084597475827\n",
      "Step 150 (8191); Episode 86/100; Loss: 0.0013455331791192293\n",
      "Step 151 (8192); Episode 86/100; Loss: 0.04833778738975525\n",
      "Step 152 (8193); Episode 86/100; Loss: 0.0015447420300915837\n",
      "Step 153 (8194); Episode 86/100; Loss: 0.0481850765645504\n",
      "Step 154 (8195); Episode 86/100; Loss: 0.0018953183898702264\n",
      "Step 155 (8196); Episode 86/100; Loss: 0.06681619584560394\n",
      "Step 156 (8197); Episode 86/100; Loss: 0.0031841613817960024\n",
      "Step 157 (8198); Episode 86/100; Loss: 0.00048346759285777807\n",
      "Step 158 (8199); Episode 86/100; Loss: 0.04798822104930878\n",
      "Step 159 (8200); Episode 86/100; Loss: 0.04229380935430527\n",
      "Step 160 (8201); Episode 86/100; Loss: 0.0029040926601737738\n",
      "Step 161 (8202); Episode 86/100; Loss: 0.0016600649105384946\n",
      "Step 162 (8203); Episode 86/100; Loss: 0.0012542627518996596\n",
      "Step 0 (8204); Episode 87/100; Loss: 0.0014597075060009956\n",
      "Step 1 (8205); Episode 87/100; Loss: 0.002491079503670335\n",
      "Step 2 (8206); Episode 87/100; Loss: 0.022144313901662827\n",
      "Step 3 (8207); Episode 87/100; Loss: 0.04276494309306145\n",
      "Step 4 (8208); Episode 87/100; Loss: 0.001003170502372086\n",
      "Step 5 (8209); Episode 87/100; Loss: 0.02075282856822014\n",
      "Step 6 (8210); Episode 87/100; Loss: 0.013329987414181232\n",
      "Step 7 (8211); Episode 87/100; Loss: 0.0010821947362273932\n",
      "Step 8 (8212); Episode 87/100; Loss: 0.014556444250047207\n",
      "Step 9 (8213); Episode 87/100; Loss: 0.0012218685587868094\n",
      "Step 10 (8214); Episode 87/100; Loss: 0.0014175532851368189\n",
      "Step 11 (8215); Episode 87/100; Loss: 0.0012971780961379409\n",
      "Step 12 (8216); Episode 87/100; Loss: 0.0014230760280042887\n",
      "Step 13 (8217); Episode 87/100; Loss: 0.04126717150211334\n",
      "Step 14 (8218); Episode 87/100; Loss: 0.0491623692214489\n",
      "Step 15 (8219); Episode 87/100; Loss: 0.033605530858039856\n",
      "Step 16 (8220); Episode 87/100; Loss: 0.0028382400050759315\n",
      "Step 17 (8221); Episode 87/100; Loss: 0.0013712011277675629\n",
      "Step 18 (8222); Episode 87/100; Loss: 0.05825158208608627\n",
      "Step 19 (8223); Episode 87/100; Loss: 0.0031096716411411762\n",
      "Step 20 (8224); Episode 87/100; Loss: 0.007935034111142159\n",
      "Step 21 (8225); Episode 87/100; Loss: 0.0016945060342550278\n",
      "Step 22 (8226); Episode 87/100; Loss: 0.04974818229675293\n",
      "Step 23 (8227); Episode 87/100; Loss: 0.0017454514745622873\n",
      "Step 24 (8228); Episode 87/100; Loss: 0.06141366809606552\n",
      "Step 25 (8229); Episode 87/100; Loss: 0.05265799164772034\n",
      "Step 26 (8230); Episode 87/100; Loss: 0.001848219777457416\n",
      "Step 27 (8231); Episode 87/100; Loss: 0.05258079990744591\n",
      "Step 28 (8232); Episode 87/100; Loss: 0.046097297221422195\n",
      "Step 29 (8233); Episode 87/100; Loss: 0.04772712290287018\n",
      "Step 30 (8234); Episode 87/100; Loss: 0.0006555257132276893\n",
      "Step 31 (8235); Episode 87/100; Loss: 0.0024910797365009785\n",
      "Step 32 (8236); Episode 87/100; Loss: 0.002649417845532298\n",
      "Step 33 (8237); Episode 87/100; Loss: 0.0014989444753155112\n",
      "Step 34 (8238); Episode 87/100; Loss: 0.001698958338238299\n",
      "Step 35 (8239); Episode 87/100; Loss: 0.05174275487661362\n",
      "Step 36 (8240); Episode 87/100; Loss: 0.05250197649002075\n",
      "Step 37 (8241); Episode 87/100; Loss: 0.046197693794965744\n",
      "Step 38 (8242); Episode 87/100; Loss: 0.07439086586236954\n",
      "Step 39 (8243); Episode 87/100; Loss: 0.04273248091340065\n",
      "Step 40 (8244); Episode 87/100; Loss: 0.0007954799220897257\n",
      "Step 41 (8245); Episode 87/100; Loss: 0.006506945006549358\n",
      "Step 42 (8246); Episode 87/100; Loss: 0.01426691748201847\n",
      "Step 43 (8247); Episode 87/100; Loss: 0.02859213575720787\n",
      "Step 44 (8248); Episode 87/100; Loss: 0.0023709675297141075\n",
      "Step 45 (8249); Episode 87/100; Loss: 0.08424920588731766\n",
      "Step 46 (8250); Episode 87/100; Loss: 0.07982808351516724\n",
      "Step 47 (8251); Episode 87/100; Loss: 0.0005391977610997856\n",
      "Step 48 (8252); Episode 87/100; Loss: 0.056954700499773026\n",
      "Step 49 (8253); Episode 87/100; Loss: 0.0007285419851541519\n",
      "Step 50 (8254); Episode 87/100; Loss: 0.00294134090654552\n",
      "Step 51 (8255); Episode 87/100; Loss: 0.0017711161635816097\n",
      "Step 52 (8256); Episode 87/100; Loss: 0.04568580165505409\n",
      "Step 53 (8257); Episode 87/100; Loss: 0.09410271793603897\n",
      "Step 54 (8258); Episode 87/100; Loss: 0.0024097259156405926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55 (8259); Episode 87/100; Loss: 0.07462270557880402\n",
      "Step 56 (8260); Episode 87/100; Loss: 0.0010507515398785472\n",
      "Step 57 (8261); Episode 87/100; Loss: 0.0008295840816572309\n",
      "Step 58 (8262); Episode 87/100; Loss: 0.0017557895043864846\n",
      "Step 59 (8263); Episode 87/100; Loss: 0.09159386903047562\n",
      "Step 60 (8264); Episode 87/100; Loss: 0.0006641524378210306\n",
      "Step 61 (8265); Episode 87/100; Loss: 0.0020388683769851923\n",
      "Step 62 (8266); Episode 87/100; Loss: 0.0406017005443573\n",
      "Step 63 (8267); Episode 87/100; Loss: 0.02866709791123867\n",
      "Step 64 (8268); Episode 87/100; Loss: 0.05322635546326637\n",
      "Step 65 (8269); Episode 87/100; Loss: 0.043276481330394745\n",
      "Step 66 (8270); Episode 87/100; Loss: 0.09739915281534195\n",
      "Step 67 (8271); Episode 87/100; Loss: 0.0005358565249480307\n",
      "Step 68 (8272); Episode 87/100; Loss: 0.0400373600423336\n",
      "Step 69 (8273); Episode 87/100; Loss: 0.005298329517245293\n",
      "Step 70 (8274); Episode 87/100; Loss: 0.04547971859574318\n",
      "Step 71 (8275); Episode 87/100; Loss: 0.03631751239299774\n",
      "Step 72 (8276); Episode 87/100; Loss: 0.002638575853779912\n",
      "Step 73 (8277); Episode 87/100; Loss: 0.002472854917868972\n",
      "Step 74 (8278); Episode 87/100; Loss: 0.0019707148894667625\n",
      "Step 75 (8279); Episode 87/100; Loss: 0.002787550911307335\n",
      "Step 76 (8280); Episode 87/100; Loss: 0.0027772211469709873\n",
      "Step 77 (8281); Episode 87/100; Loss: 0.00378783093765378\n",
      "Step 78 (8282); Episode 87/100; Loss: 0.05004401132464409\n",
      "Step 79 (8283); Episode 87/100; Loss: 0.004990207031369209\n",
      "Step 80 (8284); Episode 87/100; Loss: 0.04618566110730171\n",
      "Step 81 (8285); Episode 87/100; Loss: 0.07462988048791885\n",
      "Step 82 (8286); Episode 87/100; Loss: 0.1049707904458046\n",
      "Step 83 (8287); Episode 87/100; Loss: 0.0025690486654639244\n",
      "Step 84 (8288); Episode 87/100; Loss: 0.07629598677158356\n",
      "Step 85 (8289); Episode 87/100; Loss: 0.00047464389353990555\n",
      "Step 86 (8290); Episode 87/100; Loss: 0.004201099276542664\n",
      "Step 87 (8291); Episode 87/100; Loss: 0.0032707699574530125\n",
      "Step 88 (8292); Episode 87/100; Loss: 0.012250128202140331\n",
      "Step 89 (8293); Episode 87/100; Loss: 0.023359157145023346\n",
      "Step 90 (8294); Episode 87/100; Loss: 0.04479729011654854\n",
      "Step 91 (8295); Episode 87/100; Loss: 0.0010384318884462118\n",
      "Step 92 (8296); Episode 87/100; Loss: 0.04115999490022659\n",
      "Step 93 (8297); Episode 87/100; Loss: 0.0563877709209919\n",
      "Step 94 (8298); Episode 87/100; Loss: 0.0010234466753900051\n",
      "Step 95 (8299); Episode 87/100; Loss: 0.0019018604652956128\n",
      "Step 96 (8300); Episode 87/100; Loss: 0.00047591436305083334\n",
      "Step 97 (8301); Episode 87/100; Loss: 0.11055013537406921\n",
      "Step 98 (8302); Episode 87/100; Loss: 0.05357525497674942\n",
      "Step 99 (8303); Episode 87/100; Loss: 0.018929295241832733\n",
      "Step 100 (8304); Episode 87/100; Loss: 0.11107528209686279\n",
      "Step 101 (8305); Episode 87/100; Loss: 0.0022122664377093315\n",
      "Step 102 (8306); Episode 87/100; Loss: 0.048923712223768234\n",
      "Step 103 (8307); Episode 87/100; Loss: 0.001994660124182701\n",
      "Step 104 (8308); Episode 87/100; Loss: 0.00364121375605464\n",
      "Step 105 (8309); Episode 87/100; Loss: 0.0035783518105745316\n",
      "Step 106 (8310); Episode 87/100; Loss: 0.0012278794310986996\n",
      "Step 107 (8311); Episode 87/100; Loss: 0.05543571338057518\n",
      "Step 108 (8312); Episode 87/100; Loss: 0.0011421393137425184\n",
      "Step 109 (8313); Episode 87/100; Loss: 0.006072992458939552\n",
      "Step 110 (8314); Episode 87/100; Loss: 0.003376707900315523\n",
      "Step 111 (8315); Episode 87/100; Loss: 0.01542618777602911\n",
      "Step 112 (8316); Episode 87/100; Loss: 0.009153383783996105\n",
      "Step 113 (8317); Episode 87/100; Loss: 0.002620345912873745\n",
      "Step 114 (8318); Episode 87/100; Loss: 0.10801564157009125\n",
      "Step 115 (8319); Episode 87/100; Loss: 0.10560489445924759\n",
      "Step 116 (8320); Episode 87/100; Loss: 0.04392661899328232\n",
      "Step 117 (8321); Episode 87/100; Loss: 0.047977179288864136\n",
      "Step 118 (8322); Episode 87/100; Loss: 0.0008944124565459788\n",
      "Step 119 (8323); Episode 87/100; Loss: 0.12069776654243469\n",
      "Step 120 (8324); Episode 87/100; Loss: 0.0015750167658552527\n",
      "Step 121 (8325); Episode 87/100; Loss: 0.05915147066116333\n",
      "Step 122 (8326); Episode 87/100; Loss: 0.0027633814606815577\n",
      "Step 123 (8327); Episode 87/100; Loss: 0.002916339784860611\n",
      "Step 124 (8328); Episode 87/100; Loss: 0.0024554210249334574\n",
      "Step 125 (8329); Episode 87/100; Loss: 0.06603380292654037\n",
      "Step 126 (8330); Episode 87/100; Loss: 0.003196264849975705\n",
      "Step 127 (8331); Episode 87/100; Loss: 0.0038177429232746363\n",
      "Step 128 (8332); Episode 87/100; Loss: 0.03000226989388466\n",
      "Step 129 (8333); Episode 87/100; Loss: 0.019936461001634598\n",
      "Step 130 (8334); Episode 87/100; Loss: 0.001368702040053904\n",
      "Step 131 (8335); Episode 87/100; Loss: 0.0017694829730316997\n",
      "Step 132 (8336); Episode 87/100; Loss: 0.002012276090681553\n",
      "Step 133 (8337); Episode 87/100; Loss: 0.0012804853031411767\n",
      "Step 134 (8338); Episode 87/100; Loss: 0.008312678895890713\n",
      "Step 135 (8339); Episode 87/100; Loss: 0.0016316997352987528\n",
      "Step 136 (8340); Episode 87/100; Loss: 0.04615956172347069\n",
      "Step 137 (8341); Episode 87/100; Loss: 0.04799036681652069\n",
      "Step 138 (8342); Episode 87/100; Loss: 0.0031933460850268602\n",
      "Step 139 (8343); Episode 87/100; Loss: 0.00165248429402709\n",
      "Step 140 (8344); Episode 87/100; Loss: 0.001397600513882935\n",
      "Step 141 (8345); Episode 87/100; Loss: 0.029463043436408043\n",
      "Step 142 (8346); Episode 87/100; Loss: 0.0012027400080114603\n",
      "Step 143 (8347); Episode 87/100; Loss: 0.0016204767161980271\n",
      "Step 144 (8348); Episode 87/100; Loss: 0.0012156438315287232\n",
      "Step 145 (8349); Episode 87/100; Loss: 0.004189525730907917\n",
      "Step 146 (8350); Episode 87/100; Loss: 0.05355830490589142\n",
      "Step 147 (8351); Episode 87/100; Loss: 0.0005847591673955321\n",
      "Step 148 (8352); Episode 87/100; Loss: 0.000696077651809901\n",
      "Step 149 (8353); Episode 87/100; Loss: 0.0007123695104382932\n",
      "Step 150 (8354); Episode 87/100; Loss: 0.045901909470558167\n",
      "Step 151 (8355); Episode 87/100; Loss: 0.04664061218500137\n",
      "Step 152 (8356); Episode 87/100; Loss: 0.03152622655034065\n",
      "Step 153 (8357); Episode 87/100; Loss: 0.00106295314617455\n",
      "Step 154 (8358); Episode 87/100; Loss: 0.05101294070482254\n",
      "Step 155 (8359); Episode 87/100; Loss: 0.04864379018545151\n",
      "Step 156 (8360); Episode 87/100; Loss: 0.02305673249065876\n",
      "Step 157 (8361); Episode 87/100; Loss: 0.05121495947241783\n",
      "Step 158 (8362); Episode 87/100; Loss: 0.0015640315832570195\n",
      "Step 159 (8363); Episode 87/100; Loss: 0.00891711562871933\n",
      "Step 160 (8364); Episode 87/100; Loss: 0.00042901228880509734\n",
      "Step 161 (8365); Episode 87/100; Loss: 0.0013788513606414199\n",
      "Step 162 (8366); Episode 87/100; Loss: 0.0019124397076666355\n",
      "Step 163 (8367); Episode 87/100; Loss: 0.05416540801525116\n",
      "Step 164 (8368); Episode 87/100; Loss: 0.04994307458400726\n",
      "Step 165 (8369); Episode 87/100; Loss: 0.0038148213643580675\n",
      "Step 166 (8370); Episode 87/100; Loss: 0.0013555475743487477\n",
      "Step 167 (8371); Episode 87/100; Loss: 0.00036002841079607606\n",
      "Step 168 (8372); Episode 87/100; Loss: 0.05599046126008034\n",
      "Step 169 (8373); Episode 87/100; Loss: 0.0013389771338552237\n",
      "Step 170 (8374); Episode 87/100; Loss: 0.0004724435566458851\n",
      "Step 171 (8375); Episode 87/100; Loss: 0.009165632538497448\n",
      "Step 0 (8376); Episode 88/100; Loss: 0.053051918745040894\n",
      "Step 1 (8377); Episode 88/100; Loss: 0.14938205480575562\n",
      "Step 2 (8378); Episode 88/100; Loss: 0.0014895590720698237\n",
      "Step 3 (8379); Episode 88/100; Loss: 0.0011784558882936835\n",
      "Step 4 (8380); Episode 88/100; Loss: 0.001526442589238286\n",
      "Step 5 (8381); Episode 88/100; Loss: 0.003693923819810152\n",
      "Step 6 (8382); Episode 88/100; Loss: 0.047628674656152725\n",
      "Step 7 (8383); Episode 88/100; Loss: 0.053029052913188934\n",
      "Step 8 (8384); Episode 88/100; Loss: 0.00566265732049942\n",
      "Step 9 (8385); Episode 88/100; Loss: 0.04391281306743622\n",
      "Step 10 (8386); Episode 88/100; Loss: 0.023175232112407684\n",
      "Step 11 (8387); Episode 88/100; Loss: 0.0008350128773599863\n",
      "Step 12 (8388); Episode 88/100; Loss: 0.0014915067004039884\n",
      "Step 13 (8389); Episode 88/100; Loss: 0.0017742137424647808\n",
      "Step 14 (8390); Episode 88/100; Loss: 0.0009879748104140162\n",
      "Step 15 (8391); Episode 88/100; Loss: 0.03794734925031662\n",
      "Step 16 (8392); Episode 88/100; Loss: 0.0050237709656357765\n",
      "Step 17 (8393); Episode 88/100; Loss: 0.047411657869815826\n",
      "Step 18 (8394); Episode 88/100; Loss: 0.002009172225371003\n",
      "Step 19 (8395); Episode 88/100; Loss: 0.070839062333107\n",
      "Step 20 (8396); Episode 88/100; Loss: 0.04166679084300995\n",
      "Step 21 (8397); Episode 88/100; Loss: 0.0010151858441531658\n",
      "Step 22 (8398); Episode 88/100; Loss: 0.0016710679046809673\n",
      "Step 23 (8399); Episode 88/100; Loss: 0.001116919913329184\n",
      "Step 24 (8400); Episode 88/100; Loss: 0.0012694249162450433\n",
      "Step 25 (8401); Episode 88/100; Loss: 0.0033248590771108866\n",
      "Step 26 (8402); Episode 88/100; Loss: 0.0013147647259756923\n",
      "Step 27 (8403); Episode 88/100; Loss: 0.044297389686107635\n",
      "Step 28 (8404); Episode 88/100; Loss: 0.035058293491601944\n",
      "Step 29 (8405); Episode 88/100; Loss: 0.0467049665749073\n",
      "Step 30 (8406); Episode 88/100; Loss: 0.0017557977698743343\n",
      "Step 31 (8407); Episode 88/100; Loss: 0.02193848416209221\n",
      "Step 32 (8408); Episode 88/100; Loss: 0.05640074610710144\n",
      "Step 33 (8409); Episode 88/100; Loss: 0.03275464102625847\n",
      "Step 34 (8410); Episode 88/100; Loss: 0.036856282502412796\n",
      "Step 35 (8411); Episode 88/100; Loss: 0.0007330729276873171\n",
      "Step 36 (8412); Episode 88/100; Loss: 0.0007763270405121148\n",
      "Step 37 (8413); Episode 88/100; Loss: 0.0014521012781187892\n",
      "Step 38 (8414); Episode 88/100; Loss: 0.04450167343020439\n",
      "Step 39 (8415); Episode 88/100; Loss: 0.05733240768313408\n",
      "Step 40 (8416); Episode 88/100; Loss: 0.04279498755931854\n",
      "Step 41 (8417); Episode 88/100; Loss: 0.0021876944229006767\n",
      "Step 42 (8418); Episode 88/100; Loss: 0.0010994723998010159\n",
      "Step 43 (8419); Episode 88/100; Loss: 0.05806047469377518\n",
      "Step 44 (8420); Episode 88/100; Loss: 0.003852908033877611\n",
      "Step 45 (8421); Episode 88/100; Loss: 0.1393764168024063\n",
      "Step 46 (8422); Episode 88/100; Loss: 0.04421890527009964\n",
      "Step 47 (8423); Episode 88/100; Loss: 0.002760079223662615\n",
      "Step 48 (8424); Episode 88/100; Loss: 0.11532531678676605\n",
      "Step 49 (8425); Episode 88/100; Loss: 0.07891525328159332\n",
      "Step 50 (8426); Episode 88/100; Loss: 0.04732769355177879\n",
      "Step 51 (8427); Episode 88/100; Loss: 0.05032685771584511\n",
      "Step 52 (8428); Episode 88/100; Loss: 0.0037557240575551987\n",
      "Step 53 (8429); Episode 88/100; Loss: 0.05661635845899582\n",
      "Step 54 (8430); Episode 88/100; Loss: 0.0009506372734904289\n",
      "Step 55 (8431); Episode 88/100; Loss: 0.05923492833971977\n",
      "Step 56 (8432); Episode 88/100; Loss: 0.03808221220970154\n",
      "Step 57 (8433); Episode 88/100; Loss: 0.016021575778722763\n",
      "Step 58 (8434); Episode 88/100; Loss: 0.05205054581165314\n",
      "Step 59 (8435); Episode 88/100; Loss: 0.043648745864629745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60 (8436); Episode 88/100; Loss: 0.0010763425379991531\n",
      "Step 61 (8437); Episode 88/100; Loss: 0.048380278050899506\n",
      "Step 62 (8438); Episode 88/100; Loss: 0.0006950398092158139\n",
      "Step 63 (8439); Episode 88/100; Loss: 0.0009100607130676508\n",
      "Step 64 (8440); Episode 88/100; Loss: 0.02829018235206604\n",
      "Step 65 (8441); Episode 88/100; Loss: 0.06604038923978806\n",
      "Step 66 (8442); Episode 88/100; Loss: 0.0008766429964452982\n",
      "Step 67 (8443); Episode 88/100; Loss: 0.051484741270542145\n",
      "Step 68 (8444); Episode 88/100; Loss: 0.012353434227406979\n",
      "Step 69 (8445); Episode 88/100; Loss: 0.01529434323310852\n",
      "Step 70 (8446); Episode 88/100; Loss: 0.046380698680877686\n",
      "Step 71 (8447); Episode 88/100; Loss: 0.0003398411499802023\n",
      "Step 72 (8448); Episode 88/100; Loss: 0.0009483642061240971\n",
      "Step 73 (8449); Episode 88/100; Loss: 0.04773855209350586\n",
      "Step 74 (8450); Episode 88/100; Loss: 0.003922183532267809\n",
      "Step 75 (8451); Episode 88/100; Loss: 0.055519651621580124\n",
      "Step 76 (8452); Episode 88/100; Loss: 0.04700904339551926\n",
      "Step 77 (8453); Episode 88/100; Loss: 0.0012427465990185738\n",
      "Step 78 (8454); Episode 88/100; Loss: 0.002198369475081563\n",
      "Step 79 (8455); Episode 88/100; Loss: 0.0015706251142546535\n",
      "Step 80 (8456); Episode 88/100; Loss: 0.03990256413817406\n",
      "Step 81 (8457); Episode 88/100; Loss: 0.08832059055566788\n",
      "Step 82 (8458); Episode 88/100; Loss: 0.0037576875183731318\n",
      "Step 83 (8459); Episode 88/100; Loss: 0.001269107568077743\n",
      "Step 84 (8460); Episode 88/100; Loss: 0.0005699716857634485\n",
      "Step 85 (8461); Episode 88/100; Loss: 0.0027819047681987286\n",
      "Step 86 (8462); Episode 88/100; Loss: 0.001482645864598453\n",
      "Step 87 (8463); Episode 88/100; Loss: 0.10514336079359055\n",
      "Step 88 (8464); Episode 88/100; Loss: 0.0012115801218897104\n",
      "Step 89 (8465); Episode 88/100; Loss: 0.0034143105149269104\n",
      "Step 90 (8466); Episode 88/100; Loss: 0.04471292346715927\n",
      "Step 91 (8467); Episode 88/100; Loss: 0.009610225446522236\n",
      "Step 92 (8468); Episode 88/100; Loss: 0.0015898464480414987\n",
      "Step 93 (8469); Episode 88/100; Loss: 0.0021396165248006582\n",
      "Step 94 (8470); Episode 88/100; Loss: 0.0014435653574764729\n",
      "Step 95 (8471); Episode 88/100; Loss: 0.002210344886407256\n",
      "Step 96 (8472); Episode 88/100; Loss: 0.008880960755050182\n",
      "Step 97 (8473); Episode 88/100; Loss: 0.03079734928905964\n",
      "Step 98 (8474); Episode 88/100; Loss: 0.04710274562239647\n",
      "Step 99 (8475); Episode 88/100; Loss: 0.09775765985250473\n",
      "Step 100 (8476); Episode 88/100; Loss: 0.0015283061657100916\n",
      "Step 101 (8477); Episode 88/100; Loss: 0.04794050753116608\n",
      "Step 102 (8478); Episode 88/100; Loss: 0.002041495405137539\n",
      "Step 103 (8479); Episode 88/100; Loss: 0.15021516382694244\n",
      "Step 104 (8480); Episode 88/100; Loss: 0.0005963276489637792\n",
      "Step 105 (8481); Episode 88/100; Loss: 0.04064038395881653\n",
      "Step 106 (8482); Episode 88/100; Loss: 0.006472171749919653\n",
      "Step 107 (8483); Episode 88/100; Loss: 0.0025520888157188892\n",
      "Step 108 (8484); Episode 88/100; Loss: 0.002207447774708271\n",
      "Step 109 (8485); Episode 88/100; Loss: 0.04453964903950691\n",
      "Step 110 (8486); Episode 88/100; Loss: 0.0019527465337887406\n",
      "Step 111 (8487); Episode 88/100; Loss: 0.002131229965016246\n",
      "Step 112 (8488); Episode 88/100; Loss: 0.0009394649532623589\n",
      "Step 113 (8489); Episode 88/100; Loss: 0.015049693174660206\n",
      "Step 114 (8490); Episode 88/100; Loss: 0.04007714241743088\n",
      "Step 115 (8491); Episode 88/100; Loss: 0.14603590965270996\n",
      "Step 116 (8492); Episode 88/100; Loss: 0.008627476170659065\n",
      "Step 117 (8493); Episode 88/100; Loss: 0.0013644014252349734\n",
      "Step 118 (8494); Episode 88/100; Loss: 0.001101509784348309\n",
      "Step 119 (8495); Episode 88/100; Loss: 0.04938766360282898\n",
      "Step 120 (8496); Episode 88/100; Loss: 0.0023610901553183794\n",
      "Step 121 (8497); Episode 88/100; Loss: 0.09752264618873596\n",
      "Step 122 (8498); Episode 88/100; Loss: 0.0039917766116559505\n",
      "Step 123 (8499); Episode 88/100; Loss: 0.02291092649102211\n",
      "Step 124 (8500); Episode 88/100; Loss: 0.04806504026055336\n",
      "Step 125 (8501); Episode 88/100; Loss: 0.0009125357610173523\n",
      "Step 126 (8502); Episode 88/100; Loss: 0.001436905236914754\n",
      "Step 127 (8503); Episode 88/100; Loss: 0.042453937232494354\n",
      "Step 128 (8504); Episode 88/100; Loss: 0.05769908428192139\n",
      "Step 129 (8505); Episode 88/100; Loss: 0.0025997976772487164\n",
      "Step 130 (8506); Episode 88/100; Loss: 0.019948739558458328\n",
      "Step 131 (8507); Episode 88/100; Loss: 0.001627519028261304\n",
      "Step 132 (8508); Episode 88/100; Loss: 0.0442943349480629\n",
      "Step 133 (8509); Episode 88/100; Loss: 0.0008224683115258813\n",
      "Step 134 (8510); Episode 88/100; Loss: 0.04901351407170296\n",
      "Step 135 (8511); Episode 88/100; Loss: 0.044496744871139526\n",
      "Step 136 (8512); Episode 88/100; Loss: 0.0566849485039711\n",
      "Step 137 (8513); Episode 88/100; Loss: 0.0027064301539212465\n",
      "Step 138 (8514); Episode 88/100; Loss: 0.002344419714063406\n",
      "Step 139 (8515); Episode 88/100; Loss: 0.12724152207374573\n",
      "Step 140 (8516); Episode 88/100; Loss: 0.006979586556553841\n",
      "Step 141 (8517); Episode 88/100; Loss: 0.001053973799571395\n",
      "Step 142 (8518); Episode 88/100; Loss: 0.004043763503432274\n",
      "Step 143 (8519); Episode 88/100; Loss: 0.08376393467187881\n",
      "Step 144 (8520); Episode 88/100; Loss: 0.00214047497138381\n",
      "Step 145 (8521); Episode 88/100; Loss: 0.02204536832869053\n",
      "Step 146 (8522); Episode 88/100; Loss: 0.0013019158504903316\n",
      "Step 147 (8523); Episode 88/100; Loss: 0.0016166913555935025\n",
      "Step 148 (8524); Episode 88/100; Loss: 0.04240003973245621\n",
      "Step 149 (8525); Episode 88/100; Loss: 0.0007006377563811839\n",
      "Step 150 (8526); Episode 88/100; Loss: 0.0006290695164352655\n",
      "Step 151 (8527); Episode 88/100; Loss: 0.10562209039926529\n",
      "Step 152 (8528); Episode 88/100; Loss: 0.0018969479715451598\n",
      "Step 153 (8529); Episode 88/100; Loss: 0.0019875147845596075\n",
      "Step 154 (8530); Episode 88/100; Loss: 0.0011886165011674166\n",
      "Step 155 (8531); Episode 88/100; Loss: 0.08659380674362183\n",
      "Step 156 (8532); Episode 88/100; Loss: 0.1002124473452568\n",
      "Step 157 (8533); Episode 88/100; Loss: 0.08403028547763824\n",
      "Step 158 (8534); Episode 88/100; Loss: 0.10081300884485245\n",
      "Step 159 (8535); Episode 88/100; Loss: 0.0016653668135404587\n",
      "Step 160 (8536); Episode 88/100; Loss: 0.038408372551202774\n",
      "Step 161 (8537); Episode 88/100; Loss: 0.044008832424879074\n",
      "Step 162 (8538); Episode 88/100; Loss: 0.03669380024075508\n",
      "Step 163 (8539); Episode 88/100; Loss: 0.0019332607043907046\n",
      "Step 164 (8540); Episode 88/100; Loss: 0.0380447618663311\n",
      "Step 165 (8541); Episode 88/100; Loss: 0.0028261616826057434\n",
      "Step 166 (8542); Episode 88/100; Loss: 0.005460499785840511\n",
      "Step 167 (8543); Episode 88/100; Loss: 0.0033230602275580168\n",
      "Step 168 (8544); Episode 88/100; Loss: 0.038377437740564346\n",
      "Step 169 (8545); Episode 88/100; Loss: 0.05234525725245476\n",
      "Step 170 (8546); Episode 88/100; Loss: 0.003028322011232376\n",
      "Step 0 (8547); Episode 89/100; Loss: 0.0008220353629440069\n",
      "Step 1 (8548); Episode 89/100; Loss: 0.09293977171182632\n",
      "Step 2 (8549); Episode 89/100; Loss: 0.0026664978358894587\n",
      "Step 3 (8550); Episode 89/100; Loss: 0.04267827421426773\n",
      "Step 4 (8551); Episode 89/100; Loss: 0.001441474654711783\n",
      "Step 5 (8552); Episode 89/100; Loss: 0.020328465849161148\n",
      "Step 6 (8553); Episode 89/100; Loss: 0.05768236145377159\n",
      "Step 7 (8554); Episode 89/100; Loss: 0.0007232836214825511\n",
      "Step 8 (8555); Episode 89/100; Loss: 0.09710923582315445\n",
      "Step 9 (8556); Episode 89/100; Loss: 0.000828066433314234\n",
      "Step 10 (8557); Episode 89/100; Loss: 0.0017832990270107985\n",
      "Step 11 (8558); Episode 89/100; Loss: 0.08150533586740494\n",
      "Step 12 (8559); Episode 89/100; Loss: 0.002099827630445361\n",
      "Step 13 (8560); Episode 89/100; Loss: 0.04042287543416023\n",
      "Step 14 (8561); Episode 89/100; Loss: 0.08931821584701538\n",
      "Step 15 (8562); Episode 89/100; Loss: 0.05610647052526474\n",
      "Step 16 (8563); Episode 89/100; Loss: 0.03943295031785965\n",
      "Step 17 (8564); Episode 89/100; Loss: 0.0024234300944954157\n",
      "Step 18 (8565); Episode 89/100; Loss: 0.050751980394124985\n",
      "Step 19 (8566); Episode 89/100; Loss: 0.0016335929976776242\n",
      "Step 20 (8567); Episode 89/100; Loss: 0.04214851185679436\n",
      "Step 21 (8568); Episode 89/100; Loss: 0.039486367255449295\n",
      "Step 22 (8569); Episode 89/100; Loss: 0.001589830033481121\n",
      "Step 23 (8570); Episode 89/100; Loss: 0.0016602990217506886\n",
      "Step 24 (8571); Episode 89/100; Loss: 0.05531485378742218\n",
      "Step 25 (8572); Episode 89/100; Loss: 0.04555436223745346\n",
      "Step 26 (8573); Episode 89/100; Loss: 0.03949378803372383\n",
      "Step 27 (8574); Episode 89/100; Loss: 0.0009064027690328658\n",
      "Step 28 (8575); Episode 89/100; Loss: 0.06821126490831375\n",
      "Step 29 (8576); Episode 89/100; Loss: 0.0013973040040582418\n",
      "Step 30 (8577); Episode 89/100; Loss: 0.056977685540914536\n",
      "Step 31 (8578); Episode 89/100; Loss: 0.04623091593384743\n",
      "Step 32 (8579); Episode 89/100; Loss: 0.0014976391103118658\n",
      "Step 33 (8580); Episode 89/100; Loss: 0.00248667411506176\n",
      "Step 34 (8581); Episode 89/100; Loss: 0.047642771154642105\n",
      "Step 35 (8582); Episode 89/100; Loss: 0.00225050444714725\n",
      "Step 36 (8583); Episode 89/100; Loss: 0.056827910244464874\n",
      "Step 37 (8584); Episode 89/100; Loss: 0.0068672168999910355\n",
      "Step 38 (8585); Episode 89/100; Loss: 0.10018734633922577\n",
      "Step 39 (8586); Episode 89/100; Loss: 0.05874199420213699\n",
      "Step 40 (8587); Episode 89/100; Loss: 0.000751508167013526\n",
      "Step 41 (8588); Episode 89/100; Loss: 0.03812795877456665\n",
      "Step 42 (8589); Episode 89/100; Loss: 0.02436099201440811\n",
      "Step 43 (8590); Episode 89/100; Loss: 0.001920325099490583\n",
      "Step 44 (8591); Episode 89/100; Loss: 0.0009069374646060169\n",
      "Step 45 (8592); Episode 89/100; Loss: 0.0013714622473344207\n",
      "Step 46 (8593); Episode 89/100; Loss: 0.143834188580513\n",
      "Step 47 (8594); Episode 89/100; Loss: 0.000601822102908045\n",
      "Step 48 (8595); Episode 89/100; Loss: 0.0018346882425248623\n",
      "Step 49 (8596); Episode 89/100; Loss: 0.04326653853058815\n",
      "Step 50 (8597); Episode 89/100; Loss: 0.0035350220277905464\n",
      "Step 51 (8598); Episode 89/100; Loss: 0.04736856743693352\n",
      "Step 52 (8599); Episode 89/100; Loss: 0.017223695293068886\n",
      "Step 53 (8600); Episode 89/100; Loss: 0.002924405736848712\n",
      "Step 54 (8601); Episode 89/100; Loss: 0.0013481647474691272\n",
      "Step 55 (8602); Episode 89/100; Loss: 0.0067579494789242744\n",
      "Step 56 (8603); Episode 89/100; Loss: 0.04896853491663933\n",
      "Step 57 (8604); Episode 89/100; Loss: 0.10004932433366776\n",
      "Step 58 (8605); Episode 89/100; Loss: 0.003333491273224354\n",
      "Step 59 (8606); Episode 89/100; Loss: 0.048913244158029556\n",
      "Step 60 (8607); Episode 89/100; Loss: 0.04979490488767624\n",
      "Step 61 (8608); Episode 89/100; Loss: 0.042174942791461945\n",
      "Step 62 (8609); Episode 89/100; Loss: 0.05037230998277664\n",
      "Step 63 (8610); Episode 89/100; Loss: 0.010849034413695335\n",
      "Step 64 (8611); Episode 89/100; Loss: 0.04502782225608826\n",
      "Step 65 (8612); Episode 89/100; Loss: 0.00175001029856503\n",
      "Step 66 (8613); Episode 89/100; Loss: 0.0027732523158192635\n",
      "Step 67 (8614); Episode 89/100; Loss: 0.041667625308036804\n",
      "Step 68 (8615); Episode 89/100; Loss: 0.043124374002218246\n",
      "Step 69 (8616); Episode 89/100; Loss: 0.0015468624187633395\n",
      "Step 70 (8617); Episode 89/100; Loss: 0.04745853692293167\n",
      "Step 71 (8618); Episode 89/100; Loss: 0.0421864315867424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 72 (8619); Episode 89/100; Loss: 0.020116090774536133\n",
      "Step 73 (8620); Episode 89/100; Loss: 0.015197508968412876\n",
      "Step 74 (8621); Episode 89/100; Loss: 0.0015786082949489355\n",
      "Step 75 (8622); Episode 89/100; Loss: 0.0010882223723456264\n",
      "Step 76 (8623); Episode 89/100; Loss: 0.0015991122927516699\n",
      "Step 77 (8624); Episode 89/100; Loss: 0.002771194791421294\n",
      "Step 78 (8625); Episode 89/100; Loss: 0.001985561102628708\n",
      "Step 79 (8626); Episode 89/100; Loss: 0.001757791731506586\n",
      "Step 80 (8627); Episode 89/100; Loss: 0.09404546022415161\n",
      "Step 81 (8628); Episode 89/100; Loss: 0.0013967155246064067\n",
      "Step 82 (8629); Episode 89/100; Loss: 0.0012205184902995825\n",
      "Step 83 (8630); Episode 89/100; Loss: 0.0016321742441505194\n",
      "Step 84 (8631); Episode 89/100; Loss: 0.02025408111512661\n",
      "Step 85 (8632); Episode 89/100; Loss: 0.046822067350149155\n",
      "Step 86 (8633); Episode 89/100; Loss: 0.005007446277886629\n",
      "Step 87 (8634); Episode 89/100; Loss: 0.001462952932342887\n",
      "Step 88 (8635); Episode 89/100; Loss: 0.04563932120800018\n",
      "Step 89 (8636); Episode 89/100; Loss: 0.10179843753576279\n",
      "Step 90 (8637); Episode 89/100; Loss: 0.09218024462461472\n",
      "Step 91 (8638); Episode 89/100; Loss: 0.002421194687485695\n",
      "Step 92 (8639); Episode 89/100; Loss: 0.0034773703664541245\n",
      "Step 93 (8640); Episode 89/100; Loss: 0.0013644261052832007\n",
      "Step 94 (8641); Episode 89/100; Loss: 0.001869149156846106\n",
      "Step 95 (8642); Episode 89/100; Loss: 0.0406356081366539\n",
      "Step 96 (8643); Episode 89/100; Loss: 0.004675583448261023\n",
      "Step 97 (8644); Episode 89/100; Loss: 0.0920252874493599\n",
      "Step 98 (8645); Episode 89/100; Loss: 0.1242004930973053\n",
      "Step 99 (8646); Episode 89/100; Loss: 0.04429304972290993\n",
      "Step 100 (8647); Episode 89/100; Loss: 0.019176406785845757\n",
      "Step 101 (8648); Episode 89/100; Loss: 0.009612193331122398\n",
      "Step 102 (8649); Episode 89/100; Loss: 0.0008753350120969117\n",
      "Step 103 (8650); Episode 89/100; Loss: 0.002302879933267832\n",
      "Step 104 (8651); Episode 89/100; Loss: 0.003313614521175623\n",
      "Step 105 (8652); Episode 89/100; Loss: 0.0007478896295651793\n",
      "Step 106 (8653); Episode 89/100; Loss: 0.04971390590071678\n",
      "Step 107 (8654); Episode 89/100; Loss: 0.00047623718273825943\n",
      "Step 108 (8655); Episode 89/100; Loss: 0.001803504885174334\n",
      "Step 109 (8656); Episode 89/100; Loss: 0.11128917336463928\n",
      "Step 110 (8657); Episode 89/100; Loss: 0.0007761897868476808\n",
      "Step 111 (8658); Episode 89/100; Loss: 0.003132982412353158\n",
      "Step 112 (8659); Episode 89/100; Loss: 0.003929183818399906\n",
      "Step 113 (8660); Episode 89/100; Loss: 0.0022699565161019564\n",
      "Step 114 (8661); Episode 89/100; Loss: 0.002239916007965803\n",
      "Step 115 (8662); Episode 89/100; Loss: 0.0009143677889369428\n",
      "Step 116 (8663); Episode 89/100; Loss: 0.0011619284050539136\n",
      "Step 117 (8664); Episode 89/100; Loss: 0.0013753600651398301\n",
      "Step 118 (8665); Episode 89/100; Loss: 0.0007140153902582824\n",
      "Step 119 (8666); Episode 89/100; Loss: 0.056294653564691544\n",
      "Step 120 (8667); Episode 89/100; Loss: 0.0005498178652487695\n",
      "Step 121 (8668); Episode 89/100; Loss: 0.0013678909745067358\n",
      "Step 122 (8669); Episode 89/100; Loss: 0.0011855352204293013\n",
      "Step 123 (8670); Episode 89/100; Loss: 0.0022172224707901478\n",
      "Step 124 (8671); Episode 89/100; Loss: 0.0011778406333178282\n",
      "Step 125 (8672); Episode 89/100; Loss: 0.046346742659807205\n",
      "Step 126 (8673); Episode 89/100; Loss: 0.001300472067669034\n",
      "Step 127 (8674); Episode 89/100; Loss: 0.0007907208055257797\n",
      "Step 128 (8675); Episode 89/100; Loss: 0.001353504485450685\n",
      "Step 129 (8676); Episode 89/100; Loss: 0.0009317983640357852\n",
      "Step 130 (8677); Episode 89/100; Loss: 0.04483022168278694\n",
      "Step 131 (8678); Episode 89/100; Loss: 0.0008589002536609769\n",
      "Step 132 (8679); Episode 89/100; Loss: 0.0029399788472801447\n",
      "Step 133 (8680); Episode 89/100; Loss: 0.0007497258484363556\n",
      "Step 134 (8681); Episode 89/100; Loss: 0.0009292447939515114\n",
      "Step 135 (8682); Episode 89/100; Loss: 0.001731647877022624\n",
      "Step 136 (8683); Episode 89/100; Loss: 0.039042163640260696\n",
      "Step 137 (8684); Episode 89/100; Loss: 0.10317711532115936\n",
      "Step 138 (8685); Episode 89/100; Loss: 0.0012716223718598485\n",
      "Step 139 (8686); Episode 89/100; Loss: 0.0924345999956131\n",
      "Step 140 (8687); Episode 89/100; Loss: 0.0015582748455926776\n",
      "Step 141 (8688); Episode 89/100; Loss: 0.004182484932243824\n",
      "Step 142 (8689); Episode 89/100; Loss: 0.04201636463403702\n",
      "Step 143 (8690); Episode 89/100; Loss: 0.0008694170392118394\n",
      "Step 144 (8691); Episode 89/100; Loss: 0.05145992338657379\n",
      "Step 145 (8692); Episode 89/100; Loss: 0.0004733307578135282\n",
      "Step 146 (8693); Episode 89/100; Loss: 0.0033211447298526764\n",
      "Step 147 (8694); Episode 89/100; Loss: 0.07928857207298279\n",
      "Step 148 (8695); Episode 89/100; Loss: 0.0008092449861578643\n",
      "Step 149 (8696); Episode 89/100; Loss: 0.002208560472354293\n",
      "Step 150 (8697); Episode 89/100; Loss: 0.0026610293425619602\n",
      "Step 151 (8698); Episode 89/100; Loss: 0.036784250289201736\n",
      "Step 152 (8699); Episode 89/100; Loss: 0.04237455502152443\n",
      "Step 153 (8700); Episode 89/100; Loss: 0.003158764448016882\n",
      "Step 154 (8701); Episode 89/100; Loss: 0.05745972320437431\n",
      "Step 155 (8702); Episode 89/100; Loss: 0.001809346373192966\n",
      "Step 156 (8703); Episode 89/100; Loss: 0.08560813218355179\n",
      "Step 157 (8704); Episode 89/100; Loss: 0.0011267410591244698\n",
      "Step 158 (8705); Episode 89/100; Loss: 0.03483177721500397\n",
      "Step 159 (8706); Episode 89/100; Loss: 0.049815695732831955\n",
      "Step 160 (8707); Episode 89/100; Loss: 0.02128562517464161\n",
      "Step 161 (8708); Episode 89/100; Loss: 0.06178894266486168\n",
      "Step 162 (8709); Episode 89/100; Loss: 0.038647543638944626\n",
      "Step 0 (8710); Episode 90/100; Loss: 0.11178063601255417\n",
      "Step 1 (8711); Episode 90/100; Loss: 0.0009003705927170813\n",
      "Step 2 (8712); Episode 90/100; Loss: 0.05319981276988983\n",
      "Step 3 (8713); Episode 90/100; Loss: 0.05335734784603119\n",
      "Step 4 (8714); Episode 90/100; Loss: 0.11017850041389465\n",
      "Step 5 (8715); Episode 90/100; Loss: 0.003483670298010111\n",
      "Step 6 (8716); Episode 90/100; Loss: 0.001913208281621337\n",
      "Step 7 (8717); Episode 90/100; Loss: 0.000783914583735168\n",
      "Step 8 (8718); Episode 90/100; Loss: 0.0014056599466130137\n",
      "Step 9 (8719); Episode 90/100; Loss: 0.0014160482678562403\n",
      "Step 10 (8720); Episode 90/100; Loss: 0.0027811978943645954\n",
      "Step 11 (8721); Episode 90/100; Loss: 0.001689460128545761\n",
      "Step 12 (8722); Episode 90/100; Loss: 0.0010769206564873457\n",
      "Step 13 (8723); Episode 90/100; Loss: 0.0011059289099648595\n",
      "Step 14 (8724); Episode 90/100; Loss: 0.0014789564302191138\n",
      "Step 15 (8725); Episode 90/100; Loss: 0.04149988666176796\n",
      "Step 16 (8726); Episode 90/100; Loss: 0.0012809026520699263\n",
      "Step 17 (8727); Episode 90/100; Loss: 0.001127132447436452\n",
      "Step 18 (8728); Episode 90/100; Loss: 0.0015082384925335646\n",
      "Step 19 (8729); Episode 90/100; Loss: 0.002394887153059244\n",
      "Step 20 (8730); Episode 90/100; Loss: 0.0010014607105404139\n",
      "Step 21 (8731); Episode 90/100; Loss: 0.0011680492898449302\n",
      "Step 22 (8732); Episode 90/100; Loss: 0.04581666365265846\n",
      "Step 23 (8733); Episode 90/100; Loss: 0.0014595170505344868\n",
      "Step 24 (8734); Episode 90/100; Loss: 0.05733673274517059\n",
      "Step 25 (8735); Episode 90/100; Loss: 0.0008646298083476722\n",
      "Step 26 (8736); Episode 90/100; Loss: 0.02500678040087223\n",
      "Step 27 (8737); Episode 90/100; Loss: 0.046395305544137955\n",
      "Step 28 (8738); Episode 90/100; Loss: 0.04656567797064781\n",
      "Step 29 (8739); Episode 90/100; Loss: 0.0006246097036637366\n",
      "Step 30 (8740); Episode 90/100; Loss: 0.05721573159098625\n",
      "Step 31 (8741); Episode 90/100; Loss: 0.0022086778189986944\n",
      "Step 32 (8742); Episode 90/100; Loss: 0.05808347463607788\n",
      "Step 33 (8743); Episode 90/100; Loss: 0.0009165940573439002\n",
      "Step 34 (8744); Episode 90/100; Loss: 0.0011562079889699817\n",
      "Step 35 (8745); Episode 90/100; Loss: 0.08781576156616211\n",
      "Step 36 (8746); Episode 90/100; Loss: 0.01201410312205553\n",
      "Step 37 (8747); Episode 90/100; Loss: 0.0008521690615452826\n",
      "Step 38 (8748); Episode 90/100; Loss: 0.0007460586493834853\n",
      "Step 39 (8749); Episode 90/100; Loss: 0.0469219833612442\n",
      "Step 40 (8750); Episode 90/100; Loss: 0.01635386049747467\n",
      "Step 41 (8751); Episode 90/100; Loss: 0.0016544481040909886\n",
      "Step 42 (8752); Episode 90/100; Loss: 0.0005130702629685402\n",
      "Step 43 (8753); Episode 90/100; Loss: 0.0022271766792982817\n",
      "Step 44 (8754); Episode 90/100; Loss: 0.058123040944337845\n",
      "Step 45 (8755); Episode 90/100; Loss: 0.042166195809841156\n",
      "Step 46 (8756); Episode 90/100; Loss: 0.08506754785776138\n",
      "Step 47 (8757); Episode 90/100; Loss: 0.07921530306339264\n",
      "Step 48 (8758); Episode 90/100; Loss: 0.000565423397347331\n",
      "Step 49 (8759); Episode 90/100; Loss: 0.003502672305330634\n",
      "Step 50 (8760); Episode 90/100; Loss: 0.05689910054206848\n",
      "Step 51 (8761); Episode 90/100; Loss: 0.05419681966304779\n",
      "Step 52 (8762); Episode 90/100; Loss: 0.09005609154701233\n",
      "Step 53 (8763); Episode 90/100; Loss: 0.052343372255563736\n",
      "Step 54 (8764); Episode 90/100; Loss: 0.04206642135977745\n",
      "Step 55 (8765); Episode 90/100; Loss: 0.04683089256286621\n",
      "Step 56 (8766); Episode 90/100; Loss: 0.0007783655310049653\n",
      "Step 57 (8767); Episode 90/100; Loss: 0.055529285222291946\n",
      "Step 58 (8768); Episode 90/100; Loss: 0.11131726205348969\n",
      "Step 59 (8769); Episode 90/100; Loss: 0.0021658488549292088\n",
      "Step 60 (8770); Episode 90/100; Loss: 0.07908204197883606\n",
      "Step 61 (8771); Episode 90/100; Loss: 0.001522265374660492\n",
      "Step 62 (8772); Episode 90/100; Loss: 0.003528024535626173\n",
      "Step 63 (8773); Episode 90/100; Loss: 0.03962548077106476\n",
      "Step 64 (8774); Episode 90/100; Loss: 0.10080241411924362\n",
      "Step 65 (8775); Episode 90/100; Loss: 0.051661357283592224\n",
      "Step 66 (8776); Episode 90/100; Loss: 0.05731767788529396\n",
      "Step 67 (8777); Episode 90/100; Loss: 0.0032376344315707684\n",
      "Step 68 (8778); Episode 90/100; Loss: 0.0005412342143245041\n",
      "Step 69 (8779); Episode 90/100; Loss: 0.0007308100466616452\n",
      "Step 70 (8780); Episode 90/100; Loss: 0.015744689851999283\n",
      "Step 71 (8781); Episode 90/100; Loss: 0.0009649760322645307\n",
      "Step 72 (8782); Episode 90/100; Loss: 0.0013703806325793266\n",
      "Step 73 (8783); Episode 90/100; Loss: 0.001679629902355373\n",
      "Step 74 (8784); Episode 90/100; Loss: 0.05900191143155098\n",
      "Step 75 (8785); Episode 90/100; Loss: 0.0013836517464369535\n",
      "Step 76 (8786); Episode 90/100; Loss: 0.002651653252542019\n",
      "Step 77 (8787); Episode 90/100; Loss: 0.0010567675344645977\n",
      "Step 78 (8788); Episode 90/100; Loss: 0.008257601410150528\n",
      "Step 79 (8789); Episode 90/100; Loss: 0.0007307243649847806\n",
      "Step 80 (8790); Episode 90/100; Loss: 0.0020354161970317364\n",
      "Step 81 (8791); Episode 90/100; Loss: 0.0006559010362252593\n",
      "Step 82 (8792); Episode 90/100; Loss: 0.0010382244363427162\n",
      "Step 83 (8793); Episode 90/100; Loss: 0.0474519319832325\n",
      "Step 84 (8794); Episode 90/100; Loss: 0.04000210016965866\n",
      "Step 85 (8795); Episode 90/100; Loss: 0.0019527404801920056\n",
      "Step 86 (8796); Episode 90/100; Loss: 0.0015998181188479066\n",
      "Step 87 (8797); Episode 90/100; Loss: 0.000438386807218194\n",
      "Step 88 (8798); Episode 90/100; Loss: 0.0014923324342817068\n",
      "Step 89 (8799); Episode 90/100; Loss: 0.05737723410129547\n",
      "Step 90 (8800); Episode 90/100; Loss: 0.0011426362907513976\n",
      "Step 91 (8801); Episode 90/100; Loss: 0.0014465950662270188\n",
      "Step 92 (8802); Episode 90/100; Loss: 0.0013292675139382482\n",
      "Step 93 (8803); Episode 90/100; Loss: 0.10637450218200684\n",
      "Step 94 (8804); Episode 90/100; Loss: 0.0010985838016495109\n",
      "Step 95 (8805); Episode 90/100; Loss: 0.00046447780914604664\n",
      "Step 96 (8806); Episode 90/100; Loss: 0.0016478208126500249\n",
      "Step 97 (8807); Episode 90/100; Loss: 0.0009196096798405051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 98 (8808); Episode 90/100; Loss: 0.053259409964084625\n",
      "Step 99 (8809); Episode 90/100; Loss: 0.006039139814674854\n",
      "Step 100 (8810); Episode 90/100; Loss: 0.001140404143370688\n",
      "Step 101 (8811); Episode 90/100; Loss: 0.0007683587609790266\n",
      "Step 102 (8812); Episode 90/100; Loss: 0.0006523545598611236\n",
      "Step 103 (8813); Episode 90/100; Loss: 0.040859416127204895\n",
      "Step 104 (8814); Episode 90/100; Loss: 0.05288618430495262\n",
      "Step 105 (8815); Episode 90/100; Loss: 0.09497412294149399\n",
      "Step 106 (8816); Episode 90/100; Loss: 0.0005724140210077167\n",
      "Step 107 (8817); Episode 90/100; Loss: 0.0011415414046496153\n",
      "Step 108 (8818); Episode 90/100; Loss: 0.0010405093198642135\n",
      "Step 109 (8819); Episode 90/100; Loss: 0.0008145570755004883\n",
      "Step 110 (8820); Episode 90/100; Loss: 0.13486172258853912\n",
      "Step 111 (8821); Episode 90/100; Loss: 0.0449184775352478\n",
      "Step 112 (8822); Episode 90/100; Loss: 0.0012108180671930313\n",
      "Step 113 (8823); Episode 90/100; Loss: 0.003619304858148098\n",
      "Step 114 (8824); Episode 90/100; Loss: 0.0022339497227221727\n",
      "Step 115 (8825); Episode 90/100; Loss: 0.035121314227581024\n",
      "Step 116 (8826); Episode 90/100; Loss: 0.045702651143074036\n",
      "Step 117 (8827); Episode 90/100; Loss: 0.048509858548641205\n",
      "Step 118 (8828); Episode 90/100; Loss: 0.0007480326457880437\n",
      "Step 119 (8829); Episode 90/100; Loss: 0.04521676525473595\n",
      "Step 120 (8830); Episode 90/100; Loss: 0.0007493146695196629\n",
      "Step 121 (8831); Episode 90/100; Loss: 0.04427175223827362\n",
      "Step 122 (8832); Episode 90/100; Loss: 0.02402348257601261\n",
      "Step 123 (8833); Episode 90/100; Loss: 0.05426082760095596\n",
      "Step 124 (8834); Episode 90/100; Loss: 0.001927318749949336\n",
      "Step 125 (8835); Episode 90/100; Loss: 0.000763855641707778\n",
      "Step 126 (8836); Episode 90/100; Loss: 0.00104324403218925\n",
      "Step 127 (8837); Episode 90/100; Loss: 0.0006318267551250756\n",
      "Step 128 (8838); Episode 90/100; Loss: 0.1543896347284317\n",
      "Step 129 (8839); Episode 90/100; Loss: 0.053596388548612595\n",
      "Step 130 (8840); Episode 90/100; Loss: 0.04035703092813492\n",
      "Step 131 (8841); Episode 90/100; Loss: 0.0015699492068961263\n",
      "Step 132 (8842); Episode 90/100; Loss: 0.0013269255869090557\n",
      "Step 133 (8843); Episode 90/100; Loss: 0.0017225241754204035\n",
      "Step 134 (8844); Episode 90/100; Loss: 0.0008583461167290807\n",
      "Step 135 (8845); Episode 90/100; Loss: 0.0026093239430338144\n",
      "Step 136 (8846); Episode 90/100; Loss: 0.13078440725803375\n",
      "Step 137 (8847); Episode 90/100; Loss: 0.0006078733131289482\n",
      "Step 138 (8848); Episode 90/100; Loss: 0.02616969309747219\n",
      "Step 139 (8849); Episode 90/100; Loss: 0.05017984285950661\n",
      "Step 140 (8850); Episode 90/100; Loss: 0.04408242553472519\n",
      "Step 141 (8851); Episode 90/100; Loss: 0.0006382664432749152\n",
      "Step 142 (8852); Episode 90/100; Loss: 0.0008762708166614175\n",
      "Step 0 (8853); Episode 91/100; Loss: 0.002644360763952136\n",
      "Step 1 (8854); Episode 91/100; Loss: 0.0010416933801025152\n",
      "Step 2 (8855); Episode 91/100; Loss: 0.09910282492637634\n",
      "Step 3 (8856); Episode 91/100; Loss: 0.007684104610234499\n",
      "Step 4 (8857); Episode 91/100; Loss: 0.0003694111364893615\n",
      "Step 5 (8858); Episode 91/100; Loss: 0.12246447056531906\n",
      "Step 6 (8859); Episode 91/100; Loss: 0.0011486613657325506\n",
      "Step 7 (8860); Episode 91/100; Loss: 0.0007838600431568921\n",
      "Step 8 (8861); Episode 91/100; Loss: 0.042523741722106934\n",
      "Step 9 (8862); Episode 91/100; Loss: 0.11712376028299332\n",
      "Step 10 (8863); Episode 91/100; Loss: 0.023584255948662758\n",
      "Step 11 (8864); Episode 91/100; Loss: 0.08077926188707352\n",
      "Step 12 (8865); Episode 91/100; Loss: 0.14417073130607605\n",
      "Step 13 (8866); Episode 91/100; Loss: 0.001780125661753118\n",
      "Step 14 (8867); Episode 91/100; Loss: 0.047971662133932114\n",
      "Step 15 (8868); Episode 91/100; Loss: 0.09247490763664246\n",
      "Step 16 (8869); Episode 91/100; Loss: 0.0007556356722488999\n",
      "Step 17 (8870); Episode 91/100; Loss: 0.0011400476796552539\n",
      "Step 18 (8871); Episode 91/100; Loss: 0.03644475340843201\n",
      "Step 19 (8872); Episode 91/100; Loss: 0.003482228610664606\n",
      "Step 20 (8873); Episode 91/100; Loss: 0.0018790836911648512\n",
      "Step 21 (8874); Episode 91/100; Loss: 0.03457076847553253\n",
      "Step 22 (8875); Episode 91/100; Loss: 0.0009541436447761953\n",
      "Step 23 (8876); Episode 91/100; Loss: 0.0007640198455192149\n",
      "Step 24 (8877); Episode 91/100; Loss: 0.008899488486349583\n",
      "Step 25 (8878); Episode 91/100; Loss: 0.0027675065211951733\n",
      "Step 26 (8879); Episode 91/100; Loss: 0.001005689613521099\n",
      "Step 27 (8880); Episode 91/100; Loss: 0.0027172884438186884\n",
      "Step 28 (8881); Episode 91/100; Loss: 0.015677224844694138\n",
      "Step 29 (8882); Episode 91/100; Loss: 0.00167400436475873\n",
      "Step 30 (8883); Episode 91/100; Loss: 0.010786150582134724\n",
      "Step 31 (8884); Episode 91/100; Loss: 0.13510988652706146\n",
      "Step 32 (8885); Episode 91/100; Loss: 0.05018151178956032\n",
      "Step 33 (8886); Episode 91/100; Loss: 0.07511605322360992\n",
      "Step 34 (8887); Episode 91/100; Loss: 0.0034370010253041983\n",
      "Step 35 (8888); Episode 91/100; Loss: 0.0827503502368927\n",
      "Step 36 (8889); Episode 91/100; Loss: 0.003111753147095442\n",
      "Step 37 (8890); Episode 91/100; Loss: 0.0015062055317685008\n",
      "Step 38 (8891); Episode 91/100; Loss: 0.0015353113412857056\n",
      "Step 39 (8892); Episode 91/100; Loss: 0.003624408971518278\n",
      "Step 40 (8893); Episode 91/100; Loss: 0.10057760775089264\n",
      "Step 41 (8894); Episode 91/100; Loss: 0.004065386019647121\n",
      "Step 42 (8895); Episode 91/100; Loss: 0.0038151703774929047\n",
      "Step 43 (8896); Episode 91/100; Loss: 0.03862274810671806\n",
      "Step 44 (8897); Episode 91/100; Loss: 0.0038475736510008574\n",
      "Step 45 (8898); Episode 91/100; Loss: 0.002787912031635642\n",
      "Step 46 (8899); Episode 91/100; Loss: 0.08077485114336014\n",
      "Step 47 (8900); Episode 91/100; Loss: 0.047133877873420715\n",
      "Step 48 (8901); Episode 91/100; Loss: 0.0019316336838528514\n",
      "Step 49 (8902); Episode 91/100; Loss: 0.04839297756552696\n",
      "Step 50 (8903); Episode 91/100; Loss: 0.003545683342963457\n",
      "Step 51 (8904); Episode 91/100; Loss: 0.039835959672927856\n",
      "Step 52 (8905); Episode 91/100; Loss: 0.0028122880030423403\n",
      "Step 53 (8906); Episode 91/100; Loss: 0.08889421075582504\n",
      "Step 54 (8907); Episode 91/100; Loss: 0.000607863359618932\n",
      "Step 55 (8908); Episode 91/100; Loss: 0.0018885517492890358\n",
      "Step 56 (8909); Episode 91/100; Loss: 0.0023402078077197075\n",
      "Step 57 (8910); Episode 91/100; Loss: 0.08788855373859406\n",
      "Step 58 (8911); Episode 91/100; Loss: 0.0019085026578977704\n",
      "Step 59 (8912); Episode 91/100; Loss: 0.05122019723057747\n",
      "Step 60 (8913); Episode 91/100; Loss: 0.05279944837093353\n",
      "Step 61 (8914); Episode 91/100; Loss: 0.0008398992940783501\n",
      "Step 62 (8915); Episode 91/100; Loss: 0.04109682887792587\n",
      "Step 63 (8916); Episode 91/100; Loss: 0.020432747900485992\n",
      "Step 64 (8917); Episode 91/100; Loss: 0.001758061465807259\n",
      "Step 65 (8918); Episode 91/100; Loss: 0.0027414376381784678\n",
      "Step 66 (8919); Episode 91/100; Loss: 0.002883222186937928\n",
      "Step 67 (8920); Episode 91/100; Loss: 0.03678407147526741\n",
      "Step 68 (8921); Episode 91/100; Loss: 0.04711929336190224\n",
      "Step 69 (8922); Episode 91/100; Loss: 0.05299369618296623\n",
      "Step 70 (8923); Episode 91/100; Loss: 0.08166249096393585\n",
      "Step 71 (8924); Episode 91/100; Loss: 0.0015736224595457315\n",
      "Step 72 (8925); Episode 91/100; Loss: 0.10118623822927475\n",
      "Step 73 (8926); Episode 91/100; Loss: 0.0458197258412838\n",
      "Step 74 (8927); Episode 91/100; Loss: 0.012145980261266232\n",
      "Step 75 (8928); Episode 91/100; Loss: 0.0028551144059747458\n",
      "Step 76 (8929); Episode 91/100; Loss: 0.0030384459532797337\n",
      "Step 77 (8930); Episode 91/100; Loss: 0.08573560416698456\n",
      "Step 78 (8931); Episode 91/100; Loss: 0.0012211001012474298\n",
      "Step 79 (8932); Episode 91/100; Loss: 0.0018487554043531418\n",
      "Step 80 (8933); Episode 91/100; Loss: 0.0014059991808608174\n",
      "Step 81 (8934); Episode 91/100; Loss: 0.05610225349664688\n",
      "Step 82 (8935); Episode 91/100; Loss: 0.045847758650779724\n",
      "Step 83 (8936); Episode 91/100; Loss: 0.03835495188832283\n",
      "Step 84 (8937); Episode 91/100; Loss: 0.0036621862091124058\n",
      "Step 85 (8938); Episode 91/100; Loss: 0.001999884145334363\n",
      "Step 86 (8939); Episode 91/100; Loss: 0.045740384608507156\n",
      "Step 87 (8940); Episode 91/100; Loss: 0.0010679764673113823\n",
      "Step 88 (8941); Episode 91/100; Loss: 0.00146689941175282\n",
      "Step 89 (8942); Episode 91/100; Loss: 0.038656894117593765\n",
      "Step 90 (8943); Episode 91/100; Loss: 0.04275345802307129\n",
      "Step 91 (8944); Episode 91/100; Loss: 0.06200582534074783\n",
      "Step 92 (8945); Episode 91/100; Loss: 0.0026173307560384274\n",
      "Step 93 (8946); Episode 91/100; Loss: 0.04600752517580986\n",
      "Step 94 (8947); Episode 91/100; Loss: 0.04291478171944618\n",
      "Step 95 (8948); Episode 91/100; Loss: 0.002326192567124963\n",
      "Step 96 (8949); Episode 91/100; Loss: 0.0018739585066214204\n",
      "Step 97 (8950); Episode 91/100; Loss: 0.045436155050992966\n",
      "Step 98 (8951); Episode 91/100; Loss: 0.010254682041704655\n",
      "Step 99 (8952); Episode 91/100; Loss: 0.004419311415404081\n",
      "Step 100 (8953); Episode 91/100; Loss: 0.0014947474701330066\n",
      "Step 101 (8954); Episode 91/100; Loss: 0.004359940066933632\n",
      "Step 102 (8955); Episode 91/100; Loss: 0.0009394338703714311\n",
      "Step 103 (8956); Episode 91/100; Loss: 0.0009914502734318376\n",
      "Step 104 (8957); Episode 91/100; Loss: 0.0006010775687173009\n",
      "Step 105 (8958); Episode 91/100; Loss: 0.0007511835428886116\n",
      "Step 106 (8959); Episode 91/100; Loss: 0.003008362604305148\n",
      "Step 107 (8960); Episode 91/100; Loss: 0.005483112297952175\n",
      "Step 108 (8961); Episode 91/100; Loss: 0.0012274588225409389\n",
      "Step 109 (8962); Episode 91/100; Loss: 0.021988868713378906\n",
      "Step 110 (8963); Episode 91/100; Loss: 0.0031983349472284317\n",
      "Step 111 (8964); Episode 91/100; Loss: 0.044420499354600906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 112 (8965); Episode 91/100; Loss: 0.058196403086185455\n",
      "Step 113 (8966); Episode 91/100; Loss: 0.001003832439891994\n",
      "Step 114 (8967); Episode 91/100; Loss: 0.09053727984428406\n",
      "Step 115 (8968); Episode 91/100; Loss: 0.019935492426156998\n",
      "Step 116 (8969); Episode 91/100; Loss: 0.0010375494603067636\n",
      "Step 117 (8970); Episode 91/100; Loss: 0.00866684503853321\n",
      "Step 118 (8971); Episode 91/100; Loss: 0.03821495920419693\n",
      "Step 119 (8972); Episode 91/100; Loss: 0.000707416154909879\n",
      "Step 120 (8973); Episode 91/100; Loss: 0.001394132967106998\n",
      "Step 121 (8974); Episode 91/100; Loss: 0.055567990988492966\n",
      "Step 122 (8975); Episode 91/100; Loss: 0.001350223203189671\n",
      "Step 123 (8976); Episode 91/100; Loss: 0.0007608287269249558\n",
      "Step 124 (8977); Episode 91/100; Loss: 0.0729326456785202\n",
      "Step 125 (8978); Episode 91/100; Loss: 0.0018432469805702567\n",
      "Step 126 (8979); Episode 91/100; Loss: 0.0020703973714262247\n",
      "Step 127 (8980); Episode 91/100; Loss: 0.05712928622961044\n",
      "Step 128 (8981); Episode 91/100; Loss: 0.09001928567886353\n",
      "Step 129 (8982); Episode 91/100; Loss: 0.05833716318011284\n",
      "Step 130 (8983); Episode 91/100; Loss: 0.0012253837194293737\n",
      "Step 131 (8984); Episode 91/100; Loss: 0.00859992764890194\n",
      "Step 132 (8985); Episode 91/100; Loss: 0.00233117351308465\n",
      "Step 133 (8986); Episode 91/100; Loss: 0.0013340885052457452\n",
      "Step 134 (8987); Episode 91/100; Loss: 0.027978669852018356\n",
      "Step 135 (8988); Episode 91/100; Loss: 0.0008209992083720863\n",
      "Step 136 (8989); Episode 91/100; Loss: 0.03160429745912552\n",
      "Step 137 (8990); Episode 91/100; Loss: 0.0026611709035933018\n",
      "Step 138 (8991); Episode 91/100; Loss: 0.10365743935108185\n",
      "Step 139 (8992); Episode 91/100; Loss: 0.022050226107239723\n",
      "Step 140 (8993); Episode 91/100; Loss: 0.0027269385755062103\n",
      "Step 141 (8994); Episode 91/100; Loss: 0.048211973160505295\n",
      "Step 142 (8995); Episode 91/100; Loss: 0.0016357648419216275\n",
      "Step 143 (8996); Episode 91/100; Loss: 0.001911472762003541\n",
      "Step 144 (8997); Episode 91/100; Loss: 0.0010210699401795864\n",
      "Step 145 (8998); Episode 91/100; Loss: 0.0007100225775502622\n",
      "Step 146 (8999); Episode 91/100; Loss: 0.0034593569580465555\n",
      "Step 147 (9000); Episode 91/100; Loss: 0.003511201124638319\n",
      "Step 148 (9001); Episode 91/100; Loss: 0.0028094593435525894\n",
      "Step 149 (9002); Episode 91/100; Loss: 0.008949465118348598\n",
      "Step 150 (9003); Episode 91/100; Loss: 0.09857242554426193\n",
      "Step 151 (9004); Episode 91/100; Loss: 0.0007283071172423661\n",
      "Step 152 (9005); Episode 91/100; Loss: 0.0014992790529504418\n",
      "Step 153 (9006); Episode 91/100; Loss: 0.012111261487007141\n",
      "Step 154 (9007); Episode 91/100; Loss: 0.0010053305886685848\n",
      "Step 155 (9008); Episode 91/100; Loss: 0.0007715192041359842\n",
      "Step 156 (9009); Episode 91/100; Loss: 0.001645225565880537\n",
      "Step 157 (9010); Episode 91/100; Loss: 0.0571829117834568\n",
      "Step 158 (9011); Episode 91/100; Loss: 0.0015559032326564193\n",
      "Step 159 (9012); Episode 91/100; Loss: 0.0029000721406191587\n",
      "Step 160 (9013); Episode 91/100; Loss: 0.0030315620824694633\n",
      "Step 161 (9014); Episode 91/100; Loss: 0.0012201807694509625\n",
      "Step 162 (9015); Episode 91/100; Loss: 0.0010768932988867164\n",
      "Step 163 (9016); Episode 91/100; Loss: 0.045912694185972214\n",
      "Step 164 (9017); Episode 91/100; Loss: 0.006250191479921341\n",
      "Step 165 (9018); Episode 91/100; Loss: 0.0011262856423854828\n",
      "Step 166 (9019); Episode 91/100; Loss: 0.04537823051214218\n",
      "Step 167 (9020); Episode 91/100; Loss: 0.081622414290905\n",
      "Step 0 (9021); Episode 92/100; Loss: 0.001575303147546947\n",
      "Step 1 (9022); Episode 92/100; Loss: 0.002382988343015313\n",
      "Step 2 (9023); Episode 92/100; Loss: 0.039500363171100616\n",
      "Step 3 (9024); Episode 92/100; Loss: 0.04342547059059143\n",
      "Step 4 (9025); Episode 92/100; Loss: 0.0014060194371268153\n",
      "Step 5 (9026); Episode 92/100; Loss: 0.08383800834417343\n",
      "Step 6 (9027); Episode 92/100; Loss: 0.0019199212547391653\n",
      "Step 7 (9028); Episode 92/100; Loss: 0.019140588119626045\n",
      "Step 8 (9029); Episode 92/100; Loss: 0.001314004766754806\n",
      "Step 9 (9030); Episode 92/100; Loss: 0.0006984144565649331\n",
      "Step 10 (9031); Episode 92/100; Loss: 0.05091754347085953\n",
      "Step 11 (9032); Episode 92/100; Loss: 0.0892757773399353\n",
      "Step 12 (9033); Episode 92/100; Loss: 0.003589176805689931\n",
      "Step 13 (9034); Episode 92/100; Loss: 0.0010537263005971909\n",
      "Step 14 (9035); Episode 92/100; Loss: 0.11307066679000854\n",
      "Step 15 (9036); Episode 92/100; Loss: 0.0013764494797214866\n",
      "Step 16 (9037); Episode 92/100; Loss: 0.00147276371717453\n",
      "Step 17 (9038); Episode 92/100; Loss: 0.039448436349630356\n",
      "Step 18 (9039); Episode 92/100; Loss: 0.0013522581430152059\n",
      "Step 19 (9040); Episode 92/100; Loss: 0.04872133582830429\n",
      "Step 20 (9041); Episode 92/100; Loss: 0.0022766240872442722\n",
      "Step 21 (9042); Episode 92/100; Loss: 0.009079180657863617\n",
      "Step 22 (9043); Episode 92/100; Loss: 0.04436512291431427\n",
      "Step 23 (9044); Episode 92/100; Loss: 0.044751204550266266\n",
      "Step 24 (9045); Episode 92/100; Loss: 0.001242705387994647\n",
      "Step 25 (9046); Episode 92/100; Loss: 0.050025686621665955\n",
      "Step 26 (9047); Episode 92/100; Loss: 0.001250302535481751\n",
      "Step 27 (9048); Episode 92/100; Loss: 0.001997164683416486\n",
      "Step 28 (9049); Episode 92/100; Loss: 0.003928945399820805\n",
      "Step 29 (9050); Episode 92/100; Loss: 0.05762786418199539\n",
      "Step 30 (9051); Episode 92/100; Loss: 0.06331000477075577\n",
      "Step 31 (9052); Episode 92/100; Loss: 0.0010499188210815191\n",
      "Step 32 (9053); Episode 92/100; Loss: 0.09397100657224655\n",
      "Step 33 (9054); Episode 92/100; Loss: 0.049532853066921234\n",
      "Step 34 (9055); Episode 92/100; Loss: 0.0593404620885849\n",
      "Step 35 (9056); Episode 92/100; Loss: 0.000810326891951263\n",
      "Step 36 (9057); Episode 92/100; Loss: 0.04733249917626381\n",
      "Step 37 (9058); Episode 92/100; Loss: 0.0014236876741051674\n",
      "Step 38 (9059); Episode 92/100; Loss: 0.0012866936158388853\n",
      "Step 39 (9060); Episode 92/100; Loss: 0.04826689884066582\n",
      "Step 40 (9061); Episode 92/100; Loss: 0.042743608355522156\n",
      "Step 41 (9062); Episode 92/100; Loss: 0.04638053849339485\n",
      "Step 42 (9063); Episode 92/100; Loss: 0.03498392924666405\n",
      "Step 43 (9064); Episode 92/100; Loss: 0.06973589956760406\n",
      "Step 44 (9065); Episode 92/100; Loss: 0.0023466777056455612\n",
      "Step 45 (9066); Episode 92/100; Loss: 0.0009035500115714967\n",
      "Step 46 (9067); Episode 92/100; Loss: 0.044673286378383636\n",
      "Step 47 (9068); Episode 92/100; Loss: 0.002404728438705206\n",
      "Step 48 (9069); Episode 92/100; Loss: 0.00280170701444149\n",
      "Step 49 (9070); Episode 92/100; Loss: 0.003480467712506652\n",
      "Step 50 (9071); Episode 92/100; Loss: 0.0026372051797807217\n",
      "Step 51 (9072); Episode 92/100; Loss: 0.09083708375692368\n",
      "Step 52 (9073); Episode 92/100; Loss: 0.0005705617368221283\n",
      "Step 53 (9074); Episode 92/100; Loss: 0.0009839654667302966\n",
      "Step 54 (9075); Episode 92/100; Loss: 0.05641356110572815\n",
      "Step 55 (9076); Episode 92/100; Loss: 0.0008280901820398867\n",
      "Step 56 (9077); Episode 92/100; Loss: 0.04186220094561577\n",
      "Step 57 (9078); Episode 92/100; Loss: 0.0031936541199684143\n",
      "Step 58 (9079); Episode 92/100; Loss: 0.001378681161440909\n",
      "Step 59 (9080); Episode 92/100; Loss: 0.003138759173452854\n",
      "Step 60 (9081); Episode 92/100; Loss: 0.0015713123138993979\n",
      "Step 61 (9082); Episode 92/100; Loss: 0.04703563451766968\n",
      "Step 62 (9083); Episode 92/100; Loss: 0.009289071895182133\n",
      "Step 63 (9084); Episode 92/100; Loss: 0.04889807850122452\n",
      "Step 64 (9085); Episode 92/100; Loss: 0.0014100497355684638\n",
      "Step 65 (9086); Episode 92/100; Loss: 0.0016658589011058211\n",
      "Step 66 (9087); Episode 92/100; Loss: 0.043105076998472214\n",
      "Step 67 (9088); Episode 92/100; Loss: 0.048766590654850006\n",
      "Step 68 (9089); Episode 92/100; Loss: 0.013738133013248444\n",
      "Step 69 (9090); Episode 92/100; Loss: 0.0024899879936128855\n",
      "Step 70 (9091); Episode 92/100; Loss: 0.028998130932450294\n",
      "Step 71 (9092); Episode 92/100; Loss: 0.08795207738876343\n",
      "Step 72 (9093); Episode 92/100; Loss: 0.04258761554956436\n",
      "Step 73 (9094); Episode 92/100; Loss: 0.003118132473900914\n",
      "Step 74 (9095); Episode 92/100; Loss: 0.001555269118398428\n",
      "Step 75 (9096); Episode 92/100; Loss: 0.0014643133617937565\n",
      "Step 76 (9097); Episode 92/100; Loss: 0.08637681603431702\n",
      "Step 77 (9098); Episode 92/100; Loss: 0.051097359508275986\n",
      "Step 78 (9099); Episode 92/100; Loss: 0.04735391587018967\n",
      "Step 79 (9100); Episode 92/100; Loss: 0.1099439263343811\n",
      "Step 80 (9101); Episode 92/100; Loss: 0.03713976964354515\n",
      "Step 81 (9102); Episode 92/100; Loss: 0.004885104019194841\n",
      "Step 82 (9103); Episode 92/100; Loss: 0.0021900534629821777\n",
      "Step 83 (9104); Episode 92/100; Loss: 0.04152928292751312\n",
      "Step 84 (9105); Episode 92/100; Loss: 0.058362968266010284\n",
      "Step 85 (9106); Episode 92/100; Loss: 0.0018499104771763086\n",
      "Step 86 (9107); Episode 92/100; Loss: 0.04122045263648033\n",
      "Step 87 (9108); Episode 92/100; Loss: 0.0008889900054782629\n",
      "Step 88 (9109); Episode 92/100; Loss: 0.0016750787617638707\n",
      "Step 89 (9110); Episode 92/100; Loss: 0.0010409546084702015\n",
      "Step 90 (9111); Episode 92/100; Loss: 0.04612353816628456\n",
      "Step 91 (9112); Episode 92/100; Loss: 0.002292443998157978\n",
      "Step 92 (9113); Episode 92/100; Loss: 0.04394347593188286\n",
      "Step 93 (9114); Episode 92/100; Loss: 0.04481833055615425\n",
      "Step 94 (9115); Episode 92/100; Loss: 0.002340785227715969\n",
      "Step 95 (9116); Episode 92/100; Loss: 0.041624803096055984\n",
      "Step 96 (9117); Episode 92/100; Loss: 0.0006150659755803645\n",
      "Step 97 (9118); Episode 92/100; Loss: 0.0012472523376345634\n",
      "Step 98 (9119); Episode 92/100; Loss: 0.019501514732837677\n",
      "Step 99 (9120); Episode 92/100; Loss: 0.002456146292388439\n",
      "Step 100 (9121); Episode 92/100; Loss: 0.04172148555517197\n",
      "Step 101 (9122); Episode 92/100; Loss: 0.005270455963909626\n",
      "Step 102 (9123); Episode 92/100; Loss: 0.0012464930769056082\n",
      "Step 103 (9124); Episode 92/100; Loss: 0.0016072971047833562\n",
      "Step 104 (9125); Episode 92/100; Loss: 0.04069104790687561\n",
      "Step 105 (9126); Episode 92/100; Loss: 0.09002764523029327\n",
      "Step 106 (9127); Episode 92/100; Loss: 0.14110678434371948\n",
      "Step 107 (9128); Episode 92/100; Loss: 0.0012637223117053509\n",
      "Step 108 (9129); Episode 92/100; Loss: 0.07581128180027008\n",
      "Step 109 (9130); Episode 92/100; Loss: 0.002034910721704364\n",
      "Step 110 (9131); Episode 92/100; Loss: 0.09940657019615173\n",
      "Step 111 (9132); Episode 92/100; Loss: 0.055526115000247955\n",
      "Step 112 (9133); Episode 92/100; Loss: 0.05842704698443413\n",
      "Step 113 (9134); Episode 92/100; Loss: 0.03087758831679821\n",
      "Step 114 (9135); Episode 92/100; Loss: 0.05069436505436897\n",
      "Step 115 (9136); Episode 92/100; Loss: 0.0012405093293637037\n",
      "Step 116 (9137); Episode 92/100; Loss: 0.0009171388810500503\n",
      "Step 117 (9138); Episode 92/100; Loss: 0.0010304105235263705\n",
      "Step 118 (9139); Episode 92/100; Loss: 0.0014953002100810409\n",
      "Step 119 (9140); Episode 92/100; Loss: 0.042785707861185074\n",
      "Step 120 (9141); Episode 92/100; Loss: 0.0016737786354497075\n",
      "Step 121 (9142); Episode 92/100; Loss: 0.039874427020549774\n",
      "Step 122 (9143); Episode 92/100; Loss: 0.10992762446403503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 123 (9144); Episode 92/100; Loss: 0.0013857630547136068\n",
      "Step 124 (9145); Episode 92/100; Loss: 0.0024020776618272066\n",
      "Step 125 (9146); Episode 92/100; Loss: 0.03713979572057724\n",
      "Step 126 (9147); Episode 92/100; Loss: 0.00207622186280787\n",
      "Step 127 (9148); Episode 92/100; Loss: 0.041572827845811844\n",
      "Step 128 (9149); Episode 92/100; Loss: 0.05762702599167824\n",
      "Step 129 (9150); Episode 92/100; Loss: 0.0012218537740409374\n",
      "Step 0 (9151); Episode 93/100; Loss: 0.05080857872962952\n",
      "Step 1 (9152); Episode 93/100; Loss: 0.0023981169797480106\n",
      "Step 2 (9153); Episode 93/100; Loss: 0.04313848540186882\n",
      "Step 3 (9154); Episode 93/100; Loss: 0.0015629070112481713\n",
      "Step 4 (9155); Episode 93/100; Loss: 0.0019239748362451792\n",
      "Step 5 (9156); Episode 93/100; Loss: 0.0005027335719205439\n",
      "Step 6 (9157); Episode 93/100; Loss: 0.005479064304381609\n",
      "Step 7 (9158); Episode 93/100; Loss: 0.04312485083937645\n",
      "Step 8 (9159); Episode 93/100; Loss: 0.03904423862695694\n",
      "Step 9 (9160); Episode 93/100; Loss: 0.05655724182724953\n",
      "Step 10 (9161); Episode 93/100; Loss: 0.08997410535812378\n",
      "Step 11 (9162); Episode 93/100; Loss: 0.0014365209499374032\n",
      "Step 12 (9163); Episode 93/100; Loss: 0.03918348252773285\n",
      "Step 13 (9164); Episode 93/100; Loss: 0.03666609153151512\n",
      "Step 14 (9165); Episode 93/100; Loss: 0.0012798986863344908\n",
      "Step 15 (9166); Episode 93/100; Loss: 0.038861069828271866\n",
      "Step 16 (9167); Episode 93/100; Loss: 0.0384337455034256\n",
      "Step 17 (9168); Episode 93/100; Loss: 0.034079283475875854\n",
      "Step 18 (9169); Episode 93/100; Loss: 0.0019648736342787743\n",
      "Step 19 (9170); Episode 93/100; Loss: 0.001039817463606596\n",
      "Step 20 (9171); Episode 93/100; Loss: 0.021324848756194115\n",
      "Step 21 (9172); Episode 93/100; Loss: 0.0013072233414277434\n",
      "Step 22 (9173); Episode 93/100; Loss: 0.03874918073415756\n",
      "Step 23 (9174); Episode 93/100; Loss: 0.002760213566944003\n",
      "Step 24 (9175); Episode 93/100; Loss: 0.0007272255606949329\n",
      "Step 25 (9176); Episode 93/100; Loss: 0.002053976058959961\n",
      "Step 26 (9177); Episode 93/100; Loss: 0.0010106746340170503\n",
      "Step 27 (9178); Episode 93/100; Loss: 0.0013278457336127758\n",
      "Step 28 (9179); Episode 93/100; Loss: 0.09753256291151047\n",
      "Step 29 (9180); Episode 93/100; Loss: 0.00353172211907804\n",
      "Step 30 (9181); Episode 93/100; Loss: 0.0013517459155991673\n",
      "Step 31 (9182); Episode 93/100; Loss: 0.04891118407249451\n",
      "Step 32 (9183); Episode 93/100; Loss: 0.0026047546416521072\n",
      "Step 33 (9184); Episode 93/100; Loss: 0.01534702256321907\n",
      "Step 34 (9185); Episode 93/100; Loss: 0.08619992434978485\n",
      "Step 35 (9186); Episode 93/100; Loss: 0.0011246419744566083\n",
      "Step 36 (9187); Episode 93/100; Loss: 0.002650802955031395\n",
      "Step 37 (9188); Episode 93/100; Loss: 0.0016348932404071093\n",
      "Step 38 (9189); Episode 93/100; Loss: 0.04768289253115654\n",
      "Step 39 (9190); Episode 93/100; Loss: 0.002489021047949791\n",
      "Step 40 (9191); Episode 93/100; Loss: 0.04352731630206108\n",
      "Step 41 (9192); Episode 93/100; Loss: 0.09148599207401276\n",
      "Step 42 (9193); Episode 93/100; Loss: 0.0033635864965617657\n",
      "Step 43 (9194); Episode 93/100; Loss: 0.002424169098958373\n",
      "Step 44 (9195); Episode 93/100; Loss: 0.009105990640819073\n",
      "Step 45 (9196); Episode 93/100; Loss: 0.0026375004090368748\n",
      "Step 46 (9197); Episode 93/100; Loss: 0.00159257254563272\n",
      "Step 47 (9198); Episode 93/100; Loss: 0.09951038658618927\n",
      "Step 48 (9199); Episode 93/100; Loss: 0.04354463517665863\n",
      "Step 49 (9200); Episode 93/100; Loss: 0.037497568875551224\n",
      "Step 50 (9201); Episode 93/100; Loss: 0.002614492317661643\n",
      "Step 51 (9202); Episode 93/100; Loss: 0.0021606427617371082\n",
      "Step 52 (9203); Episode 93/100; Loss: 0.00110064004547894\n",
      "Step 53 (9204); Episode 93/100; Loss: 0.07490938901901245\n",
      "Step 54 (9205); Episode 93/100; Loss: 0.002087800996378064\n",
      "Step 55 (9206); Episode 93/100; Loss: 0.001793937641195953\n",
      "Step 56 (9207); Episode 93/100; Loss: 0.09924435615539551\n",
      "Step 57 (9208); Episode 93/100; Loss: 0.007085469551384449\n",
      "Step 58 (9209); Episode 93/100; Loss: 0.042934346944093704\n",
      "Step 59 (9210); Episode 93/100; Loss: 0.0023251387756317854\n",
      "Step 60 (9211); Episode 93/100; Loss: 0.001198178855702281\n",
      "Step 61 (9212); Episode 93/100; Loss: 0.0013080073986202478\n",
      "Step 62 (9213); Episode 93/100; Loss: 0.0006570213008671999\n",
      "Step 63 (9214); Episode 93/100; Loss: 0.0010002899216488004\n",
      "Step 64 (9215); Episode 93/100; Loss: 0.0008154920069500804\n",
      "Step 65 (9216); Episode 93/100; Loss: 0.03263053297996521\n",
      "Step 66 (9217); Episode 93/100; Loss: 0.04248438775539398\n",
      "Step 67 (9218); Episode 93/100; Loss: 0.04609484225511551\n",
      "Step 68 (9219); Episode 93/100; Loss: 0.0013770110672339797\n",
      "Step 69 (9220); Episode 93/100; Loss: 0.0006844510207884014\n",
      "Step 70 (9221); Episode 93/100; Loss: 0.0013152812607586384\n",
      "Step 71 (9222); Episode 93/100; Loss: 0.0017666646745055914\n",
      "Step 72 (9223); Episode 93/100; Loss: 0.0013728707563132048\n",
      "Step 73 (9224); Episode 93/100; Loss: 0.0006494861445389688\n",
      "Step 74 (9225); Episode 93/100; Loss: 0.09911568462848663\n",
      "Step 75 (9226); Episode 93/100; Loss: 0.0008246881770901382\n",
      "Step 76 (9227); Episode 93/100; Loss: 0.0013174823252484202\n",
      "Step 77 (9228); Episode 93/100; Loss: 0.0013027670793235302\n",
      "Step 78 (9229); Episode 93/100; Loss: 0.040874745696783066\n",
      "Step 79 (9230); Episode 93/100; Loss: 0.09053100645542145\n",
      "Step 80 (9231); Episode 93/100; Loss: 0.0009255376644432545\n",
      "Step 81 (9232); Episode 93/100; Loss: 0.036912620067596436\n",
      "Step 82 (9233); Episode 93/100; Loss: 0.0012655237223953009\n",
      "Step 83 (9234); Episode 93/100; Loss: 0.000747230660635978\n",
      "Step 84 (9235); Episode 93/100; Loss: 0.04349503666162491\n",
      "Step 85 (9236); Episode 93/100; Loss: 0.05036785081028938\n",
      "Step 86 (9237); Episode 93/100; Loss: 0.06369906663894653\n",
      "Step 87 (9238); Episode 93/100; Loss: 0.004712873604148626\n",
      "Step 88 (9239); Episode 93/100; Loss: 0.08328485488891602\n",
      "Step 89 (9240); Episode 93/100; Loss: 0.001447123009711504\n",
      "Step 90 (9241); Episode 93/100; Loss: 0.011904306709766388\n",
      "Step 91 (9242); Episode 93/100; Loss: 0.0012712444877251983\n",
      "Step 92 (9243); Episode 93/100; Loss: 0.04763194918632507\n",
      "Step 93 (9244); Episode 93/100; Loss: 0.03516928106546402\n",
      "Step 94 (9245); Episode 93/100; Loss: 0.003876045811921358\n",
      "Step 95 (9246); Episode 93/100; Loss: 0.001635479275137186\n",
      "Step 96 (9247); Episode 93/100; Loss: 0.002445428166538477\n",
      "Step 97 (9248); Episode 93/100; Loss: 0.002276643645018339\n",
      "Step 98 (9249); Episode 93/100; Loss: 0.001089376164600253\n",
      "Step 99 (9250); Episode 93/100; Loss: 0.017272088676691055\n",
      "Step 100 (9251); Episode 93/100; Loss: 0.0013302037259563804\n",
      "Step 101 (9252); Episode 93/100; Loss: 0.049203820526599884\n",
      "Step 102 (9253); Episode 93/100; Loss: 0.0021331508178263903\n",
      "Step 103 (9254); Episode 93/100; Loss: 0.002056942554190755\n",
      "Step 104 (9255); Episode 93/100; Loss: 0.004457182716578245\n",
      "Step 105 (9256); Episode 93/100; Loss: 0.0013727358309552073\n",
      "Step 106 (9257); Episode 93/100; Loss: 0.04664142429828644\n",
      "Step 107 (9258); Episode 93/100; Loss: 0.001002592733129859\n",
      "Step 108 (9259); Episode 93/100; Loss: 0.0007899265037849545\n",
      "Step 109 (9260); Episode 93/100; Loss: 0.05112474411725998\n",
      "Step 110 (9261); Episode 93/100; Loss: 0.0027835899963974953\n",
      "Step 111 (9262); Episode 93/100; Loss: 0.0019749263301491737\n",
      "Step 112 (9263); Episode 93/100; Loss: 0.004427924752235413\n",
      "Step 113 (9264); Episode 93/100; Loss: 0.0009477586718276143\n",
      "Step 114 (9265); Episode 93/100; Loss: 0.0022422424517571926\n",
      "Step 115 (9266); Episode 93/100; Loss: 0.0012625104282051325\n",
      "Step 116 (9267); Episode 93/100; Loss: 0.0018214822048321366\n",
      "Step 117 (9268); Episode 93/100; Loss: 0.03458070755004883\n",
      "Step 118 (9269); Episode 93/100; Loss: 0.0015285077970474958\n",
      "Step 119 (9270); Episode 93/100; Loss: 0.004002979025244713\n",
      "Step 120 (9271); Episode 93/100; Loss: 0.11179034411907196\n",
      "Step 121 (9272); Episode 93/100; Loss: 0.0007866949308663607\n",
      "Step 122 (9273); Episode 93/100; Loss: 0.001523688668385148\n",
      "Step 123 (9274); Episode 93/100; Loss: 0.042854420840740204\n",
      "Step 124 (9275); Episode 93/100; Loss: 0.027612537145614624\n",
      "Step 125 (9276); Episode 93/100; Loss: 0.0012792916968464851\n",
      "Step 126 (9277); Episode 93/100; Loss: 0.05583104491233826\n",
      "Step 127 (9278); Episode 93/100; Loss: 0.042049482464790344\n",
      "Step 128 (9279); Episode 93/100; Loss: 0.000695732596796006\n",
      "Step 129 (9280); Episode 93/100; Loss: 0.0021154326386749744\n",
      "Step 130 (9281); Episode 93/100; Loss: 0.053154345601797104\n",
      "Step 131 (9282); Episode 93/100; Loss: 0.05841342732310295\n",
      "Step 132 (9283); Episode 93/100; Loss: 0.04859703779220581\n",
      "Step 133 (9284); Episode 93/100; Loss: 0.0005543215083889663\n",
      "Step 134 (9285); Episode 93/100; Loss: 0.08445743471384048\n",
      "Step 135 (9286); Episode 93/100; Loss: 0.001123742200434208\n",
      "Step 136 (9287); Episode 93/100; Loss: 0.0007807490765117109\n",
      "Step 137 (9288); Episode 93/100; Loss: 0.022598551586270332\n",
      "Step 138 (9289); Episode 93/100; Loss: 0.0765608698129654\n",
      "Step 139 (9290); Episode 93/100; Loss: 0.002922779880464077\n",
      "Step 140 (9291); Episode 93/100; Loss: 0.002385591622442007\n",
      "Step 141 (9292); Episode 93/100; Loss: 0.0005568002234213054\n",
      "Step 142 (9293); Episode 93/100; Loss: 0.052131205797195435\n",
      "Step 143 (9294); Episode 93/100; Loss: 0.02615676261484623\n",
      "Step 144 (9295); Episode 93/100; Loss: 0.0006478849682025611\n",
      "Step 145 (9296); Episode 93/100; Loss: 0.05184782296419144\n",
      "Step 146 (9297); Episode 93/100; Loss: 0.042426638305187225\n",
      "Step 147 (9298); Episode 93/100; Loss: 0.0016048257239162922\n",
      "Step 148 (9299); Episode 93/100; Loss: 0.001255309907719493\n",
      "Step 149 (9300); Episode 93/100; Loss: 0.0024362963158637285\n",
      "Step 150 (9301); Episode 93/100; Loss: 0.0012289953883737326\n",
      "Step 151 (9302); Episode 93/100; Loss: 0.0010088131530210376\n",
      "Step 152 (9303); Episode 93/100; Loss: 0.001154668745584786\n",
      "Step 153 (9304); Episode 93/100; Loss: 0.0021116884890943766\n",
      "Step 154 (9305); Episode 93/100; Loss: 0.003716196632012725\n",
      "Step 155 (9306); Episode 93/100; Loss: 0.000572557735722512\n",
      "Step 156 (9307); Episode 93/100; Loss: 0.04331992194056511\n",
      "Step 157 (9308); Episode 93/100; Loss: 0.10769672691822052\n",
      "Step 158 (9309); Episode 93/100; Loss: 0.0010850693797692657\n",
      "Step 159 (9310); Episode 93/100; Loss: 0.0033863489516079426\n",
      "Step 160 (9311); Episode 93/100; Loss: 0.04909993335604668\n",
      "Step 161 (9312); Episode 93/100; Loss: 0.0008128109620884061\n",
      "Step 162 (9313); Episode 93/100; Loss: 0.02349546179175377\n",
      "Step 163 (9314); Episode 93/100; Loss: 0.01631777174770832\n",
      "Step 164 (9315); Episode 93/100; Loss: 0.0009958278387784958\n",
      "Step 165 (9316); Episode 93/100; Loss: 0.0013273999793455005\n",
      "Step 166 (9317); Episode 93/100; Loss: 0.001960069639608264\n",
      "Step 167 (9318); Episode 93/100; Loss: 0.0018104312475770712\n",
      "Step 168 (9319); Episode 93/100; Loss: 0.0010599473025649786\n",
      "Step 169 (9320); Episode 93/100; Loss: 0.04381958022713661\n",
      "Step 170 (9321); Episode 93/100; Loss: 0.0012771968031302094\n",
      "Step 171 (9322); Episode 93/100; Loss: 0.000877940037753433\n",
      "Step 172 (9323); Episode 93/100; Loss: 0.045783527195453644\n",
      "Step 173 (9324); Episode 93/100; Loss: 0.0008011516183614731\n",
      "Step 174 (9325); Episode 93/100; Loss: 0.0019303396111354232\n",
      "Step 0 (9326); Episode 94/100; Loss: 0.001640662201680243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (9327); Episode 94/100; Loss: 0.008432072587311268\n",
      "Step 2 (9328); Episode 94/100; Loss: 0.10030659288167953\n",
      "Step 3 (9329); Episode 94/100; Loss: 0.003860699711367488\n",
      "Step 4 (9330); Episode 94/100; Loss: 0.0027482318691909313\n",
      "Step 5 (9331); Episode 94/100; Loss: 0.0006807903992012143\n",
      "Step 6 (9332); Episode 94/100; Loss: 0.03737837076187134\n",
      "Step 7 (9333); Episode 94/100; Loss: 0.04584446921944618\n",
      "Step 8 (9334); Episode 94/100; Loss: 0.0018469600472599268\n",
      "Step 9 (9335); Episode 94/100; Loss: 0.0012113986304029822\n",
      "Step 10 (9336); Episode 94/100; Loss: 0.0016752867959439754\n",
      "Step 11 (9337); Episode 94/100; Loss: 0.00442696874961257\n",
      "Step 12 (9338); Episode 94/100; Loss: 0.0011862394167110324\n",
      "Step 13 (9339); Episode 94/100; Loss: 0.036040812730789185\n",
      "Step 14 (9340); Episode 94/100; Loss: 0.0010092369047924876\n",
      "Step 15 (9341); Episode 94/100; Loss: 0.026215963065624237\n",
      "Step 16 (9342); Episode 94/100; Loss: 0.001506366184912622\n",
      "Step 17 (9343); Episode 94/100; Loss: 0.08308809995651245\n",
      "Step 18 (9344); Episode 94/100; Loss: 0.0007047744584269822\n",
      "Step 19 (9345); Episode 94/100; Loss: 0.0009367243619635701\n",
      "Step 20 (9346); Episode 94/100; Loss: 0.044196777045726776\n",
      "Step 21 (9347); Episode 94/100; Loss: 0.0006607912364415824\n",
      "Step 22 (9348); Episode 94/100; Loss: 0.0010640484979376197\n",
      "Step 23 (9349); Episode 94/100; Loss: 0.0023285788483917713\n",
      "Step 24 (9350); Episode 94/100; Loss: 0.0005813696188852191\n",
      "Step 25 (9351); Episode 94/100; Loss: 0.001105891540646553\n",
      "Step 26 (9352); Episode 94/100; Loss: 0.0038994853384792805\n",
      "Step 27 (9353); Episode 94/100; Loss: 0.0023827990517020226\n",
      "Step 28 (9354); Episode 94/100; Loss: 0.0019243973074480891\n",
      "Step 29 (9355); Episode 94/100; Loss: 0.017011286690831184\n",
      "Step 30 (9356); Episode 94/100; Loss: 0.0569416806101799\n",
      "Step 31 (9357); Episode 94/100; Loss: 0.056460343301296234\n",
      "Step 32 (9358); Episode 94/100; Loss: 0.0008427990251220763\n",
      "Step 33 (9359); Episode 94/100; Loss: 0.0009702637325972319\n",
      "Step 34 (9360); Episode 94/100; Loss: 0.0006177526665851474\n",
      "Step 35 (9361); Episode 94/100; Loss: 0.0016138608334586024\n",
      "Step 36 (9362); Episode 94/100; Loss: 0.04635360464453697\n",
      "Step 37 (9363); Episode 94/100; Loss: 0.048397742211818695\n",
      "Step 38 (9364); Episode 94/100; Loss: 0.002303246408700943\n",
      "Step 39 (9365); Episode 94/100; Loss: 0.0025961000937968493\n",
      "Step 40 (9366); Episode 94/100; Loss: 0.10007810592651367\n",
      "Step 41 (9367); Episode 94/100; Loss: 0.001274390728212893\n",
      "Step 42 (9368); Episode 94/100; Loss: 0.0016440164763480425\n",
      "Step 43 (9369); Episode 94/100; Loss: 0.049524642527103424\n",
      "Step 44 (9370); Episode 94/100; Loss: 0.041899025440216064\n",
      "Step 45 (9371); Episode 94/100; Loss: 0.04673715680837631\n",
      "Step 46 (9372); Episode 94/100; Loss: 0.0005160649889148772\n",
      "Step 47 (9373); Episode 94/100; Loss: 0.01828904263675213\n",
      "Step 48 (9374); Episode 94/100; Loss: 0.0009820081759244204\n",
      "Step 49 (9375); Episode 94/100; Loss: 0.043651215732097626\n",
      "Step 50 (9376); Episode 94/100; Loss: 0.0023326007649302483\n",
      "Step 51 (9377); Episode 94/100; Loss: 0.04483508691191673\n",
      "Step 52 (9378); Episode 94/100; Loss: 0.05042935535311699\n",
      "Step 53 (9379); Episode 94/100; Loss: 0.09453175216913223\n",
      "Step 54 (9380); Episode 94/100; Loss: 0.036384325474500656\n",
      "Step 55 (9381); Episode 94/100; Loss: 0.0012748668668791652\n",
      "Step 56 (9382); Episode 94/100; Loss: 0.1276044249534607\n",
      "Step 57 (9383); Episode 94/100; Loss: 0.04098457843065262\n",
      "Step 58 (9384); Episode 94/100; Loss: 0.002929402980953455\n",
      "Step 59 (9385); Episode 94/100; Loss: 0.009598071686923504\n",
      "Step 60 (9386); Episode 94/100; Loss: 0.035691436380147934\n",
      "Step 61 (9387); Episode 94/100; Loss: 0.0024502871092408895\n",
      "Step 62 (9388); Episode 94/100; Loss: 0.0033753300085663795\n",
      "Step 63 (9389); Episode 94/100; Loss: 0.020443903282284737\n",
      "Step 64 (9390); Episode 94/100; Loss: 0.017983688041567802\n",
      "Step 65 (9391); Episode 94/100; Loss: 0.04401594027876854\n",
      "Step 66 (9392); Episode 94/100; Loss: 0.0026075500063598156\n",
      "Step 67 (9393); Episode 94/100; Loss: 0.002219440182670951\n",
      "Step 68 (9394); Episode 94/100; Loss: 0.0008509797626174986\n",
      "Step 69 (9395); Episode 94/100; Loss: 0.0007472425349988043\n",
      "Step 70 (9396); Episode 94/100; Loss: 0.0055844830349087715\n",
      "Step 71 (9397); Episode 94/100; Loss: 0.001083292649127543\n",
      "Step 72 (9398); Episode 94/100; Loss: 0.0011141225695610046\n",
      "Step 73 (9399); Episode 94/100; Loss: 0.05366924777626991\n",
      "Step 74 (9400); Episode 94/100; Loss: 0.001415533828549087\n",
      "Step 75 (9401); Episode 94/100; Loss: 0.0010023220675066113\n",
      "Step 76 (9402); Episode 94/100; Loss: 0.0011045726714655757\n",
      "Step 77 (9403); Episode 94/100; Loss: 0.0009195441962219775\n",
      "Step 78 (9404); Episode 94/100; Loss: 0.001731599448248744\n",
      "Step 79 (9405); Episode 94/100; Loss: 0.0015788089949637651\n",
      "Step 80 (9406); Episode 94/100; Loss: 0.001465042238123715\n",
      "Step 81 (9407); Episode 94/100; Loss: 0.0017845373367890716\n",
      "Step 82 (9408); Episode 94/100; Loss: 0.0013177397195249796\n",
      "Step 83 (9409); Episode 94/100; Loss: 0.0026541082188487053\n",
      "Step 84 (9410); Episode 94/100; Loss: 0.012508601881563663\n",
      "Step 85 (9411); Episode 94/100; Loss: 0.0011222228640690446\n",
      "Step 86 (9412); Episode 94/100; Loss: 0.041248422116041183\n",
      "Step 87 (9413); Episode 94/100; Loss: 0.010393626987934113\n",
      "Step 88 (9414); Episode 94/100; Loss: 0.10294265300035477\n",
      "Step 89 (9415); Episode 94/100; Loss: 0.0014101744163781404\n",
      "Step 90 (9416); Episode 94/100; Loss: 0.0014057453954592347\n",
      "Step 91 (9417); Episode 94/100; Loss: 0.02920568734407425\n",
      "Step 92 (9418); Episode 94/100; Loss: 0.0053444006480276585\n",
      "Step 93 (9419); Episode 94/100; Loss: 0.0004995372728444636\n",
      "Step 94 (9420); Episode 94/100; Loss: 0.0017058338271453977\n",
      "Step 95 (9421); Episode 94/100; Loss: 0.0957663506269455\n",
      "Step 96 (9422); Episode 94/100; Loss: 0.0018667313270270824\n",
      "Step 97 (9423); Episode 94/100; Loss: 0.130634605884552\n",
      "Step 98 (9424); Episode 94/100; Loss: 0.0040067825466394424\n",
      "Step 99 (9425); Episode 94/100; Loss: 0.0017095794901251793\n",
      "Step 100 (9426); Episode 94/100; Loss: 0.0016366721829399467\n",
      "Step 101 (9427); Episode 94/100; Loss: 0.002071097493171692\n",
      "Step 102 (9428); Episode 94/100; Loss: 0.0018672184087336063\n",
      "Step 103 (9429); Episode 94/100; Loss: 0.0047400337643921375\n",
      "Step 104 (9430); Episode 94/100; Loss: 0.0064733256585896015\n",
      "Step 105 (9431); Episode 94/100; Loss: 0.09842295944690704\n",
      "Step 106 (9432); Episode 94/100; Loss: 0.004287314135581255\n",
      "Step 107 (9433); Episode 94/100; Loss: 0.0066922069527208805\n",
      "Step 108 (9434); Episode 94/100; Loss: 0.0022520744241774082\n",
      "Step 109 (9435); Episode 94/100; Loss: 0.0009713105391710997\n",
      "Step 110 (9436); Episode 94/100; Loss: 0.031063374131917953\n",
      "Step 111 (9437); Episode 94/100; Loss: 0.0025096482131630182\n",
      "Step 112 (9438); Episode 94/100; Loss: 0.000586304347962141\n",
      "Step 113 (9439); Episode 94/100; Loss: 0.050066255033016205\n",
      "Step 114 (9440); Episode 94/100; Loss: 0.0024091219529509544\n",
      "Step 115 (9441); Episode 94/100; Loss: 0.045640744268894196\n",
      "Step 116 (9442); Episode 94/100; Loss: 0.06541377305984497\n",
      "Step 117 (9443); Episode 94/100; Loss: 0.004956467542797327\n",
      "Step 118 (9444); Episode 94/100; Loss: 0.0006073758122511208\n",
      "Step 119 (9445); Episode 94/100; Loss: 0.048016007989645004\n",
      "Step 120 (9446); Episode 94/100; Loss: 0.001170103088952601\n",
      "Step 121 (9447); Episode 94/100; Loss: 0.0010216685477644205\n",
      "Step 122 (9448); Episode 94/100; Loss: 0.04160443693399429\n",
      "Step 123 (9449); Episode 94/100; Loss: 0.06649070978164673\n",
      "Step 124 (9450); Episode 94/100; Loss: 0.036828555166721344\n",
      "Step 125 (9451); Episode 94/100; Loss: 0.04306699335575104\n",
      "Step 126 (9452); Episode 94/100; Loss: 0.055283673107624054\n",
      "Step 127 (9453); Episode 94/100; Loss: 0.05061313509941101\n",
      "Step 128 (9454); Episode 94/100; Loss: 0.0044538783840835094\n",
      "Step 129 (9455); Episode 94/100; Loss: 0.0043968986719846725\n",
      "Step 130 (9456); Episode 94/100; Loss: 0.0836344063282013\n",
      "Step 131 (9457); Episode 94/100; Loss: 0.01487736590206623\n",
      "Step 132 (9458); Episode 94/100; Loss: 0.05351126194000244\n",
      "Step 133 (9459); Episode 94/100; Loss: 0.053531162440776825\n",
      "Step 134 (9460); Episode 94/100; Loss: 0.0023291928227990866\n",
      "Step 135 (9461); Episode 94/100; Loss: 0.0011470403987914324\n",
      "Step 136 (9462); Episode 94/100; Loss: 0.05792165920138359\n",
      "Step 137 (9463); Episode 94/100; Loss: 0.041382286697626114\n",
      "Step 138 (9464); Episode 94/100; Loss: 0.04117818921804428\n",
      "Step 139 (9465); Episode 94/100; Loss: 0.03551282733678818\n",
      "Step 140 (9466); Episode 94/100; Loss: 0.0013265577144920826\n",
      "Step 141 (9467); Episode 94/100; Loss: 0.09357722848653793\n",
      "Step 142 (9468); Episode 94/100; Loss: 0.06123823672533035\n",
      "Step 143 (9469); Episode 94/100; Loss: 0.08898137509822845\n",
      "Step 144 (9470); Episode 94/100; Loss: 0.07868713140487671\n",
      "Step 145 (9471); Episode 94/100; Loss: 0.039948947727680206\n",
      "Step 146 (9472); Episode 94/100; Loss: 0.06876872479915619\n",
      "Step 147 (9473); Episode 94/100; Loss: 0.004597820807248354\n",
      "Step 148 (9474); Episode 94/100; Loss: 0.10509409755468369\n",
      "Step 149 (9475); Episode 94/100; Loss: 0.001531897927634418\n",
      "Step 150 (9476); Episode 94/100; Loss: 0.0024696183390915394\n",
      "Step 151 (9477); Episode 94/100; Loss: 0.012775197625160217\n",
      "Step 152 (9478); Episode 94/100; Loss: 0.044798463582992554\n",
      "Step 153 (9479); Episode 94/100; Loss: 0.003613410284742713\n",
      "Step 154 (9480); Episode 94/100; Loss: 0.002153935609385371\n",
      "Step 155 (9481); Episode 94/100; Loss: 0.06452789902687073\n",
      "Step 156 (9482); Episode 94/100; Loss: 0.04268977418541908\n",
      "Step 157 (9483); Episode 94/100; Loss: 0.04716713726520538\n",
      "Step 158 (9484); Episode 94/100; Loss: 0.044468533247709274\n",
      "Step 159 (9485); Episode 94/100; Loss: 0.0036837828811258078\n",
      "Step 160 (9486); Episode 94/100; Loss: 0.014569878578186035\n",
      "Step 161 (9487); Episode 94/100; Loss: 0.0022777768317610025\n",
      "Step 162 (9488); Episode 94/100; Loss: 0.044502533972263336\n",
      "Step 163 (9489); Episode 94/100; Loss: 0.03645867481827736\n",
      "Step 164 (9490); Episode 94/100; Loss: 0.0006743362173438072\n",
      "Step 165 (9491); Episode 94/100; Loss: 0.09584429860115051\n",
      "Step 166 (9492); Episode 94/100; Loss: 0.0017469937447458506\n",
      "Step 0 (9493); Episode 95/100; Loss: 0.03303343802690506\n",
      "Step 1 (9494); Episode 95/100; Loss: 0.0026623753365129232\n",
      "Step 2 (9495); Episode 95/100; Loss: 0.0031882040202617645\n",
      "Step 3 (9496); Episode 95/100; Loss: 0.04491594433784485\n",
      "Step 4 (9497); Episode 95/100; Loss: 0.04970184341073036\n",
      "Step 5 (9498); Episode 95/100; Loss: 0.020520636811852455\n",
      "Step 6 (9499); Episode 95/100; Loss: 0.0004317406564950943\n",
      "Step 7 (9500); Episode 95/100; Loss: 0.032480694353580475\n",
      "Step 8 (9501); Episode 95/100; Loss: 0.0014768545515835285\n",
      "Step 9 (9502); Episode 95/100; Loss: 0.011600648052990437\n",
      "Step 10 (9503); Episode 95/100; Loss: 0.0030755053739994764\n",
      "Step 11 (9504); Episode 95/100; Loss: 0.002679018769413233\n",
      "Step 12 (9505); Episode 95/100; Loss: 0.047762710601091385\n",
      "Step 13 (9506); Episode 95/100; Loss: 0.0010160449892282486\n",
      "Step 14 (9507); Episode 95/100; Loss: 0.0010947190457955003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 (9508); Episode 95/100; Loss: 0.04701234772801399\n",
      "Step 16 (9509); Episode 95/100; Loss: 0.06409293413162231\n",
      "Step 17 (9510); Episode 95/100; Loss: 0.0045458413660526276\n",
      "Step 18 (9511); Episode 95/100; Loss: 0.007892930880188942\n",
      "Step 19 (9512); Episode 95/100; Loss: 0.0016190401511266828\n",
      "Step 20 (9513); Episode 95/100; Loss: 0.036577530205249786\n",
      "Step 21 (9514); Episode 95/100; Loss: 0.0012239854549989104\n",
      "Step 22 (9515); Episode 95/100; Loss: 0.0013091698056086898\n",
      "Step 23 (9516); Episode 95/100; Loss: 0.04424915462732315\n",
      "Step 24 (9517); Episode 95/100; Loss: 0.044013068079948425\n",
      "Step 25 (9518); Episode 95/100; Loss: 0.07523903995752335\n",
      "Step 26 (9519); Episode 95/100; Loss: 0.005097138229757547\n",
      "Step 27 (9520); Episode 95/100; Loss: 0.0018701791996136308\n",
      "Step 28 (9521); Episode 95/100; Loss: 0.045156899839639664\n",
      "Step 29 (9522); Episode 95/100; Loss: 0.044620778411626816\n",
      "Step 30 (9523); Episode 95/100; Loss: 0.04684174805879593\n",
      "Step 31 (9524); Episode 95/100; Loss: 0.001821849960833788\n",
      "Step 32 (9525); Episode 95/100; Loss: 0.08541969209909439\n",
      "Step 33 (9526); Episode 95/100; Loss: 0.0016427724622189999\n",
      "Step 34 (9527); Episode 95/100; Loss: 0.002137943636626005\n",
      "Step 35 (9528); Episode 95/100; Loss: 0.0007902920478954911\n",
      "Step 36 (9529); Episode 95/100; Loss: 0.0020077195949852467\n",
      "Step 37 (9530); Episode 95/100; Loss: 0.0009056827402673662\n",
      "Step 38 (9531); Episode 95/100; Loss: 0.00042543746531009674\n",
      "Step 39 (9532); Episode 95/100; Loss: 0.005136216524988413\n",
      "Step 40 (9533); Episode 95/100; Loss: 0.08617959916591644\n",
      "Step 41 (9534); Episode 95/100; Loss: 0.04597301036119461\n",
      "Step 42 (9535); Episode 95/100; Loss: 0.054801907390356064\n",
      "Step 43 (9536); Episode 95/100; Loss: 0.005822401959449053\n",
      "Step 44 (9537); Episode 95/100; Loss: 0.0011582609731703997\n",
      "Step 45 (9538); Episode 95/100; Loss: 0.02741897478699684\n",
      "Step 46 (9539); Episode 95/100; Loss: 0.04575815796852112\n",
      "Step 47 (9540); Episode 95/100; Loss: 0.003783885855227709\n",
      "Step 48 (9541); Episode 95/100; Loss: 0.055862583220005035\n",
      "Step 49 (9542); Episode 95/100; Loss: 0.002042382024228573\n",
      "Step 50 (9543); Episode 95/100; Loss: 0.0015186227392405272\n",
      "Step 51 (9544); Episode 95/100; Loss: 0.11613640189170837\n",
      "Step 52 (9545); Episode 95/100; Loss: 0.005206440109759569\n",
      "Step 53 (9546); Episode 95/100; Loss: 0.0973292663693428\n",
      "Step 54 (9547); Episode 95/100; Loss: 0.04348541796207428\n",
      "Step 55 (9548); Episode 95/100; Loss: 0.08340487629175186\n",
      "Step 56 (9549); Episode 95/100; Loss: 0.05882278457283974\n",
      "Step 57 (9550); Episode 95/100; Loss: 0.0015943110920488834\n",
      "Step 58 (9551); Episode 95/100; Loss: 0.03404543921351433\n",
      "Step 59 (9552); Episode 95/100; Loss: 0.042531803250312805\n",
      "Step 60 (9553); Episode 95/100; Loss: 0.0017612810479477048\n",
      "Step 61 (9554); Episode 95/100; Loss: 0.12150996178388596\n",
      "Step 62 (9555); Episode 95/100; Loss: 0.04081207141280174\n",
      "Step 63 (9556); Episode 95/100; Loss: 0.0018318159272894263\n",
      "Step 64 (9557); Episode 95/100; Loss: 0.03128909319639206\n",
      "Step 65 (9558); Episode 95/100; Loss: 0.0014663522597402334\n",
      "Step 66 (9559); Episode 95/100; Loss: 0.11230456829071045\n",
      "Step 67 (9560); Episode 95/100; Loss: 0.0013649098109453917\n",
      "Step 68 (9561); Episode 95/100; Loss: 0.11018310487270355\n",
      "Step 69 (9562); Episode 95/100; Loss: 0.00420283991843462\n",
      "Step 70 (9563); Episode 95/100; Loss: 0.001956298016011715\n",
      "Step 71 (9564); Episode 95/100; Loss: 0.0006891668890602887\n",
      "Step 72 (9565); Episode 95/100; Loss: 0.001830780180171132\n",
      "Step 73 (9566); Episode 95/100; Loss: 0.04858052358031273\n",
      "Step 74 (9567); Episode 95/100; Loss: 0.02636227197945118\n",
      "Step 75 (9568); Episode 95/100; Loss: 0.09859475493431091\n",
      "Step 76 (9569); Episode 95/100; Loss: 0.0019146590493619442\n",
      "Step 77 (9570); Episode 95/100; Loss: 0.13609294593334198\n",
      "Step 78 (9571); Episode 95/100; Loss: 0.0011832885211333632\n",
      "Step 79 (9572); Episode 95/100; Loss: 0.00967437494546175\n",
      "Step 80 (9573); Episode 95/100; Loss: 0.047058816999197006\n",
      "Step 81 (9574); Episode 95/100; Loss: 0.024396151304244995\n",
      "Step 82 (9575); Episode 95/100; Loss: 0.0024400497786700726\n",
      "Step 83 (9576); Episode 95/100; Loss: 0.001697274507023394\n",
      "Step 84 (9577); Episode 95/100; Loss: 0.001275234972126782\n",
      "Step 85 (9578); Episode 95/100; Loss: 0.0011589644709601998\n",
      "Step 86 (9579); Episode 95/100; Loss: 0.018219338729977608\n",
      "Step 87 (9580); Episode 95/100; Loss: 0.048818740993738174\n",
      "Step 88 (9581); Episode 95/100; Loss: 0.0016002744669094682\n",
      "Step 89 (9582); Episode 95/100; Loss: 0.04894308000802994\n",
      "Step 90 (9583); Episode 95/100; Loss: 0.002951332600787282\n",
      "Step 91 (9584); Episode 95/100; Loss: 0.04484984651207924\n",
      "Step 92 (9585); Episode 95/100; Loss: 0.05541827902197838\n",
      "Step 93 (9586); Episode 95/100; Loss: 0.0013904017396271229\n",
      "Step 94 (9587); Episode 95/100; Loss: 0.04858937859535217\n",
      "Step 95 (9588); Episode 95/100; Loss: 0.0018104523187503219\n",
      "Step 96 (9589); Episode 95/100; Loss: 0.040199678391218185\n",
      "Step 97 (9590); Episode 95/100; Loss: 0.04888338968157768\n",
      "Step 98 (9591); Episode 95/100; Loss: 0.03933097422122955\n",
      "Step 99 (9592); Episode 95/100; Loss: 0.05442550405859947\n",
      "Step 100 (9593); Episode 95/100; Loss: 0.09861702471971512\n",
      "Step 101 (9594); Episode 95/100; Loss: 0.04231901094317436\n",
      "Step 102 (9595); Episode 95/100; Loss: 0.0025867132935673\n",
      "Step 103 (9596); Episode 95/100; Loss: 0.054938774555921555\n",
      "Step 104 (9597); Episode 95/100; Loss: 0.042135339230298996\n",
      "Step 105 (9598); Episode 95/100; Loss: 0.0013530048308894038\n",
      "Step 106 (9599); Episode 95/100; Loss: 0.046288106590509415\n",
      "Step 107 (9600); Episode 95/100; Loss: 0.08369354903697968\n",
      "Step 108 (9601); Episode 95/100; Loss: 0.0010494862217456102\n",
      "Step 109 (9602); Episode 95/100; Loss: 0.0011754250153899193\n",
      "Step 110 (9603); Episode 95/100; Loss: 0.16807128489017487\n",
      "Step 111 (9604); Episode 95/100; Loss: 0.04288645461201668\n",
      "Step 112 (9605); Episode 95/100; Loss: 0.04438205808401108\n",
      "Step 113 (9606); Episode 95/100; Loss: 0.0021071715746074915\n",
      "Step 114 (9607); Episode 95/100; Loss: 0.0031728672329336405\n",
      "Step 115 (9608); Episode 95/100; Loss: 0.0015400273259729147\n",
      "Step 116 (9609); Episode 95/100; Loss: 0.0029992940835654736\n",
      "Step 117 (9610); Episode 95/100; Loss: 0.0007664100849069655\n",
      "Step 118 (9611); Episode 95/100; Loss: 0.05475710704922676\n",
      "Step 119 (9612); Episode 95/100; Loss: 0.08140424638986588\n",
      "Step 120 (9613); Episode 95/100; Loss: 0.09531185775995255\n",
      "Step 121 (9614); Episode 95/100; Loss: 0.0006989979301579297\n",
      "Step 122 (9615); Episode 95/100; Loss: 0.09868816286325455\n",
      "Step 123 (9616); Episode 95/100; Loss: 0.0012948171934112906\n",
      "Step 124 (9617); Episode 95/100; Loss: 0.0683746188879013\n",
      "Step 125 (9618); Episode 95/100; Loss: 0.11578024178743362\n",
      "Step 126 (9619); Episode 95/100; Loss: 0.03862153738737106\n",
      "Step 127 (9620); Episode 95/100; Loss: 0.0009396707755513489\n",
      "Step 128 (9621); Episode 95/100; Loss: 0.0016774031100794673\n",
      "Step 129 (9622); Episode 95/100; Loss: 0.0029655196703970432\n",
      "Step 130 (9623); Episode 95/100; Loss: 0.08506549149751663\n",
      "Step 131 (9624); Episode 95/100; Loss: 0.001963472692295909\n",
      "Step 132 (9625); Episode 95/100; Loss: 0.0020137170795351267\n",
      "Step 133 (9626); Episode 95/100; Loss: 0.05507862567901611\n",
      "Step 134 (9627); Episode 95/100; Loss: 0.045637909322977066\n",
      "Step 135 (9628); Episode 95/100; Loss: 0.0020998637191951275\n",
      "Step 136 (9629); Episode 95/100; Loss: 0.0023164902813732624\n",
      "Step 137 (9630); Episode 95/100; Loss: 0.0013461781200021505\n",
      "Step 138 (9631); Episode 95/100; Loss: 0.0007321996963582933\n",
      "Step 139 (9632); Episode 95/100; Loss: 0.02610383741557598\n",
      "Step 140 (9633); Episode 95/100; Loss: 0.0023734127171337605\n",
      "Step 141 (9634); Episode 95/100; Loss: 0.0008562647853977978\n",
      "Step 142 (9635); Episode 95/100; Loss: 0.0016183691332116723\n",
      "Step 143 (9636); Episode 95/100; Loss: 0.010788102634251118\n",
      "Step 144 (9637); Episode 95/100; Loss: 0.0008201858727261424\n",
      "Step 145 (9638); Episode 95/100; Loss: 0.044928718358278275\n",
      "Step 146 (9639); Episode 95/100; Loss: 0.0017586771864444017\n",
      "Step 147 (9640); Episode 95/100; Loss: 0.0009660269715823233\n",
      "Step 148 (9641); Episode 95/100; Loss: 0.0011702303308993578\n",
      "Step 149 (9642); Episode 95/100; Loss: 0.0032996830996125937\n",
      "Step 150 (9643); Episode 95/100; Loss: 0.004825043026357889\n",
      "Step 151 (9644); Episode 95/100; Loss: 0.001421976019628346\n",
      "Step 152 (9645); Episode 95/100; Loss: 0.04713139310479164\n",
      "Step 153 (9646); Episode 95/100; Loss: 0.0024187809322029352\n",
      "Step 154 (9647); Episode 95/100; Loss: 0.0018336776411160827\n",
      "Step 155 (9648); Episode 95/100; Loss: 0.001777025987394154\n",
      "Step 156 (9649); Episode 95/100; Loss: 0.053813524544239044\n",
      "Step 157 (9650); Episode 95/100; Loss: 0.0011690625688061118\n",
      "Step 158 (9651); Episode 95/100; Loss: 0.021606221795082092\n",
      "Step 159 (9652); Episode 95/100; Loss: 0.0015706508420407772\n",
      "Step 160 (9653); Episode 95/100; Loss: 0.0012342717964202166\n",
      "Step 161 (9654); Episode 95/100; Loss: 0.022993139922618866\n",
      "Step 162 (9655); Episode 95/100; Loss: 0.024994194507598877\n",
      "Step 163 (9656); Episode 95/100; Loss: 0.0010705876629799604\n",
      "Step 164 (9657); Episode 95/100; Loss: 0.059252504259347916\n",
      "Step 165 (9658); Episode 95/100; Loss: 0.0016575297340750694\n",
      "Step 166 (9659); Episode 95/100; Loss: 0.0007981356466189027\n",
      "Step 167 (9660); Episode 95/100; Loss: 0.0007576104835607111\n",
      "Step 168 (9661); Episode 95/100; Loss: 0.06260456889867783\n",
      "Step 169 (9662); Episode 95/100; Loss: 0.0007141332025639713\n",
      "Step 170 (9663); Episode 95/100; Loss: 0.030291613191366196\n",
      "Step 171 (9664); Episode 95/100; Loss: 0.07745618373155594\n",
      "Step 172 (9665); Episode 95/100; Loss: 0.002226955723017454\n",
      "Step 173 (9666); Episode 95/100; Loss: 0.039861809462308884\n",
      "Step 174 (9667); Episode 95/100; Loss: 0.0009599407785572112\n",
      "Step 175 (9668); Episode 95/100; Loss: 0.051681045442819595\n",
      "Step 176 (9669); Episode 95/100; Loss: 0.0018700223881751299\n",
      "Step 177 (9670); Episode 95/100; Loss: 0.0018540662713348866\n",
      "Step 178 (9671); Episode 95/100; Loss: 0.006607063114643097\n",
      "Step 179 (9672); Episode 95/100; Loss: 0.0035871181171387434\n",
      "Step 180 (9673); Episode 95/100; Loss: 0.002168650971725583\n",
      "Step 181 (9674); Episode 95/100; Loss: 0.006291534751653671\n",
      "Step 182 (9675); Episode 95/100; Loss: 0.06399639695882797\n",
      "Step 183 (9676); Episode 95/100; Loss: 0.0015930216759443283\n",
      "Step 184 (9677); Episode 95/100; Loss: 0.10845662653446198\n",
      "Step 185 (9678); Episode 95/100; Loss: 0.06076952815055847\n",
      "Step 186 (9679); Episode 95/100; Loss: 0.059948332607746124\n",
      "Step 187 (9680); Episode 95/100; Loss: 0.0013221529079601169\n",
      "Step 188 (9681); Episode 95/100; Loss: 0.0022551303263753653\n",
      "Step 189 (9682); Episode 95/100; Loss: 0.06692567467689514\n",
      "Step 190 (9683); Episode 95/100; Loss: 0.05044233053922653\n",
      "Step 191 (9684); Episode 95/100; Loss: 0.14134672284126282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 192 (9685); Episode 95/100; Loss: 0.0012239061761647463\n",
      "Step 193 (9686); Episode 95/100; Loss: 0.0005816453485749662\n",
      "Step 194 (9687); Episode 95/100; Loss: 0.0022208807058632374\n",
      "Step 195 (9688); Episode 95/100; Loss: 0.0006897593848407269\n",
      "Step 196 (9689); Episode 95/100; Loss: 0.054926756769418716\n",
      "Step 197 (9690); Episode 95/100; Loss: 0.001703205518424511\n",
      "Step 198 (9691); Episode 95/100; Loss: 0.0019106800900772214\n",
      "Step 199 (9692); Episode 95/100; Loss: 0.0006139024626463652\n",
      "Step 0 (9693); Episode 96/100; Loss: 0.0011446888092905283\n",
      "Step 1 (9694); Episode 96/100; Loss: 0.0007321504526771605\n",
      "Step 2 (9695); Episode 96/100; Loss: 0.049208637326955795\n",
      "Step 3 (9696); Episode 96/100; Loss: 0.0008360531064681709\n",
      "Step 4 (9697); Episode 96/100; Loss: 0.09388034045696259\n",
      "Step 5 (9698); Episode 96/100; Loss: 0.038955703377723694\n",
      "Step 6 (9699); Episode 96/100; Loss: 0.04317227378487587\n",
      "Step 7 (9700); Episode 96/100; Loss: 0.029452309012413025\n",
      "Step 8 (9701); Episode 96/100; Loss: 0.037730176001787186\n",
      "Step 9 (9702); Episode 96/100; Loss: 0.0010117257479578257\n",
      "Step 10 (9703); Episode 96/100; Loss: 0.0008976447861641645\n",
      "Step 11 (9704); Episode 96/100; Loss: 0.0015821197303012013\n",
      "Step 12 (9705); Episode 96/100; Loss: 0.038098860532045364\n",
      "Step 13 (9706); Episode 96/100; Loss: 0.0027704962994903326\n",
      "Step 14 (9707); Episode 96/100; Loss: 0.07866145670413971\n",
      "Step 15 (9708); Episode 96/100; Loss: 0.045892518013715744\n",
      "Step 16 (9709); Episode 96/100; Loss: 0.0025207996368408203\n",
      "Step 17 (9710); Episode 96/100; Loss: 0.0074860937893390656\n",
      "Step 18 (9711); Episode 96/100; Loss: 0.03174431994557381\n",
      "Step 19 (9712); Episode 96/100; Loss: 0.0064238691702485085\n",
      "Step 20 (9713); Episode 96/100; Loss: 0.07457606494426727\n",
      "Step 21 (9714); Episode 96/100; Loss: 0.0022003527265042067\n",
      "Step 22 (9715); Episode 96/100; Loss: 0.0011793049052357674\n",
      "Step 23 (9716); Episode 96/100; Loss: 0.006323653273284435\n",
      "Step 24 (9717); Episode 96/100; Loss: 0.05085150897502899\n",
      "Step 25 (9718); Episode 96/100; Loss: 0.0025338262785226107\n",
      "Step 26 (9719); Episode 96/100; Loss: 0.002601080108433962\n",
      "Step 27 (9720); Episode 96/100; Loss: 0.035564228892326355\n",
      "Step 28 (9721); Episode 96/100; Loss: 0.0016367286443710327\n",
      "Step 29 (9722); Episode 96/100; Loss: 0.0022979588247835636\n",
      "Step 30 (9723); Episode 96/100; Loss: 0.11708240956068039\n",
      "Step 31 (9724); Episode 96/100; Loss: 0.05858970806002617\n",
      "Step 32 (9725); Episode 96/100; Loss: 0.002998646115884185\n",
      "Step 33 (9726); Episode 96/100; Loss: 0.002539775101467967\n",
      "Step 34 (9727); Episode 96/100; Loss: 0.003366599790751934\n",
      "Step 35 (9728); Episode 96/100; Loss: 0.0844816192984581\n",
      "Step 36 (9729); Episode 96/100; Loss: 0.0010219516698271036\n",
      "Step 37 (9730); Episode 96/100; Loss: 0.039114758372306824\n",
      "Step 38 (9731); Episode 96/100; Loss: 0.0025519428309053183\n",
      "Step 39 (9732); Episode 96/100; Loss: 0.0010435531148687005\n",
      "Step 40 (9733); Episode 96/100; Loss: 0.08411551266908646\n",
      "Step 41 (9734); Episode 96/100; Loss: 0.002478281268849969\n",
      "Step 42 (9735); Episode 96/100; Loss: 0.03404047340154648\n",
      "Step 43 (9736); Episode 96/100; Loss: 0.0007439558976329863\n",
      "Step 44 (9737); Episode 96/100; Loss: 0.0020112614147365093\n",
      "Step 45 (9738); Episode 96/100; Loss: 0.05455141142010689\n",
      "Step 46 (9739); Episode 96/100; Loss: 0.04206416755914688\n",
      "Step 47 (9740); Episode 96/100; Loss: 0.0013925241073593497\n",
      "Step 48 (9741); Episode 96/100; Loss: 0.1361592411994934\n",
      "Step 49 (9742); Episode 96/100; Loss: 0.0013903715880587697\n",
      "Step 50 (9743); Episode 96/100; Loss: 0.04539116099476814\n",
      "Step 51 (9744); Episode 96/100; Loss: 0.0004138684889767319\n",
      "Step 52 (9745); Episode 96/100; Loss: 0.028258340433239937\n",
      "Step 53 (9746); Episode 96/100; Loss: 0.04925383999943733\n",
      "Step 54 (9747); Episode 96/100; Loss: 0.06411634385585785\n",
      "Step 55 (9748); Episode 96/100; Loss: 0.002299692714586854\n",
      "Step 56 (9749); Episode 96/100; Loss: 0.048240773379802704\n",
      "Step 57 (9750); Episode 96/100; Loss: 0.050601668655872345\n",
      "Step 58 (9751); Episode 96/100; Loss: 0.0006941712344996631\n",
      "Step 59 (9752); Episode 96/100; Loss: 0.001654186169616878\n",
      "Step 60 (9753); Episode 96/100; Loss: 0.04326273500919342\n",
      "Step 61 (9754); Episode 96/100; Loss: 0.0017944879364222288\n",
      "Step 62 (9755); Episode 96/100; Loss: 0.0008692876435816288\n",
      "Step 63 (9756); Episode 96/100; Loss: 0.0010457729222252965\n",
      "Step 64 (9757); Episode 96/100; Loss: 0.0004687197506427765\n",
      "Step 65 (9758); Episode 96/100; Loss: 0.0010552809108048677\n",
      "Step 66 (9759); Episode 96/100; Loss: 0.04353867843747139\n",
      "Step 67 (9760); Episode 96/100; Loss: 0.012922177091240883\n",
      "Step 68 (9761); Episode 96/100; Loss: 0.0013387547805905342\n",
      "Step 69 (9762); Episode 96/100; Loss: 0.020253846421837807\n",
      "Step 70 (9763); Episode 96/100; Loss: 0.04872802272439003\n",
      "Step 71 (9764); Episode 96/100; Loss: 0.0016405548667535186\n",
      "Step 72 (9765); Episode 96/100; Loss: 0.045179061591625214\n",
      "Step 73 (9766); Episode 96/100; Loss: 0.0014209728688001633\n",
      "Step 74 (9767); Episode 96/100; Loss: 0.015243098139762878\n",
      "Step 75 (9768); Episode 96/100; Loss: 0.0015666225226595998\n",
      "Step 76 (9769); Episode 96/100; Loss: 0.14669397473335266\n",
      "Step 77 (9770); Episode 96/100; Loss: 0.00032927034772001207\n",
      "Step 78 (9771); Episode 96/100; Loss: 0.0030389009043574333\n",
      "Step 79 (9772); Episode 96/100; Loss: 0.0011591240763664246\n",
      "Step 80 (9773); Episode 96/100; Loss: 0.0006875377730466425\n",
      "Step 81 (9774); Episode 96/100; Loss: 0.07513689249753952\n",
      "Step 82 (9775); Episode 96/100; Loss: 0.04623882472515106\n",
      "Step 83 (9776); Episode 96/100; Loss: 0.00541229872033\n",
      "Step 84 (9777); Episode 96/100; Loss: 0.041506219655275345\n",
      "Step 85 (9778); Episode 96/100; Loss: 0.04020285978913307\n",
      "Step 86 (9779); Episode 96/100; Loss: 0.07392561435699463\n",
      "Step 87 (9780); Episode 96/100; Loss: 0.03923977538943291\n",
      "Step 88 (9781); Episode 96/100; Loss: 0.0017746021039783955\n",
      "Step 89 (9782); Episode 96/100; Loss: 0.01650775410234928\n",
      "Step 90 (9783); Episode 96/100; Loss: 0.003145553870126605\n",
      "Step 91 (9784); Episode 96/100; Loss: 0.03797508776187897\n",
      "Step 92 (9785); Episode 96/100; Loss: 0.0028345161117613316\n",
      "Step 93 (9786); Episode 96/100; Loss: 0.01752038672566414\n",
      "Step 94 (9787); Episode 96/100; Loss: 0.108897365629673\n",
      "Step 95 (9788); Episode 96/100; Loss: 0.0021014350932091475\n",
      "Step 96 (9789); Episode 96/100; Loss: 0.04383295774459839\n",
      "Step 97 (9790); Episode 96/100; Loss: 0.042894791811704636\n",
      "Step 98 (9791); Episode 96/100; Loss: 0.002328217262402177\n",
      "Step 99 (9792); Episode 96/100; Loss: 0.01583406887948513\n",
      "Step 100 (9793); Episode 96/100; Loss: 0.05436088517308235\n",
      "Step 101 (9794); Episode 96/100; Loss: 0.0012344835558906198\n",
      "Step 102 (9795); Episode 96/100; Loss: 0.001382532762363553\n",
      "Step 103 (9796); Episode 96/100; Loss: 0.0011125270975753665\n",
      "Step 104 (9797); Episode 96/100; Loss: 0.06500666588544846\n",
      "Step 105 (9798); Episode 96/100; Loss: 0.04057350382208824\n",
      "Step 106 (9799); Episode 96/100; Loss: 0.04718390852212906\n",
      "Step 107 (9800); Episode 96/100; Loss: 0.0008959602564573288\n",
      "Step 108 (9801); Episode 96/100; Loss: 0.0026230381336063147\n",
      "Step 109 (9802); Episode 96/100; Loss: 0.003593796631321311\n",
      "Step 110 (9803); Episode 96/100; Loss: 0.04624355211853981\n",
      "Step 111 (9804); Episode 96/100; Loss: 0.004414196126163006\n",
      "Step 112 (9805); Episode 96/100; Loss: 0.009689709171652794\n",
      "Step 113 (9806); Episode 96/100; Loss: 0.040971141308546066\n",
      "Step 114 (9807); Episode 96/100; Loss: 0.0912555605173111\n",
      "Step 115 (9808); Episode 96/100; Loss: 0.0006653322489000857\n",
      "Step 116 (9809); Episode 96/100; Loss: 0.04093918576836586\n",
      "Step 117 (9810); Episode 96/100; Loss: 0.0017558214021846652\n",
      "Step 118 (9811); Episode 96/100; Loss: 0.018483083695173264\n",
      "Step 119 (9812); Episode 96/100; Loss: 0.04437229782342911\n",
      "Step 120 (9813); Episode 96/100; Loss: 0.04787827655673027\n",
      "Step 121 (9814); Episode 96/100; Loss: 0.012025793083012104\n",
      "Step 122 (9815); Episode 96/100; Loss: 0.0005981478607282043\n",
      "Step 123 (9816); Episode 96/100; Loss: 0.0008283915813080966\n",
      "Step 124 (9817); Episode 96/100; Loss: 0.09308240562677383\n",
      "Step 125 (9818); Episode 96/100; Loss: 0.0005003882106393576\n",
      "Step 126 (9819); Episode 96/100; Loss: 0.04717930778861046\n",
      "Step 127 (9820); Episode 96/100; Loss: 0.03352660685777664\n",
      "Step 128 (9821); Episode 96/100; Loss: 0.0009825685992836952\n",
      "Step 129 (9822); Episode 96/100; Loss: 0.09253814816474915\n",
      "Step 130 (9823); Episode 96/100; Loss: 0.0009688591817393899\n",
      "Step 131 (9824); Episode 96/100; Loss: 0.013394842855632305\n",
      "Step 132 (9825); Episode 96/100; Loss: 0.005330247338861227\n",
      "Step 133 (9826); Episode 96/100; Loss: 0.0007994864718057215\n",
      "Step 134 (9827); Episode 96/100; Loss: 0.04291677474975586\n",
      "Step 135 (9828); Episode 96/100; Loss: 0.04841047525405884\n",
      "Step 136 (9829); Episode 96/100; Loss: 0.00045820328523404896\n",
      "Step 137 (9830); Episode 96/100; Loss: 0.0014037590008229017\n",
      "Step 138 (9831); Episode 96/100; Loss: 0.055519040673971176\n",
      "Step 139 (9832); Episode 96/100; Loss: 0.045408181846141815\n",
      "Step 140 (9833); Episode 96/100; Loss: 0.0018339694943279028\n",
      "Step 141 (9834); Episode 96/100; Loss: 0.04906614124774933\n",
      "Step 142 (9835); Episode 96/100; Loss: 0.0015072883106768131\n",
      "Step 143 (9836); Episode 96/100; Loss: 0.003056608373299241\n",
      "Step 144 (9837); Episode 96/100; Loss: 0.004407222848385572\n",
      "Step 145 (9838); Episode 96/100; Loss: 0.09331440180540085\n",
      "Step 146 (9839); Episode 96/100; Loss: 0.002071259543299675\n",
      "Step 147 (9840); Episode 96/100; Loss: 0.07162695378065109\n",
      "Step 148 (9841); Episode 96/100; Loss: 0.04664567857980728\n",
      "Step 149 (9842); Episode 96/100; Loss: 0.0007092141313478351\n",
      "Step 150 (9843); Episode 96/100; Loss: 0.0017693231347948313\n",
      "Step 151 (9844); Episode 96/100; Loss: 0.0560951754450798\n",
      "Step 152 (9845); Episode 96/100; Loss: 0.0019420189782977104\n",
      "Step 153 (9846); Episode 96/100; Loss: 0.05025136098265648\n",
      "Step 154 (9847); Episode 96/100; Loss: 0.0013137749629095197\n",
      "Step 155 (9848); Episode 96/100; Loss: 0.040981605648994446\n",
      "Step 0 (9849); Episode 97/100; Loss: 0.0012184090446680784\n",
      "Step 1 (9850); Episode 97/100; Loss: 0.01224343292415142\n",
      "Step 2 (9851); Episode 97/100; Loss: 0.0006231529987417161\n",
      "Step 3 (9852); Episode 97/100; Loss: 0.0017768933903425932\n",
      "Step 4 (9853); Episode 97/100; Loss: 0.042335476726293564\n",
      "Step 5 (9854); Episode 97/100; Loss: 0.05646280199289322\n",
      "Step 6 (9855); Episode 97/100; Loss: 0.0012935521081089973\n",
      "Step 7 (9856); Episode 97/100; Loss: 0.0017858680803328753\n",
      "Step 8 (9857); Episode 97/100; Loss: 0.04774530231952667\n",
      "Step 9 (9858); Episode 97/100; Loss: 0.05107414722442627\n",
      "Step 10 (9859); Episode 97/100; Loss: 0.044700365513563156\n",
      "Step 11 (9860); Episode 97/100; Loss: 0.05386336147785187\n",
      "Step 12 (9861); Episode 97/100; Loss: 0.005189085379242897\n",
      "Step 13 (9862); Episode 97/100; Loss: 0.004443670157343149\n",
      "Step 14 (9863); Episode 97/100; Loss: 0.08476541936397552\n",
      "Step 15 (9864); Episode 97/100; Loss: 0.0016027716919779778\n",
      "Step 16 (9865); Episode 97/100; Loss: 0.04312865063548088\n",
      "Step 17 (9866); Episode 97/100; Loss: 0.05864959582686424\n",
      "Step 18 (9867); Episode 97/100; Loss: 0.0014080412220209837\n",
      "Step 19 (9868); Episode 97/100; Loss: 0.05646466463804245\n",
      "Step 20 (9869); Episode 97/100; Loss: 0.03807063028216362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21 (9870); Episode 97/100; Loss: 0.01148518268018961\n",
      "Step 22 (9871); Episode 97/100; Loss: 0.03239544481039047\n",
      "Step 23 (9872); Episode 97/100; Loss: 0.08432266861200333\n",
      "Step 24 (9873); Episode 97/100; Loss: 0.016395606100559235\n",
      "Step 25 (9874); Episode 97/100; Loss: 0.04026535525918007\n",
      "Step 26 (9875); Episode 97/100; Loss: 0.00046377943363040686\n",
      "Step 27 (9876); Episode 97/100; Loss: 0.0016582509269937873\n",
      "Step 28 (9877); Episode 97/100; Loss: 0.0021771006286144257\n",
      "Step 29 (9878); Episode 97/100; Loss: 0.019884221255779266\n",
      "Step 30 (9879); Episode 97/100; Loss: 0.015309259295463562\n",
      "Step 31 (9880); Episode 97/100; Loss: 0.0019691563211381435\n",
      "Step 32 (9881); Episode 97/100; Loss: 0.002387243090197444\n",
      "Step 33 (9882); Episode 97/100; Loss: 0.11394887417554855\n",
      "Step 34 (9883); Episode 97/100; Loss: 0.05327068269252777\n",
      "Step 35 (9884); Episode 97/100; Loss: 0.07226590812206268\n",
      "Step 36 (9885); Episode 97/100; Loss: 0.03939427062869072\n",
      "Step 37 (9886); Episode 97/100; Loss: 0.05228262394666672\n",
      "Step 38 (9887); Episode 97/100; Loss: 0.0023263851180672646\n",
      "Step 39 (9888); Episode 97/100; Loss: 0.09888078272342682\n",
      "Step 40 (9889); Episode 97/100; Loss: 0.002237573964521289\n",
      "Step 41 (9890); Episode 97/100; Loss: 0.0012723255204036832\n",
      "Step 42 (9891); Episode 97/100; Loss: 0.05701316520571709\n",
      "Step 43 (9892); Episode 97/100; Loss: 0.0522645078599453\n",
      "Step 44 (9893); Episode 97/100; Loss: 0.00299772247672081\n",
      "Step 45 (9894); Episode 97/100; Loss: 0.00240038032643497\n",
      "Step 46 (9895); Episode 97/100; Loss: 0.001695874147117138\n",
      "Step 47 (9896); Episode 97/100; Loss: 0.0015304272528737783\n",
      "Step 48 (9897); Episode 97/100; Loss: 0.013961619697511196\n",
      "Step 49 (9898); Episode 97/100; Loss: 0.020183401182293892\n",
      "Step 50 (9899); Episode 97/100; Loss: 0.0006688864668831229\n",
      "Step 51 (9900); Episode 97/100; Loss: 0.0018302558455616236\n",
      "Step 52 (9901); Episode 97/100; Loss: 0.0013713565422222018\n",
      "Step 53 (9902); Episode 97/100; Loss: 0.0012062484165653586\n",
      "Step 54 (9903); Episode 97/100; Loss: 0.0018869773484766483\n",
      "Step 55 (9904); Episode 97/100; Loss: 0.002220895839855075\n",
      "Step 56 (9905); Episode 97/100; Loss: 0.002930666320025921\n",
      "Step 57 (9906); Episode 97/100; Loss: 0.04321381077170372\n",
      "Step 58 (9907); Episode 97/100; Loss: 0.0009007215267047286\n",
      "Step 59 (9908); Episode 97/100; Loss: 0.0009191252756863832\n",
      "Step 60 (9909); Episode 97/100; Loss: 0.0009013948729261756\n",
      "Step 61 (9910); Episode 97/100; Loss: 0.004065645858645439\n",
      "Step 62 (9911); Episode 97/100; Loss: 0.048792365938425064\n",
      "Step 63 (9912); Episode 97/100; Loss: 0.0013891177950426936\n",
      "Step 64 (9913); Episode 97/100; Loss: 0.0010634851641952991\n",
      "Step 65 (9914); Episode 97/100; Loss: 0.0007651310879737139\n",
      "Step 66 (9915); Episode 97/100; Loss: 0.001878032460808754\n",
      "Step 67 (9916); Episode 97/100; Loss: 0.00032366893719881773\n",
      "Step 68 (9917); Episode 97/100; Loss: 0.0009614660521037877\n",
      "Step 69 (9918); Episode 97/100; Loss: 0.0008543245494365692\n",
      "Step 70 (9919); Episode 97/100; Loss: 0.0007264937157742679\n",
      "Step 71 (9920); Episode 97/100; Loss: 0.0008679241873323917\n",
      "Step 72 (9921); Episode 97/100; Loss: 0.048109497874975204\n",
      "Step 73 (9922); Episode 97/100; Loss: 0.0021509556099772453\n",
      "Step 74 (9923); Episode 97/100; Loss: 0.0016794389812275767\n",
      "Step 75 (9924); Episode 97/100; Loss: 0.0008736084564588964\n",
      "Step 76 (9925); Episode 97/100; Loss: 0.0013786533381789923\n",
      "Step 77 (9926); Episode 97/100; Loss: 0.058278266340494156\n",
      "Step 78 (9927); Episode 97/100; Loss: 0.04429154843091965\n",
      "Step 79 (9928); Episode 97/100; Loss: 0.00144860427826643\n",
      "Step 80 (9929); Episode 97/100; Loss: 0.0029143933206796646\n",
      "Step 81 (9930); Episode 97/100; Loss: 0.009668419137597084\n",
      "Step 82 (9931); Episode 97/100; Loss: 0.0025750771164894104\n",
      "Step 83 (9932); Episode 97/100; Loss: 0.04643324390053749\n",
      "Step 84 (9933); Episode 97/100; Loss: 0.0009507854119874537\n",
      "Step 85 (9934); Episode 97/100; Loss: 0.05019092187285423\n",
      "Step 86 (9935); Episode 97/100; Loss: 0.0014572764048352838\n",
      "Step 87 (9936); Episode 97/100; Loss: 0.001743349595926702\n",
      "Step 88 (9937); Episode 97/100; Loss: 0.0873732715845108\n",
      "Step 89 (9938); Episode 97/100; Loss: 0.05025476962327957\n",
      "Step 90 (9939); Episode 97/100; Loss: 0.001231442322023213\n",
      "Step 91 (9940); Episode 97/100; Loss: 0.0018144784262403846\n",
      "Step 92 (9941); Episode 97/100; Loss: 0.10948257148265839\n",
      "Step 93 (9942); Episode 97/100; Loss: 0.0014857719652354717\n",
      "Step 94 (9943); Episode 97/100; Loss: 0.0015212246216833591\n",
      "Step 95 (9944); Episode 97/100; Loss: 0.0012553356355056167\n",
      "Step 96 (9945); Episode 97/100; Loss: 0.03513577580451965\n",
      "Step 97 (9946); Episode 97/100; Loss: 0.004008042626082897\n",
      "Step 98 (9947); Episode 97/100; Loss: 0.0013317338889464736\n",
      "Step 99 (9948); Episode 97/100; Loss: 0.0017035036580637097\n",
      "Step 100 (9949); Episode 97/100; Loss: 0.038680657744407654\n",
      "Step 101 (9950); Episode 97/100; Loss: 0.05735304206609726\n",
      "Step 102 (9951); Episode 97/100; Loss: 0.04587765410542488\n",
      "Step 103 (9952); Episode 97/100; Loss: 0.0013022698694840074\n",
      "Step 104 (9953); Episode 97/100; Loss: 0.004674416035413742\n",
      "Step 105 (9954); Episode 97/100; Loss: 0.04721653461456299\n",
      "Step 106 (9955); Episode 97/100; Loss: 0.04319761320948601\n",
      "Step 107 (9956); Episode 97/100; Loss: 0.0018343314295634627\n",
      "Step 108 (9957); Episode 97/100; Loss: 0.0009520729654468596\n",
      "Step 109 (9958); Episode 97/100; Loss: 0.0005897036753594875\n",
      "Step 110 (9959); Episode 97/100; Loss: 0.1024286225438118\n",
      "Step 111 (9960); Episode 97/100; Loss: 0.0011588019551709294\n",
      "Step 112 (9961); Episode 97/100; Loss: 0.0017549882177263498\n",
      "Step 113 (9962); Episode 97/100; Loss: 0.05567019060254097\n",
      "Step 114 (9963); Episode 97/100; Loss: 0.0027158453594893217\n",
      "Step 115 (9964); Episode 97/100; Loss: 0.060131579637527466\n",
      "Step 116 (9965); Episode 97/100; Loss: 0.08175015449523926\n",
      "Step 117 (9966); Episode 97/100; Loss: 0.0015042510349303484\n",
      "Step 118 (9967); Episode 97/100; Loss: 0.10813075304031372\n",
      "Step 119 (9968); Episode 97/100; Loss: 0.0005501546547748148\n",
      "Step 120 (9969); Episode 97/100; Loss: 0.004168575629591942\n",
      "Step 121 (9970); Episode 97/100; Loss: 0.0012421609135344625\n",
      "Step 122 (9971); Episode 97/100; Loss: 0.0005238704616203904\n",
      "Step 123 (9972); Episode 97/100; Loss: 0.0005245018983259797\n",
      "Step 124 (9973); Episode 97/100; Loss: 0.0564609058201313\n",
      "Step 125 (9974); Episode 97/100; Loss: 0.0015197470784187317\n",
      "Step 126 (9975); Episode 97/100; Loss: 0.08593292534351349\n",
      "Step 127 (9976); Episode 97/100; Loss: 0.0022474087309092283\n",
      "Step 128 (9977); Episode 97/100; Loss: 0.10023132711648941\n",
      "Step 129 (9978); Episode 97/100; Loss: 0.03711719810962677\n",
      "Step 130 (9979); Episode 97/100; Loss: 0.05707572400569916\n",
      "Step 131 (9980); Episode 97/100; Loss: 0.04126818850636482\n",
      "Step 132 (9981); Episode 97/100; Loss: 0.0012657196493819356\n",
      "Step 133 (9982); Episode 97/100; Loss: 0.003307613544166088\n",
      "Step 134 (9983); Episode 97/100; Loss: 0.002438765950500965\n",
      "Step 135 (9984); Episode 97/100; Loss: 0.002853283192962408\n",
      "Step 136 (9985); Episode 97/100; Loss: 0.0030445833690464497\n",
      "Step 137 (9986); Episode 97/100; Loss: 0.019720571115612984\n",
      "Step 138 (9987); Episode 97/100; Loss: 0.0016993819735944271\n",
      "Step 139 (9988); Episode 97/100; Loss: 0.0015055970288813114\n",
      "Step 140 (9989); Episode 97/100; Loss: 0.0507986880838871\n",
      "Step 141 (9990); Episode 97/100; Loss: 0.0458456315100193\n",
      "Step 142 (9991); Episode 97/100; Loss: 0.0011478009400889277\n",
      "Step 143 (9992); Episode 97/100; Loss: 0.0016312592197209597\n",
      "Step 144 (9993); Episode 97/100; Loss: 0.01976046711206436\n",
      "Step 145 (9994); Episode 97/100; Loss: 0.0049163708463311195\n",
      "Step 146 (9995); Episode 97/100; Loss: 0.0012911180965602398\n",
      "Step 147 (9996); Episode 97/100; Loss: 0.046782419085502625\n",
      "Step 148 (9997); Episode 97/100; Loss: 0.03261776268482208\n",
      "Step 149 (9998); Episode 97/100; Loss: 0.04471069574356079\n",
      "Step 150 (9999); Episode 97/100; Loss: 0.08703602105379105\n",
      "Step 151 (10000); Episode 97/100; Loss: 0.0023399621713906527\n",
      "Step 0 (10001); Episode 98/100; Loss: 0.0015270364237949252\n",
      "Step 1 (10002); Episode 98/100; Loss: 0.002099078381434083\n",
      "Step 2 (10003); Episode 98/100; Loss: 0.04536246880888939\n",
      "Step 3 (10004); Episode 98/100; Loss: 0.030022289603948593\n",
      "Step 4 (10005); Episode 98/100; Loss: 0.04860859364271164\n",
      "Step 5 (10006); Episode 98/100; Loss: 0.000797314802184701\n",
      "Step 6 (10007); Episode 98/100; Loss: 0.0007473760051652789\n",
      "Step 7 (10008); Episode 98/100; Loss: 0.10088825225830078\n",
      "Step 8 (10009); Episode 98/100; Loss: 0.00107279559597373\n",
      "Step 9 (10010); Episode 98/100; Loss: 0.001738073187880218\n",
      "Step 10 (10011); Episode 98/100; Loss: 0.0003158663457725197\n",
      "Step 11 (10012); Episode 98/100; Loss: 0.0018843779107555747\n",
      "Step 12 (10013); Episode 98/100; Loss: 0.0005952559877187014\n",
      "Step 13 (10014); Episode 98/100; Loss: 0.14878389239311218\n",
      "Step 14 (10015); Episode 98/100; Loss: 0.08322860300540924\n",
      "Step 15 (10016); Episode 98/100; Loss: 0.0035553877241909504\n",
      "Step 16 (10017); Episode 98/100; Loss: 0.001221513724885881\n",
      "Step 17 (10018); Episode 98/100; Loss: 0.0006218276685103774\n",
      "Step 18 (10019); Episode 98/100; Loss: 0.041440680623054504\n",
      "Step 19 (10020); Episode 98/100; Loss: 0.10044027864933014\n",
      "Step 20 (10021); Episode 98/100; Loss: 0.000488203892018646\n",
      "Step 21 (10022); Episode 98/100; Loss: 0.0014652691315859556\n",
      "Step 22 (10023); Episode 98/100; Loss: 0.0013784101465716958\n",
      "Step 23 (10024); Episode 98/100; Loss: 0.03754427656531334\n",
      "Step 24 (10025); Episode 98/100; Loss: 0.0008691688417457044\n",
      "Step 25 (10026); Episode 98/100; Loss: 0.0002776785986497998\n",
      "Step 26 (10027); Episode 98/100; Loss: 0.0009117070003412664\n",
      "Step 27 (10028); Episode 98/100; Loss: 0.002548510441556573\n",
      "Step 28 (10029); Episode 98/100; Loss: 0.05138148367404938\n",
      "Step 29 (10030); Episode 98/100; Loss: 0.00213774130679667\n",
      "Step 30 (10031); Episode 98/100; Loss: 0.0007733659003861248\n",
      "Step 31 (10032); Episode 98/100; Loss: 0.04128042608499527\n",
      "Step 32 (10033); Episode 98/100; Loss: 0.0004088413843419403\n",
      "Step 33 (10034); Episode 98/100; Loss: 0.07602068781852722\n",
      "Step 34 (10035); Episode 98/100; Loss: 0.04813940078020096\n",
      "Step 35 (10036); Episode 98/100; Loss: 0.09065541625022888\n",
      "Step 36 (10037); Episode 98/100; Loss: 0.0018801686819642782\n",
      "Step 37 (10038); Episode 98/100; Loss: 0.07233516871929169\n",
      "Step 38 (10039); Episode 98/100; Loss: 0.0013804197078570724\n",
      "Step 39 (10040); Episode 98/100; Loss: 0.046131204813718796\n",
      "Step 40 (10041); Episode 98/100; Loss: 0.0005075349472463131\n",
      "Step 41 (10042); Episode 98/100; Loss: 0.0004124346887692809\n",
      "Step 42 (10043); Episode 98/100; Loss: 0.0008018659427762032\n",
      "Step 43 (10044); Episode 98/100; Loss: 0.002090100198984146\n",
      "Step 44 (10045); Episode 98/100; Loss: 0.0005032829358242452\n",
      "Step 45 (10046); Episode 98/100; Loss: 0.04657846316695213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46 (10047); Episode 98/100; Loss: 0.0853191614151001\n",
      "Step 47 (10048); Episode 98/100; Loss: 0.0009787498274818063\n",
      "Step 48 (10049); Episode 98/100; Loss: 0.04068128019571304\n",
      "Step 49 (10050); Episode 98/100; Loss: 0.01559426449239254\n",
      "Step 50 (10051); Episode 98/100; Loss: 0.00320799951441586\n",
      "Step 51 (10052); Episode 98/100; Loss: 0.008381486870348454\n",
      "Step 52 (10053); Episode 98/100; Loss: 0.0018068704521283507\n",
      "Step 53 (10054); Episode 98/100; Loss: 0.08408821374177933\n",
      "Step 54 (10055); Episode 98/100; Loss: 0.03343378007411957\n",
      "Step 55 (10056); Episode 98/100; Loss: 0.0007888046093285084\n",
      "Step 56 (10057); Episode 98/100; Loss: 0.0944836363196373\n",
      "Step 57 (10058); Episode 98/100; Loss: 0.11522629112005234\n",
      "Step 58 (10059); Episode 98/100; Loss: 0.0006139031029306352\n",
      "Step 59 (10060); Episode 98/100; Loss: 0.004751518368721008\n",
      "Step 60 (10061); Episode 98/100; Loss: 0.002700241282582283\n",
      "Step 61 (10062); Episode 98/100; Loss: 0.0525725893676281\n",
      "Step 62 (10063); Episode 98/100; Loss: 0.05670039355754852\n",
      "Step 63 (10064); Episode 98/100; Loss: 0.04205039143562317\n",
      "Step 64 (10065); Episode 98/100; Loss: 0.0007581223617307842\n",
      "Step 65 (10066); Episode 98/100; Loss: 0.0016026436351239681\n",
      "Step 66 (10067); Episode 98/100; Loss: 0.04870588704943657\n",
      "Step 67 (10068); Episode 98/100; Loss: 0.001885729841887951\n",
      "Step 68 (10069); Episode 98/100; Loss: 0.04032447934150696\n",
      "Step 69 (10070); Episode 98/100; Loss: 0.04443696513772011\n",
      "Step 70 (10071); Episode 98/100; Loss: 0.0014361533103510737\n",
      "Step 71 (10072); Episode 98/100; Loss: 0.049178171902894974\n",
      "Step 72 (10073); Episode 98/100; Loss: 0.002424580045044422\n",
      "Step 73 (10074); Episode 98/100; Loss: 0.09248019009828568\n",
      "Step 74 (10075); Episode 98/100; Loss: 0.0015278158243745565\n",
      "Step 75 (10076); Episode 98/100; Loss: 0.002001630375161767\n",
      "Step 76 (10077); Episode 98/100; Loss: 0.04461725428700447\n",
      "Step 77 (10078); Episode 98/100; Loss: 0.002003913978114724\n",
      "Step 78 (10079); Episode 98/100; Loss: 0.02279134839773178\n",
      "Step 79 (10080); Episode 98/100; Loss: 0.001856412971392274\n",
      "Step 80 (10081); Episode 98/100; Loss: 0.041036102920770645\n",
      "Step 81 (10082); Episode 98/100; Loss: 0.05314945802092552\n",
      "Step 82 (10083); Episode 98/100; Loss: 0.0007125571719370782\n",
      "Step 83 (10084); Episode 98/100; Loss: 0.0007655840599909425\n",
      "Step 84 (10085); Episode 98/100; Loss: 0.0008375072502531111\n",
      "Step 85 (10086); Episode 98/100; Loss: 0.0007935684989206493\n",
      "Step 86 (10087); Episode 98/100; Loss: 0.043069709092378616\n",
      "Step 87 (10088); Episode 98/100; Loss: 0.041161056607961655\n",
      "Step 88 (10089); Episode 98/100; Loss: 0.05878234654664993\n",
      "Step 89 (10090); Episode 98/100; Loss: 0.0479595884680748\n",
      "Step 90 (10091); Episode 98/100; Loss: 0.002263322938233614\n",
      "Step 91 (10092); Episode 98/100; Loss: 0.04130666330456734\n",
      "Step 92 (10093); Episode 98/100; Loss: 0.0038344240747392178\n",
      "Step 93 (10094); Episode 98/100; Loss: 0.08020095527172089\n",
      "Step 94 (10095); Episode 98/100; Loss: 0.0016306056641042233\n",
      "Step 95 (10096); Episode 98/100; Loss: 0.0007746724877506495\n",
      "Step 96 (10097); Episode 98/100; Loss: 0.03398435190320015\n",
      "Step 97 (10098); Episode 98/100; Loss: 0.0029818806797266006\n",
      "Step 98 (10099); Episode 98/100; Loss: 0.002362453378736973\n",
      "Step 99 (10100); Episode 98/100; Loss: 0.0008814847678877413\n",
      "Step 100 (10101); Episode 98/100; Loss: 0.0027634252328425646\n",
      "Step 101 (10102); Episode 98/100; Loss: 0.05107777565717697\n",
      "Step 102 (10103); Episode 98/100; Loss: 0.0011686929501593113\n",
      "Step 103 (10104); Episode 98/100; Loss: 0.03883909434080124\n",
      "Step 104 (10105); Episode 98/100; Loss: 0.07330088317394257\n",
      "Step 105 (10106); Episode 98/100; Loss: 0.0442940928041935\n",
      "Step 106 (10107); Episode 98/100; Loss: 0.03910401090979576\n",
      "Step 107 (10108); Episode 98/100; Loss: 0.0048207687214016914\n",
      "Step 108 (10109); Episode 98/100; Loss: 0.08135101944208145\n",
      "Step 109 (10110); Episode 98/100; Loss: 0.08255887776613235\n",
      "Step 110 (10111); Episode 98/100; Loss: 0.0021334742195904255\n",
      "Step 111 (10112); Episode 98/100; Loss: 0.04228867590427399\n",
      "Step 112 (10113); Episode 98/100; Loss: 0.0047507272101938725\n",
      "Step 113 (10114); Episode 98/100; Loss: 0.017266012728214264\n",
      "Step 114 (10115); Episode 98/100; Loss: 0.006671573966741562\n",
      "Step 115 (10116); Episode 98/100; Loss: 0.07221488654613495\n",
      "Step 116 (10117); Episode 98/100; Loss: 0.004188867285847664\n",
      "Step 117 (10118); Episode 98/100; Loss: 0.0018749578157439828\n",
      "Step 118 (10119); Episode 98/100; Loss: 0.002295127836987376\n",
      "Step 119 (10120); Episode 98/100; Loss: 0.0030824514105916023\n",
      "Step 120 (10121); Episode 98/100; Loss: 0.034150123596191406\n",
      "Step 121 (10122); Episode 98/100; Loss: 0.0012375960359349847\n",
      "Step 122 (10123); Episode 98/100; Loss: 0.03906579315662384\n",
      "Step 123 (10124); Episode 98/100; Loss: 0.04085196554660797\n",
      "Step 124 (10125); Episode 98/100; Loss: 0.04586568847298622\n",
      "Step 125 (10126); Episode 98/100; Loss: 0.03901498764753342\n",
      "Step 126 (10127); Episode 98/100; Loss: 0.05208999291062355\n",
      "Step 127 (10128); Episode 98/100; Loss: 0.057560015469789505\n",
      "Step 128 (10129); Episode 98/100; Loss: 0.03789437189698219\n",
      "Step 129 (10130); Episode 98/100; Loss: 0.03544893488287926\n",
      "Step 130 (10131); Episode 98/100; Loss: 0.0022783703170716763\n",
      "Step 131 (10132); Episode 98/100; Loss: 0.04259273782372475\n",
      "Step 132 (10133); Episode 98/100; Loss: 0.002074928255751729\n",
      "Step 133 (10134); Episode 98/100; Loss: 0.057327501475811005\n",
      "Step 134 (10135); Episode 98/100; Loss: 0.031052106991410255\n",
      "Step 135 (10136); Episode 98/100; Loss: 0.05595981702208519\n",
      "Step 136 (10137); Episode 98/100; Loss: 0.003199232742190361\n",
      "Step 137 (10138); Episode 98/100; Loss: 0.0031387433409690857\n",
      "Step 138 (10139); Episode 98/100; Loss: 0.0026475100312381983\n",
      "Step 139 (10140); Episode 98/100; Loss: 0.0008033200283534825\n",
      "Step 140 (10141); Episode 98/100; Loss: 0.005003328900784254\n",
      "Step 141 (10142); Episode 98/100; Loss: 0.00475161150097847\n",
      "Step 142 (10143); Episode 98/100; Loss: 0.002736733527854085\n",
      "Step 143 (10144); Episode 98/100; Loss: 0.04482618719339371\n",
      "Step 144 (10145); Episode 98/100; Loss: 0.09828133881092072\n",
      "Step 145 (10146); Episode 98/100; Loss: 0.0029505928978323936\n",
      "Step 146 (10147); Episode 98/100; Loss: 0.0016361527377739549\n",
      "Step 147 (10148); Episode 98/100; Loss: 0.0011942742858082056\n",
      "Step 148 (10149); Episode 98/100; Loss: 0.00205886154435575\n",
      "Step 149 (10150); Episode 98/100; Loss: 0.0017806292744353414\n",
      "Step 150 (10151); Episode 98/100; Loss: 0.044421639293432236\n",
      "Step 151 (10152); Episode 98/100; Loss: 0.07337744534015656\n",
      "Step 152 (10153); Episode 98/100; Loss: 0.0024315635673701763\n",
      "Step 153 (10154); Episode 98/100; Loss: 0.0032493979670107365\n",
      "Step 154 (10155); Episode 98/100; Loss: 0.008417196571826935\n",
      "Step 155 (10156); Episode 98/100; Loss: 0.0012333402410149574\n",
      "Step 156 (10157); Episode 98/100; Loss: 0.05160265043377876\n",
      "Step 157 (10158); Episode 98/100; Loss: 0.0013169398298487067\n",
      "Step 0 (10159); Episode 99/100; Loss: 0.0014521150151267648\n",
      "Step 1 (10160); Episode 99/100; Loss: 0.002796481130644679\n",
      "Step 2 (10161); Episode 99/100; Loss: 0.0037795708049088717\n",
      "Step 3 (10162); Episode 99/100; Loss: 0.0314040407538414\n",
      "Step 4 (10163); Episode 99/100; Loss: 0.0935947522521019\n",
      "Step 5 (10164); Episode 99/100; Loss: 0.0012423759326338768\n",
      "Step 6 (10165); Episode 99/100; Loss: 0.001891677500680089\n",
      "Step 7 (10166); Episode 99/100; Loss: 0.049304988235235214\n",
      "Step 8 (10167); Episode 99/100; Loss: 0.0530993826687336\n",
      "Step 9 (10168); Episode 99/100; Loss: 0.01801873929798603\n",
      "Step 10 (10169); Episode 99/100; Loss: 0.06203092262148857\n",
      "Step 11 (10170); Episode 99/100; Loss: 0.0016148933209478855\n",
      "Step 12 (10171); Episode 99/100; Loss: 0.0034287627786397934\n",
      "Step 13 (10172); Episode 99/100; Loss: 0.05786849558353424\n",
      "Step 14 (10173); Episode 99/100; Loss: 0.0010624862043187022\n",
      "Step 15 (10174); Episode 99/100; Loss: 0.003337625879794359\n",
      "Step 16 (10175); Episode 99/100; Loss: 0.002638068748638034\n",
      "Step 17 (10176); Episode 99/100; Loss: 0.0007162928231991827\n",
      "Step 18 (10177); Episode 99/100; Loss: 0.0012972709955647588\n",
      "Step 19 (10178); Episode 99/100; Loss: 0.0023247860372066498\n",
      "Step 20 (10179); Episode 99/100; Loss: 0.0009136132430285215\n",
      "Step 21 (10180); Episode 99/100; Loss: 0.0009715757332742214\n",
      "Step 22 (10181); Episode 99/100; Loss: 0.045860741287469864\n",
      "Step 23 (10182); Episode 99/100; Loss: 0.04536082223057747\n",
      "Step 24 (10183); Episode 99/100; Loss: 0.0007705710595473647\n",
      "Step 25 (10184); Episode 99/100; Loss: 0.05074997618794441\n",
      "Step 26 (10185); Episode 99/100; Loss: 0.0020710795652121305\n",
      "Step 27 (10186); Episode 99/100; Loss: 0.0010302389273419976\n",
      "Step 28 (10187); Episode 99/100; Loss: 0.0018260396318510175\n",
      "Step 29 (10188); Episode 99/100; Loss: 0.03569269925355911\n",
      "Step 30 (10189); Episode 99/100; Loss: 0.050794314593076706\n",
      "Step 31 (10190); Episode 99/100; Loss: 0.008335508406162262\n",
      "Step 32 (10191); Episode 99/100; Loss: 0.047577787190675735\n",
      "Step 33 (10192); Episode 99/100; Loss: 0.045166585594415665\n",
      "Step 34 (10193); Episode 99/100; Loss: 0.001598228933289647\n",
      "Step 35 (10194); Episode 99/100; Loss: 0.047797612845897675\n",
      "Step 36 (10195); Episode 99/100; Loss: 0.007877596653997898\n",
      "Step 37 (10196); Episode 99/100; Loss: 0.002379643963649869\n",
      "Step 38 (10197); Episode 99/100; Loss: 0.059685688465833664\n",
      "Step 39 (10198); Episode 99/100; Loss: 0.007056690752506256\n",
      "Step 40 (10199); Episode 99/100; Loss: 0.0012645351234823465\n",
      "Step 41 (10200); Episode 99/100; Loss: 0.0010820392053574324\n",
      "Step 42 (10201); Episode 99/100; Loss: 0.049217864871025085\n",
      "Step 43 (10202); Episode 99/100; Loss: 0.005617863032966852\n",
      "Step 44 (10203); Episode 99/100; Loss: 0.0007065048557706177\n",
      "Step 45 (10204); Episode 99/100; Loss: 0.08500120043754578\n",
      "Step 46 (10205); Episode 99/100; Loss: 0.12707631289958954\n",
      "Step 47 (10206); Episode 99/100; Loss: 0.09096410125494003\n",
      "Step 48 (10207); Episode 99/100; Loss: 0.0013884243089705706\n",
      "Step 49 (10208); Episode 99/100; Loss: 0.04838663339614868\n",
      "Step 50 (10209); Episode 99/100; Loss: 0.0018586835358291864\n",
      "Step 51 (10210); Episode 99/100; Loss: 0.0047022015787661076\n",
      "Step 52 (10211); Episode 99/100; Loss: 0.03405109420418739\n",
      "Step 53 (10212); Episode 99/100; Loss: 0.005621586926281452\n",
      "Step 54 (10213); Episode 99/100; Loss: 0.06265915185213089\n",
      "Step 55 (10214); Episode 99/100; Loss: 0.00208460446447134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56 (10215); Episode 99/100; Loss: 0.0016975480830296874\n",
      "Step 57 (10216); Episode 99/100; Loss: 0.05945237725973129\n",
      "Step 58 (10217); Episode 99/100; Loss: 0.0016074032755568624\n",
      "Step 59 (10218); Episode 99/100; Loss: 0.0027593711856752634\n",
      "Step 60 (10219); Episode 99/100; Loss: 0.0018425099551677704\n",
      "Step 61 (10220); Episode 99/100; Loss: 0.003030742285773158\n",
      "Step 62 (10221); Episode 99/100; Loss: 0.002981407567858696\n",
      "Step 63 (10222); Episode 99/100; Loss: 0.048322323709726334\n",
      "Step 64 (10223); Episode 99/100; Loss: 0.0393410325050354\n",
      "Step 65 (10224); Episode 99/100; Loss: 0.050812978297472\n",
      "Step 66 (10225); Episode 99/100; Loss: 0.08722206205129623\n",
      "Step 67 (10226); Episode 99/100; Loss: 0.041076015681028366\n",
      "Step 68 (10227); Episode 99/100; Loss: 0.03823743015527725\n",
      "Step 69 (10228); Episode 99/100; Loss: 0.0010543836979195476\n",
      "Step 70 (10229); Episode 99/100; Loss: 0.0012858854606747627\n",
      "Step 71 (10230); Episode 99/100; Loss: 0.0034536465536803007\n",
      "Step 72 (10231); Episode 99/100; Loss: 0.08737600594758987\n",
      "Step 73 (10232); Episode 99/100; Loss: 0.0011696001747623086\n",
      "Step 74 (10233); Episode 99/100; Loss: 0.00206113513559103\n",
      "Step 75 (10234); Episode 99/100; Loss: 0.09474936872720718\n",
      "Step 76 (10235); Episode 99/100; Loss: 0.0029280756134539843\n",
      "Step 77 (10236); Episode 99/100; Loss: 0.05441342294216156\n",
      "Step 78 (10237); Episode 99/100; Loss: 0.0019239677349105477\n",
      "Step 79 (10238); Episode 99/100; Loss: 0.0021983326878398657\n",
      "Step 80 (10239); Episode 99/100; Loss: 0.0020203902386128902\n",
      "Step 81 (10240); Episode 99/100; Loss: 0.0013463024515658617\n",
      "Step 82 (10241); Episode 99/100; Loss: 0.0008824833785183728\n",
      "Step 83 (10242); Episode 99/100; Loss: 0.04166233539581299\n",
      "Step 84 (10243); Episode 99/100; Loss: 0.0008550563943572342\n",
      "Step 85 (10244); Episode 99/100; Loss: 0.04931202158331871\n",
      "Step 86 (10245); Episode 99/100; Loss: 0.04771842807531357\n",
      "Step 87 (10246); Episode 99/100; Loss: 0.057417888194322586\n",
      "Step 88 (10247); Episode 99/100; Loss: 0.02945137582719326\n",
      "Step 89 (10248); Episode 99/100; Loss: 0.042577940970659256\n",
      "Step 90 (10249); Episode 99/100; Loss: 0.001791604794561863\n",
      "Step 91 (10250); Episode 99/100; Loss: 0.08548390865325928\n",
      "Step 92 (10251); Episode 99/100; Loss: 0.0005063140415586531\n",
      "Step 93 (10252); Episode 99/100; Loss: 0.0008907412993721664\n",
      "Step 94 (10253); Episode 99/100; Loss: 0.04130123183131218\n",
      "Step 95 (10254); Episode 99/100; Loss: 0.04080398008227348\n",
      "Step 96 (10255); Episode 99/100; Loss: 0.001098460634239018\n",
      "Step 97 (10256); Episode 99/100; Loss: 0.0019045424414798617\n",
      "Step 98 (10257); Episode 99/100; Loss: 0.057028044015169144\n",
      "Step 99 (10258); Episode 99/100; Loss: 0.0004093774768989533\n",
      "Step 100 (10259); Episode 99/100; Loss: 0.0035594720393419266\n",
      "Step 101 (10260); Episode 99/100; Loss: 0.07578472048044205\n",
      "Step 102 (10261); Episode 99/100; Loss: 0.0042264582589268684\n",
      "Step 103 (10262); Episode 99/100; Loss: 0.04563228785991669\n",
      "Step 104 (10263); Episode 99/100; Loss: 0.04729943722486496\n",
      "Step 105 (10264); Episode 99/100; Loss: 0.04839262738823891\n",
      "Step 106 (10265); Episode 99/100; Loss: 0.0019721335265785456\n",
      "Step 107 (10266); Episode 99/100; Loss: 0.0014057387597858906\n",
      "Step 108 (10267); Episode 99/100; Loss: 0.002044981112703681\n",
      "Step 109 (10268); Episode 99/100; Loss: 0.05637139827013016\n",
      "Step 110 (10269); Episode 99/100; Loss: 0.0030196327716112137\n",
      "Step 111 (10270); Episode 99/100; Loss: 0.0024301724042743444\n",
      "Step 112 (10271); Episode 99/100; Loss: 0.0024094737600535154\n",
      "Step 113 (10272); Episode 99/100; Loss: 0.002494022250175476\n",
      "Step 114 (10273); Episode 99/100; Loss: 0.0008351085125468671\n",
      "Step 115 (10274); Episode 99/100; Loss: 0.0019215377978980541\n",
      "Step 116 (10275); Episode 99/100; Loss: 0.04970217123627663\n",
      "Step 117 (10276); Episode 99/100; Loss: 0.0014211857924237847\n",
      "Step 118 (10277); Episode 99/100; Loss: 0.017293578013777733\n",
      "Step 119 (10278); Episode 99/100; Loss: 0.0006541626062244177\n",
      "Step 120 (10279); Episode 99/100; Loss: 0.03760022297501564\n",
      "Step 121 (10280); Episode 99/100; Loss: 0.044346198439598083\n",
      "Step 122 (10281); Episode 99/100; Loss: 0.000943782040849328\n",
      "Step 123 (10282); Episode 99/100; Loss: 0.0007469187839888036\n",
      "Step 124 (10283); Episode 99/100; Loss: 0.001238568453118205\n",
      "Step 125 (10284); Episode 99/100; Loss: 0.0011660419404506683\n",
      "Step 126 (10285); Episode 99/100; Loss: 0.0026954589411616325\n",
      "Step 127 (10286); Episode 99/100; Loss: 0.0015957950381562114\n",
      "Step 128 (10287); Episode 99/100; Loss: 0.04341316223144531\n",
      "Step 129 (10288); Episode 99/100; Loss: 0.0022098699118942022\n",
      "Step 130 (10289); Episode 99/100; Loss: 0.00036060676211491227\n",
      "Step 131 (10290); Episode 99/100; Loss: 0.04105459898710251\n",
      "Step 132 (10291); Episode 99/100; Loss: 0.0007440460612997413\n",
      "Step 133 (10292); Episode 99/100; Loss: 0.0019345857435837388\n",
      "Step 134 (10293); Episode 99/100; Loss: 0.0032136335503309965\n",
      "Step 135 (10294); Episode 99/100; Loss: 0.009941741824150085\n",
      "Step 136 (10295); Episode 99/100; Loss: 0.043709103018045425\n",
      "Step 137 (10296); Episode 99/100; Loss: 0.0019396821735426784\n",
      "Step 138 (10297); Episode 99/100; Loss: 0.0019453249406069517\n",
      "Step 139 (10298); Episode 99/100; Loss: 0.04118899628520012\n",
      "Step 140 (10299); Episode 99/100; Loss: 0.044112563133239746\n",
      "Step 141 (10300); Episode 99/100; Loss: 0.0026717090513557196\n",
      "Step 142 (10301); Episode 99/100; Loss: 0.0010340362787246704\n",
      "Step 143 (10302); Episode 99/100; Loss: 0.0007896166061982512\n",
      "Step 144 (10303); Episode 99/100; Loss: 0.002434872090816498\n",
      "Step 145 (10304); Episode 99/100; Loss: 0.04302617907524109\n",
      "Step 146 (10305); Episode 99/100; Loss: 0.05766252800822258\n",
      "Step 147 (10306); Episode 99/100; Loss: 0.05534352362155914\n",
      "Step 148 (10307); Episode 99/100; Loss: 0.11132894456386566\n",
      "Step 149 (10308); Episode 99/100; Loss: 0.000577668659389019\n",
      "Step 150 (10309); Episode 99/100; Loss: 0.001264285994693637\n",
      "Step 151 (10310); Episode 99/100; Loss: 0.038825202733278275\n",
      "Step 152 (10311); Episode 99/100; Loss: 0.0020084581337869167\n",
      "Step 153 (10312); Episode 99/100; Loss: 0.0010813463013619184\n",
      "Step 0 (10313); Episode 100/100; Loss: 0.04796459153294563\n",
      "Step 1 (10314); Episode 100/100; Loss: 0.0811082199215889\n",
      "Step 2 (10315); Episode 100/100; Loss: 0.0017025197157636285\n",
      "Step 3 (10316); Episode 100/100; Loss: 0.0016525221290066838\n",
      "Step 4 (10317); Episode 100/100; Loss: 0.002319352701306343\n",
      "Step 5 (10318); Episode 100/100; Loss: 0.0008589957724325359\n",
      "Step 6 (10319); Episode 100/100; Loss: 0.0018983435584232211\n",
      "Step 7 (10320); Episode 100/100; Loss: 0.000583540415391326\n",
      "Step 8 (10321); Episode 100/100; Loss: 0.0006313047488220036\n",
      "Step 9 (10322); Episode 100/100; Loss: 0.043777983635663986\n",
      "Step 10 (10323); Episode 100/100; Loss: 0.07958737015724182\n",
      "Step 11 (10324); Episode 100/100; Loss: 0.040167324244976044\n",
      "Step 12 (10325); Episode 100/100; Loss: 0.039010535925626755\n",
      "Step 13 (10326); Episode 100/100; Loss: 0.0046807508915662766\n",
      "Step 14 (10327); Episode 100/100; Loss: 0.03964276239275932\n",
      "Step 15 (10328); Episode 100/100; Loss: 0.04007839784026146\n",
      "Step 16 (10329); Episode 100/100; Loss: 0.0021220319904386997\n",
      "Step 17 (10330); Episode 100/100; Loss: 0.0437910221517086\n",
      "Step 18 (10331); Episode 100/100; Loss: 0.042611587792634964\n",
      "Step 19 (10332); Episode 100/100; Loss: 0.03628690540790558\n",
      "Step 20 (10333); Episode 100/100; Loss: 0.043456751853227615\n",
      "Step 21 (10334); Episode 100/100; Loss: 0.05635160952806473\n",
      "Step 22 (10335); Episode 100/100; Loss: 0.001731880009174347\n",
      "Step 23 (10336); Episode 100/100; Loss: 0.03654060885310173\n",
      "Step 24 (10337); Episode 100/100; Loss: 0.0030221769120544195\n",
      "Step 25 (10338); Episode 100/100; Loss: 0.08512155711650848\n",
      "Step 26 (10339); Episode 100/100; Loss: 0.001311550964601338\n",
      "Step 27 (10340); Episode 100/100; Loss: 0.0017004271503537893\n",
      "Step 28 (10341); Episode 100/100; Loss: 0.0023267625365406275\n",
      "Step 29 (10342); Episode 100/100; Loss: 0.0016924240626394749\n",
      "Step 30 (10343); Episode 100/100; Loss: 0.03408021107316017\n",
      "Step 31 (10344); Episode 100/100; Loss: 0.05782279744744301\n",
      "Step 32 (10345); Episode 100/100; Loss: 0.00360338413156569\n",
      "Step 33 (10346); Episode 100/100; Loss: 0.0009018866112455726\n",
      "Step 34 (10347); Episode 100/100; Loss: 0.09205719828605652\n",
      "Step 35 (10348); Episode 100/100; Loss: 0.0010575918713584542\n",
      "Step 36 (10349); Episode 100/100; Loss: 0.001400874461978674\n",
      "Step 37 (10350); Episode 100/100; Loss: 0.0012038920540362597\n",
      "Step 38 (10351); Episode 100/100; Loss: 0.0006649774732068181\n",
      "Step 39 (10352); Episode 100/100; Loss: 0.03443349897861481\n",
      "Step 40 (10353); Episode 100/100; Loss: 0.0005671961116604507\n",
      "Step 41 (10354); Episode 100/100; Loss: 0.042381785809993744\n",
      "Step 42 (10355); Episode 100/100; Loss: 0.056446537375450134\n",
      "Step 43 (10356); Episode 100/100; Loss: 0.0014761937782168388\n",
      "Step 44 (10357); Episode 100/100; Loss: 0.0012476941337808967\n",
      "Step 45 (10358); Episode 100/100; Loss: 0.0005477019585669041\n",
      "Step 46 (10359); Episode 100/100; Loss: 0.04741770029067993\n",
      "Step 47 (10360); Episode 100/100; Loss: 0.09151943773031235\n",
      "Step 48 (10361); Episode 100/100; Loss: 0.0005729977856390178\n",
      "Step 49 (10362); Episode 100/100; Loss: 0.04957444965839386\n",
      "Step 50 (10363); Episode 100/100; Loss: 0.0014915971551090479\n",
      "Step 51 (10364); Episode 100/100; Loss: 0.04223325103521347\n",
      "Step 52 (10365); Episode 100/100; Loss: 0.0010488377884030342\n",
      "Step 53 (10366); Episode 100/100; Loss: 0.08325733989477158\n",
      "Step 54 (10367); Episode 100/100; Loss: 0.0009755554492585361\n",
      "Step 55 (10368); Episode 100/100; Loss: 0.04771167412400246\n",
      "Step 56 (10369); Episode 100/100; Loss: 0.04716913029551506\n",
      "Step 57 (10370); Episode 100/100; Loss: 0.04064391553401947\n",
      "Step 58 (10371); Episode 100/100; Loss: 0.03979707881808281\n",
      "Step 59 (10372); Episode 100/100; Loss: 0.030500004068017006\n",
      "Step 60 (10373); Episode 100/100; Loss: 0.0009610748966224492\n",
      "Step 61 (10374); Episode 100/100; Loss: 0.0007510216091759503\n",
      "Step 62 (10375); Episode 100/100; Loss: 0.03910677507519722\n",
      "Step 63 (10376); Episode 100/100; Loss: 0.00201021577231586\n",
      "Step 64 (10377); Episode 100/100; Loss: 0.0014826079132035375\n",
      "Step 65 (10378); Episode 100/100; Loss: 0.0012257426278665662\n",
      "Step 66 (10379); Episode 100/100; Loss: 0.00235976604744792\n",
      "Step 67 (10380); Episode 100/100; Loss: 0.04995987191796303\n",
      "Step 68 (10381); Episode 100/100; Loss: 0.000898425467312336\n",
      "Step 69 (10382); Episode 100/100; Loss: 0.040417302399873734\n",
      "Step 70 (10383); Episode 100/100; Loss: 0.055932361632585526\n",
      "Step 71 (10384); Episode 100/100; Loss: 0.0013208371819928288\n",
      "Step 72 (10385); Episode 100/100; Loss: 0.043067026883363724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 73 (10386); Episode 100/100; Loss: 0.0436834916472435\n",
      "Step 74 (10387); Episode 100/100; Loss: 0.055981725454330444\n",
      "Step 75 (10388); Episode 100/100; Loss: 0.0007925386307761073\n",
      "Step 76 (10389); Episode 100/100; Loss: 0.001075383392162621\n",
      "Step 77 (10390); Episode 100/100; Loss: 0.0008905464201234281\n",
      "Step 78 (10391); Episode 100/100; Loss: 0.0009320885874330997\n",
      "Step 79 (10392); Episode 100/100; Loss: 0.08566639572381973\n",
      "Step 80 (10393); Episode 100/100; Loss: 0.002422996796667576\n",
      "Step 81 (10394); Episode 100/100; Loss: 0.002138355979695916\n",
      "Step 82 (10395); Episode 100/100; Loss: 0.03771420940756798\n",
      "Step 83 (10396); Episode 100/100; Loss: 0.0016823573969304562\n",
      "Step 84 (10397); Episode 100/100; Loss: 0.03335904702544212\n",
      "Step 85 (10398); Episode 100/100; Loss: 0.0006539135938510299\n",
      "Step 86 (10399); Episode 100/100; Loss: 0.0014608930796384811\n",
      "Step 87 (10400); Episode 100/100; Loss: 0.0026840129867196083\n",
      "Step 88 (10401); Episode 100/100; Loss: 0.0004646643064916134\n",
      "Step 89 (10402); Episode 100/100; Loss: 0.04511044919490814\n",
      "Step 90 (10403); Episode 100/100; Loss: 0.0011018648510798812\n",
      "Step 91 (10404); Episode 100/100; Loss: 0.0008486086735501885\n",
      "Step 92 (10405); Episode 100/100; Loss: 0.05139974132180214\n",
      "Step 93 (10406); Episode 100/100; Loss: 0.0013569688890129328\n",
      "Step 94 (10407); Episode 100/100; Loss: 0.0010376168647781014\n",
      "Step 95 (10408); Episode 100/100; Loss: 0.050184961408376694\n",
      "Step 96 (10409); Episode 100/100; Loss: 0.00038731296081095934\n",
      "Step 97 (10410); Episode 100/100; Loss: 0.0010169297456741333\n",
      "Step 98 (10411); Episode 100/100; Loss: 0.0005231685936450958\n",
      "Step 99 (10412); Episode 100/100; Loss: 0.042466916143894196\n",
      "Step 100 (10413); Episode 100/100; Loss: 0.0014246132923290133\n",
      "Step 101 (10414); Episode 100/100; Loss: 0.09383708983659744\n",
      "Step 102 (10415); Episode 100/100; Loss: 0.00025294575607404113\n",
      "Step 103 (10416); Episode 100/100; Loss: 0.05610379949212074\n",
      "Step 104 (10417); Episode 100/100; Loss: 0.0038857616018503904\n",
      "Step 105 (10418); Episode 100/100; Loss: 0.09536243230104446\n",
      "Step 106 (10419); Episode 100/100; Loss: 0.0016309985658153892\n",
      "Step 107 (10420); Episode 100/100; Loss: 0.0016668359749019146\n",
      "Step 108 (10421); Episode 100/100; Loss: 0.0019856279250234365\n",
      "Step 109 (10422); Episode 100/100; Loss: 0.0015965051716193557\n",
      "Step 110 (10423); Episode 100/100; Loss: 0.002319654216989875\n",
      "Step 111 (10424); Episode 100/100; Loss: 0.0005268110544420779\n",
      "Step 112 (10425); Episode 100/100; Loss: 0.0942663699388504\n",
      "Step 113 (10426); Episode 100/100; Loss: 0.046120669692754745\n",
      "Step 114 (10427); Episode 100/100; Loss: 0.03968484699726105\n",
      "Step 115 (10428); Episode 100/100; Loss: 0.0018149529350921512\n",
      "Step 116 (10429); Episode 100/100; Loss: 0.08904344588518143\n",
      "Step 117 (10430); Episode 100/100; Loss: 0.000530039076693356\n",
      "Step 118 (10431); Episode 100/100; Loss: 0.04780351370573044\n",
      "Step 119 (10432); Episode 100/100; Loss: 0.041549261659383774\n",
      "Step 120 (10433); Episode 100/100; Loss: 0.0008409586152993143\n",
      "Step 121 (10434); Episode 100/100; Loss: 0.04659206047654152\n",
      "Step 122 (10435); Episode 100/100; Loss: 0.08260733634233475\n",
      "Step 123 (10436); Episode 100/100; Loss: 0.0011694609420374036\n",
      "Step 124 (10437); Episode 100/100; Loss: 0.0010096065234392881\n",
      "Step 125 (10438); Episode 100/100; Loss: 0.0011692134430631995\n",
      "Step 126 (10439); Episode 100/100; Loss: 0.0006880807341076434\n",
      "Step 127 (10440); Episode 100/100; Loss: 0.05720978602766991\n",
      "Step 128 (10441); Episode 100/100; Loss: 0.0015942816389724612\n",
      "Step 129 (10442); Episode 100/100; Loss: 0.0005734764854423702\n",
      "Step 130 (10443); Episode 100/100; Loss: 0.027759458869695663\n",
      "Step 131 (10444); Episode 100/100; Loss: 0.08674135059118271\n",
      "Step 132 (10445); Episode 100/100; Loss: 0.0005055589717812836\n",
      "Step 133 (10446); Episode 100/100; Loss: 0.040928058326244354\n",
      "Step 134 (10447); Episode 100/100; Loss: 0.0004644952714443207\n",
      "Step 135 (10448); Episode 100/100; Loss: 0.0015314194606617093\n",
      "Step 136 (10449); Episode 100/100; Loss: 0.0010189226595684886\n",
      "Step 137 (10450); Episode 100/100; Loss: 0.04051054269075394\n",
      "Step 138 (10451); Episode 100/100; Loss: 0.00045435907668434083\n",
      "Step 139 (10452); Episode 100/100; Loss: 0.03968149051070213\n",
      "Step 140 (10453); Episode 100/100; Loss: 0.0005980557762086391\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations = run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70d16eb61eae34605e8d7813a70a604a",
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f5e85e8aa15e9cb9117b17265435eae",
     "grade": false,
     "grade_id": "cell-6607b79e73a101a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Policy Gradient (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "951b88e9cd8396d088d3f80e6da9690c",
     "grade": false,
     "grade_id": "cell-083fe71da94aa7aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So we have spent a lot of time working on *value based* methods. We will now switch to *policy based* methods, i.e. learn a policy directly rather than learn a value function from which the policy follows. Mention two advantages of using a policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5c1f505cb22eca6eb3b8213ff23e60f",
     "grade": true,
     "grade_id": "cell-134510705650d5ac",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "233ca94abc32f0e510c5d8a164206d05",
     "grade": false,
     "grade_id": "cell-76a10fe31897025f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3.1 Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2bc16b45e6145226b8a6f5117003b7f5",
     "grade": false,
     "grade_id": "cell-34f0712f792bbcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to do so, we will implement a Policy network. Although in general this does not have to be the case, we will use an architecture very similar to the Q-network (two layers with ReLU activation for the hidden layer). Since we have discrete actions, our model will output one value per action, where each value represents the (normalized!) log-probability of selecting that action. *Use the (log-)softmax activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "155baf230fd6deb5f6ccf93138fa3419",
     "grade": false,
     "grade_id": "cell-6a31440f9477f963",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3cb94e04b03fa4b663bcf38a96ef656d",
     "grade": true,
     "grade_id": "cell-9d280fe6520edc91",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "log_p = model(x)\n",
    "\n",
    "# Does the outcome make sense?\n",
    "print(log_p.exp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b0ff099a335c248a91df00e975494d0",
     "grade": false,
     "grade_id": "cell-35294ca4eda15b11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3.2 Monte Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93ed9cbcf70541f5a04709ee89a16e78",
     "grade": false,
     "grade_id": "cell-44f33e587542974d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the *Monte Carlo* policy gradient algorithm. Remember from lab 1 that this means that we will estimate returns for states by sample episodes. Compared to DQN, this means that we do *not* perform an update step at every environment step, but only at the end of each episode. This means that we should generate an episode of data, compute the REINFORCE loss (which requires computing the returns) and then perform a gradient step.\n",
    "\n",
    "To help you, we already implemented a few functions that you can (but do not have to) use.\n",
    "\n",
    "* You can use `torch.multinomial` to sample from a categorical distribution.\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$. Make sure that you do this in **linear time**, otherwise your algorithm will be very slow! Note the - (minus) since you want to maximize return while you want to minimize the loss.\n",
    "* Importantly, you should **normalize the returns** (not the rewards!, e.g. subtract mean and divide by standard deviation within the episode) before computing the loss, or your estimator will have very high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b2c75181678fed25fcc7c8b39bb7de3",
     "grade": true,
     "grade_id": "cell-3f6e32c4931392bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return action, log_p[action]\n",
    "\n",
    "def run_episode(env, model):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return episode\n",
    "\n",
    "def compute_reinforce_loss(episode, discount_factor):\n",
    "    # Compute the reinforce loss\n",
    "    # Make sure that your function runs in LINEAR TIME\n",
    "    # Don't forget to normalize your RETURNS (not rewards)\n",
    "    # Note that the rewards/returns should be maximized \n",
    "    # while the loss should be minimized so you need a - somewhere\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss\n",
    "\n",
    "def run_episodes_policy_gradient(model, env, num_episodes, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "                           \n",
    "        if i % 10 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(i, len(episode), '\\033[92m' if len(episode) >= 195 else '\\033[99m'))\n",
    "        episode_durations.append(len(episode))\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play around with the parameters!\n",
    "num_episodes = 200\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.01\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(\n",
    "    model, env, num_episodes, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "454f1fb392b88af636d085896efb2aad",
     "grade": false,
     "grade_id": "cell-ad1138b69e6728a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4. Deep Reinforcement Learning (5 bonus points)\n",
    "Note that so far we used the state variables as input. However, the true power of Deep Learning is that we can directly learn from raw inputs, e.g. we can learn to balance the cart pole *by just looking at the screen*. This probably means that you need a deep(er) (convolutional) network, as well as tweaking some parameters, running for more iterations (perhaps on GPU) and do other tricks to stabilize learning. Can you get this to work? This will earn you bonus points!\n",
    "\n",
    "Hints:\n",
    "* You may want to use [Google Colab](https://colab.research.google.com/) such that you can benefit from GPU acceleration.\n",
    "* Even if you don't use Colab, save the weights of your final model and load it in the code here (see example below). Hand in the model file with the .ipynb in a .zip. We likely won't be able to run your training code during grading!\n",
    "* Preprocessing is already done for you, and the observation is the difference between two consequtive frames such that the model can 'see' (angular) speed from a single image. Now do you see why we (sometimes) use the word observation (and not state)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f660e1484fe2bf60d66467326eacb1ba",
     "grade": false,
     "grade_id": "cell-9c9dfa80827c5680",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class CartPoleRawEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('CartPole-v0', *args, **kwargs)  #.unwrapped\n",
    "        self.action_space = self._env.action_space\n",
    "        screen_height, screen_width = 40, 80  # TODO\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        return self._env.seed(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        self.prev_screen = self.screen = self.get_screen()\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        s, r, done, info = self._env.step(action)\n",
    "        self.prev_screen = self.screen\n",
    "        self.screen = self.get_screen()\n",
    "        return self._get_observation(), r, done, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.screen - self.prev_screen\n",
    "    \n",
    "    def _get_cart_location(self, screen_width):\n",
    "        _env = self._env.unwrapped\n",
    "        world_width = _env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(_env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self._env.unwrapped.render(mode='rgb_array').transpose(\n",
    "            (2, 0, 1))  # transpose into torch order (CHW)\n",
    "        # Strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, screen_height * 4 // 10:screen_height * 8 // 10]\n",
    "        view_width = screen_height * 8 // 10\n",
    "        cart_location = self._get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescare, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        #return screen.unsqueeze(0).to(device)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        return self._env.close()\n",
    "\n",
    "raw_env = CartPoleRawEnv()\n",
    "s = raw_env.reset()\n",
    "\n",
    "# \n",
    "s, r, done, _ = raw_env.step(env.action_space.sample())\n",
    "\n",
    "raw_env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(raw_env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n",
    "# Observations are (-1, 1) while we need to plot (0, 1) so show (rgb + 1) / 2\n",
    "plt.figure()\n",
    "plt.imshow((s.cpu().squeeze(0).permute(1, 2, 0).numpy() + 1) / 2,\n",
    "           interpolation='none')\n",
    "plt.title('Example observation')\n",
    "plt.show()\n",
    "raw_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe you should make it a bit deeper?\n",
    "class DeepPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(40 * 80 * 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        return F.log_softmax(self.l1(x.view(x.size(0), -1)), -1)\n",
    "    \n",
    "policy = DeepPolicy()\n",
    "filename = 'weights.pt'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(f\"Loading weights from {filename}\")\n",
    "    weights = torch.load(filename, map_location='cpu')\n",
    "    \n",
    "    policy.load_state_dict(weights['policy'])\n",
    "    \n",
    "else:\n",
    "    # Train\n",
    "    \n",
    "    ### TODO some training here, maybe? Or run this on a different machine?\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    print(f\"Saving weights to {filename}\")\n",
    "    torch.save({\n",
    "        # You can add more here if you need, e.g. critic\n",
    "        'policy': policy.state_dict()  # Always save weights rather than objects\n",
    "    },\n",
    "    filename)\n",
    "    \n",
    "def bonus_get_action(x):\n",
    "    return policy(x).exp().multinomial(1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4502e425cdd9d5db2ec0e9e8e972fa0b",
     "grade": true,
     "grade_id": "cell-0d7bd58a23fdfabb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "episode_durations = []\n",
    "for i in range(20):  # Not too many since it may take forever to render\n",
    "    test_env = CartPoleRawEnv()\n",
    "    test_env.seed(seed + i)\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        with torch.no_grad():\n",
    "            action = bonus_get_action(state).item()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "    episode_durations.append(steps)\n",
    "    test_env.close()\n",
    "    \n",
    "plt.plot(episode_durations)\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
